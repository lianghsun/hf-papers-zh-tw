<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2026-02-27 — HF Papers 繁中</title>
  <link rel="stylesheet" href="../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../index.html" class="site-logo">📄 HF Papers 繁中</a>
      <nav>
        <a href="../index.html">首頁</a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="date-nav">
  
  <span class="current">2026-02-27</span>
  
</div>

<h1 class="page-title">2026-02-27</h1>
<p class="page-subtitle">9 篇論文</p>

<div class="paper-list">
  
  <div class="paper-card">
    <div class="paper-card-meta">
      <span>2602.22897</span>
      <span>▲ 1</span>
    </div>
    <div class="paper-card-title">
      <a href="../paper/2602.22897/index.html" style="color:inherit;">
        全能GAIA：邁向原生全模態AI代理
      </a>
    </div>
    
    <div class="paper-card-title-en">OmniGAIA: Towards Native Omni-Modal AI Agents</div>
    
    <div class="paper-card-abstract"># 論文摘要翻譯

人類智能自然地將全模態感知——跨越視覺、音訊和語言——與複雜推理和工具使用相互交織，以與世界互動。然而，當前的多模態大語言模型主要侷限於雙模態互動（例如，視覺-語言），缺乏通用AI助手所需的統一認知能力。為彌補此差距，我們提出OmniGAIA，一個綜合基準旨在評估全模態智能體在需要深度推理和跨越視訊、音訊和影像模態的多輪工具執行的任務上的表現。通過新穎的全模態事件圖方法構建，OmniGAIA 綜合了源自真實世界資料的複雜多跳查詢，這些查詢需要跨模態推理和外部工具整合。此外，我們提出OmniAtlas，一個在工具整合推理範式下的原生全模態基礎智能體，具有主動全模態感知能力。通過後見之明引導樹探索策略和OmniDPO 合成的軌跡進行訓練以進行細粒度錯誤修正，OmniAtlas 有效增強了現有開源模型的工具使用能力。此項工作標誌著邁向針對真實場景的下一代原生全模態AI助手的一步。</div>
    <div class="paper-card-footer">
      <div class="tags">
        <span class="tag domain">NLP</span><span class="tag domain">CV</span><span class="tag domain">Audio</span><span class="tag domain">Multimodal</span><span class="tag domain">RL</span>
        <span class="tag method">Transformer</span><span class="tag method">Tool Integration</span><span class="tag method">Tree Search</span><span class="tag method">DPO</span><span class="tag method">Agent Architecture</span>
        <span class="tag task">Multi-modal Reasoning</span><span class="tag task">Tool Use</span><span class="tag task">Question Answering</span><span class="tag task">Agent Planning</span>
        <span class="tag open">Open Source</span>
      </div>
      <div class="paper-card-links">
        <a href="../paper/2602.22897/index.html">繁中全文</a>
        <a href="https://arxiv.org/abs/2602.22897" target="_blank" rel="noopener">arXiv</a>
      </div>
    </div>
  </div>
  
  <div class="paper-card">
    <div class="paper-card-meta">
      <span>2602.23152</span>
      <span>▲ 1</span>
    </div>
    <div class="paper-card-title">
      <a href="../paper/2602.23152/index.html" style="color:inherit;">
        一致性的三位一體作為通用世界模型的定義原則
      </a>
    </div>
    
    <div class="paper-card-title-en">The Trinity of Consistency as a Defining Principle for General World Models</div>
    
    <div class="paper-card-abstract"># 論文摘要翻譯

能夠學習、模擬和推理客觀物理規律的世界模型的構建構成了追求人工通用智慧的基礎性挑戰。以 Sora 等視頻生成模型為代表的最近進展已經展示了資料驅動縮放律近似物理動力學的潛力，而新興的統一多模態模型（UMM）為整合感知、語言和推理提供了一個有前景的架構範式。儘管取得了這些進展，該領域仍然缺乏定義通用世界模型所需基本特性的有原則的理論框架。在本文中，我們提出世界模型必須植根於一致性三角（Trinity of Consistency）：作為語義介面的模態一致性、作為幾何基礎的空間一致性以及作為因果引擎的時間一致性。通過這個三層面的視角，我們系統地回顧了多模態學習的演變，揭示了從鬆散耦合的專用模組向能夠實現內部世界模擬器協同湧現的統一架構的發展軌跡。為了補充這個概念框架，我們引入了 CoW-Bench，一個以多幀推理和生成場景為中心的基準。CoW-Bench 在統一的評估協議下評估視頻生成模型和 UMM。我們的工作為通用世界模型建立了一條有原則的路徑，明確了當前系統的局限性以及未來進展的架構需求。</div>
    <div class="paper-card-footer">
      <div class="tags">
        <span class="tag domain">CV</span><span class="tag domain">Multimodal</span><span class="tag domain">Theory</span>
        <span class="tag method">Transformer</span><span class="tag method">Diffusion</span><span class="tag method">Multimodal Learning</span><span class="tag method">Video Generation</span>
        <span class="tag task">Video Generation</span><span class="tag task">Multi-frame Reasoning</span><span class="tag task">World Modeling</span><span class="tag task">Physical Dynamics Simulation</span>
        
      </div>
      <div class="paper-card-links">
        <a href="../paper/2602.23152/index.html">繁中全文</a>
        <a href="https://arxiv.org/abs/2602.23152" target="_blank" rel="noopener">arXiv</a>
      </div>
    </div>
  </div>
  
  <div class="paper-card">
    <div class="paper-card-meta">
      <span>2602.22766</span>
      <span>▲ 1</span>
    </div>
    <div class="paper-card-title">
      <a href="../paper/2602.22766/index.html" style="color:inherit;">
        想象有助於視覺推理，但在潛在空間中尚未實現
      </a>
    </div>
    
    <div class="paper-card-title-en">Imagination Helps Visual Reasoning, But Not Yet in Latent Space</div>
    
    <div class="paper-card-abstract"># 論文摘要翻譯

潛在視覺推理旨在透過 Multimodal Large Language Models 的隱藏狀態進行沉思，以模擬人類的想像過程。儘管被認為是視覺推理的有前景範式，但推動其有效性的潛在機制仍不清楚。受到揭示其真正有效性根源的動機驅使，我們使用因果中介分析來調查潛在推理的有效性。我們將此過程建模為因果鏈：輸入作為處理、潛在令牌作為中介、最終答案作為結果。我們的研究結果揭示了兩個關鍵的斷裂：（a）輸入-潛在斷裂：對輸入的劇烈擾動導致潛在令牌的變化可忽略不計，表明潛在令牌無法有效地關注輸入序列。（b）潛在-答案斷裂：對潛在令牌的擾動對最終答案的影響最小，表明潛在令牌對結果的因果效應有限。此外，廣泛的探測分析表明潛在令牌編碼的視覺信息有限，且表現出高度相似性。因此，我們質疑潛在推理的必要性，並提出一個名為 CapImagine 的直接替代方案，該方案教導模型使用文本明確地進行想像。在以視覺為中心的基準上的實驗表明，CapImagine 明顯優於複雜的潛在空間基線，突顯了透過明確想像進行視覺推理的更優越潛力。</div>
    <div class="paper-card-footer">
      <div class="tags">
        <span class="tag domain">NLP</span><span class="tag domain">CV</span><span class="tag domain">Multimodal</span>
        <span class="tag method">Causal Mediation Analysis</span><span class="tag method">Multimodal Large Language Models</span><span class="tag method">Probing Analysis</span>
        <span class="tag task">Visual Reasoning</span><span class="tag task">Visual Question Answering</span>
        
      </div>
      <div class="paper-card-links">
        <a href="../paper/2602.22766/index.html">繁中全文</a>
        <a href="https://arxiv.org/abs/2602.22766" target="_blank" rel="noopener">arXiv</a>
      </div>
    </div>
  </div>
  
  <div class="paper-card">
    <div class="paper-card-meta">
      <span>2602.23008</span>
      <span>▲ 1</span>
    </div>
    <div class="paper-card-title">
      <a href="../paper/2602.23008/index.html" style="color:inherit;">
        通過混合在策略和離策略最優化的探索性記憶增強LLM代理
      </a>
    </div>
    
    <div class="paper-card-title-en">Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization</div>
    
    <div class="paper-card-abstract"># 論文摘要翻譯

探索仍然是使用強化學習訓練的大型語言模型代理的關鍵瓶頸。雖然先前的方法利用預訓練知識，但它們在需要發現新穎狀態的環境中表現不佳。我們提出 Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO²)，一個混合強化學習框架，利用記憶來進行探索，並結合同策略和異策略更新，使大型語言模型在具有記憶的情況下表現良好，同時也確保在沒有記憶時的穩健性。在 ScienceWorld 和 WebShop 上，EMPO² 分別相對於 GRPO 達到 128.6% 和 11.3% 的改進。此外，在分布外測試中，EMPO² 展示了對新任務卓越的適應性，僅需少數幾次帶有記憶的試驗且無需參數更新。這些結果突出了 EMPO² 作為構建更具探索性和可泛化之大型語言模型代理的有前景框架。</div>
    <div class="paper-card-footer">
      <div class="tags">
        <span class="tag domain">NLP</span><span class="tag domain">RL</span>
        <span class="tag method">Memory-Augmented LLM</span><span class="tag method">On-Policy Optimization</span><span class="tag method">Off-Policy Optimization</span><span class="tag method">Hybrid RL</span>
        <span class="tag task">Agent</span><span class="tag task">Exploration</span><span class="tag task">Decision Making</span>
        
      </div>
      <div class="paper-card-links">
        <a href="../paper/2602.23008/index.html">繁中全文</a>
        <a href="https://arxiv.org/abs/2602.23008" target="_blank" rel="noopener">arXiv</a>
      </div>
    </div>
  </div>
  
  <div class="paper-card">
    <div class="paper-card-meta">
      <span>2602.22594</span>
      <span>▲ 1</span>
    </div>
    <div class="paper-card-title">
      <a href="../paper/2602.22594/index.html" style="color:inherit;">
        因果運動擴散模型用於自迴歸運動生成
      </a>
    </div>
    
    <div class="paper-card-title-en">Causal Motion Diffusion Models for Autoregressive Motion Generation</div>
    
    <div class="paper-card-abstract">Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.</div>
    <div class="paper-card-footer">
      <div class="tags">
        <span class="tag domain">CV</span><span class="tag domain">Multimodal</span>
        <span class="tag method">Diffusion Models</span><span class="tag method">Transformer</span><span class="tag method">VAE</span><span class="tag method">Autoregressive Models</span>
        <span class="tag task">Motion Generation</span><span class="tag task">Text-to-Motion Generation</span><span class="tag task">Motion Synthesis</span>
        
      </div>
      <div class="paper-card-links">
        <a href="../paper/2602.22594/index.html">繁中全文</a>
        <a href="https://arxiv.org/abs/2602.22594" target="_blank" rel="noopener">arXiv</a>
      </div>
    </div>
  </div>
  
  <div class="paper-card">
    <div class="paper-card-meta">
      <span>2602.23259</span>
      
    </div>
    <div class="paper-card-title">
      <a href="../paper/2602.23259/index.html" style="color:inherit;">
        風險感知型世界模型預測控制用於可泛化端到端自動駕駛
      </a>
    </div>
    
    <div class="paper-card-title-en">Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving</div>
    
    <div class="paper-card-abstract">隨著模仿學習（IL）和大規模駕駛資料集的進步，端到端自動駕駛（E2E-AD）近期取得了重大進展。目前，基於 IL 的方法已成為主流範式：模型依賴於專家提供的標準駕駛行為，並學習最小化其行動與專家行動之間的差異。然而，這個「僅依照專家行駛」的目標存在泛化能力有限的問題：當遇到超出專家示範分佈的罕見或未見過的長尾場景時，在缺乏先前經驗的情況下，模型傾向於做出不安全的決策。這引發了一個根本性問題：E2E-AD 系統能否在沒有任何專家行動監督的情況下做出可靠的決策？受此啟發，我們提出了一個名為 Risk-aware World Model Predictive Control（RaWMPC）的統一框架，透過魯棒控制來解決這個泛化困境，而不依賴於專家示範。在實踐中，RaWMPC 利用世界模型來預測多個候選行動的後果，並透過明確的風險評估選擇低風險行動。為了賦予世界模型預測風險駕駛行為結果的能力，我們設計了一種風險感知的交互策略，系統性地將世界模型暴露於危險行為，使得災難性後果可預測，進而可以避免。此外，為了在測試時生成低風險的候選行動，我們引入了自評估蒸餾方法，將經過充分訓練的世界模型中的避風險能力蒸餾到一個生成式行動提案網絡中，無需任何專家示範。廣泛的實驗表明，RaWMPC 在分佈內和分佈外場景中都優於最先進的方法，同時提供了優越的決策可解釋性。</div>
    <div class="paper-card-footer">
      <div class="tags">
        <span class="tag domain">CV</span><span class="tag domain">RL</span><span class="tag domain">Robotics</span>
        <span class="tag method">World Model</span><span class="tag method">Model Predictive Control</span><span class="tag method">Imitation Learning</span><span class="tag method">Risk Evaluation</span><span class="tag method">Distillation</span>
        <span class="tag task">Autonomous Driving</span><span class="tag task">End-to-End Learning</span><span class="tag task">Action Prediction</span>
        
      </div>
      <div class="paper-card-links">
        <a href="../paper/2602.23259/index.html">繁中全文</a>
        <a href="https://arxiv.org/abs/2602.23259" target="_blank" rel="noopener">arXiv</a>
      </div>
    </div>
  </div>
  
  <div class="paper-card">
    <div class="paper-card-meta">
      <span>2602.23165</span>
      
    </div>
    <div class="paper-card-title">
      <a href="../paper/2602.23165/index.html" style="color:inherit;">
        DyaDiT：用於社交友善的二人互動手勢生成的多模態擴散變壓器
      </a>
    </div>
    
    <div class="paper-card-title-en">DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation</div>
    
    <div class="paper-card-abstract"># 論文摘要翻譯

生成逼真的對話手勢對於實現與數位人類的自然、社交互動至關重要。然而，現有方法通常將單一音訊流映射到單一說話者的動作，而未考慮社交背景或建模兩人進行對話之間的相互動態。我們提出 DyaDiT，一個多模態擴散 Transformer，從二人組音訊信號生成情境相適應的人類動作。在 Seamless Interaction Dataset 上進行訓練，DyaDiT 接收二人組音訊及可選的社交背景標記，以產生情境相適應的動作。它融合來自兩位說話者的信息以捕捉交互動態，使用動作字典來編碼動作先驗，並可選擇性地利用對話夥伴的手勢來產生更具回應性的動作。我們在標準動作生成指標上評估 DyaDiT，並進行定量用戶研究，證明它不僅在客觀指標上超越現有方法，而且受到用戶的強烈偏好，突出了其穩健性和社交有利的動作生成。程式碼和模型將在接收後發布。</div>
    <div class="paper-card-footer">
      <div class="tags">
        <span class="tag domain">CV</span><span class="tag domain">Multimodal</span><span class="tag domain">Audio</span>
        <span class="tag method">Diffusion</span><span class="tag method">Transformer</span>
        <span class="tag task">Motion Generation</span><span class="tag task">Gesture Generation</span>
        <span class="tag open">Open Source</span>
      </div>
      <div class="paper-card-links">
        <a href="../paper/2602.23165/index.html">繁中全文</a>
        <a href="https://arxiv.org/abs/2602.23165" target="_blank" rel="noopener">arXiv</a>
      </div>
    </div>
  </div>
  
  <div class="paper-card">
    <div class="paper-card-meta">
      <span>2602.23058</span>
      <span>▲ 2</span>
    </div>
    <div class="paper-card-title">
      <a href="../paper/2602.23058/index.html" style="color:inherit;">
        GeoWorld：幾何世界模型
      </a>
    </div>
    
    <div class="paper-card-title-en">GeoWorld: Geometric World Models</div>
    
    <div class="paper-card-abstract"># 能量基礎預測性世界模型的幾何增強方法

能量基礎預測性世界模型通過在潛在能量景觀上進行推理而非生成像素，為多步驟視覺規劃提供了強大的方法。然而，現有方法面臨兩個主要挑戰：(i) 其潛在表示通常在歐幾里得空間中學習，忽視了狀態之間的潛在幾何結構和層級關係；(ii) 它們在長地平線預測中表現不佳，導致在擴展展開過程中快速退化。為解決這些挑戰，我們引入 GeoWorld，一個幾何世界模型，通過 Hyperbolic JEPA 保存幾何結構和層級關係，該模型將潛在表示從歐幾里得空間映射到雙曲流形上。我們進一步引入用於能量基礎優化的幾何強化學習，在雙曲潛在空間中實現穩定的多步驟規劃。在 CrossTask 和 COIN 上的廣泛實驗表明，與最先進的 V-JEPA 2 相比，3 步規劃中取得約 3% 的 SR 提升，以及 4 步規劃中取得 2% 的 SR 提升。專案網站：https://steve-zeyu-zhang.github.io/GeoWorld。</div>
    <div class="paper-card-footer">
      <div class="tags">
        <span class="tag domain">CV</span><span class="tag domain">RL</span>
        <span class="tag method">Energy-based Models</span><span class="tag method">Hyperbolic JEPA</span><span class="tag method">World Models</span><span class="tag method">Geometric Learning</span>
        <span class="tag task">Visual Planning</span><span class="tag task">Multi-step Prediction</span>
        <span class="tag open">Open Source</span>
      </div>
      <div class="paper-card-links">
        <a href="../paper/2602.23058/index.html">繁中全文</a>
        <a href="https://arxiv.org/abs/2602.23058" target="_blank" rel="noopener">arXiv</a>
      </div>
    </div>
  </div>
  
  <div class="paper-card">
    <div class="paper-card-meta">
      <span>2602.22437</span>
      <span>▲ 1</span>
    </div>
    <div class="paper-card-title">
      <a href="../paper/2602.22437/index.html" style="color:inherit;">
        veScale-FSDP：靈活且高效能的大規模 FSDP
      </a>
    </div>
    
    <div class="paper-card-title-en">veScale-FSDP: Flexible and High-Performance FSDP at Scale</div>
    
    <div class="paper-card-abstract"># Fully Sharded Data Parallel (FSDP) 論文摘要翻譯

完全分片數據並行（Fully Sharded Data Parallel，FSDP），也稱為 ZeRO，廣泛用於大規模模型訓練，具有靈活性強且對模型代碼入侵最小的特點。然而，現有的 FSDP 系統在結構感知訓練方法（例如分塊量化訓練）和非逐元素優化器（例如 Shampoo 和 Muon，用於 Gemini、Kimi K2 等尖端模型）方面存在困難。FSDP 固定的逐元素或逐列分片格式與分塊結構計算相衝突。此外，當前的實現在通訊和記憶體效率方面表現不足，限制了擴展至數萬個 GPU 的能力。我們推出 veScale-FSDP，一個重新設計的 FSDP 系統，將靈活的分片格式 RaggedShard 與結構感知規劃演算法相結合，在大規模下提供靈活性和性能。veScale-FSDP 原生支持 FSDP 所需的高效數據放置，賦能分塊量化和非逐元素優化器。因此，veScale-FSDP 相比現有 FSDP 系統實現了 5~66% 的吞吐量提升和 16~30% 的記憶體使用降低，同時能夠高效地擴展至數萬個 GPU。</div>
    <div class="paper-card-footer">
      <div class="tags">
        <span class="tag domain">Theory</span>
        <span class="tag method">FSDP</span><span class="tag method">ZeRO</span><span class="tag method">RaggedShard</span><span class="tag method">Distributed Training</span><span class="tag method">Quantization</span>
        <span class="tag task">Large-scale Model Training</span>
        
      </div>
      <div class="paper-card-links">
        <a href="../paper/2602.22437/index.html">繁中全文</a>
        <a href="https://arxiv.org/abs/2602.22437" target="_blank" rel="noopener">arXiv</a>
      </div>
    </div>
  </div>
  
</div>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>每日自動抓取 <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>，以 Claude AI 翻譯為繁體中文。</p>
    </div>
  </footer>
</body>
</html>