<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DyaDiTï¼šç”¨æ–¼ç¤¾äº¤å‹å–„çš„äºŒäººäº’å‹•æ‰‹å‹¢ç”Ÿæˆçš„å¤šæ¨¡æ…‹æ“´æ•£è®Šå£“å™¨ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-27/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-27 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">DyaDiTï¼šç”¨æ–¼ç¤¾äº¤å‹å–„çš„äºŒäººäº’å‹•æ‰‹å‹¢ç”Ÿæˆçš„å¤šæ¨¡æ…‹æ“´æ•£è®Šå£“å™¨</h1>
  
  <div class="en-title">DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation</div>
  

  <div class="paper-meta">
    
    <span>Yichen Peng, Jyun-Ting Song, Siyeol Jung, Ruofan Liu, Haiyang Liu, Xuangeng Chu, Ruicong Liu, Erwin Wu, Hideki Koike, Kris Kitani</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.23165" target="_blank">2602.23165</a></span>
    
    <span style="color:var(--text-muted);font-size:0.8rem;">
      ä¾†æºï¼šarxiv HTML
    </span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.23165" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.23165" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.23165" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">Multimodal</span><span class="tag domain">Audio</span>
    <span class="tag method">Diffusion</span><span class="tag method">Transformer</span>
    <span class="tag task">Motion Generation</span><span class="tag task">Gesture Generation</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

ç”Ÿæˆé€¼çœŸçš„å°è©±æ‰‹å‹¢å°æ–¼å¯¦ç¾èˆ‡æ•¸ä½äººé¡çš„è‡ªç„¶ã€ç¤¾äº¤äº’å‹•è‡³é—œé‡è¦ã€‚ç„¶è€Œï¼Œç¾æœ‰æ–¹æ³•é€šå¸¸å°‡å–®ä¸€éŸ³è¨Šæµæ˜ å°„åˆ°å–®ä¸€èªªè©±è€…çš„å‹•ä½œï¼Œè€Œæœªè€ƒæ…®ç¤¾äº¤èƒŒæ™¯æˆ–å»ºæ¨¡å…©äººé€²è¡Œå°è©±ä¹‹é–“çš„ç›¸äº’å‹•æ…‹ã€‚æˆ‘å€‘æå‡º DyaDiTï¼Œä¸€å€‹å¤šæ¨¡æ…‹æ“´æ•£ Transformerï¼Œå¾äºŒäººçµ„éŸ³è¨Šä¿¡è™Ÿç”Ÿæˆæƒ…å¢ƒç›¸é©æ‡‰çš„äººé¡å‹•ä½œã€‚åœ¨ Seamless Interaction Dataset ä¸Šé€²è¡Œè¨“ç·´ï¼ŒDyaDiT æ¥æ”¶äºŒäººçµ„éŸ³è¨ŠåŠå¯é¸çš„ç¤¾äº¤èƒŒæ™¯æ¨™è¨˜ï¼Œä»¥ç”¢ç”Ÿæƒ…å¢ƒç›¸é©æ‡‰çš„å‹•ä½œã€‚å®ƒèåˆä¾†è‡ªå…©ä½èªªè©±è€…çš„ä¿¡æ¯ä»¥æ•æ‰äº¤äº’å‹•æ…‹ï¼Œä½¿ç”¨å‹•ä½œå­—å…¸ä¾†ç·¨ç¢¼å‹•ä½œå…ˆé©—ï¼Œä¸¦å¯é¸æ“‡æ€§åœ°åˆ©ç”¨å°è©±å¤¥ä¼´çš„æ‰‹å‹¢ä¾†ç”¢ç”Ÿæ›´å…·å›æ‡‰æ€§çš„å‹•ä½œã€‚æˆ‘å€‘åœ¨æ¨™æº–å‹•ä½œç”ŸæˆæŒ‡æ¨™ä¸Šè©•ä¼° DyaDiTï¼Œä¸¦é€²è¡Œå®šé‡ç”¨æˆ¶ç ”ç©¶ï¼Œè­‰æ˜å®ƒä¸åƒ…åœ¨å®¢è§€æŒ‡æ¨™ä¸Šè¶…è¶Šç¾æœ‰æ–¹æ³•ï¼Œè€Œä¸”å—åˆ°ç”¨æˆ¶çš„å¼·çƒˆåå¥½ï¼Œçªå‡ºäº†å…¶ç©©å¥æ€§å’Œç¤¾äº¤æœ‰åˆ©çš„å‹•ä½œç”Ÿæˆã€‚ç¨‹å¼ç¢¼å’Œæ¨¡å‹å°‡åœ¨æ¥æ”¶å¾Œç™¼å¸ƒã€‚</p>
  
  <div class="abstract-en">Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker&#39;s motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner&#39;s gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <h1 id="dyadit-transformer">DyaDiTï¼šç”¨æ–¼ç¤¾äº¤å‹å–„äºŒäººäº’å‹•æ‰‹å‹¢ç”Ÿæˆçš„å¤šæ¨¡æ…‹æ“´æ•£ Transformer</h1>
<p>ç”Ÿæˆé€¼çœŸçš„å°è©±æ‰‹å‹¢å°æ–¼å¯¦ç¾èˆ‡æ•¸ä½äººé¡çš„è‡ªç„¶ã€ç¤¾äº¤äº’å‹•è‡³é—œé‡è¦ã€‚ç„¶è€Œï¼Œç¾æœ‰æ–¹æ³•é€šå¸¸å°‡å–®ä¸€éŸ³é »æµå°æ˜ åˆ°å–®ä¸€è¬›è€…çš„å‹•ä½œï¼Œè€Œæœªè€ƒæ…®ç¤¾äº¤èƒŒæ™¯æˆ–å»ºæ¨¡å…©å€‹é€²è¡Œå°è©±çš„äººä¹‹é–“çš„ç›¸äº’å‹•æ…‹ã€‚æˆ‘å€‘æå‡º DyaDiTï¼Œé€™æ˜¯ä¸€å€‹å¤šæ¨¡æ…‹æ“´æ•£ Transformerï¼Œèƒ½å¾äºŒäººäº’å‹•éŸ³é »ä¿¡è™Ÿç”ŸæˆèƒŒæ™¯ç›¸é—œçš„äººé¡å‹•ä½œã€‚DyaDiT åœ¨ Seamless Interaction Dataset [ 1 ] ä¸Šé€²è¡Œè¨“ç·´ï¼Œä»¥äºŒäººäº’å‹•éŸ³é »å’Œå¯é¸çš„ç¤¾äº¤èƒŒæ™¯ä»£å¹£ä½œç‚ºè¼¸å…¥ï¼Œä»¥ç”¢ç”ŸèƒŒæ™¯ç›¸é—œçš„å‹•ä½œã€‚å®ƒèåˆä¾†è‡ªå…©ä½è¬›è€…çš„è¨Šæ¯ä»¥æ•æ‰äº’å‹•å‹•æ…‹ï¼Œä½¿ç”¨å‹•ä½œå­—å…¸ä¾†ç·¨ç¢¼å‹•ä½œå…ˆé©—ï¼Œä¸¦å¯é¸æ“‡æ€§åœ°åˆ©ç”¨å°è©±å¤¥ä¼´çš„æ‰‹å‹¢ä»¥ç”¢ç”Ÿæ›´å…·å›æ‡‰æ€§çš„å‹•ä½œã€‚æˆ‘å€‘åœ¨æ¨™æº–å‹•ä½œç”ŸæˆæŒ‡æ¨™ä¸Šè©•ä¼° DyaDiTï¼Œä¸¦é€²è¡Œé‡åŒ–ä½¿ç”¨è€…ç ”ç©¶ï¼Œè­‰æ˜å®ƒä¸åƒ…åœ¨å®¢è§€æŒ‡æ¨™ä¸Šè¶…è¶Šç¾æœ‰æ–¹æ³•ï¼Œè€Œä¸”ä¹Ÿå¼·çƒˆå—åˆ°ä½¿ç”¨è€…åå¥½ï¼Œçªé¡¯å…¶ç©©å¥æ€§å’Œç¤¾äº¤å‹å–„çš„å‹•ä½œç”Ÿæˆã€‚ç¨‹å¼ç¢¼å’Œæ¨¡å‹å°‡åœ¨æ¥æ”¶å¾Œç™¼ä½ˆã€‚</p>
<h2 id="1">1 ç°¡ä»‹</h2>
<p>å»ºæ§‹å¯ä»¥èˆ‡äººè‡ªç„¶äº’å‹•çš„åˆæˆä»£ç†ï¼ˆä¹Ÿç¨±ç‚ºæ•¸ä½äººé¡ã€AI ä»£ç†ã€è™›æ“¬åŒ–èº«æˆ–æ©Ÿå™¨äººï¼‰æ˜¯äººæ©Ÿä»‹é¢æœªä¾†ç™¼å±•çš„é—œéµã€‚æœ€è¿‘çš„èªè¨€æ¨¡å‹å¦‚ GPT-4.5 [33] å’Œ LLaMA-3.1 [12] å·²ç¶“å±•ç¾å‡ºä»¤äººå°è±¡æ·±åˆ»çš„å°è©±èƒ½åŠ›ï¼Œè¨±å¤šä½¿ç”¨è€…ç”šè‡³æ„Ÿè¦ºè‡ªå·±åœ¨èˆ‡å¦ä¸€å€‹äººäº¤è«‡ [15]ã€‚ç„¶è€Œï¼Œç›®å‰é€™ç¨®å‡è±¡ä»ç„¶ä¾·é™æ–¼æ–‡å­—è¦–çª—å…§ã€‚å¯¦éš›ä¸Šï¼Œäººé¡äº’å‹•é ä¸æ­¢æ–¼å£é ­è¨€èªã€‚äººå€‘æœƒåšå‡ºæ‰‹å‹¢ã€ç›¸äº’å›æ‡‰ï¼Œä¸¦é€éèº«é«”å‹•ä½œè¡¨é”å¾®å¦™çš„ç¤¾äº¤ç·šç´¢ã€‚ç‚ºäº†è®“äººé¡çœŸæ­£æ„Ÿè¦ºåˆ°åˆæˆä»£ç†æ˜¯äº’å‹•çš„ï¼Œè©²ä»£ç†å¿…é ˆä¼´éš¨å…¶è¨€èªè€Œåšå‡ºéš¨è‘—å°è©±è‡ªç„¶æ¼”é€²çš„æ‰‹å‹¢ã€‚</p>
<p>ç„¶è€Œï¼Œç”Ÿæˆé€™æ¨£çš„æ‰‹å‹¢å…·æœ‰æŒ‘æˆ°æ€§ã€‚æ‰‹å‹¢å—åˆ°ç¤¾æœƒå› ç´ çš„å¼·çƒˆå½±éŸ¿ï¼Œä¾‹å¦‚å€‹æ€§ã€èªªè©±è€…ä¹‹é–“çš„é—œä¿‚å’Œä»–å€‘çš„å°è©±è§’è‰²ã€‚é€™äº›ç·šç´¢å½±éŸ¿äººå€‘å¦‚ä½•ç§»å‹•ã€å¦‚ä½•åæ‡‰ä»¥åŠå¦‚ä½•å½¼æ­¤å”èª¿ï¼Œä½†å¤§å¤šæ•¸ç¾æœ‰çš„æ‰‹å‹¢ç”Ÿæˆæ¨¡å‹ä¸¦æœªæ˜ç¢ºå»ºæ¨¡é€™äº›å› ç´  [42ã€26ã€27]ã€‚å› æ­¤ï¼Œå®ƒå€‘ç”Ÿæˆçš„æ‰‹å‹¢å¾€å¾€é¡¯å¾—æ™®é€šæˆ–è™›å‡ã€‚é™¤äº†ç¤¾äº¤è„ˆçµ¡å¤–ï¼Œç™¼ç”Ÿåœ¨å…©å€‹äº’å‹•å€‹é«”ä¹‹é–“çš„é›™äººå°è©±å‘ˆç¾å‡ºæ‰‹å‹¢ç”Ÿæˆæ¨¡å‹é›£ä»¥è™•ç†çš„èªéŸ³å‹•æ…‹ã€‚å…©å€‹äººå¯ä»¥åŒæ™‚èªªè©±ã€ç›¸äº’æ‰“æ–·ï¼Œæˆ–åœ¨èªªè©±å’Œè†è½ä¹‹é–“å¿«é€Ÿåˆ‡æ› [8ã€28]ã€‚ä»–å€‘çš„éŸ³è¨Šæµæ··åˆåœ¨ä¸€èµ·ï¼Œä½¿å¾—é›£ä»¥å€åˆ†èª°åœ¨èªªè©±ï¼Œèª°åœ¨å›æ‡‰ [39ã€30ã€19]ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•¸ç¾æœ‰çš„é›™äººæ‰‹å‹¢ç”Ÿæˆæ¨¡å‹ [32ã€29] ç°¡å–®åœ°å°‡å…©å€‹èªéŸ³æµèåˆç‚ºä¸€å€‹ï¼Œä½¿å¾—æ¯å€‹èªªè©±è€…çš„è²¢ç»è®Šå¾—æ¨¡ç³Šï¼Œå› æ­¤å‰Šå¼±äº†ç”Ÿæˆçš„æ‰‹å‹¢èˆ‡åº•å±¤äº’å‹•ä¹‹é–“çš„å°æ‡‰é—œä¿‚ã€‚</p>
<p>ç‚ºäº†è§£æ±ºé€™äº›é™åˆ¶ï¼Œæˆ‘å€‘ä»‹ç´¹ DyaDiTï¼Œä¸€å€‹åŸºæ–¼æ“´æ•£çš„ Transformerï¼Œå¯å¾é›™äººéŸ³è¨Šç”Ÿæˆå…·æœ‰ç¤¾äº¤æ„è­˜çš„æ‰‹å‹¢ã€‚èˆ‡ä»¥å¾€åªå°ˆæ³¨æ–¼éŸ³è¨Šå’Œç”Ÿæˆå‹•ä½œä¹‹é–“å°é½çš„å·¥ä½œ [22ã€23ã€36ã€10] ä¸åŒï¼ŒDyaDiT çš„ç”Ÿæˆä»¥æ˜ç¢ºçš„ç¤¾äº¤ç·šç´¢ï¼ˆå¦‚é—œä¿‚å’Œå€‹æ€§ï¼‰ç‚ºæ¢ä»¶ã€‚æ­¤å¤–ï¼Œç‚ºäº†è§£æ±ºå°è©±æœŸé–“å…©å€‹é‡ç–ŠéŸ³è¨Šæµçš„å¼·çƒˆç³¾çºå•é¡Œï¼Œæˆ‘å€‘æå‡ºäº†æ­£äº¤åŒ–äº¤å‰æ³¨æ„åŠ›ï¼ˆORCAï¼‰ï¼Œä¸€å€‹ç°¡å–®è€Œæœ‰æ•ˆçš„æ¨¡çµ„ï¼Œå¯æ¶ˆé™¤å…©å€‹éŸ³è¨Šæµçš„æ­§ç¾©ï¼Œç”¢ç”Ÿæ›´æ¸…æ½”çš„éŸ³è¨Šè¡¨å¾µä»¥ä¾¿æ›´å¥½åœ°ç”Ÿæˆæ‰‹å‹¢ã€‚æœ€å¾Œï¼Œç”±æ–¼é›™äººè¨­å®šä¸­çš„äººé¡å‹•ä½œå¾€å¾€å—åˆ°å¤¥ä¼´æ‰‹å‹¢çš„å½±éŸ¿ï¼ŒDyaDiT å¯ä»¥é¸æ“‡æ€§åœ°å°‡å¤¥ä¼´çš„å‹•ä½œä½œç‚ºé¡å¤–è¼¸å…¥ï¼Œå…è¨±æ¨¡å‹ç”Ÿæˆæ›´å”èª¿ã€æ›´æœ‰å›æ‡‰æ€§ä¸”æ›´è‡ªç„¶çš„æ‰‹å‹¢ã€‚</p>
<p>æˆ‘å€‘çš„å¯¦é©—é¡¯ç¤º DyaDiT åœ¨æ¨™æº–é‡åŒ–æŒ‡æ¨™ä¸Šå§‹çµ‚å„ªæ–¼ç¾æœ‰çš„é›™äººæ‰‹å‹¢ç”Ÿæˆæ–¹æ³•ï¼Œåœ¨æ‰‹å‹¢å“è³ªå’Œåˆ†ä½ˆå°é½æ–¹é¢å–å¾—é¡¯è‘—é€²å±•ã€‚æ­¤å¤–ï¼Œæˆ‘å€‘é€²è¡Œäº†å»£æ³›çš„ä½¿ç”¨è€…ç ”ç©¶ä»¥è©•ä¼°äººé¡æ„ŸçŸ¥åå¥½ã€‚çµæœé¡¯ç¤ºåƒèˆ‡è€…å¼·çƒˆåå¥½ DyaDiT ç”Ÿæˆçš„æ‰‹å‹¢ï¼Œè¡¨æ˜æˆ‘å€‘ç”Ÿæˆçš„å‹•ä½œåœ¨ç¤¾äº¤æ„è­˜ä¸Šæ›´é«˜ï¼Œæ›´é©åˆçœŸå¯¦å°è©±è¨­å®šã€‚</p>
<p>ç¸½çµè€Œè¨€ï¼Œæˆ‘å€‘çš„ä¸»è¦è²¢ç»å¦‚ä¸‹ï¼š</p>
<ul>
<li>â€¢ æˆ‘å€‘æå‡º DyaDiTï¼Œä¸€å€‹æ“´æ•£ Transformer (DiT)ï¼Œå¯åœ¨é›™äººå°è©±ä¸­ç”Ÿæˆå…·æœ‰ç¤¾äº¤è„ˆçµ¡æ„è­˜çš„æ‰‹å‹¢ã€‚</li>
<li>â€¢ æˆ‘å€‘å¼•å…¥æ­£äº¤åŒ–äº¤å‰æ³¨æ„åŠ›ï¼ˆORCAï¼‰æ¨¡çµ„ï¼Œæ¸›å°‘å…©å€‹å€‹é«”éŸ³è¨Šæµä¹‹é–“çš„å¹²æ“¾ï¼Œç‚ºé›™äººå°è©±ä¸­æ›´å¥½çš„æ‰‹å‹¢ç”Ÿæˆæä¾›æ›´æ¸…æ½”çš„éŸ³è¨Šè¡¨å¾µã€‚</li>
<li>â€¢ é€éå»£æ³›çš„é‡åŒ–è©•ä¼°å’Œä½¿ç”¨è€…ç ”ç©¶ï¼Œæˆ‘å€‘å±•ç¤ºäº† DyaDiT åœ¨æ¨™æº–æŒ‡æ¨™ä¸Šå§‹çµ‚å„ªæ–¼ç¾æœ‰æ–¹æ³•ï¼Œä¸¦åœ¨æ„ŸçŸ¥çœŸå¯¦æ€§å’Œç¤¾äº¤ä¸€è‡´æ€§æ–¹é¢å—åˆ°ä½¿ç”¨è€…åå¥½ã€‚</li>
</ul>
<p>æˆ‘å€‘æå‡º DyaDiTï¼Œä¸€å€‹æ“´æ•£ Transformer (DiT)ï¼Œå¯åœ¨é›™äººå°è©±ä¸­ç”Ÿæˆå…·æœ‰ç¤¾äº¤è„ˆçµ¡æ„è­˜çš„æ‰‹å‹¢ã€‚</p>
<p>æˆ‘å€‘å¼•å…¥æ­£äº¤åŒ–äº¤å‰æ³¨æ„åŠ›ï¼ˆORCAï¼‰æ¨¡çµ„ï¼Œæ¸›å°‘å…©å€‹å€‹é«”éŸ³è¨Šæµä¹‹é–“çš„å¹²æ“¾ï¼Œç‚ºé›™äººå°è©±ä¸­æ›´å¥½çš„æ‰‹å‹¢ç”Ÿæˆæä¾›æ›´æ¸…æ½”çš„éŸ³è¨Šè¡¨å¾µã€‚</p>
<p>é€éå»£æ³›çš„é‡åŒ–è©•ä¼°å’Œä½¿ç”¨è€…ç ”ç©¶ï¼Œæˆ‘å€‘å±•ç¤ºäº† DyaDiT åœ¨æ¨™æº–æŒ‡æ¨™ä¸Šå§‹çµ‚å„ªæ–¼ç¾æœ‰æ–¹æ³•ï¼Œä¸¦åœ¨æ„ŸçŸ¥çœŸå¯¦æ€§å’Œç¤¾äº¤ä¸€è‡´æ€§æ–¹é¢å—åˆ°ä½¿ç”¨è€…åå¥½ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23165/x2.png" loading="lazy"></figure> åœ– 2ï¼šDyaDiT æ¦‚è¦½ã€‚DyaDiT ä»¥å¤šç¨®è¼¸å…¥æ¨¡å¼ç‚ºæ¢ä»¶ï¼ŒåŒ…æ‹¬éŸ³è¨Šã€å¤¥ä¼´å‹•ä½œã€é—œä¿‚é¡å‹å’Œå€‹æ€§å¾—åˆ†ã€‚å®ƒæ¡ç”¨éŸ³è¨Šæ­£äº¤åŒ–äº¤å‰æ³¨æ„åŠ›ï¼ˆORCAï¼‰æ¨¡çµ„ä»¥ç²å¾—æ›´æ¸…æ½”çš„éŸ³è¨Šè¡¨å¾µï¼Œä»¥åŠä¸€å€‹å‹•ä½œå­—å…¸ä¾†å¼•å°é¢¨æ ¼æ„ŸçŸ¥çš„æ‰‹å‹¢ç”Ÿæˆã€‚</p>
<h2 id="2">2 ç›¸é—œå·¥ä½œ</h2>
<h3 id="21">2.1 å…±èªæ‰‹å‹¢ç”Ÿæˆ</h3>
<p>å…±èªæ‰‹å‹¢ç”Ÿæˆå°ˆæ³¨æ–¼åˆæˆèˆ‡å–®ä¸€èªªè©±è€…èªéŸ³å°é½Šçš„èº«é«”å‹•ä½œã€‚æ—©æœŸæ–¹æ³•å°‡æ‰‹å‹¢ç”Ÿæˆè¡¨è¿°ç‚ºå¤šæ¨¡æ…‹èªéŸ³ç·šç´¢çš„ç¿»è­¯å•é¡Œï¼Œé€šééè¿´æˆ–å°æŠ—æ¨¡å‹çµåˆæ–‡å­—ã€éŸ³è¨Šå’Œèªªè©±è€…èº«ä»½ï¼Œä¾‹å¦‚ Yoon ç­‰äºº [49] çš„ä¸‰æ¨¡æ…‹æ¡†æ¶ï¼Œå°‡æ‰‹å‹¢ç”Ÿæˆè¦–ç‚ºä¸€å€‹ç¿»è­¯å•é¡Œï¼Œé€šéå°æŠ—éè¿´æ¡†æ¶çµåˆèªéŸ³æ–‡å­—ç¨¿ã€éŸ³è¨Šå’Œèªªè©±è€…èº«ä»½ã€‚Liu ç­‰äºº [23] å¼•å…¥äº† BEAT è³‡æ–™é›†å’Œ CaMNï¼Œä¸€å€‹èƒ½å¤ ç”ŸæˆåŒæ­¥èº«é«”å’Œæ‰‹éƒ¨æ‰‹å‹¢çš„ç´šè¯å¤šæ¨¡æ…‹å°æŠ—ç¶²è·¯ã€‚å…¶ä»–å·¥ä½œï¼Œä¾‹å¦‚ EMAGE [22]ã€TalkSHOW [47]ã€MECo [5] ç­‰ [34, 24, 11, 21, 45] é€šéè§£ç³¾çºç¯€å¥å’Œèªç¾©é‹å‹•ç‰¹å¾µæˆ–é€é VQ-VAEs æ•´åˆé›¢æ•£æ½›åœ¨ç©ºé–“ä¾†å¢å¼·çœŸå¯¦æ€§ã€‚</p>
<p>å„˜ç®¡é€™äº›é€²å±•å€¼å¾—æ³¨æ„ï¼Œä½†å…¶ä¸­å¤§å¤šæ•¸å°ˆæ³¨æ–¼å–®ä¸€èªªè©±è€…å…±èªæ‰‹å‹¢ï¼Œå¿½ç•¥äº†äººé¡æºé€šçš„äº’å‹•æœ¬è³ªã€‚æ›´è¤‡é›œçš„è¨­å®šï¼Œä¾‹å¦‚é›™äººæ‰‹å‹¢ç”Ÿæˆï¼Œéœ€è¦å»ºæ¨¡å…©å€‹åƒèˆ‡è€…çš„è¡Œç‚ºåŠå…¶äººéš›å‹•æ…‹ï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚</p>
<h3 id="22">2.2 é›™äººæ‰‹å‹¢å’Œåæ‡‰ç”Ÿæˆ</h3>
<p>èˆ‡å…±èªæ‰‹å‹¢åˆæˆä¸åŒï¼Œé›™äººæ‰‹å‹¢ç”Ÿæˆå¿…é ˆå»ºæ¨¡å…©å€‹åƒèˆ‡è€…ä¹‹é–“çš„å”èª¿è¡Œç‚ºï¼ŒåŒ…æ‹¬äººéš›æ™‚åºã€ç›¸äº’æ³¨æ„åŠ›å’Œåæ‡‰èƒ½åŠ›ã€‚è©²é ˜åŸŸå¤§å¤šæ•¸å…ˆå‰å·¥ä½œä¾†è‡ªæ–¼è‡‰éƒ¨åæ‡‰ç”Ÿæˆ (FRG)ï¼Œå…¶ç›®æ¨™æ˜¯é æ¸¬è½çœ¾å°èªªè©±è€…è¡Œç‚ºçš„éç¢ºå®šæ€§è‡‰éƒ¨åæ‡‰ [50, 16, 26, 27, 25, 38, 31]ã€‚</p>
<p>å„˜ç®¡ FRG ç‚ºå»ºæ¨¡é›™äººäº’å‹•æä¾›äº†åŸºç¤ï¼Œä½†å®ƒä¸»è¦å°ˆæ³¨æ–¼è‡‰éƒ¨æˆ–é ­éƒ¨å‹•ä½œï¼Œè€Œéå…¨èº«æ‰‹å‹¢ã€‚ç”±æ–¼é€™ä¸€é™åˆ¶ï¼Œæœ€è¿‘çš„ä¸€äº›å·¥ä½œå·²é–‹å§‹æ¢ç´¢é›™äººè¨­å®šä¸­çš„èº«é«”æ‰‹å‹¢ç”Ÿæˆã€‚</p>
<p>ä¸€äº›åŠªåŠ›ï¼Œä¾‹å¦‚ Audio2Photoreal [32]ã€ConvoFusion [29] å’Œ TAG2G [9] å°‡å–®ä¸€èªªè©±è€…æ¡†æ¶æ“´å±•åˆ°é›™æ–¹è¨­å®šã€‚</p>
<p>ç„¶è€Œï¼Œé€™äº›æ¨¡å‹é€šå¸¸è¦å˜› (1) å¿½ç•¥å…©å€‹å€‹é«”ä¹‹é–“çš„ç¤¾æœƒèƒŒæ™¯ï¼Œè¦å˜› (2) å°‡é›™äººéŸ³è¨Šè¦–ç‚ºå–®ä¸€æ··åˆä¿¡è™Ÿï¼Œè€Œä¸æ˜ç¢ºå»ºæ¨¡è·¨äººå“¡å‹•æ…‹ï¼Œé€™å¾€å¾€å°è‡´æ¨¡å‹å‘ˆç¾çš„è§’è‰²å’Œäº’å‹•æ¨¡å¼å‡ºç¾æ­§ç¾©ã€‚é€™äº›é™åˆ¶çªå‡ºäº†å°æ˜ç¢ºç‰¹å¾µè§£ç³¾çºå’Œæ›´å¥½çš„ç¤¾æœƒèƒŒæ™¯æ¨ç†æ”¯æ´çš„éœ€æ±‚ã€‚</p>
<h3 id="23">2.3 æ“´æ•£æ¨¡å‹ç‚ºåŸºç¤çš„æ‰‹å‹¢ç”Ÿæˆ</h3>
<p>æ“´æ•£æ¨¡å‹å› å…¶å»ºæ¨¡å¤šæ¨¡æ…‹å’Œå¤šå°å¤šåˆ†ä½ˆçš„èƒ½åŠ›ï¼Œå·²æˆç‚ºäººé«”å‹•ä½œçš„å¼·å¤§ç”Ÿæˆå·¥å…·ã€‚
è¨±å¤šç ”ç©¶å·²å°‡åŸæœ¬ç‚ºæ–‡æœ¬æ¢ä»¶å‹•ä½œç”Ÿæˆè¨­è¨ˆçš„æ“´æ•£æ¡†æ¶ï¼Œå¦‚ MotionDiffuse [ 51 ]ã€FineMoGen [ 52 ] å’Œ MDM [ 41 ] ç­‰ [ 37ã€40ã€6ã€3 ]ï¼Œæ”¹ç·¨æ‡‰ç”¨æ–¼èªéŸ³â€“æ‰‹å‹¢é ˜åŸŸã€‚Alexanderson ç­‰äºº [ 2 ] ç‚ºå…±ç¾èªéŸ³æ‰‹å‹¢é‡æ–°åˆ¶å®šäº† DiffWaveï¼Œè€Œ Zhu ç­‰äºº [ 55 ] æå‡ºäº† DiffGestureï¼Œå°‡å¸¶é›œè¨Šçš„æ‰‹å‹¢åºåˆ—èˆ‡èªå¢ƒåµŒå…¥æ•´åˆç”¨æ–¼æ™‚é–“å»ºæ¨¡ã€‚DiffuseStyleGesture+ [ 46 ] é€²ä¸€æ­¥å¼•å…¥äº†å°éŸ³è¨Šã€æ–‡æœ¬ã€é¢¨æ ¼å’Œç¨®å­æ‰‹å‹¢çš„æ¢ä»¶é™åˆ¶ï¼Œåˆ©ç”¨åŸºæ–¼ Transformer çš„å»é›œè¨Šä»¥åŠæ³¨æ„åŠ›æ§åˆ¶ã€‚å…¶ä»–ç ”ç©¶å¦‚ UnifiedGesture [ 44 ]ã€LivelySpeaker [ 53 ] å’Œ AMUSE [ 7 ]ï¼Œå¼·èª¿äº†èªç¾©å’ŒéŸ»å¾‹çš„ä¸€è‡´æ€§ï¼Œä¸¦åˆ†é›¢äº†æƒ…æ„Ÿå’Œé¢¨æ ¼æ½›åœ¨å› ç´ ã€‚è€ƒæ…®åˆ°æ“´æ•£æ¨¡å‹åœ¨æ‰‹å‹¢ç”Ÿæˆä¸­çš„æˆåŠŸï¼Œä»¥åŠäºŒäººå°è©±å›ºæœ‰çš„éæ±ºå®šæ€§æœ¬è³ªï¼Œæˆ‘å€‘åŸºæ–¼æ“´æ•£æ¨¡å‹æ§‹å»ºæˆ‘å€‘çš„æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°æ•æ‰é›™äººäº’å‹•ä¸­å­˜åœ¨çš„å¯è®Šæ€§å’Œå‹•æ…‹ç‰¹æ€§ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23165/x3.png" loading="lazy"></figure> åœ– 3ï¼šORCA æ¸›å°‘äº†å…©å€‹éŸ³è¨Šä¸²æµä¹‹é–“çš„æ­§ç¾©ï¼Œä½¿ DyaDiT å³ä½¿åœ¨ä¸€å€‹äººæ‰“æ–·å°è©±ä¸­çš„å¦ä¸€å€‹äººæ™‚ï¼Œä¹Ÿèƒ½ç”Ÿæˆé€¼çœŸçš„å‹•ä½œã€‚è©²ç¤ºä¾‹å±•ç¤ºäº†éš¨è‘—å°è©±è½‰è®Šï¼Œç”Ÿæˆçš„å‹•ä½œå¦‚ä½•è‡ªç„¶èª¿æ•´ã€‚</p>
<h2 id="3-dyadit">3 DyaDiT</h2>
<p>æˆ‘å€‘ä»‹ç´¹ DyaDiTï¼Œä¸€å€‹å…·æœ‰å¤šæ¨¡æ…‹è¼¸å…¥çš„æ“´æ•£ Transformerï¼ˆDiTï¼‰ï¼Œç”¨æ–¼äºŒäººæ‰‹å‹¢ç”Ÿæˆã€‚çµ¦å®šä¾†è‡ªå…©å€‹å€‹é«”çš„å°è©±éŸ³è¨Šä¸²æµï¼ˆè¡¨ç¤ºç‚ºè‡ªæˆ‘å’Œä»–äººï¼‰ï¼Œæˆ‘å€‘çš„ç›®æ¨™æ˜¯æ ¹æ“šå°è©±èªå¢ƒç‚ºå¦ä¸€ä½èªªè©±è€…ç”Ÿæˆåˆç†çš„ä¸Šèº«æ‰‹å‹¢ã€‚ç‚ºäº†é€²ä¸€æ­¥å¢å¼·ç”Ÿæˆæ‰‹å‹¢çš„çœŸå¯¦æ€§ï¼ŒDyaDiT åœ¨æ¨è«–æœŸé–“å¯é¸æ“‡æ€§åœ°ç´å…¥è‡ªæˆ‘çš„é—œä¿‚ã€æ€§æ ¼ç‰¹å¾µå’Œæ‰‹å‹¢åºåˆ—ä½œç‚ºè¼”åŠ©æ¢ä»¶ä¿¡è™Ÿã€‚æˆ‘å€‘åœ¨ Seamless Interaction è³‡æ–™é›† [ 1 ] çš„å­é›†ä¸Šè¨“ç·´æˆ‘å€‘çš„æ¨¡å‹ï¼Œè³‡æ–™é›†è¦ç¯„å’Œå‰è™•ç†ç¨‹åºè©³è¦‹ç¬¬ 3.1 ç¯€ã€‚å®Œæ•´æ¶æ§‹æ¦‚è¦½å¦‚åœ– 2 æ‰€ç¤ºã€‚éŸ³è¨Šæ­£äº¤åŒ–äº¤å‰æ³¨æ„åŠ›ï¼ˆORCAï¼‰æ¨¡çµ„çš„è©³ç´°è³‡è¨Šåœ¨ç¬¬ 3.3 ç¯€æä¾›ï¼Œè€Œå‹•ä½œå­—å…¸å’Œå‹•ä½œåˆ†è©å™¨åˆ†åˆ¥å‘ˆç¾åœ¨ç¬¬ 3.4 å’Œ 3.5 ç¯€ã€‚</p>
<h3 id="31-seamless-interaction">3.1 Seamless Interaction è³‡æ–™é›†</h3>
<p>Seamless Interaction è³‡æ–™é›† [ 1 ] æ˜¯ä¸€å€‹å¤§è¦æ¨¡çš„é›™äººå°è©±èªæ–™åº«ï¼ŒåŒ…å«åŒæ­¥çš„éŸ³è¨Šã€å…¨èº«å‹•ä½œå’Œè‡‰éƒ¨è¡¨æƒ…ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘ä½¿ç”¨è©²è³‡æ–™é›†è‡ªç„¶å ´æ™¯é›†åˆä¸­çš„ç­–åŠƒå­é›†ï¼Œå…± 3,000 å€‹ç‰‡æ®µï¼ˆç´„ 182 å°æ™‚ï¼‰ï¼Œå…¶ä¸­åŒ…å«è±å¯Œä¸”è‡ªç™¼æ€§çš„é›™äººå°è©±ï¼Œé©åˆç”¨æ–¼å»ºæ¨¡å°è©±æ‰‹å‹¢ã€‚æˆ‘å€‘å°ˆæ³¨æ–¼ç”Ÿæˆä¸Šèº«æ‰‹å‹¢ï¼Œè¡¨ç¤ºç‚º $g \in \mathbb{R}^{T \times J \times 6}$ï¼Œä½¿ç”¨ 6D æ—‹è½‰è¡¨ç¤ºæ³• [ 54 ]ï¼Œå…¶ä¸­ $J$ è¡¨ç¤ºä¸Šèº«é—œç¯€çš„æ•¸é‡ã€‚é™¤äº†å‹•ä½œè³‡æ–™å¤–ï¼Œæˆ‘å€‘é‚„ç´å…¥è³‡æ–™é›†ä¸­æä¾›çš„å…©å€‹é«˜å±¤æ¬¡ç¤¾äº¤æ¨™ç±¤ï¼šé—œä¿‚é¡å‹æ¨™ç±¤ $f_{rs} \in {0,1}^{4}$ï¼Œè¡¨ç¤ºç™¼è¨€è€…æ˜¯æœ‹å‹ã€é™Œç”Ÿäººã€å®¶åº­æˆå“¡æˆ–ç´„æœƒå°è±¡ï¼›ä»¥åŠäººæ ¼åˆ†æ•¸å‘é‡ $f_{ps} \in \mathbb{R}^{5}$ï¼Œé‡åŒ–äº”å€‹ä¸»è¦äººæ ¼ç‰¹è³ªï¼Œåˆ†åˆ¥æ˜¯å¤–å‘æ€§ã€è¦ªå’Œæ€§ã€å‹¤å‹æ€§ã€ç¥ç¶“è³ªå’Œé–‹æ”¾æ€§ã€‚å°æ–¼éŸ³è¨Šè¼¸å…¥ï¼Œæˆ‘å€‘å¾å…©å€‹å€‹é«”æå–é›™äººèªéŸ³ä¿¡è™Ÿï¼Œä¸¦ä½¿ç”¨é è¨“ç·´çš„ Wav2Vec2 ç·¨ç¢¼å™¨ [ 4 ] è™•ç†å®ƒå€‘ï¼Œä»¥ç²å¾—éŸ³è¨Šç‰¹å¾µåµŒå…¥ä»¥åœ¨æˆ‘å€‘çš„æ¨¡å‹ä¸­é€²è¡Œæ¢ä»¶è¨­ç½®ã€‚</p>
<h3 id="32-dit">3.2 DiT éª¨å¹¹</h3>
<p>æˆ‘å€‘çš„æ¨¡å‹æ¡ç”¨éµå¾ªå»å™ªæ“´æ•£æ©Ÿç‡æ¨¡å‹ï¼ˆDDPMï¼‰[ 13 ] æ¡†æ¶çš„æ“´æ•£è½‰æ›å™¨ï¼ˆDiTï¼‰éª¨å¹¹ã€‚è©²ç¶²è·¯æ¡ç”¨å¸¶é›œè¨Šçš„æ½›åœ¨å§¿æ…‹ $\mathbf{x}<em>{t}$ï¼Œä¸¦æ ¹æ“šæ™‚é–“æ­¥ $t$ é æ¸¬æ·»åŠ çš„é«˜æ–¯é›œè¨Š $\boldsymbol{\epsilon}</em>{\theta}(\mathbf{x}<em>{t},t,\mathbf{c})$ï¼Œæ¢ä»¶åŸºæ–¼å¤šå€‹èƒŒæ™¯è¼¸å…¥ $\mathbf{c}$ï¼Œ
å…¶ä¸­ $\mathbf{c}=(ORCA(a</em>{\text{self}},a_{\text{other}}),p_{\text{self}},f_{\text{relat}},f_{\text{ps}})$ åŒ…æ‹¬éŸ³è¨Šã€å¤¥ä¼´çš„å‹•ä½œã€é—œä¿‚é¡å‹å’Œäººæ ¼åˆ†æ•¸ã€‚è¨“ç·´ç›®æ¨™éµå¾ªæ¨™æº–çš„ $\epsilon$ é æ¸¬æå¤±ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathcal{L}<em>{\text{diff}}=\mathbb{E}</em>{\mathbf{x}<em>{0},t,\boldsymbol{\epsilon}}\left[\left|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}</em>{\theta}(\mathbf{x}<em>{t},t,\mathbf{c})\right|</em>{2}^{2}\right]$,</th>
<th></th>
<th>(1)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ $\mathbf{x}<em>{t}=\sqrt{\bar{\alpha}</em>{t}}\mathbf{x}<em>{0}+\sqrt{1-\bar{\alpha}</em>{t}}\boldsymbol{\epsilon}$ã€‚</p>
<p>å¦‚åœ– 2 (a) æ‰€ç¤ºï¼Œæ¯å€‹ DiT å€å¡ŠåŒ…å«ä¸€å€‹è‡ªæ³¨æ„å±¤ï¼Œç”¨æ–¼å»ºæ¨¡æ½›åœ¨å§¿æ…‹åºåˆ—ä¸­çš„æ™‚é–“ä¾è³´æ€§ï¼Œä»¥åŠä¸€å€‹äº¤å‰æ³¨æ„å±¤ï¼Œç”¨æ–¼æ•´åˆä¾†è‡ªå¤šæ¨¡æ…‹ç·šç´¢çš„èƒŒæ™¯ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé—œä¿‚å’Œäººæ ¼åµŒå…¥é€éåŸºæ–¼ FiLM [ 35 ] çš„èª¿ç¯€å’Œäº¤å‰æ³¨æ„æ³¨å…¥ï¼Œä½¿ DyaDiT èƒ½å¤ è¯åˆæ•æ‰æ‰‹å‹¢ç”Ÿæˆä¸­çš„ç¤¾äº¤å±¬æ€§å’Œå€‹äººè¡¨ç¾é¢¨æ ¼ã€‚</p>
<h3 id="33-orca">3.3 éŸ³è¨Šæ­£äº¤åŒ–äº¤å‰æ³¨æ„åŠ›ï¼ˆORCAï¼‰</h3>
<p>ç‚ºäº†æœ‰æ•ˆåœ°æ•æ‰å…©ä½èªªè©±è€…ä¹‹é–“çš„å°è©±å‹•æ…‹ï¼Œæˆ‘å€‘å¼•å…¥äº†ç”¨æ–¼éŸ³è¨Šèåˆçš„æ­£äº¤åŒ–äº¤å‰æ³¨æ„åŠ›æ¨¡çµ„ï¼ˆåœ– 2 (b)ï¼‰ã€‚çµ¦å®šç”± Wav2Vec2 ç·¨ç¢¼çš„ä¾†è‡ªå…©ä½èªªè©±è€…çš„éŸ³è¨Šç‰¹å¾µ $a_{\text{self}}$ å’Œ $a_{\text{other}}$ï¼Œæˆ‘å€‘çš„ç›®æ¨™æ˜¯åˆ†é›¢å†—é¤˜å› ç´ ï¼ŒåŒæ™‚å°‡äº’è£œè³‡è¨Šå°é½Šè‡³è¯åˆè¡¨ç¤ºã€‚</p>
<p>æˆ‘å€‘é¦–å…ˆæ‡‰ç”¨æ­£äº¤åŒ–éç¨‹ä»¥éæ¿¾å‡ºå…©å€‹éŸ³è¨Šæµä¹‹é–“çš„å†—é¤˜æˆåˆ†ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$a_{\text{self}}^{\perp}=a_{\text{self}}-\mathrm{Proj}<em>{a</em>{\text{other}}}(a_{\text{self}})$ ,</th>
<th></th>
<th>(2)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ $\mathrm{Proj}<em>{a</em>{\text{other}}}(\cdot)$ è¡¨ç¤ºè‡ªèº«éŸ³è¨Šç‰¹å¾µ $a_{\text{self}}$ åœ¨ä»–äººéŸ³è¨Šç‰¹å¾µå­ç©ºé–“ä¸Šçš„æŠ•å½±ã€‚æ­¤æ“ä½œå¼·åˆ¶å¯¦ç¾äº’è£œèª¿ç¯€ä¸¦æ¸›å°‘äºŒäººçµ„éŸ³è¨Šä¸­çš„ç›¸é—œè³‡è¨Šã€‚</p>
<p>éš¨å¾Œï¼Œæˆ‘å€‘æ¡ç”¨å…©å€‹å°ç¨±çš„äº¤å‰æ³¨æ„åŠ›æ¨¡çµ„ä»¥å¯¦ç¾é›™å‘è³‡è¨Šäº¤æ›ã€‚ç¬¬ä¸€å€‹æ¨¡çµ„ä½¿ç”¨ $a_{\text{other}}$ ä½œç‚ºæŸ¥è©¢ä¸¦é—œæ³¨ $a_{\text{self}}^{\perp}$ï¼Œæ•æ‰èªªè©±è€…å°å¤¥ä¼´è©±èªçš„å›æ‡‰ã€‚åä¹‹ï¼Œç¬¬äºŒå€‹æ¨¡çµ„ä»¥ $a_{\text{self}}^{\perp}$ ä½œç‚ºæŸ¥è©¢ä¸¦é—œæ³¨ $a_{\text{other}}$ï¼Œå»ºæ¨¡è½è€…çš„åæ‡‰ç·šç´¢ã€‚å…©å€‹äº¤å‰æ³¨æ„åŠ›æµçš„è¼¸å‡ºéš¨å¾Œé€šéå¯å­¸ç¿’çš„é–€æ§æ©Ÿåˆ¶é€²è¡Œè‡ªé©æ‡‰èåˆï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$f_{\text{audio}}=\sigma(\mathbf{W}<em>{g})\cdot h</em>{\text{self}\rightarrow\text{other}}+(1-\sigma(\mathbf{W}<em>{g}))\cdot h</em>{\text{other}\rightarrow\text{self}}$ ,</th>
<th></th>
<th>(3)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ $\sigma(\cdot)$ æ˜¯ sigmoid å‡½æ•¸ï¼Œ$\mathbf{W}_{g}$ æ˜¯å¯å­¸ç¿’çš„é–€æ§åƒæ•¸ã€‚</p>
<p>å¾—åˆ°çš„èåˆæ¨™è¨˜ $f_{\text{audio}}$ è¢«ç”¨ä½œ DyaDiT çš„æœ€çµ‚éŸ³è¨Šèª¿ç¯€è¼¸å…¥ï¼Œæä¾›åæ˜ å…©ä½å°è©±è€…è²éŸ³è¡Œç‚ºçš„åˆç†è²å­¸åµŒå…¥ã€‚å¦‚åœ– 3 æ‰€ç¤ºï¼Œé€é ORCAï¼ŒDyaDiT å…·å‚™ç”Ÿæˆèªªè©±è€…æˆ–è½è€…å§¿æ…‹çš„èƒ½åŠ›ã€‚</p>
<h3 id="34-motion-dictionary-md">3.4 Motion Dictionary (MD)</h3>
<p>Inspired by prior work LIA [ 43 ] which learns a set of orthogonal motion-directions by enforcing orthonormality at initialization, we similarly introduce a learnable orthogonal motion dictionary to modulate the partnerâ€™s audio feature a o â€‹ t â€‹ h â€‹ e â€‹ r a_{other} according to the current style motion. The motion dictionary is a module designed to incorporate motion style conditioning when user requires motion style control. As shown in Figure 2 (c), it consists of a set of learnable motion bases { d 0 , d 1 , â€¦ , d n } {d_{0},d_{1},\dots,d_{n}} that encode representative gesture primitives. During training, we include ground truth motion style features f motion = [ m 0 , m 1 , â€¦ , m n ] f_{\text{motion}}=[m_{0},m_{1},\dots,m_{n}] to guide the model in learning style-aware correspondences between audio cues and motion patterns.</p>
<p>Given the other-speaker audio feature a other a_{\text{other}} , the dictionary is integrated through a cross-attention (CA) operation followed by a weighted combination with the motion bases:</p>
<table>
<thead>
<tr>
<th></th>
<th>a other â€² = CA â€‹ ( a other , âˆ‘ k = 0 n m k â€‹ d k ) + a other , a_{\text{other}}^{\prime}=\mathrm{CA}(a_{\text{other}},\sum_{k=0}^{n}m_{k}d_{k})+a_{\text{other}},</th>
<th></th>
<th>(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>where Dict = [ d 0 , d 1 , â€¦ , d n ] \mathrm{Dict}=[d_{0},d_{1},\dots,d_{n}] and m k m_{k} denotes the style-dependent modulation weight derived from f motion f_{\text{motion}} .</p>
<p>At inference, the motion dictionary can be optionally activated. With classifier-free guidance (CFG) [ 14 ] , the style conditioning can be strengthened to generate gestures with distinct stylistic patterns, or completely dropped to produce an unconditional, style-agnostic motion driven purely by the learned audioâ€“motion prior.</p>
<h3 id="35-motion-tokenizer">3.5 Motion Tokenizer</h3>
<p>æˆ‘å€‘ä¸ç›´æ¥ä½¿ç”¨åŸå§‹æ‰‹å‹¢åºåˆ— $x_0^{\prime}\in\mathbb{R}^{T\times J}$ è¨“ç·´ DiT æ¨¡å‹ï¼Œè€Œæ˜¯å…ˆè¨“ç·´ä¸€å€‹ VQ-VAE å°‡é€£çºŒå‹•ä½œç©ºé–“é›¢æ•£åŒ–ç‚ºç·Šæ¹Šçš„æ½›åœ¨è¡¨ç¤ºã€‚æˆ‘å€‘çš„å¯¦ä½œéµå¾ªå…ˆå‰ç ”ç©¶ä¸­å¼•å…¥çš„æ®˜å·®å‘é‡é‡åŒ–æ¡†æ¶ [18]ï¼Œå…¶ä¸­å¤šå€‹ç¢¼å†Šç´šè¯ä»¥é€æ­¥ç²¾åŒ–é‡åŒ–èª¤å·®ã€‚å…·é«”ä¾†èªªï¼Œæˆ‘å€‘æ¡ç”¨ä¸€å€‹æ®˜å·®é•·åº¦ç‚º 4 çš„æ®˜å·® VQ-VAEï¼Œåˆ©ç”¨å››å€‹åˆ†å±¤ç¢¼å†Šä¾†æ•æ‰é€æ¼¸æ›´ç´°ç²’åº¦çš„å‹•ä½œç´°ç¯€ã€‚æ­¤å¤–ï¼Œç‚ºäº†æ¸›å°‘æ™‚é–“å†—é¤˜ï¼Œæˆ‘å€‘åœ¨ç·¨ç¢¼å™¨ä¸­ä½¿ç”¨ 1D å·ç©å±¤å°‡è¼¸å…¥æ‰‹å‹¢åºåˆ—ä¸‹æ¡æ¨£ 4 å€ï¼Œä½¿å¾—æ¯å››å€‹ç•«å¹€å°æ‡‰ä¸€å€‹æ½›åœ¨ä»¤ç‰Œã€‚åœ¨è¨“ç·´ DyaDiT æœŸé–“ï¼ŒVQ-VAE ç·¨ç¢¼å™¨å°‡æ‰‹å‹¢åºåˆ—å£“ç¸®ç‚ºé‡åŒ–ä»¤ç‰Œåºåˆ— $x_t^{\prime}\in\mathbb{R}^{T/4\times d}$ï¼Œå…¶ä¸­åœ¨æˆ‘å€‘çš„å¯¦é©—ä¸­ $d=64$ã€‚åœ¨æ¨è«–æ™‚ï¼ŒVQ-VAE è§£ç¢¼å™¨éš¨å¾Œå¾ç”Ÿæˆçš„é‡åŒ–åµŒå…¥é‡å»ºé€£çºŒå‹•ä½œã€‚è«‹æ³¨æ„ï¼Œç”±æ–¼æˆ‘å€‘çš„æ“´æ•£æ¨¡å‹ç›´æ¥åœ¨æ½›åœ¨ç©ºé–“ä¸­é‹ä½œï¼Œç·¨ç¢¼å™¨åœ¨æ¨è«–æœŸé–“ä¸è¢«ä½¿ç”¨ï¼Œè§£ç¢¼å™¨åœ¨è¨“ç·´æœŸé–“ä¹Ÿä¸è¢«ä½¿ç”¨ã€‚é€™ç¨®åˆ†å±¤ä¸”æ™‚é–“ä¸Šç·Šæ¹Šçš„é‡åŒ–å…è¨±æ“´æ•£æ¨¡å‹æ•æ‰é•·è·é›¢ä¾è³´æ€§ï¼ŒåŒæ™‚ä¿æŒé«˜ä¿çœŸå‹•ä½œé‡å»ºã€‚</p>
<h2 id="4">4 å¯¦é©—</h2>
<p>æˆ‘å€‘é€éå®šé‡å’Œå®šæ€§å¯¦é©—è©•ä¼° DyaDiT åœ¨äºŒäººæ‰‹å‹¢ç”Ÿæˆä¸Šçš„æœ‰æ•ˆæ€§ã€‚å°æ–¼å®šé‡è©•ä¼°ï¼Œæˆ‘å€‘è¨ˆç®— FrÃ©chet Distance (FD) å’Œå¤šæ¨£æ€§æŒ‡æ¨™ï¼Œå°‡ DyaDiT èˆ‡å…©å€‹ç¾æœ‰çš„äºŒäººæ‰‹å‹¢ç”Ÿæˆæ¨¡å‹ ConvoFusion [29] å’Œ Audio2PhotoReal [32] é€²è¡Œæ¯”è¼ƒã€‚æ­¤å¤–ï¼Œæˆ‘å€‘å ±å‘Šé—œæ–¼çœŸå¯¦æ‰‹å‹¢çš„åˆ†æ•¸ï¼Œé€™äº›åˆ†æ•¸å¯ä½œç‚ºè³‡æ–™é›†ä¸­åº•å±¤å‹•ä½œåˆ†ä½ˆçš„åƒè€ƒã€‚å°æ–¼å®šæ€§è©•ä¼°ï¼Œæˆ‘å€‘é€²è¡Œä½¿ç”¨è€…åå¥½ç ”ç©¶ï¼Œä»¥è©•ä¼°ç”Ÿæˆæ‰‹å‹¢çš„æ•´é«”æ„ŸçŸ¥å“è³ªåŠå…¶èˆ‡ç¤¾äº¤é—œä¿‚å’Œå€‹æ€§ç‰¹å¾µçš„å°é½ã€‚</p>
<h3 id="41-evaluation-metrics">4.1 Evaluation Metrics</h3>
<p>Table 1: Quantitative comparison of DyaDiT and baselines in terms of FrÃ©chet Distance (FD) and Diversity. Lower FD indicates higher realism, and higher diversity values indicate more varied motion generation.</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23165/x4.png" loading="lazy"></figure> Figure 4 : Qualitative Results. Comparison of visualization results between DyaDiT , ConvoFusion [ 29 ] , and Audio2PhotoReal [ 3 ] . The gestures generated by DyaDiT exhibit higher diversity and greater realism compared to the other methods.</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23165/x5.png" loading="lazy"></figure> Figure 5 : Visualization results under different personality score conditionings. All samples are generated using classifier-free guidance with CFG = 2.5.</p>
<p>Following prior works on gesture generation [ 32 , 3 , 2 ] , we adopt both distribution and diversity metrics for quantitative evaluation. Specifically, we report the FrÃ©chet Distance (FD) to measure the realism of generated motions, Diversity [ 20 ] to quantify motion variability across samples
For reference, we also compute all metrics on ground truth samples from the Seamless Interaction dataset [ 1 ] to provide as baseline for comparison. Below, we provide the definitions of the quantitative metrics used in our evaluation.</p>
<ul>
<li>â€¢ FD (Static) measures the pose realism for individual frames in the pose space â„ 3 Ã— j \mathbb{R}^{3\times j} , where j = 43 j{=}43 (including finger joints).</li>
<li>â€¢ FD (Kinetic) evaluates the realism of temporal dynamics by computing the FD over gesture velocities in â„ T Ã— 3 Ã— j \mathbb{R}^{T\times 3\times j} , where T = 300 T{=}300 frames in our experiments.</li>
<li>â€¢ Diversity (Static) quantifies the variation of single-frame poses by computing the average mean squared error (MSE) between clips.</li>
<li>â€¢ Diversity (Kinetic) measures temporal motion diversity by averaging the pairwise velocity differences across generated gesture sequences.</li>
</ul>
<p>FD (Static) measures the pose realism for individual frames in the pose space â„ 3 Ã— j \mathbb{R}^{3\times j} , where j = 43 j{=}43 (including finger joints).</p>
<p>FD (Kinetic) evaluates the realism of temporal dynamics by computing the FD over gesture velocities in â„ T Ã— 3 Ã— j \mathbb{R}^{T\times 3\times j} , where T = 300 T{=}300 frames in our experiments.</p>
<p>Diversity (Static) quantifies the variation of single-frame poses by computing the average mean squared error (MSE) between clips.</p>
<p>Diversity (Kinetic) measures temporal motion diversity by averaging the pairwise velocity differences across generated gesture sequences.</p>
<h3 id="42-baseline-methods">4.2 Baseline Methods</h3>
<p>We compare DyaDiT with two representative dyadic gesture generation models:</p>
<ul>
<li>â€¢ ConvoFusion [ 29 ] implement a diffusion-based framework with multiple layers of cross-attention to fuse multimodal inputs, including audio and visual features. For a fair comparison, we retrain their model on our training subset of the Seamless Interaction Dataset [ 1 ] to match our experimental domain and output representation.</li>
<li>â€¢ Audio2PhotoReal [ 32 ] is a full-body gesture generation model that jointly synthesizes body and facial motion. We extract and adapt only the body gesture generation branch for comparison. Specifically, the model first predicts keyframe poses from audio using a causal transformer, and then employs a diffusion transformer to interpolate between keyframes to produce temporally coherent motion sequences. Likewise, we retrain the model on our dataset to ensure domain consistency with our evaluation study setup.</li>
</ul>
<p>ConvoFusion [ 29 ] implement a diffusion-based framework with multiple layers of cross-attention to fuse multimodal inputs, including audio and visual features. For a fair comparison, we retrain their model on our training subset of the Seamless Interaction Dataset [ 1 ] to match our experimental domain and output representation.</p>
<p>Audio2PhotoReal [ 32 ] is a full-body gesture generation model that jointly synthesizes body and facial motion. We extract and adapt only the body gesture generation branch for comparison. Specifically, the model first predicts keyframe poses from audio using a causal transformer, and then employs a diffusion transformer to interpolate between keyframes to produce temporally coherent motion sequences. Likewise, we retrain the model on our dataset to ensure domain consistency with our evaluation study setup.</p>
<p>During inference, both methods use a DDIM scheduler with 50 denoising steps (CFG = 2) to generate 300-frame gesture sequences (10 seconds).</p>
<h3 id="43-quantitative-results">4.3 Quantitative Results</h3>
<p>The quantitative comparison between DyaDiT and the baseline models is shown in Table 1 .
For interpretability, the GT diversity reflects the datasetâ€™s upper bound, while the GT Random FD serves as its lower bound. Across all metrics, DyaDiT consistently outperforms existing baselines, achieving substantially lower FD(static) and FD(kinetic) scores while preserving high motion diversity. These results demonstrate that the proposed social-context-aware DiT framework is more effective than the multi-branch fusion design in ConvoFusion [ 29 ] and the keyframe interpolation approach used in Audio2PhotoReal [ 20 ] .</p>
<h3 id="44">4.4 æ¶ˆèç ”ç©¶</h3>
<p>æˆ‘å€‘é€²ä¸€æ­¥é€²è¡Œäº†å¤šé …æ¶ˆèç ”ç©¶ï¼Œä»¥åˆ†æä¸åŒçš„çµ„ä»¶å’Œæ¢ä»¶ä¿¡è™Ÿå¦‚ä½•å° DyaDiT çš„æ€§èƒ½åšå‡ºè²¢ç»ã€‚ä»¥ä¸‹ï¼Œæˆ‘å€‘æ˜ç¢ºè§£é‡‹äº†å„å€‹è®Šé«”ä¹‹é–“çš„å·®ç•°ã€‚</p>
<ul>
<li>â€¢ DyaDiT (w/o ORCA) ç§»é™¤ ORCA æ¨¡çµ„ï¼Œç›´æ¥ä¸²è¯åŸå§‹çš„é›™äººå°è©±éŸ³è¨Šç‰¹å¾µã€‚</li>
<li>â€¢ DyaDiT (w/o MD) ç§»é™¤å‹•ä½œå­—å…¸ã€‚</li>
<li>â€¢ DyaDiT (Uncond) åœ¨æ²’æœ‰ä»»ä½•æ¢ä»¶çš„æƒ…æ³ä¸‹ç”Ÿæˆæ‰‹å‹¢ã€‚</li>
<li>â€¢ DyaDiT (Random) ä½¿ç”¨éš¨æ©Ÿåˆ†é…çš„é—œä¿‚å’Œäººæ ¼æ¨™ç±¤åŸ·è¡Œæ¨ç†ã€‚</li>
</ul>
<p>DyaDiT (w/o ORCA) ç§»é™¤ ORCA æ¨¡çµ„ï¼Œç›´æ¥ä¸²è¯åŸå§‹çš„é›™äººå°è©±éŸ³è¨Šç‰¹å¾µã€‚</p>
<p>DyaDiT (w/o MD) ç§»é™¤å‹•ä½œå­—å…¸ã€‚</p>
<p>DyaDiT (Uncond) åœ¨æ²’æœ‰ä»»ä½•æ¢ä»¶çš„æƒ…æ³ä¸‹ç”Ÿæˆæ‰‹å‹¢ã€‚</p>
<p>DyaDiT (Random) ä½¿ç”¨éš¨æ©Ÿåˆ†é…çš„é—œä¿‚å’Œäººæ ¼æ¨™ç±¤åŸ·è¡Œæ¨ç†ã€‚</p>
<p>ä¸å‡ºæ‰€æ–™ï¼Œæˆ‘å€‘è§€å¯Ÿåˆ°ç§»é™¤ ORCA æ¨¡çµ„å°è‡´ FD æ€§èƒ½å‡ºç¾æ˜é¡¯ä¸‹é™ï¼Œç¢ºèªäº†å…¶å°å‹•ä½œçœŸå¯¦æ€§çš„è²¢ç»ã€‚åŒæ¨£åœ°ï¼Œç§»é™¤å‹•ä½œå­—å…¸ä¹Ÿæœƒé™ä½æ•´é«”æ€§èƒ½ï¼Œç‰¹åˆ¥æ˜¯åœ¨å¤šæ¨£æ€§ï¼ˆéœæ…‹ï¼‰ä¸Šï¼Œä¸‹é™äº† 9.12ï¼Œé€™è¡¨æ˜å­¸ç¿’åˆ°çš„å‹•ä½œåŸºç¤åœ¨ä¿ƒé€²é¢¨æ ¼è®ŠåŒ–æ–¹é¢æ‰®æ¼”è‘—é‡è¦è§’è‰²ã€‚</p>
<p>æ­¤å¤–ï¼Œæˆ‘å€‘è§€å¯Ÿåˆ° DyaDiT åœ¨ç”Ÿæˆå“è³ªæ–¹é¢å„ªæ–¼ DyaDiT (Uncond) å’Œ DyaDiT (Random)ã€‚é€™è¡¨æ˜ç¤¾æœƒèªå¢ƒå°æ–¼ç”¢ç”Ÿæ›´é€¼çœŸçš„æ‰‹å‹¢ç”Ÿæˆè‡³é—œé‡è¦ã€‚é€²ä¸€æ­¥åœ°ï¼Œæˆ‘å€‘ç™¼ç¾ç•¶ç§»é™¤æˆ–éš¨æ©Ÿåˆ†é…ç¤¾æœƒèªå¢ƒæ™‚ï¼Œå¤šæ¨£æ€§ï¼ˆéœæ…‹ï¼‰æŒ‡æ¨™ä¸‹é™äº†ç´„ 6 é»ã€‚æˆ‘å€‘èªç‚ºé€™æ˜¯å› ç‚ºéŸ³è¨Šä¿¡è™Ÿèˆ‡é—œä¿‚å’Œäººæ ¼ç‰¹å¾µå‘ˆæ­£ç›¸é—œï¼Œä¸åŒ¹é…æˆ–ç¼ºå¤±çš„ç¤¾æœƒèªå¢ƒå°è‡´æ›´å—é™çš„å‹•ä½œåˆ†ä½ˆã€‚</p>
<p>æœ€å¾Œï¼Œæˆ‘å€‘è§€å¯Ÿåˆ° DyaDiT (Uncond) ä¸¦æœªè¡¨ç¾å‡ºæ¯”å®Œå…¨æœ‰æ¢ä»¶çš„ DyaDiT æ¨¡å‹æ›´é«˜çš„å¤šæ¨£æ€§ã€‚æˆ‘å€‘èªç‚ºé€™æ˜¯ç”±æ–¼é›™äººå°è©±çš„æ€§è³ªï¼Œå…¶ä¸­å¤§éƒ¨åˆ†è³‡æ–™åŒ…å«è½è€…è¡Œç‚ºï¼Œèˆ‡èªªè©±è€…è¡Œç‚ºç›¸æ¯”ï¼Œé€™è‡ªç„¶åœ¨éœæ…‹å’Œå‹•æ…‹æ‰‹å‹¢ä¸Šè¡¨ç¾å‡ºè¼ƒå°‘çš„è®ŠåŒ–ã€‚å› æ­¤ï¼Œé¡å¤–çš„æ¢ä»¶ä¿¡è™Ÿæœ‰åŠ©æ–¼å¼•å°æ¨¡å‹ç”¢ç”Ÿæ›´å»£æ³›çš„æ‰‹å‹¢è®ŠåŒ–ï¼Œå¾è€Œå¯¦ç¾æ›´é«˜çš„æ•´é«”å¤šæ¨£æ€§ã€‚</p>
<h2 id="5">5 ä½¿ç”¨è€…ç ”ç©¶</h2>
<p>æˆ‘å€‘èˆ‡åå…­ååƒèˆ‡è€…é€²è¡Œäº† A/B åå¥½ç ”ç©¶ï¼Œä»¥è©•ä¼°ä½¿ç”¨è€…å°äºŒäººå°è©±è¨­ç½®ä¸­äººé¡å‹•ä½œçš„åå¥½ã€‚æˆ‘å€‘å°‡æˆ‘å€‘çš„æ–¹æ³•èˆ‡ ConvoFusion [29] ä»¥åŠçœŸå¯¦å‹•ä½œé€²è¡Œäº†æ¯”è¼ƒã€‚ä½¿ç”¨è€…ç ”ç©¶è©•ä¼°äº†ç”Ÿæˆæ‰‹å‹¢çš„ä¸‰å€‹æ–¹é¢ï¼šæ•´é«”å“è³ªã€é—œä¿‚ä¸€è‡´æ€§å’Œå€‹æ€§ä¸€è‡´æ€§ã€‚åƒèˆ‡è€…è¢«å±•ç¤ºæˆå°çš„å½±ç‰‡ç‰‡æ®µï¼Œä¸¦é¸æ“‡ä»–å€‘åå¥½çš„æ‰‹å‹¢åºåˆ—ï¼Œå½±ç‰‡å°ä»¥éš¨æ©Ÿé †åºå‘ˆç¾ä»¥é¿å…æ’åºåå·®ã€‚åœ– 6 å±•ç¤ºäº†ä¸€å€‹ç¯„ä¾‹å½±ç‰‡å°ã€‚</p>
<p>æˆ‘å€‘å¾ Seamless Interaction è³‡æ–™é›†çš„é©—è­‰é›†ä¸­éš¨æ©Ÿé¸æ“‡äº† 56 å€‹åç§’çš„åºåˆ—ä¾†é€²è¡Œå¯¦é©—ã€‚ä»¥ä¸‹é¡¯ç¤ºäº†åœ¨æ¯å€‹éƒ¨åˆ†ä¸­å‘ˆç¾çµ¦åƒèˆ‡è€…çš„æ¨£æœ¬å•é¡Œã€‚</p>
<ul>
<li>â€¢ æ•´é«”å“è³ªï¼šã€Œæ©™è‰²è§’è‰²çš„å“ªå€‹æ‰‹å‹¢çœ‹èµ·ä¾†æ›´åƒäººé¡ï¼Ÿã€</li>
<li>â€¢ é—œä¿‚ä¸€è‡´æ€§ï¼šã€Œå“ªä¸€å°çœ‹èµ·ä¾†æ›´åƒæ˜¯æœ‹å‹ï¼ˆæˆ–é™Œç”Ÿäººã€æˆ–å®¶åº­æˆå“¡ã€æˆ–ç´„æœƒå°è±¡ï¼‰ï¼Ÿã€</li>
<li>â€¢ å€‹æ€§ä¸€è‡´æ€§ï¼šã€Œå“ªå€‹æ‰‹å‹¢æ›´èƒ½åæ˜ èªåŒæ€§ï¼ˆæˆ–ç›¡è²¬æ€§ã€æˆ–å¤–å‘æ€§ã€æˆ–ç¥ç¶“è³ªï¼‰çš„å€‹æ€§ç‰¹è³ªï¼Ÿã€</li>
</ul>
<p>æ•´é«”å“è³ªï¼šã€Œæ©™è‰²è§’è‰²çš„å“ªå€‹æ‰‹å‹¢çœ‹èµ·ä¾†æ›´åƒäººé¡ï¼Ÿã€</p>
<p>é—œä¿‚ä¸€è‡´æ€§ï¼šã€Œå“ªä¸€å°çœ‹èµ·ä¾†æ›´åƒæ˜¯æœ‹å‹ï¼ˆæˆ–é™Œç”Ÿäººã€æˆ–å®¶åº­æˆå“¡ã€æˆ–ç´„æœƒå°è±¡ï¼‰ï¼Ÿã€</p>
<p>å€‹æ€§ä¸€è‡´æ€§ï¼šã€Œå“ªå€‹æ‰‹å‹¢æ›´èƒ½åæ˜ èªåŒæ€§ï¼ˆæˆ–ç›¡è²¬æ€§ã€æˆ–å¤–å‘æ€§ã€æˆ–ç¥ç¶“è³ªï¼‰çš„å€‹æ€§ç‰¹è³ªï¼Ÿã€</p>
<p>å°æ–¼æ¯å€‹å½±ç‰‡å°ï¼Œåƒèˆ‡è€…è¢«è¦æ±‚æä¾›ä»–å€‘çš„åå¥½å’Œä¿¡å¿ƒç¨‹åº¦ï¼ˆå¼·çƒˆæˆ–è¼•å¾®ï¼‰ã€‚æœ‰é—œå®Œæ•´å•å·å’Œå¯¦é©—ä¸­ä½¿ç”¨çš„ä»‹é¢è¨­è¨ˆï¼Œè«‹åƒé–±è£œå……ææ–™ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23165/x6.png" loading="lazy"></figure> åœ– 6ï¼šä½¿ç”¨è€…ç ”ç©¶ä¸­ä½¿ç”¨çš„ç¯„ä¾‹å½±ç‰‡å°ï¼Œç”¨æ–¼è©•ä¼°åƒèˆ‡è€…å°å°è©±æ‰‹å‹¢ç”Ÿæˆçš„åå¥½ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23165/x7.png" loading="lazy"></figure> åœ– 7ï¼šA/B ä¸»è§€è©•ä¼°ç™¾åˆ†æ¯”ï¼Œæ¯”è¼ƒæˆ‘å€‘çš„æ–¹æ³•èˆ‡ ConvoFusion [29] å’ŒçœŸå¯¦å‹•ä½œã€‚åƒèˆ‡è€…åå¥½æˆ‘å€‘ç”Ÿæˆçš„å‹•ä½œï¼Œå› ç‚ºå…¶å…·æœ‰æ›´è‡ªç„¶å’Œç¤¾äº¤æ„ŸçŸ¥çš„å°è©±è¡Œç‚ºã€‚</p>
<p>å¦‚åœ– 7 æ‰€ç¤ºï¼ŒDyaDiT ç›¸æ¯” ConvoFusion [29] é”åˆ°äº†æ˜é¡¯æ›´é«˜çš„ä½¿ç”¨è€…åå¥½åˆ†æ•¸ï¼Œæœ‰è¶£çš„æ˜¯ï¼Œç”šè‡³ç•¥å¾®è¶…è¶Šäº†ä¾†è‡ª Seamless Interaction è³‡æ–™é›†çš„çœŸå¯¦å‹•ä½œæ‰‹å‹¢ã€‚å…·é«”ä¾†èªªï¼Œåœ¨æ•´é«”æ‰‹å‹¢å“è³ªã€é—œä¿‚ä¸€è‡´æ€§å’Œå€‹æ€§ä¸€è‡´æ€§éƒ¨åˆ†ä¸­ï¼Œåˆ†åˆ¥æœ‰ 73.9%ã€69.8% å’Œ 66.7% çš„åƒèˆ‡è€…åå¥½ DyaDiTã€‚é€™äº›çµæœè¡¨æ˜ DyaDiT ä¸åƒ…èƒ½å¤ ç”Ÿæˆæ›´é«˜å“è³ªçš„æ‰‹å‹¢ï¼Œè€Œä¸”ç”Ÿæˆçš„å‹•ä½œåœ¨äºŒäººå°è©±è¨­ç½®ä¸­æ›´å…·ç¤¾äº¤é©ç•¶æ€§ã€‚æ­¤å¤–ï¼Œåœ¨å…©å€‹æ¯”è¼ƒè¨­ç½®ä¸­ï¼Œæˆ‘å€‘ç”Ÿæˆçš„æ‰‹å‹¢ç›¸æ¯”çœŸå¯¦å‹•ä½œåˆ†åˆ¥è¢«åå¥½ 1% å’Œ 1.7%ã€‚æˆ‘å€‘å‡è¨­é€™ä¸»è¦æ˜¯å› ç‚ºï¼ˆ1ï¼‰çœŸå¯¦å‹•ä½œè³‡æ–™æœ‰æ™‚åŒ…å«æŠ–å‹•å½å½±ï¼Œè€Œæ“´æ•£éç¨‹éš±å¼åœ°æ­£å‰‡åŒ–å‹•ä½œä¸¦ç”¢ç”Ÿæ›´å¹³æ»‘ã€æ™‚é–“ä¸Šæ›´ä¸€è‡´çš„æ‰‹å‹¢ï¼›ä»¥åŠï¼ˆ2ï¼‰ä»¥ç¤¾äº¤èƒŒæ™¯ä½œç‚ºæ¢ä»¶ä¿ƒä½¿æ¨¡å‹ç”Ÿæˆç¨å¾®æ›´å¯Œæœ‰è¡¨ç¾åŠ›æˆ–èª‡å¼µçš„å‹•ä½œï¼Œå„˜ç®¡é€™åœ¨çœŸå¯¦äº’å‹•ä¸­è¼ƒç‚ºç½•è¦‹ï¼Œä½†å‚¾å‘æ–¼å°æˆ‘å€‘çš„åƒèˆ‡è€…æ›´å…·è¦–è¦ºå¸å¼•åŠ›ã€‚æœ€å¾Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘å€‘æ²’æœ‰åœ¨å€‹æ€§ä¸€è‡´æ€§éƒ¨åˆ†å°‡æˆ‘å€‘ç”Ÿæˆçš„æ‰‹å‹¢èˆ‡çœŸå¯¦å‹•ä½œé€²è¡Œæ¯”è¼ƒã€‚é€™æ˜¯å› ç‚ºè³‡æ–™é›†ä¸­çš„å€‹æ€§è¨»è§£æ˜¯ä½œç‚ºé€£çºŒå€¼è€Œéé›¢æ•£é¡åˆ¥æä¾›çš„ï¼Œé€™ä½¿å¾—åƒèˆ‡è€…å¾ˆé›£å¯é åœ°åˆ¤æ–·ä¸€å€‹æ‰‹å‹¢æ˜¯å¦æ›´å¼·çƒˆåœ°ç¬¦åˆç‰¹å®šçš„å€‹æ€§ç‰¹è³ªã€‚</p>
<h2 id="6">6 çµè«–</h2>
<p>åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘æå‡º DyaDiTï¼Œä¸€å€‹å¤šæ¨¡æ…‹çš„äºŒäººäº’å‹•æ‰‹å‹¢ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨ç”¢ç”Ÿå…©å€‹èªªè©±è€…ä¹‹é–“ç¤¾äº¤ä¸€è‡´ä¸”è‡ªç„¶çš„å°è©±è¡Œç‚ºã€‚é€šéè¯åˆèª¿ç¯€äºŒäººäº’å‹•éŸ³é »ã€ç¤¾äº¤å±¬æ€§å’Œå¤¥ä¼´çš„å‹•ä½œï¼ŒDyaDiT æœ‰æ•ˆåœ°æ•æ‰äº†äºŒäººå°è©±ä¸­çš„äººéš›å‹•æ…‹ã€‚æˆ‘å€‘çš„æ­£äº¤åŒ–äº¤å‰æ³¨æ„åŠ›ï¼ˆORCAï¼‰æ¨¡çµ„æœ‰åŠ©æ–¼æ¾„æ¸…æ¯ä½èªªè©±è€…éŸ³é »çš„è²¢ç»ï¼Œè€Œé‹å‹•å­—å…¸é€éæä¾›é¢¨æ ¼æ„ŸçŸ¥çš„é‹å‹•å…ˆé©—ä¾†å¢å¼·è¡¨ç¾åŠ›çš„è±å¯Œæ€§ã€‚å®šé‡å’Œä¸»è§€è©•ä¼°éƒ½è­‰æ˜äº† DyaDiT åœ¨ç¾å¯¦æ€§ã€å¤šæ¨£æ€§å’Œç¤¾äº¤ä¸€è‡´æ€§æ–¹é¢ç›¸æ¯”ç¾æœ‰æ–¹æ³•é”åˆ°äº†å„ªè¶Šçš„æ•ˆèƒ½ã€‚æˆ‘å€‘ç›¸ä¿¡æœ¬ç ”ç©¶å‘ç¤¾äº¤å‹å–„çš„åˆæˆä»£ç†é‚å‡ºäº†é‡è¦çš„ä¸€æ­¥ï¼Œä¸¦ç‚ºæœªä¾†çš„ç ”ç©¶æ–¹å‘é–‹é—Šäº†é“è·¯ï¼Œä¾‹å¦‚ç¤¾äº¤æ„ŸçŸ¥çš„é›™ä»£ç†æ‰‹å‹¢ç”Ÿæˆã€‚</p>
<h2 id="7">7 é™åˆ¶èˆ‡æœªä¾†å·¥ä½œ</h2>
<p>ç›®å‰ï¼Œç”±æ–¼è³‡æ–™é›†çš„é™åˆ¶ï¼Œæˆ‘å€‘çš„æ¨¡å‹åƒ…ç”Ÿæˆä¸ŠåŠèº«æ‰‹å‹¢ã€‚ç„¶è€Œï¼Œæˆ‘å€‘ç›¸ä¿¡æå‡ºçš„æ¡†æ¶å¯ä»¥è‡ªç„¶åœ°æ“´å±•è‡³å…¨èº«æ‰‹å‹¢å’Œé¢éƒ¨è¡¨æƒ…ç”Ÿæˆã€‚æ­¤å¤–ï¼Œé›–ç„¶æˆ‘å€‘çš„æ¨¡å‹å¯ä»¥æ ¹æ“šçµ¦å®šçš„é—œä¿‚æˆ–å€‹æ€§èª¿ç¯€ä¾†ç”Ÿæˆç¤¾äº¤å‹å–„çš„æ‰‹å‹¢ï¼Œä½†æŸäº›å€‹æ€§ç·šç´¢å¯èƒ½ç„¡æ„ä¸­åŒ…å«åœ¨éŸ³é »è¼¸å…¥ä¸­ï¼Œå°è‡´èª¿ç¯€è¡çªä¸¦é™ä½ç”Ÿæˆé‹å‹•çš„å¤šæ¨£æ€§ã€‚ç‚ºäº†é€²ä¸€æ­¥æ”¹é€²å¯æ§æ€§å’Œç©©å®šå¤šæ¨£æ€§ï¼Œæˆ‘å€‘çš„æœªä¾†å·¥ä½œå°‡æ¢ç´¢éŸ³é »ä¸­æ€§åŒ–æŠ€è¡“ï¼Œä»¥å¯¦ç¾æ›´å¤šæ¨£åŒ–çš„æ‰‹å‹¢ç”Ÿæˆã€‚</p>
<h2 id="_1">åƒè€ƒæ–‡ç»</h2>
<ul>
<li>[1] V. Agrawal, A. Akinyemi, K. Alvero, M. Behrooz, and et al. (2025) Seamless interaction: dyadic audiovisual motion modeling and large-scale dataset . External Links: Link Cited by: Â§3.1 , Â§3 , 1st item , Â§4.1 , Â§9 , DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation .</li>
<li>[2] S. Alexanderson, R. Nagy, J. Beskow, and G. E. Henter (2023-07) Listen, denoise, action! audio-driven motion synthesis with diffusion models . ACM Trans. Graph. 42 ( 4 ). External Links: ISSN 0730-0301 , Link , Document Cited by: Â§2.3 , Â§4.1 .</li>
<li>[3] T. Ao, Z. Zhang, and L. Liu GestureDiffuCLIP: gesture diffusion model with clip latents . ACM Trans. Graph. . External Links: Document Cited by: Â§2.3 , Figure 4 , Figure 4 , Â§4.1 .</li>
<li>[4] A. Baevski, H. Zhou, A. Mohamed, and M. Auli (2020) Wav2vec 2.0: a framework for self-supervised learning of speech representations . In Proceedings of the 34th International Conference on Neural Information Processing Systems , NIPS '20 , Red Hook, NY, USA . External Links: ISBN 9781713829546 Cited by: Â§3.1 , Â§9 .</li>
<li>[5] B. Chen, Y. Li, Y. Zheng, Y. Ding, and K. Zhou (2025) Motion-example-controlled co-speech gesture generation leveraging large language models . In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers , SIGGRAPH Conference Papers '25 , New York, NY, USA . External Links: ISBN 9798400715402 , Link , Document Cited by: Â§2.1 .</li>
<li>[6] X. Chen, B. Jiang, W. Liu, Z. Huang, B. Fu, T. Chen, and G. Yu (2023) Executing your commands via motion diffusion in latent space . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18000â€“18010 . Cited by: Â§2.3 .</li>
<li>[7] K. Chhatre, R. DanÄ›Äek, N. Athanasiou, G. Becherini, C. Peters, M. J. Black, and T. Bolkart (2024-06) AMUSE: emotional speech-driven 3D body animation via disentangled latent diffusion . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 1942â€“1953 . External Links: Link Cited by: Â§2.3 .</li>
<li>[8] M. Dunne and S. H. Ng (1994) Simultaneous speech in small group conversation: all-together-now and one-at-a-time? . Journal of Language and Social Psychology 13 ( 1 ), pp. 45â€“71 . Cited by: Â§1 .</li>
<li>[9] F. Favali, V. Schmuck, V. Villani, and O. Celiktutan (2024) TAG2G: a diffusion-based approach to interlocutor-aware co-speech gesture generation . Electronics 13 ( 17 ). External Links: Link , ISSN 2079-9292 , Document Cited by: Â§2.2 .</li>
<li>[10] C. Fu, Y. Wang, J. Zhang, Z. Jiang, X. Mao, J. Wu, W. Cao, C. Wang, Y. Ge, and Y. Liu (2024) MambaGesture: enhancing co-speech gesture generation with mamba and disentangled multi-modality fusion . In Proceedings of the 32nd ACM International Conference on Multimedia , MM '24 , New York, NY, USA , pp. 10794â€“10803 . External Links: ISBN 9798400706868 , Link , Document Cited by: Â§1 .</li>
<li>[11] S. Ginosar, A. Bar, G. Kohavi, C. Chan, A. Owens, and J. Malik (2019) Learning individual styles of conversational gesture . In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Vol. , pp. 3492â€“3501 . External Links: Document Cited by: Â§2.1 .</li>
<li>[12] A. Grattafiori, A. Dubey, and A. J. et al. (2024) The llama 3 herd of models . External Links: 2407.21783 , Link Cited by: Â§1 .</li>
<li>[13] J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probabilistic models . arXiv preprint arxiv:2006.11239 . Cited by: Â§3.2 .</li>
<li>[14] J. Ho and T. Salimans (2021) Classifier-free diffusion guidance . In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications , External Links: Link Cited by: Â§3.4 .</li>
<li>[15] C. R. Jones and B. K. Bergen (2025) Large language models pass the turing test . External Links: 2503.23674 , Link Cited by: Â§1 .</li>
<li>[16] S. Jung and T. Kim (2025) DiffListener: discrete diffusion model for listener generation . In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Vol. , pp. 1â€“5 . External Links: Document Cited by: Â§2.2 .</li>
<li>[17] R. Khirodkar, J. Song, J. Cao, Z. Luo, and K. Kitani (2024) Harmony4D: a video dataset for in-the-wild close human interactions . In Advances in Neural Information Processing Systems , A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.) , Vol. 37 , pp. 107270â€“107285 . External Links: Document , Link Cited by: Â§9 .</li>
<li>[18] D. Lee, C. Kim, S. Kim, M. Cho, and W. Han (2022) Autoregressive image generation using residual quantization . External Links: 2203.01941 , Link Cited by: Â§3.5 , Â§9 .</li>
<li>[19] Y. Lee, S. Choi, B. Kim, Z. Wang, and S. Watanabe (2024) Boosting unknown-number speaker separation with transformer decoder-based attractor . In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 446â€“450 . Cited by: Â§1 .</li>
<li>[20] J. Li, D. Kang, W. Pei, X. Zhe, Y. Zhang, L. Bao, and Z. He (2024-08) Audio2Gestures: generating diverse gestures from audio . IEEE Transactions on Visualization and Computer Graphics 30 ( 8 ), pp. 4752â€“4766 . External Links: ISSN 1077-2626 , Link , Document Cited by: Â§4.1 , Â§4.3 , Table 1 .</li>
<li>[21] H. Liu, N. Iwamoto, Z. Zhu, Z. Li, Y. Zhou, E. Bozkurt, and B. Zheng (2022) DisCo: disentangled implicit content and rhythm learning for diverse co-speech gestures synthesis . In Proceedings of the 30th ACM International Conference on Multimedia , MM '22 , New York, NY, USA , pp. 3764â€“3773 . External Links: ISBN 9781450392037 , Link , Document Cited by: Â§2.1 .</li>
<li>[22] H. Liu, Z. Zhu, G. Becherini, Y. Peng, M. Su, Y. Zhou, X. Zhe, N. Iwamoto, B. Zheng, and M. J. Black (2024-06) EMAGE: towards unified holistic co-speech gesture generation via expressive masked audio gesture modeling . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 1144â€“1154 . Cited by: Â§1 , Â§2.1 .</li>
<li>[23] H. Liu, Z. Zhu, N. Iwamoto, Y. Peng, Z. Li, Y. Zhou, E. Bozkurt, and B. Zheng (2022) BEAT: a large-scale semantic and emotional multi-modal dataset for conversational gestures synthesis . In Computer Vision â€“ ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part VII , Berlin, Heidelberg , pp. 612â€“630 . External Links: ISBN 978-3-031-20070-0 , Link , Document Cited by: Â§1 , Â§2.1 .</li>
<li>[24] X. Liu, Q. Wu, H. Zhou, Y. Xu, R. Qian, X. Lin, X. Zhou, W. Wu, B. Dai, and B. Zhou (2022) Learning hierarchical cross-modal association for co-speech gesture generation . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 10462â€“10472 . Cited by: Â§2.1 .</li>
<li>[25] C. Luo, S. Song, W. Xie, M. Spitale, Z. Ge, L. Shen, and H. Gunes (2025) ReactFace: online multiple appropriate facial reaction generation in dyadic interactions . IEEE Transactions on Visualization and Computer Graphics 31 ( 9 ), pp. 6190â€“6207 . External Links: Document Cited by: Â§2.2 .</li>
<li>[26] C. Luo, S. Song, S. Yan, Z. Yu, and Z. Ge (2025) ReactDiff: fundamental multiple appropriate facial reaction diffusion model . In Proceedings of the 33rd ACM International Conference on Multimedia , MM '25 , New York, NY, USA , pp. 5607â€“5616 . External Links: ISBN 9798400720352 , Link , Document Cited by: Â§1 , Â§2.2 .</li>
<li>[27] C. Luo, J. Wang, B. Li, S. Song, and B. Ghanem (2025) OmniResponse: online multimodal conversational response generation in dyadic interactions . arXiv preprint arXiv:2505.21724 . Cited by: Â§1 , Â§2.2 .</li>
<li>[28] A. S. Meyer (2023) Timing in conversation . Journal of Cognition 6 ( 1 ), pp. 20 . Cited by: Â§1 .</li>
<li>[29] M. H. Mughal, R. Dabral, I. Habibie, L. Donatelli, M. Habermann, and C. Theobalt (2024-06) ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Vol. , Los Alamitos, CA, USA , pp. 1388â€“1398 . External Links: ISSN , Document , Link Cited by: Â§1 , Â§2.2 , Figure 4 , Figure 4 , 1st item , Â§4.3 , Table 1 , Â§4 , Figure 7 , Figure 7 , Â§5 , Â§5 .</li>
<li>[30] J. Neri and S. Braun (2023) Towards real-time single-channel speech separation in noisy and reverberant environments . External Links: 2303.07569 , Link Cited by: Â§1 .</li>
<li>[31] E. Ng, H. Joo, L. Hu, H. Li, T. Darrell, A. Kanazawa, and S. Ginosar (2022-06) Learning to listen: modeling non-deterministic dyadic facial motion . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 20395â€“20405 . Cited by: Â§2.2 .</li>
<li>[32] E. Ng, J. Romero, T. Bagautdinov, S. Bai, T. Darrell, A. Kanazawa, and A. Richard (2024-06) From audio to photoreal embodiment: synthesizing humans in conversations . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 1001â€“1010 . Cited by: Â§1 , Â§2.2 , 2nd item , Â§4.1 , Â§4 .</li>
<li>[33] OpenAI, J. Achiam, and S. A. et al. (2024) GPT-4 technical report . External Links: 2303.08774 , Link Cited by: Â§1 .</li>
<li>[34] K. Pang, D. Qin, Y. Fan, J. Habekost, T. Shiratori, J. Yamagishi, and T. Komura (2023-07) Bodyformer: semantics-guided 3d body gesture synthesis with transformer . ACM Trans. Graph. 42 ( 4 ). External Links: ISSN 0730-0301 , Link , Document Cited by: Â§2.1 .</li>
<li>[35] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. Courville (2018) FiLM: visual reasoning with a general conditioning layer . In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence , AAAI'18/IAAI'18/EAAI'18 . External Links: ISBN 978-1-57735-800-8 Cited by: Â§3.2 , Â§9 .</li>
<li>[36] X. Qi, C. Liu, L. Li, J. Hou, H. Xin, and X. Yu (2024-01) EmotionGesture: audio-driven diverse emotional co-speech 3d gesture generation . Trans. Multi. 26 , pp. 10420â€“10430 . External Links: ISSN 1520-9210 , Link , Document Cited by: Â§1 .</li>
<li>[37] Y. Shafir, G. Tevet, R. Kapon, and A. H. Bermano (2024) Human motion diffusion as a generative prior . In The Twelfth International Conference on Learning Representations , Cited by: Â§2.3 .</li>
<li>[38] S. Song, M. Spitale, X. Kong, H. Zhu, C. Luo, C. Palmero, G. Barquero, S. Escalera, M. Valstar, M. Daoudi, et al. (2025) REACT 2025: the third multiple appropriate facial reaction generation challenge . In Proceedings of the ACM International Conference on Multimedia , Cited by: Â§2.2 .</li>
<li>[39] H. Taherian and D. Wang (2024) Multi-channel conversational speaker separation via neural diarization . IEEE/ACM Transactions on Audio, Speech, and Language Processing 32 , pp. 2467â€“2476 . Cited by: Â§1 .</li>
<li>[40] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or (2022) Motionclip: exposing human motion generation to clip space . In Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XXII , pp. 358â€“374 . Cited by: Â§2.3 .</li>
<li>[41] G. Tevet, S. Raab, B. Gordon, Y. Shafir, D. Cohen-or, and A. H. Bermano (2023) Human motion diffusion model . In The Eleventh International Conference on Learning Representations , External Links: Link Cited by: Â§2.3 .</li>
<li>[42] M. Tran, D. Chang, M. Siniukov, and M. Soleymani (2024) Dyadic interaction modeling for social behavior generation . arXiv preprint arXiv:2403.09069 . Cited by: Â§1 .</li>
<li>[43] Y. Wang, D. Yang, F. Bremond, and A. Dantcheva (2024) LIA: latent image animator . IEEE Transactions on Pattern Analysis and Machine Intelligence , pp. 1â€“16 . Cited by: Â§3.4 .</li>
<li>[44] S. Yang, Z. Wang, Z. Wu, M. Li, Z. Zhang, Q. Huang, L. Hao, S. Xu, X. Wu, C. Yang, and Z. Dai (2023) UnifiedGesture: a unified gesture synthesis model for multiple skeletons . In Proceedings of the 31st ACM International Conference on Multimedia , MM '23 , New York, NY, USA , pp. 1033â€“1044 . External Links: ISBN 9798400701085 , Link , Document Cited by: Â§2.3 .</li>
<li>[45] S. Yang, Z. Xu, H. Xue, Y. Cheng, S. Huang, M. Gong, and Z. Wu (2024) Freetalker: controllable speech and text-driven gesture generation based on diffusion models for enhanced speaker naturalness . In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Cited by: Â§2.1 .</li>
<li>[46] S. Yang, H. Xue, Z. Zhang, M. Li, Z. Wu, X. Wu, S. Xu, and Z. Dai (2023) The diffusestylegesture+ entry to the genea challenge 2023 . In Proceedings of the 25th International Conference on Multimodal Interaction , ICMI '23 , New York, NY, USA , pp. 779â€“785 . External Links: ISBN 9798400700552 , Link , Document Cited by: Â§2.3 .</li>
<li>[47] H. Yi, H. Liang, Y. Liu, Q. Cao, Y. Wen, T. Bolkart, D. Tao, and M. J. Black (2023-06) Generating holistic 3d human motion from speech . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 469â€“480 . Cited by: Â§2.1 .</li>
<li>[48] W. Yin, Z. Cai, R. Wang, A. Zeng, C. Wei, Q. Sun, H. Mei, Y. Wang, H. E. Pang, M. Zhang, L. Zhang, C. C. Loy, A. Yamashita, L. Yang, and Z. Liu (2025) SMPLest-x: ultimate scaling for expressive human pose and shape estimation . arXiv preprint arXiv:2501.09782 . Cited by: Â§9 .</li>
<li>[49] Y. Yoon, B. Cha, J. Lee, M. Jang, J. Lee, J. Kim, and G. Lee (2020-11) Speech gesture generation from the trimodal context of text, audio, and speaker identity . ACM Trans. Graph. 39 ( 6 ). External Links: ISSN 0730-0301 , Link , Document Cited by: Â§2.1 .</li>
<li>[50] J. Zhang, L. Liang, Z. Xue, and Y. Liu (2020-05) APB2FACE: audio-guided face reenactment with auxiliary pose and blink signals . pp. 4402â€“4406 . External Links: Document Cited by: Â§2.2 .</li>
<li>[51] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu (2024-06) MotionDiffuse: text-driven human motion generation with diffusion model . IEEE Trans. Pattern Anal. Mach. Intell. 46 ( 6 ), pp. 4115â€“4128 . External Links: ISSN 0162-8828 , Link , Document Cited by: Â§2.3 .</li>
<li>[52] M. Zhang, H. Li, Z. Cai, J. Ren, L. Yang, and Z. Liu (2023) FineMoGen: fine-grained spatio-temporal motion generation and editing . NeurIPS . Cited by: Â§2.3 .</li>
<li>[53] Y. Zhi, X. Cun, X. Chen, X. Shen, W. Guo, S. Huang, and S. Gao (2023-10) LivelySpeaker: towards semantic-aware co-speech gesture generation . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 20807â€“20817 . Cited by: Â§2.3 .</li>
<li>[54] Y. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li (2018) On the continuity of rotation representations in neural networks . CoRR abs/1812.07035 . External Links: Link , 1812.07035 Cited by: Â§3.1 , Â§9 .</li>
<li>[55] L. Zhu, X. Liu, X. Liu, R. Qian, Z. Liu, and L. Yu (2023) Taming diffusion models for audio-driven co-speech gesture generation . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 10544â€“10553 . Cited by: Â§2.3 .</li>
</ul>
<h2 id="_2">è£œå……ææ–™</h2>
<p>æœ¬è£œå……ææ–™åŒ…å«ä»¥ä¸‹å„ç¯€ï¼š</p>
<ul>
<li>â€¢ 1. ç”Ÿæˆçµæœçš„é—œä¿‚èˆ‡å€‹æ€§èšé¡</li>
<li>â€¢ 2. DyaDiT çš„å¯¦ä½œç´°ç¯€</li>
<li>
<p>â€¢ 3. A/B æ¸¬è©¦å•å·çš„è©³ç´°è³‡è¨Š</p>
</li>
<li>
<p>ç”Ÿæˆçµæœçš„é—œä¿‚èˆ‡å€‹æ€§èšé¡</p>
</li>
<li>
<p>DyaDiT çš„å¯¦ä½œç´°ç¯€</p>
</li>
<li>
<p>A/B æ¸¬è©¦å•å·çš„è©³ç´°è³‡è¨Š</p>
</li>
</ul>
<p>é™¤äº†æœ¬è£œå…… supplementary.pdf å¤–ï¼Œæˆ‘å€‘é‚„æä¾›äº† narration_video.mp4ï¼Œå…¶ä¸­æä¾›äº†è«–æ–‡çš„ç°¡è¦æ¦‚è¿°ä»¥åŠå¤šå€‹å®šæ€§æ‰‹å‹¢ç”Ÿæˆç¯„ä¾‹ã€‚</p>
<p>æˆ‘å€‘é€²ä¸€æ­¥åœ¨ dyadit_code.zip ä¸­æä¾›äº†æˆ‘å€‘çš„å¯¦ä½œï¼›è«‹åƒè€ƒæ‰€å«çš„ README.md ä»¥å–å¾—åŸ·è¡Œç¨‹å¼ç¢¼çš„èªªæ˜ã€‚</p>
<p>è¨“ç·´å¥½çš„æ¨¡å‹å°‡åœ¨æ¥å—å¾Œç™¼ä½ˆã€‚</p>
<h2 id="8">8 ç”Ÿæˆæ‰‹å‹¢çš„èšé¡åˆ†æ</h2>
<p>åœ¨ä¸»è«–æ–‡ä¸­ï¼Œæˆ‘å€‘é€²è¡Œäº†ä¸€é … A/B æ¸¬è©¦ä¾†è©•ä¼°ç”Ÿæˆæ‰‹å‹¢çš„é—œä¿‚å’Œå€‹æ€§ä¸€è‡´æ€§ã€‚ç‚ºäº†é€²ä¸€æ­¥è©•ä¼° DyaDiT ä¸­æ¢ä»¶è¼¸å…¥çš„å¯æ§æ€§ï¼Œæˆ‘å€‘å°ç”Ÿæˆçš„å‹•ä½œåµŒå…¥é€²è¡Œäº† t-SNE èšé¡åˆ†æã€‚</p>
<p>åœ– 8 å±•ç¤ºäº†åœ¨ä¸åŒæ¢ä»¶ä¿¡è™Ÿä¸‹ç”Ÿæˆæ‰‹å‹¢çš„ t-SNE åµŒå…¥ã€‚åœ¨å·¦å´ï¼Œæˆ‘å€‘åœ¨å›ºå®šå€‹æ€§åˆ†æ•¸çš„æƒ…æ³ä¸‹ï¼Œä½¿ç”¨å„ç¨®é—œä¿‚é¡å‹ç”Ÿæˆæ‰‹å‹¢ã€‚åœ¨å³å´ï¼Œæˆ‘å€‘å°‡é€£çºŒçš„å€‹æ€§åˆ†æ•¸ç‰¹å¾µé›¢æ•£åŒ–ç‚ºäº”å€‹ã€Œone-hotã€å‘é‡ï¼Œä¸¦ç‚ºæ¯å€‹å‘é‡ç”Ÿæˆæ‰‹å‹¢ä»¥æª¢é©—å€‹æ€§å¯æ§æ€§ã€‚</p>
<p>æˆ‘å€‘è§€å¯Ÿåˆ°å€‹æ€§èšé¡å½¢æˆäº†æ¸…æ™°å¯åˆ†çš„ç¾¤çµ„ï¼Œè¡¨æ˜ DyaDiT æœ‰æ•ˆåœ°æ•æ‰äº†èˆ‡ä¸åŒå€‹æ€§ç‰¹å¾µç›¸é—œçš„å…¨å±€è¡Œç‚ºå‚¾å‘ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé—œä¿‚èšé¡çš„åˆ†é›¢ç¨‹åº¦ä¸å¤ æ˜é¡¯ã€‚æˆ‘å€‘èªç‚ºé€™èˆ‡äºŒå…ƒå°è©±æ‰‹å‹¢çš„æ€§è³ªç›¸ç¬¦ï¼šFriendã€Family å’Œ Dating ä¹‹é–“çš„é¢¨æ ¼å­˜åœ¨æŸäº›é‡ç–Šã€‚å› æ­¤ï¼Œç”Ÿæˆçš„æ‰‹å‹¢åœ¨é€™äº›é¡åˆ¥ä¸­ä¹Ÿè¡¨ç¾ç‚ºæ›´é€£çºŒçš„æµå½¢ï¼Œè€Œä¸æ˜¯éŠ³åˆ©çš„èšé¡é‚Šç•Œã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23165/x8.png" loading="lazy"></figure> åœ– 8ï¼šé—œä¿‚ï¼ˆå·¦ï¼‰å’Œå€‹æ€§åˆ†æ•¸ï¼ˆå³ï¼‰çš„ t-SNE èšé¡çµæœã€‚</p>
<h2 id="9">9 å¯¦ç¾ç´°ç¯€</h2>
<h4 id="diffusion-transformer">Diffusion Transformer</h4>
<p>è¼¸å…¥å§¿æ…‹åºåˆ—è¢«ç·¨ç¢¼åˆ°èˆ‡ VQ-VAE è¡¨ç¤ºç›¸å°æ‡‰çš„æ½›åœ¨ç©ºé–“ä¸­ï¼Œç”¢ç”Ÿ â„^{64} ä¸­çš„æ½›åœ¨åµŒå…¥ï¼Œè€Œä¸æ˜¯åŸå§‹çš„ â„^{43Ã—6} 6D æ—‹è½‰çŸ©é™£ [54] é—œç¯€è¡¨ç¤ºã€‚
ä¸€å€‹ç·šæ€§å±¤å°‡å¸¶æœ‰å™ªè²çš„æ½›åœ¨è¼¸å…¥å¾ â„^{64} æŠ•å½±åˆ° â„^{512} ä¸­çš„éš±è—ç©ºé–“ï¼Œéš¨å¾Œåœ¨è¼¸å‡ºè™•é€²è¡Œå°ç¨±æŠ•å½±å› â„^{64}ã€‚</p>
<p>è©²æ¨¡å‹åŒ…å« 4 å€‹ Transformer å¡Šï¼Œæ¯å€‹å‡é…å‚™ 4 é ­å¤šé ­æ³¨æ„åŠ›ï¼ˆâ„^{128}ï¼‰å’Œä¸€å€‹ â„^{2048} å‰é¥‹ç¶²è·¯ã€‚
æˆ‘å€‘æ¡ç”¨ â„^{512} æ­£å¼¦æ™‚é–“åµŒå…¥ï¼Œé€šé FiLM [35] èª¿åˆ¶æ³¨å…¥åˆ°æ¯å€‹å¡Šä¸­ã€‚</p>
<p>å…©å€‹ç¨ç«‹çš„ Wav2Vec2 [4] è™•ç†å™¨å¾å…©ä½èªªè©±è€…çš„å°è©±èªéŸ³ä¸­æå–é«˜éšéŸ³è¨Šç‰¹å¾µã€‚
é€™äº›ç”¢ç”Ÿç‰¹å¾µåºåˆ—è¨˜ç‚º $a_{\text{self}},a_{\text{other}}\in\mathbb{R}^{T\times 768}$ï¼Œå…¶ä¸­ T è¡¨ç¤ºéŸ³è¨Šå¹€çš„æ•¸é‡ã€‚
æ¯å€‹ç‰¹å¾µåºåˆ—é€šéç·šæ€§è½‰æ›æŠ•å½±åˆ° â„^{TÃ—512}ï¼Œéš¨å¾Œé€²è¡Œ LayerNorm å’Œé–€æ§èåˆæ©Ÿåˆ¶ä»¥çµåˆè‡ªæˆ‘å’Œå°æ–¹èªªè©±è€…ç·šç´¢ã€‚</p>
<p>ä¸€å€‹å¯å­¸ç¿’çš„å‹•ä½œåº«åŒ…å« 1000 å€‹åŸå‹å‘é‡ï¼Œä½æ–¼ â„^{512} ä¸­ï¼Œé€šéäº¤å‰æ³¨æ„åŠ›æä¾›ä¸Šä¸‹æ–‡å…ˆé©—ã€‚
é¡ä¼¼æ–¼æ™‚é–“åµŒå…¥ï¼Œé—œä¿‚å’Œå€‹æ€§åµŒå…¥è¢«æŠ•å½±åˆ° â„^{512}ã€‚
é€™äº›å‘é‡é€šé FiLM å¼è‡ªé©æ‡‰ç¸®æ”¾æ³¨å…¥åˆ° DiT å¡Šä¸­ã€‚
æ‰€æœ‰ä¸Šä¸‹æ–‡ç·šç´¢è¢«é€£æ¥æˆ â„^{512} ä¸­çš„çµ±ä¸€åºåˆ—ï¼Œä¸¦é€šéäº¤å‰æ³¨æ„åŠ›æ³¨å…¥åˆ°æ¯å€‹ DiT å¡Šä¸­ã€‚</p>
<h4 id="vq-vae">å‹•ä½œåˆ†è©å™¨ï¼ˆVQ-VAEï¼‰</h4>
<p>æˆ‘å€‘å¯¦ç¾ä¸€å€‹æ™‚åº VQ-VAE [18] ä»¥åœ¨æ“´æ•£ä¹‹å‰é›¢æ•£åŒ–å§¿æ…‹åºåˆ—ã€‚
çµ¦å®šé—œç¯€ç‰¹å¾µçš„è¼¸å…¥åºåˆ— $X\in\mathbb{R}^{T\times 6\times 43}$ï¼Œç·¨ç¢¼å™¨æ˜¯ç”±ä¸‰å€‹ Conv1d å±¤çµ„æˆçš„ 1D CNNï¼Œåœ¨å‰å…©å±¤ä¹‹å¾Œæ‡‰ç”¨ LeakyReLUï¼Œæ•´é«”æ™‚åºä¸‹æ¡æ¨£å› å­ç‚º 4ï¼Œç”¢ç”Ÿ â„^{(T/4)Ã—64} ä¸­çš„æ½›åœ¨åºåˆ—ã€‚
è©²é€£çºŒæ½›åœ¨ç©ºé–“ç”±æ·±åº¦ç‚º 4 çš„æ®˜å·®å‘é‡é‡åŒ–å™¨é€²è¡Œé‡åŒ–ï¼Œæ¯å€‹é‡åŒ–å™¨å‡é…å‚™ 512 é …ç¢¼æœ¬ï¼Œå°‡æ¯å€‹æ™‚é–“æ­¥æ˜ å°„åˆ°ä¸€å †é›¢æ•£ç¢¼ç´¢å¼•ã€‚
è§£ç¢¼å™¨æ˜¯ä¸€å€‹ 1D CNNï¼ŒåŒ…å«åˆå§‹ Conv1d-LeakyReLU å±¤ã€å…©å€‹ä¸Šæ¡æ¨£å¡Šï¼ˆç·šæ€§æ’å€¼ï¼Œéš¨å¾Œæ˜¯ Conv1d å’Œ LeakyReLUï¼‰èˆ‡é¡å¤–çš„ Conv1d-LeakyReLU ç´°åŒ–å±¤äº¤éŒ¯ï¼Œä»¥åŠæœ€çµ‚ Conv1d æŠ•å½±ï¼Œå¯¦ç¾æ•´é«”æ™‚åºä¸Šæ¡æ¨£å› å­ 4 ä¸¦æ¢å¾©åŸå§‹æ™‚åºè§£æåº¦ä»¥é‡å»º â„^{TÃ—6Ã—43} ä¸­çš„å§¿æ…‹ã€‚
DiT å»å™ªå™¨ä½¿ç”¨çš„æœ€çµ‚æ½›åœ¨è¡¨ç¤ºæ˜¯å¾é‡åŒ–ç¢¼ç²å¾—çš„ï¼Œæ¯ $4\times T$ å¹€åœ¨ â„^{64} ä¸­æ˜¯ç·Šæ¹Šçš„ 64 ç¶­åµŒå…¥ã€‚</p>
<h4 id="_3">ç„¡ç¸«äº’å‹•è³‡æ–™é›†</h4>
<p>æˆ‘å€‘åœ¨ Seamless Interaction è³‡æ–™é›† [1] çš„ä¸€éƒ¨åˆ†ä¸Šé€²è¡Œå¯¦é©—ã€‚ç‰¹åˆ¥åœ°ï¼Œæˆ‘å€‘æ¡ç”¨è©²è³‡æ–™é›†çš„è‡ªç„¶ä¸»ç¾©åŠƒåˆ†ã€‚å°æ–¼è¨“ç·´ï¼Œæˆ‘å€‘åˆ©ç”¨å‰ 10 å€‹å®˜æ–¹è¨“ç·´æª”æ¡ˆï¼ˆä»¥ zip æ–‡ä»¶å½¢å¼æä¾›ï¼‰ï¼ŒåŒ…å«ç´„ 182 å°æ™‚çš„è‡ªç„¶ä¸»ç¾©äº’å‹•å’Œ 3000 å°é…å°å‹•ä½œ-éŸ³è¨Šæ¨£æœ¬ã€‚å°æ–¼æ¸¬è©¦ï¼Œæˆ‘å€‘é¸æ“‡å®˜æ–¹æ¸¬è©¦åŠƒåˆ†ä¸­çš„ç¬¬ä¸€å€‹æª”æ¡ˆä»¥ç¢ºä¿ä¸€è‡´çš„è©•ä¼°è¨­ç½®ã€‚</p>
<p>æˆ‘å€‘è§€å¯Ÿåˆ°è³‡æ–™é›†ä¸­æä¾›çš„ SMPL-H åƒæ•¸åœ¨ä¸‹èº«ä¼°è¨ˆä¸­è¡¨ç¾å‡ºé¡¯è‘—çš„ä¸æº–ç¢ºæ€§ï¼Œå¯èƒ½æ˜¯ç”±æ–¼è³‡æ–™æ¡é›†éç¨‹ä¸­ç›¸æ©Ÿè¦–è§’æœ‰é™å’Œèº«é«”é®æ“‹æ‰€è‡´ã€‚ç‚ºäº†é¿å…å°‡äººå·¥è£½å“å¼•å…¥æˆ‘å€‘çš„å‹•ä½œå»ºæ¨¡ä¸­ï¼Œæˆ‘å€‘ä¸Ÿæ£„ä¸‹èº«é—œç¯€ï¼Œåƒ…ä¿ç•™åŒ…å« 43 å€‹é—œç¯€ï¼ˆåŒ…æ‹¬æ‰‹æŒ‡ï¼‰çš„ä¸Šèº«ã€‚å°æ–¼è¦–è¦ºåŒ–ï¼Œæ‰€æœ‰æœªä½¿ç”¨çš„é—œç¯€ä»¥åŠå…¨å±€æ–¹å‘å’Œæ ¹éƒ¨å¹³ç§»å‡è¨­ç½®ç‚ºé›¶ã€‚</p>
<p>é™¤äº†å§¿æ…‹è³‡æ–™å¤–ï¼Œè³‡æ–™é›†é‚„åŒ…å«é«˜éšè¨»é‡‹ï¼Œä¾‹å¦‚é—œä¿‚å’Œå€‹æ€§è©•åˆ†ã€‚é›–ç„¶è³‡æ–™é›†ç‚ºç¤¾äº¤å‹•æ…‹æä¾›äº†äººéš›å‚³æ’­å‹•æ…‹ï¼ˆIPCï¼‰æ¨™ç±¤ï¼Œä½†æˆ‘å€‘ç™¼ç¾è¨»é‡‹éæ–¼å˜ˆé›œä¸”æ¨¡ç³Šï¼Œä¸æ¸…æ¥šå®ƒå€‘é©ç”¨æ–¼å“ªå€‹èªªè©±è€…ã€‚å› æ­¤ï¼Œæˆ‘å€‘åœ¨ç•¶å‰ç ”ç©¶ä¸­ä¸æ¡ç”¨ IPC æ¨™ç±¤ç›£ç£ï¼Œè€Œæ˜¯å°ˆæ³¨æ–¼æ›´æ¸…æ½”çš„é—œä¿‚å’Œå€‹æ€§ç·šç´¢ã€‚æˆ‘å€‘æ³¨æ„åˆ°ï¼Œä¸€æ—¦æœªä¾†è³‡æ–™é›†ç‰ˆæœ¬ä¸­ IPC è¨»é‡‹å¾—åˆ°æ”¹é€²ï¼Œæˆ‘å€‘è¨ˆç•«ä½¿ç”¨ IPC æ„ŸçŸ¥æ¢ä»¶æ¨¡çµ„æ“´å±•æˆ‘å€‘çš„æ¡†æ¶ï¼Œä»¥é€²ä¸€æ­¥æ•æ‰äºŒäººäº’å‹•æ‰‹å‹¢ä¸­çš„äº¤æµæ„åœ–ã€‚</p>
<p>æœªä¾†ï¼Œæˆ‘å€‘è¨ˆç•«ä½¿ç”¨å…ˆé€²çš„äººé«”å§¿æ…‹ä¼°è¨ˆå·¥å…·ï¼ˆä¾‹å¦‚ SMPLest-x [48]ã€Harmony4D [17] æˆ–æœ€è¿‘çš„æœ€å…ˆé€²æ¨¡å‹ï¼‰é‡æ–°è¨»é‡‹è¦–è¨Šè³‡æ–™ï¼Œç›®çš„æ˜¯ç²å¾—æ›´å¯é çš„å…¨èº«å‹•ä½œç›£ç£ã€‚</p>
<h2 id="10">10 å•å·</h2>
<p>æˆ‘å€‘æä¾›äº†åœ¨ä½¿ç”¨è€…ç ”ç©¶ä¸­ä½¿ç”¨çš„ A/B æ¸¬è©¦å•å·çš„é‡æ§‹ç‰ˆæœ¬ã€‚
è‹¥è¦æŸ¥çœ‹å•å·ï¼Œè«‹å…ˆè§£å£“ç¸®å•å·è³‡æ–™å¤¾å…§çš„ questionnaire_video.zip æª”æ¡ˆã€‚è§£å£“ç¸®å¾Œï¼Œåœ¨ä»»ä½•ç¾ä»£ç¶²é ç€è¦½å™¨ä¸­é–‹å•Ÿ questionnaire.htmlã€‚</p>
<p>åŸå§‹å•å·æ˜¯é€é Google Forms é€²è¡Œçš„ï¼ˆè¦‹åœ– 9ï¼‰ã€‚ç¸½å…±åŒ…å« 28 Ã— 2 å€‹å•é¡Œï¼Œå…¶ä¸­åŒ…æ‹¬ 10 å€‹é—œæ–¼æ•´é«”æ‰‹å‹¢å“è³ªçš„å•é¡Œã€
8 å€‹é—œæ–¼é—œä¿‚ä¸€è‡´æ€§çš„å•é¡Œï¼Œä»¥åŠ 10 å€‹é—œæ–¼äººæ ¼ä¸€è‡´æ€§çš„å•é¡Œã€‚
æ¯å€‹å•é¡Œå‘ˆç¾åœ¨å…©ç¨®è¨­å®šä¸‹é€²è¡Œæ¯”è¼ƒçš„é…å°æ‰‹å‹¢å½±ç‰‡ï¼šDyaDiT vs. ConvoFusion å’Œ DyaDiT vs. Ground Truthã€‚</p>
<p>ç‚ºäº†ç²å¾—æº–ç¢ºçš„è§€çœ‹é«”é©—ï¼Œè«‹ä½©æˆ´è€³æ©Ÿã€‚
å·¦éŸ³é »é€šé“å°æ‡‰æ–¼å°è©±å¤¥ä¼´çš„èªéŸ³ï¼Œè€Œå³éŸ³é »é€šé“
å°æ‡‰æ–¼ç›®æ¨™èªªè©±è€…çš„èªéŸ³ã€‚</p>
<p>é‡æ§‹çš„ä»‹é¢å…è¨±å¯©æŸ¥è€…ç€è¦½æ‰€æœ‰å•é¡Œä¸¦æ’­æ”¾
ç›¸æ‡‰çš„å½±ç‰‡ï¼Œä»¥é«”é©—èˆ‡æˆ‘å€‘çš„åƒèˆ‡è€…ç›¸åŒçš„è©•ä¼°éç¨‹ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23165/x9.jpg" loading="lazy"></figure> åœ– 9ï¼šGoogle Form ä¸­çš„å•å·ç¯„ä¾‹</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>