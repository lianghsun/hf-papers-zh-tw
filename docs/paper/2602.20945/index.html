<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>é«˜æ•ˆæ¨ç†çš„è—è¡“ï¼šæ•¸æ“šã€çå‹µå’Œå„ªåŒ– â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">é«˜æ•ˆæ¨ç†çš„è—è¡“ï¼šæ•¸æ“šã€çå‹µå’Œå„ªåŒ–</h1>
  
  <div class="en-title">The Art of Efficient Reasoning: Data, Reward, and Optimization</div>
  

  <div class="paper-meta">
    
    <span>Taiqiang Wu, Zenan Zu, Bo Zhou, Ngai Wong</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.20945" target="_blank">2602.20945</a></span>
    
    <span style="color:var(--text-muted);font-size:0.8rem;">
      ä¾†æºï¼šPDF + DotsOCR
    </span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.20945" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.20945" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.20945" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span><span class="tag domain">RL</span>
    <span class="tag method">Chain-of-Thought</span><span class="tag method">Reinforcement Learning</span><span class="tag method">Reward Shaping</span>
    <span class="tag task">Reasoning</span><span class="tag task">Text Generation</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰æŒçºŒå—ç›Šæ–¼å¤§è¦æ¨¡æ€ç¶­éˆï¼ˆChain-of-Thought, CoTï¼‰æ¨ç†ï¼Œä½†åŒæ™‚ä¹Ÿé¢è‡¨æ²‰é‡çš„è¨ˆç®—é–‹éŠ·ã€‚ç‚ºè§£æ±ºæ­¤å•é¡Œï¼Œé«˜æ•ˆæ¨ç†æ—¨åœ¨é€éå¼·åŒ–å­¸ç¿’ï¼ˆReinforcement Learning, RLï¼‰é€²è¡Œçå‹µå¡‘å½¢ï¼Œæ¿€å‹µç°¡çŸ­è€Œæº–ç¢ºçš„æ€è€ƒè»Œè·¡ã€‚åœ¨æœ¬è«–æ–‡ä¸­ï¼Œæˆ‘å€‘ç³»çµ±æ€§åœ°èª¿æŸ¥ LLMs é«˜æ•ˆæ¨ç†çš„æ©Ÿåˆ¶ã€‚ç‚ºé€²è¡Œå…¨é¢è©•ä¼°ï¼Œæˆ‘å€‘å€¡å°æ¡ç”¨æ›´ç²¾ç´°çš„æŒ‡æ¨™ï¼ŒåŒ…æ‹¬åŸºæ–¼æ­£ç¢ºæ€§çš„é•·åº¦åˆ†ä½ˆä»¥åŠæ¶µè“‹ 2k åˆ° 32k tokens å»£æ³›ç¯„åœå…§çš„æ€§èƒ½è¡¨ç¾ã€‚é¦–å…ˆï¼Œæˆ‘å€‘æ­ç¤ºè¨“ç·´éç¨‹éµå¾ªå…©éšæ®µç¯„å¼ï¼šé•·åº¦è‡ªé©æ‡‰èˆ‡æ¨ç†ç²¾åŒ–ã€‚éš¨å¾Œï¼Œæˆ‘å€‘é€²è¡Œå»£æ³›å¯¦é©—ï¼ˆç´„ 0.2 ç™¾è¬ GPU å°æ™‚ï¼‰ï¼Œåœ¨çµ±ä¸€å”è­°ä¸‹è§£æ§‹è¨“ç·´æç¤ºè©èˆ‡æ¨å‡ºã€çå‹µå¡‘å½¢ä»¥åŠæœ€ä½³åŒ–ç­–ç•¥ã€‚ç‰¹åˆ¥æ˜¯ï¼Œä¸€å€‹é—œéµç™¼ç¾æ˜¯åœ¨ç›¸å°è¼ƒç°¡å–®çš„æç¤ºè©ä¸Šé€²è¡Œè¨“ç·´ï¼Œç¢ºä¿æ­£çå‹µä¿¡è™Ÿçš„å¯†åº¦ï¼Œå¾è€Œé¿å…é•·åº¦å´©æ½°ã€‚åŒæ™‚ï¼Œæ‰€å­¸ç¿’çš„é•·åº¦åå·®å¯ä»¥è·¨é ˜åŸŸæ¨å»£ã€‚æˆ‘å€‘å°‡æ‰€æœ‰ç™¼ç¾å‡ç…‰ç‚ºå¯¶è²´çš„æ´å¯Ÿèˆ‡å¯¦è¸æŒ‡å—ï¼Œä¸¦é€²ä¸€æ­¥åœ¨ Qwen3 ç³»åˆ—ä¸­é©—è­‰ï¼Œæ¶µè“‹ 0.6B åˆ° 30Bï¼Œå±•ç¤ºå…¶ç©©å¥æ€§èˆ‡æ³›åŒ–èƒ½åŠ›ã€‚</p>
  
  <div class="abstract-en">Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>