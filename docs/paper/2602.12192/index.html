<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>æŸ¥è©¢ç„¦é»å’Œè¨˜æ†¶æ„ŸçŸ¥é‡æ’å™¨ç”¨æ–¼é•·æ–‡æœ¬è™•ç† â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">æŸ¥è©¢ç„¦é»å’Œè¨˜æ†¶æ„ŸçŸ¥é‡æ’å™¨ç”¨æ–¼é•·æ–‡æœ¬è™•ç†</h1>
  
  <div class="en-title">Query-focused and Memory-aware Reranker for Long Context Processing</div>
  

  <div class="paper-meta">
    
    <span>Yuqing Li, Jiangnan Li, Mo Yu, Guoxuan Ding, Zheng Lin, Weiping Wang, Jie Zhou</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.12192" target="_blank">2602.12192</a></span>
    
    <span style="color:var(--text-muted);font-size:0.8rem;">
      ä¾†æºï¼šarxiv HTML
    </span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.12192" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.12192" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.12192" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span>
    <span class="tag method">Transformer</span><span class="tag method">Attention Mechanism</span><span class="tag method">Reranking</span>
    <span class="tag task">Information Retrieval</span><span class="tag task">Passage Ranking</span><span class="tag task">Question Answering</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

åŸºæ–¼å°å¤§å‹èªè¨€æ¨¡å‹ä¸­æª¢ç´¢é ­ï¼ˆretrieval headsï¼‰çš„ç¾æœ‰åˆ†æï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹æ›¿ä»£æ€§çš„é‡æ’åºæ¡†æ¶ï¼Œè©²æ¡†æ¶è¨“ç·´æ¨¡å‹ä½¿ç”¨æ‰€é¸é ­éƒ¨çš„æ³¨æ„åŠ›åˆ†æ•¸ä¾†ä¼°è¨ˆæ®µè½-æŸ¥è©¢çš„ç›¸é—œæ€§ã€‚é€™ç¨®æ–¹æ³•æä¾›äº†ä¸€å€‹åˆ—è¡¨å¼è§£æ±ºæ–¹æ¡ˆï¼Œåœ¨æ’åºéç¨‹ä¸­å……åˆ†åˆ©ç”¨æ•´å€‹å€™é¸åˆ—è¡¨ä¸­çš„æ•´é«”è³‡è¨Šã€‚åŒæ™‚ï¼Œå®ƒè‡ªç„¶åœ°ç”¢ç”Ÿé€£çºŒçš„ç›¸é—œæ€§åˆ†æ•¸ï¼Œèƒ½å¤ åœ¨ä»»æ„æª¢ç´¢è³‡æ–™é›†ä¸Šé€²è¡Œè¨“ç·´ï¼Œç„¡éœ€ä¾è³´ Likert é‡è¡¨ç›£ç£ã€‚æˆ‘å€‘çš„æ¡†æ¶è¼•é‡ä¸”æœ‰æ•ˆï¼Œåªéœ€è¦å°è¦æ¨¡æ¨¡å‹ï¼ˆä¾‹å¦‚ 4B åƒæ•¸ï¼‰å³å¯é”åˆ°å¼·å¤§çš„æ€§èƒ½ã€‚å»£æ³›çš„å¯¦é©—è¡¨æ˜ï¼Œæˆ‘å€‘çš„æ–¹æ³•åœ¨å¤šå€‹é ˜åŸŸï¼ˆåŒ…æ‹¬ç¶­åŸºç™¾ç§‘å’Œé•·æ•˜è¿°è³‡æ–™é›†ï¼‰ä¸Šå„ªæ–¼ç¾æœ‰çš„æœ€å…ˆé€²çš„é€é»å¼å’Œåˆ—è¡¨å¼é‡æ’åºå™¨ã€‚å®ƒé€²ä¸€æ­¥åœ¨ LoCoMo åŸºæº–ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆé€²çµæœï¼Œè©²åŸºæº–è©•ä¼°å°è©±ç†è§£å’Œè¨˜æ†¶ä½¿ç”¨çš„èƒ½åŠ›ã€‚æˆ‘å€‘é€²ä¸€æ­¥å±•ç¤ºäº†æˆ‘å€‘çš„æ¡†æ¶æ”¯æŒéˆæ´»çš„æ“´å±•ã€‚ä¾‹å¦‚ï¼Œç”¨ä¸Šä¸‹æ–‡è³‡è¨Šå¢å¼·å€™é¸æ®µè½é€²ä¸€æ­¥æé«˜äº†æ’åºæº–ç¢ºæ€§ï¼Œè€Œå¾ä¸­é–“å±¤è¨“ç·´æ³¨æ„åŠ›é ­å¢å¼·äº†æ•ˆç‡ï¼ŒåŒæ™‚ä¸çŠ§ç‰²æ€§èƒ½ã€‚</p>
  
  <div class="abstract-en">Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <h1 id="_1">é¢å‘æŸ¥è©¢å’Œè¨˜æ†¶æ„ŸçŸ¥çš„é•·ä¸Šä¸‹æ–‡è™•ç†é‡æ’å™¨</h1>
<p>åŸºæ–¼ç¾æœ‰çš„å¤§èªè¨€æ¨¡å‹ä¸­æª¢ç´¢é ­çš„åˆ†æï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹æ›¿ä»£æ€§çš„é‡æ’æ¡†æ¶ï¼Œè¨“ç·´æ¨¡å‹ä½¿ç”¨é¸å®šé ­éƒ¨çš„æ³¨æ„åŠ›åˆ†æ•¸ä¾†ä¼°è¨ˆæ–‡ç« -æŸ¥è©¢ç›¸é—œæ€§ã€‚
æ­¤æ–¹æ³•æä¾›äº†ä¸€å€‹åˆ—è¡¨ç´šåˆ¥çš„è§£æ±ºæ–¹æ¡ˆï¼Œåœ¨æ’åºéç¨‹ä¸­åˆ©ç”¨æ•´å€‹å€™é¸ç°¡è¡¨ä¸­çš„æ•´é«”è³‡è¨Šã€‚åŒæ™‚ï¼Œå®ƒè‡ªç„¶ç”¢ç”Ÿé€£çºŒçš„ç›¸é—œæ€§åˆ†æ•¸ï¼Œä½¿å¾—å¯ä»¥åœ¨ä»»æ„æª¢ç´¢è³‡æ–™é›†ä¸Šé€²è¡Œè¨“ç·´ï¼Œç„¡éœ€ Likert å°ºåº¦æ¨™è¨»ã€‚
æˆ‘å€‘çš„æ¡†æ¶è¼•é‡ç´šä¸”æœ‰æ•ˆï¼Œåƒ…éœ€å°è¦æ¨¡æ¨¡å‹ï¼ˆå¦‚ 4B åƒæ•¸ï¼‰å³å¯é”åˆ°å¼·å¤§æ€§èƒ½ã€‚
å»£æ³›çš„å¯¦é©—è¡¨æ˜ï¼Œæˆ‘å€‘çš„æ–¹æ³•åœ¨å¤šå€‹é ˜åŸŸï¼ˆåŒ…æ‹¬ Wikipedia å’Œé•·æ•˜äº‹è³‡æ–™é›†ï¼‰ä¸Šå„ªæ–¼ç¾æœ‰çš„æœ€å…ˆé€²çš„é»ç´šåˆ¥å’Œåˆ—è¡¨ç´šåˆ¥é‡æ’å™¨ã€‚å®ƒé€²ä¸€æ­¥åœ¨ LoCoMo åŸºæº–ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆé€²çµæœï¼Œè©²åŸºæº–è©•ä¼°å°è©±ç†è§£å’Œè¨˜æ†¶ä½¿ç”¨çš„èƒ½åŠ›ã€‚
æˆ‘å€‘é€²ä¸€æ­¥è­‰æ˜äº†æˆ‘å€‘çš„æ¡†æ¶æ”¯æŒéˆæ´»çš„æ“´å±•ã€‚ä¾‹å¦‚ï¼Œç”¨ä¸Šä¸‹æ–‡è³‡è¨Šå¢å¼·å€™é¸æ–‡ç« é€²ä¸€æ­¥æ”¹é€²æ’åºæº–ç¢ºåº¦ï¼Œè€Œå¾ä¸­é–“å±¤è¨“ç·´æ³¨æ„åŠ›é ­å¢å¼·äº†æ•ˆç‡è€Œä¸çŠ§ç‰²æ€§èƒ½ 1 1 1 æ¨¡å‹å¯åœ¨ https://huggingface.co/MindscapeRAG/QRRanker ç²å¾—ã€‚</p>
<p>é¢å‘æŸ¥è©¢å’Œè¨˜æ†¶æ„ŸçŸ¥çš„é•·ä¸Šä¸‹æ–‡è™•ç†é‡æ’å™¨</p>
<p>Yuqing Li 1,2 â€  â€  è…³è¨»ï¼šå¹³ç­‰è²¢ç»ã€‚Jiangnan Li 3 1 1 è…³è¨»: 1 Mo Yu 3 1 1 è…³è¨»: 1 Guoxuan Ding 1,2 Zheng Lin 1,2 â€  â€  è…³è¨»ï¼šé€šè¨Šä½œè€…ã€‚Weiping Wang 1 Jie Zhou 3 1 ä¸­åœ‹ç§‘å­¸é™¢ä¿¡æ¯å·¥ç¨‹ç ”ç©¶æ‰€ 2 ä¸­åœ‹ç§‘å­¸é™¢å¤§å­¸ç¶²çµ¡ç©ºé–“å®‰å…¨å­¸é™¢ 3 é¨°è¨Šå…¬å¸å¾®ä¿¡äººå·¥æ™ºèƒ½ä¸­å¿ƒæ¨¡å¼è­˜åˆ¥ä¸­å¿ƒ liyuqing@iie.ac.cn {jiangnanli,moyumyu}@tencent.com</p>
<h2 id="1">1 ä»‹ç´¹</h2>
<p>åµŒå…¥æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åŸºæ–¼ LLM æ§‹å»ºçš„æ¨¡å‹ï¼Œå·²å–å¾—æˆåŠŸä¸¦ä½¿ç”Ÿæˆæ¨¡å‹ï¼ˆRAGï¼‰å’Œä»£ç†èƒ½å¤ é«˜æ•ˆåœ°è™•ç†é•·è¼¸å…¥æˆ–å¤§å‹è¼¸å…¥èªæ–™åº« Zhang et al. ( 2025b ); Zhao et al. ( 2025 ); Babakhin et al. ( 2025 ); Li et al. ( 2025a )ã€‚
ç„¶è€Œï¼ŒåµŒå…¥æ¨¡å‹ä¹Ÿå­˜åœ¨é™åˆ¶ï¼Œé€™ä¸€é»å·²è¢« Weller et al. ( 2025 ) å¾ç†è«–ä¸Šè­‰æ˜ä¸¦é€šéå¯¦é©—èªªæ˜ã€‚ä»–å€‘æ­ç¤ºäº†ä¸€å€‹ã€Œå¹¾ä½•ç“¶é ¸ã€ï¼Œå…¶ä¸­å›ºå®šç¶­åº¦çš„å‘é‡ç„¡æ³•ç·¨ç¢¼æŸ¥è©¢-æ–‡æª”äº¤äº’çš„çµ„åˆè¤‡é›œæ€§ã€‚
æ­¤å¤–ï¼Œç›¸ä¼¼åº¦æ¸¬åº¦çš„æ­¸ç´åç½®é™åˆ¶äº†å…¶é©ç”¨é ˜åŸŸï¼Œè€Œåœ¨é€™äº›é ˜åŸŸä¸­å¯èƒ½éœ€è¦å…¶ä»–é¡å‹çš„é—œä¿‚ä¾†é€²è¡Œæª¢ç´¢ï¼Œä¾‹å¦‚å› æœé—œä¿‚ã€é—œè¯å’Œé¡æ¯”ã€‚</p>
<p>é•·æœŸä»¥ä¾†çš„ç ”ç©¶åœ¨åµŒå…¥æ¨¡å‹è¿”å›çš„å€™é¸åˆ—è¡¨ä¸Šæ‡‰ç”¨é¡å¤–çš„é‡æ’åºæ¨¡å¡Šä¾†è§£æ±ºé€™ä¸€æŒ‘æˆ°ã€‚
é‡æ’åºå™¨ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¼·å¤§çš„è¡¨ç¤ºæ–¹å¼ï¼ˆå¦‚äº¤å‰æ³¨æ„åŠ›ï¼‰ã€‚LLM çš„å¿«é€Ÿç™¼å±•æ¨å‹•äº†è¨±å¤šåŸºæ–¼ LLM çš„é‡æ’åºå™¨çš„ç™¼ä½ˆï¼Œä»¥å—ç›Šæ–¼ LLM çš„æ¨ç†èƒ½åŠ› Zhang et al. ( 2025b ); Sun et al. ( 2025 ); Liu et al. ( 2025a ); Pradeep et al. ( 2023b )ã€‚
é€™äº›é‡æ’åºå™¨å¯ä»¥æ¡ç”¨é€é»æˆ–åˆ—è¡¨å¼çš„æ–¹æ³•ã€‚
é€é»æ–¹æ³•æœƒå–ªå¤±å°å€™é¸åˆ—è¡¨çš„å…¨å±€è¦–åœ–ï¼Œä½†å¯ä»¥çµ¦å‡ºåˆ†æ•¸ã€‚
å¦ä¸€æ–¹é¢ï¼Œåˆ—è¡¨å¼æ–¹æ³•ç›´æ¥ç¹¼æ‰¿äº†ä¸»å¹¹ LLM çš„é•·ä¸Šä¸‹æ–‡æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œå…·æœ‰å°å€™é¸åˆ—è¡¨çš„æ•´é«”è¦–åœ–ï¼Œä½†ä¸‹ä¸€å€‹ token é æ¸¬é™åˆ¶äº†ç´°ç²’åº¦åˆ†æ•¸çš„é æ¸¬ï¼Œé æ¸¬çš„æµ®é»æ•¸ä¸¦ä¸ç¸½æ˜¯èƒ½æº–ç¢ºåæ˜ çœŸå¯¦çš„ä¿¡åº¦ Liu et al. ( 2025b ); Lin et al. ( 2024 )ã€‚
å› æ­¤ï¼Œå®ƒå€‘æ¡ç”¨ Likert è©•åˆ†åˆ¶åº¦ï¼Œè¦æ±‚æ¨¡å‹ç‚ºæ¯å€‹è¼¸å…¥æ–‡æª”è¼¸å‡ºäº”é»æˆ–åé»é‡è¡¨åˆ†æ•¸ã€‚
é€™é™åˆ¶äº†å¯ç”¨è¨“ç·´æ•¸æ“šçš„æ•¸é‡ã€‚</p>
<p>åœ¨é€™é …å·¥ä½œä¸­ï¼Œæˆ‘å€‘åŸºæ–¼ç¾æœ‰å° LLM ä¸­æª¢ç´¢é ­çš„åˆ†ææå‡ºäº†ä¸€ç¨®æ›¿ä»£æ–¹æ¡ˆ Wu et al. ( 2024 ); Zhang et al. ( 2025a )ã€‚
é€™äº›å·¥ä½œè­˜åˆ¥äº†å…©ç¨®ç›¸é—œé¡å‹çš„é ­ï¼šæª¢ç´¢é ­å’ŒæŸ¥è©¢èšç„¦æª¢ç´¢ (QR) é ­ã€‚å…©è€…éƒ½æŒ‡æ³¨æ„åŠ›æ¨¡å¼åæ˜ æª¢ç´¢è¡Œç‚ºçš„æ³¨æ„åŠ›é ­ã€‚
å…·é«”ä¾†èªªï¼Œç•¶å°‡ç›¸é—œå’Œå¹²æ“¾æ®µè½çš„é•·ä¸Šä¸‹æ–‡èˆ‡æŸ¥è©¢é€£æ¥æ™‚ï¼Œé€™äº›é ­è¢«å®šç¾©ç‚ºåœ¨ç›¸é—œæ®µè½ä¸Šæ”¾ç½®é¡¯è‘—æ³¨æ„åŠ›æ¬Šé‡çš„é ­ï¼Œä½¿å¾—æ³¨æ„åŠ›æ¬Šé‡çš„æ’åèˆ‡é—œè¯åº¦æ’åç›¸é—œã€‚</p>
<p>é›–ç„¶ç¾æœ‰ç ”ç©¶ä¸»è¦é—œæ³¨æ–¼æ¢æ¸¬å’Œç†è§£æ­¤é¡é ­çš„åŠŸèƒ½ï¼Œä½†æˆ‘å€‘çš„å·¥ä½œæ›´é€²ä¸€æ­¥ï¼Œé€šéè¨“ç·´ LLM ä¾†å„ªåŒ–ä¸€å°çµ„æª¢ç´¢é ­çš„æ’åæº–ç¢ºæ€§ã€‚é€šéé€™ç¨®æ–¹å¼ï¼Œæˆ‘å€‘å¯¦ç¾äº†ä¸€å€‹ LLM æ’åºå™¨ï¼Œå…¶è¢«å„ªåŒ–ä»¥ç”¨æ³¨æ„åŠ›æ¬Šé‡å°æ®µè½é€²è¡Œæ’åã€‚é€™ç¨®åˆ—è¡¨å¼è§£æ±ºæ–¹æ¡ˆï¼Œåç‚º QRRankerï¼Œå¯ä»¥è‡ªç„¶åœ°ä½¿ç”¨é€£çºŒé—œè¯åº¦åˆ†æ•¸ï¼Œè€Œä¸å— Likert é‡è¡¨ç›£ç£çš„é™åˆ¶ï¼Œå› æ­¤å¯ä»¥åœ¨ä»»æ„æª¢ç´¢æ•¸æ“šé›†ä¸Šé€²è¡Œè¨“ç·´ã€‚</p>
<p>æˆ‘å€‘çš„ QRRanker åœ¨å¯¦è¸ä¸­å…·æœ‰å¤šå€‹è‰¯å¥½ç‰¹æ€§ã€‚é¦–å…ˆï¼Œæª¢ç´¢é ­å³ä½¿åœ¨ä¸»å¹¹æ¨¡å‹ç›¸å°è¼ƒå°çš„æƒ…æ³ä¸‹ï¼ˆä¾‹å¦‚ 4B åƒæ•¸ï¼‰ä¹Ÿèƒ½å¾—åˆ°æœ‰æ•ˆè¨“ç·´ã€‚é€™ä½¿å¾—åˆ—è¡¨å¼æ–¹æ³•èƒ½å¤ ä»¥æ›´é«˜çš„æ•ˆç‡é‹è¡Œã€‚
å…¶æ¬¡ï¼Œé€šéåœ¨è¨“ç·´éç¨‹ä¸­å°‡å…±äº«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯é ç½®åˆ°å€™é¸é …çš„åŸºç¤ä¸Šï¼Œå¯ä»¥è¼•é¬†ä¸”é«˜æ•ˆåœ°ç”¨å…¨å±€ä¸Šä¸‹æ–‡å¢å¼·è¼¸å…¥å€™é¸æ®µè½ï¼Œé€™å°æ–¼é•·æ•˜è¿°ç†è§£è‡³é—œé‡è¦ã€‚
æœ€å¾Œï¼Œæˆ‘å€‘è§€å¯Ÿåˆ°æˆ‘å€‘çš„ QRRanker å°é ­çš„é¸æ“‡ç›¸ç•¶ç©©å¥ï¼Œä½¿ç”¨ä¾†è‡ªä¸­é–“å±¤çš„é ­é€²è¡Œè¨“ç·´ä¸æœƒå°è‡´æ€§èƒ½ä¸‹é™ã€‚é€™å…è¨±æˆ‘å€‘åœ¨è¨“ç·´å’Œæ¨ç†éç¨‹ä¸­å»æ‰ LLM çš„æ›´é«˜å±¤ï¼Œé€™å¯ä»¥å¤§å¤§é™ä½æ¨¡å‹çš„å»¶é²ã€‚</p>
<p>åœ¨å„ç¨®é ˜åŸŸçš„å¯¦é©—ä¸­ï¼ŒåŒ…æ‹¬ Wikipedia QA ä»»å‹™ï¼ˆMusique Trivedi et al. ( 2022 )ã€HotpotQA Yang et al. ( 2018 )ï¼‰ã€é•·æ•˜è¿° QA ä»»å‹™ï¼ˆNarrativeQA KoÄiská»³ et al. ( 2018 )ã€DetectiveQA Xu et al. ( 2025b )ï¼‰å’Œé•·ä¸Šä¸‹æ–‡å°è©±ï¼ˆLoCoMo Maharana et al. ( 2024 )ï¼‰ï¼Œéƒ½è­‰æ˜äº†æˆ‘å€‘ QRRanker çš„å„ªå‹¢ã€‚
ä½œç‚ºä¸€å€‹å¤šåŠŸèƒ½çš„æ’åºæ¡†æ¶ï¼Œæˆ‘å€‘çš„æ–¹æ³•ä¸åƒ…å„ªæ–¼æœ€å…ˆé€²çš„é€šç”¨é€é»å’Œåˆ—è¡¨å¼æ¨¡å‹ï¼ˆå¦‚ Qwen-Rerank å’Œ GroupRankï¼‰ï¼Œè€Œä¸”åœ¨é ˜åŸŸç‰¹å®šçš„æ’åºæ–¹æ³•ä¸Šä¹ŸæŒçºŒæ”¹é€²ï¼Œä¾‹å¦‚ç”¨æ–¼ Wikipedia QA çš„ HippoRAG-v2 Guti'errez et al. ( 2025 ) ä»¥åŠåœ¨ LoCoMo ä¸Šçš„ä¸€ç³»åˆ—æœ€è¿‘çš„è¨˜æ†¶å¢å¼·æ–¹æ³• Li et al. ( 2025b ); Rasmussen et al. ( 2025 ); Hu et al. ( 2026a )ã€‚</p>
<h2 id="2">2 ç›¸é—œå·¥ä½œ</h2>
<p><strong>é‡æ’åº (Reranking)</strong></p>
<p>æ’åºæŠ€è¡“å…¬èªåŸºæ–¼å…©ç¨®çµæ§‹æ§‹å»ºï¼šSiamese networkï¼ˆBi-encoderï¼›Koch et al. 2015ï¼‰å’Œ Cross-encoderï¼ˆThakur et al., 2021ï¼‰ã€‚åµŒå…¥æ¨¡å‹ï¼ˆZhang et al., 2025bï¼‰æ˜¯é€šå¸¸ç”¨æ–¼ä½¿ç”¨å„²å­˜çš„åµŒå…¥å°æ•´å€‹æ–‡ä»¶èªæ–™åº«é€²è¡Œæ’åºçš„ç¬¬ä¸€ç¨®æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒå€‘å—åˆ°ã€Œå¹¾ä½•ç“¶é ¸ã€çš„é™åˆ¶ï¼Œç„¡æ³•ç·¨ç¢¼æŸ¥è©¢å’Œæ–‡ä»¶ä¹‹é–“æ›´ç´°ç²’åº¦çš„äº¤äº’ã€‚Cross-encoder å¯ä»¥ç·©è§£é€™ä¸€é™åˆ¶ï¼Œå®ƒé€éèˆ‡æŸ¥è©¢çš„äº¤å‰æ³¨æ„åŠ›å°æ¯å€‹æ–‡ä»¶é€²è¡Œè©•åˆ†ã€‚å°æ¯å°æŸ¥è©¢-æ–‡ä»¶é€²è¡Œå°ˆé–€é‡æ–°ç·¨ç¢¼çš„è¨ˆç®—è² æ“”é™åˆ¶äº† cross-encoder åªèƒ½å°ç”± bi-encoder æ’åºçš„å‰ n å€‹æ–‡ä»¶é€²è¡Œé‡æ’åºï¼Œå¾è€Œç”¢ç”Ÿäº†å‰æ–‡ä»¶çš„ç²¾ç·»æ’åºã€‚å› æ­¤ï¼Œcross-encoder è¢«ç¨±ç‚ºé‡æ’åºå™¨ï¼ˆRerankerï¼‰ã€‚</p>
<p>åœ¨ LLM æ™‚ä»£ï¼Œé‡æ’åºå™¨ä¹Ÿè¢«æ·±å…¥æ¢ç´¢ã€‚å®ƒå€‘å¯ä»¥åˆ†ç‚ºå…©é¡ï¼šPointwise å’Œ Listwiseã€‚Pointwise æè¿°æ–‡ä»¶æˆå°è©•åˆ†çš„ç¯„ä¾‹ï¼Œé€™æ˜¯å¯¦è¸ä¸­çš„ä¸»è¦æ–¹å‘ï¼ˆQin et al. 2024ï¼›Sun et al. 2023ï¼›Liu et al. 2025aï¼›Zhuang et al. 2025ï¼‰ï¼Œä¾‹å¦‚ Qwen3ï¼ˆZhang et al., 2025bï¼‰ã€Jinaã€mGTEï¼ˆZhang et al., 2024ï¼‰ã€BGE-m3ï¼ˆChen et al. 2024ï¼‰é‡æ’åºå™¨ã€‚Pointwise æ¨¡å‹ç¨ç«‹ç·¨ç¢¼æ–‡ä»¶ï¼Œç„¡æ³•æŒæ¡å…¨å±€è³‡è¨Šã€‚ç‚ºæ­¤ï¼ŒListwise æ¨¡å‹å……åˆ†åˆ©ç”¨ LLM çš„ç”Ÿæˆèƒ½åŠ›ã€‚å®ƒå€‘ï¼ˆPradeep et al. 2023a, bï¼‰å°‡æ–‡ä»¶é€£æ¥æˆåˆ—è¡¨ä¸¦ç›¸æ‡‰åœ°ç”Ÿæˆé‡æ’åºçµæœã€‚é€²ä¸€æ­¥åœ°ï¼Œä½¿ç”¨ RL é€²è¡Œå¾®èª¿å¾Œï¼Œæ¨¡å‹å¯ä»¥å…ˆæ€è€ƒï¼ˆSun et al. 2023ï¼›Liu et al. 2025aï¼›Qin et al. 2025ï¼›Ma et al. 2023ï¼›Sun et al. 2025ï¼‰ï¼Œç„¶å¾Œçµ¦å‡ºç­”æ¡ˆï¼Œé”åˆ°é¡¯è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒListwise æ¨¡å‹éœ€è¦è¨“ç·´è³‡æ–™æä¾›ç‰¹å®šçš„æ–‡ä»¶æ’åºç”šè‡³åˆ†æ•¸ï¼Œå°è‡´è³‡æ–™æ”¶é›†å’Œæ§‹å»ºçš„è² æ“”ã€‚æ­¤å¤–ï¼ŒLLM çš„ç”Ÿæˆä¸ç©©å®šï¼ˆä¾‹å¦‚ï¼Œç”Ÿæˆä¸æ­£ç¢ºçš„æ ¼å¼ï¼‰ï¼Œç‰¹åˆ¥æ˜¯åœ¨å¼•å…¥æ€è€ƒéç¨‹æ™‚ã€‚æ­£å¦‚ Wu et al.ï¼ˆ2024ï¼‰å’Œ Zhang et al.ï¼ˆ2025aï¼‰ç ”ç©¶çš„é‚£æ¨£ï¼ŒLLM æœ¬è³ªä¸Šå…·æœ‰æª¢ç´¢èƒ½åŠ›ï¼Œæª¢ç´¢æ³¨æ„åŠ›é ­å¯ä»¥è¢«æå–ä¾†æ’åºæ–‡ä»¶ï¼Œé”åˆ°å…·æœ‰ç«¶çˆ­åŠ›çš„æ€§èƒ½ã€‚å„˜ç®¡å¦‚æ­¤ï¼Œé€™äº›é ­åœ¨è½‰ç§»åˆ°æ–°ä»»å‹™æ™‚å¯èƒ½æœƒæ”¹è®Šï¼Œéœ€è¦é¡å¤–çš„ç¨®å­è³‡æ–™é›†ä¾†æå–å®ƒå€‘ã€‚ç‚ºæ­¤ï¼Œæˆ‘å€‘æè­°è¨“ç·´é¸å®šçš„é ­ï¼Œé€™ç¢ºä¿äº†æ›´å¥½çš„å¯é·ç§»æ€§ã€‚</p>
<p><strong>è¨˜æ†¶é«”åˆ©ç”¨ (Memory Utilization)</strong></p>
<p>è¨˜æ†¶é«”æ§‹å»ºå’Œåˆ©ç”¨ä»¥ç·©è§£é•·æ–‡æœ¬è™•ç†çš„å•é¡Œæˆç‚ºç•¶ä»Šçš„ç†±é»ã€‚å°æ–¼é•·æ•…äº‹ç†è§£ï¼ŒLi et al.ï¼ˆ2025aï¼‰æ§‹å»ºå…¨å±€è¨˜æ†¶ä»¥å¢å¼·æª¢ç´¢å’Œç”Ÿæˆã€‚å°æ–¼å°è©±ç®¡ç†ï¼Œè¨­è¨ˆäº†è¤‡é›œçš„åœ–ï¼ˆJiang et al. 2026ï¼›Xu et al. 2025aï¼›Rasmussen et al. 2025ï¼›Hu et al. 2026b, aï¼‰ã€æ¨¹ï¼ˆLi et al. 2026ï¼‰å’Œäº‹ä»¶ã€äººç‰©è§’è‰²å’Œæ–‡æœ¬å¡Šçš„ç³»çµ±ï¼ˆChhikara et al. 2025ï¼›Li et al. 2025bï¼›Nan et al. 2025ï¼›Tao et al. 2026ï¼›Zou et al. 2026ï¼‰ï¼Œä»¥æº–ç¢ºæå–ç›¸é—œçš„å°è©±æ­·å²ä¾›é€²ä¸€æ­¥ä½¿ç”¨ã€‚ç„¶è€Œï¼Œä»¥ç°¡å–®çš„è¨˜æ†¶é«”æ§‹å»ºé€²è¡Œæ›´å¥½å’Œæ›´å¼·å¤§çš„æ­·å²æœå°‹ï¼Œå¯ä»¥å‹éè¤‡é›œçš„è¨˜æ†¶é«”ç®¡ç†ï¼Œæˆ‘å€‘å°‡å±•ç¤ºæˆ‘å€‘é”åˆ°æ­¤ç›®æ¨™çš„è§£æ±ºæ–¹æ¡ˆã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-25/2602.12192/x1.png" loading="lazy"></figure> åœ– 1ï¼šæª¢ç´¢åˆ†æ•¸å’Œ QR åˆ†æ•¸åŸºæ–¼ (QR) æ³¨æ„åŠ›é ­çš„æ³¨æ„åŠ›åˆ†æ•¸è¨ˆç®—ã€‚åœ¨æ­¤åœ–ä¸­ï¼ŒDoc2 æ˜¯é»ƒé‡‘æ–‡ä»¶ï¼ˆå¡Šï¼‰ã€‚</p>
<h2 id="3-qr-head">3 é å‚™çŸ¥è­˜ï¼šQR-head</h2>
<p><figure class="paper-figure"><img src="../../figures/2026-02-25/2602.12192/x2.png" loading="lazy"></figure> åœ– 2ï¼šQRRanker çš„çµæ§‹å¦‚åœ–ä¸­æ‰€ç¤ºï¼Œå…¶ä¸­é«˜äº®çš„é ­éƒ¨ç‚ºç”¨æ–¼æ–‡ä»¶è©•åˆ†çš„ QR é ­éƒ¨ã€‚ç”±æ–¼ QRRanker èƒ½å¤ åˆ©ç”¨è¨˜æ†¶å¢å¼·ä¾†æ•æ‰æ›´å¤šçš„èªå¢ƒè³‡è¨Šï¼Œæˆ‘å€‘å¯ä»¥ç‚ºæ•˜è¿°æ–‡æœ¬å’Œå°è©±æ§‹å»ºè¨˜æ†¶ï¼Œå¦‚å·¦å´æ‰€ç¤ºã€‚å³å´éƒ¨åˆ†å±•ç¤ºäº†ç”¨æ–¼æ•˜è¿°æ–‡æœ¬/ç¶­åŸºç™¾ç§‘/å°è©±çš„ QA çš„æ’å-é‡æ–°æ’åæµç¨‹ï¼Œå…¶ä¸­ä¸æ¶‰åŠè¤‡é›œçš„è¨­è¨ˆã€‚</p>
<p>æœ¬ç¯€é¦–å…ˆä»‹ç´¹æŸ¥è©¢ç„¦é»æª¢ç´¢é ­éƒ¨ï¼ˆQuery-Focused Retrieval headsï¼ŒQR-headï¼‰çš„å®šç¾©ã€‚</p>
<p>å¦‚ Wu et al. ( 2024 ) å’Œ Zhang et al. ( 2025a ) æ‰€ä»‹ç´¹çš„é‚£æ¨£ï¼Œåœ¨å¤šé ­è‡ªæ³¨æ„åŠ›æ¨¡çµ„ä¸­çš„æ‰€æœ‰é ­éƒ¨ä¸­ï¼Œæœ‰äº›æ‰®æ¼”è‘—æª¢ç´¢å™¨çš„é—œéµè§’è‰²ã€‚é€™äº›é ­éƒ¨åœ¨ç·¨ç¢¼å•é¡Œæ™‚ï¼Œæœƒæ›´å¤šåœ°é—œæ³¨èªå¢ƒä¸­åŒ…å«å›ç­”å•é¡Œçš„è³‡è¨Šçš„éƒ¨åˆ†ã€‚Zhang et al. ( 2025a ) å°‡å…¶å‘½åç‚º QR-headsï¼Œä¸¦é€šé QR score ä¾†è­˜åˆ¥å®ƒå€‘ã€‚</p>
<p>å½¢å¼ä¸Šï¼Œå°æ–¼ä¸€å€‹å•é¡Œ $Q$ï¼Œå…¶å°æ‡‰çš„èªå¢ƒ $C$ è¢«åˆ†å‰²æˆä¸€å€‹å¡Šåˆ—è¡¨ $[c_{0},c_{1},...,c_{n}]$ï¼Œå…¶ä¸­ $G=[c_{g0},...,c_{gm}]$ æ˜¯å›ç­”å•é¡Œçš„é»ƒé‡‘å¡Šã€‚ç•¶ä½¿ç”¨ $C$ å’Œ $Q$ ç·¨ç¢¼æç¤ºæ™‚ï¼Œæ³¨æ„åŠ›é ­éƒ¨ $h$ åœ¨ $Q$ å’Œæ¯å€‹å¡Š $c_{i}$ ä¹‹é–“çš„æ³¨æ„åŠ›åˆ†æ•¸è¡¨ç¤ºç‚º $A^{Q\rightarrow c_{i}}<em>{h}\in\mathbb{R}^{|Q|\times|c</em>{i}|}$ã€‚é ­éƒ¨çš„ QR score é€šéå°é»ƒé‡‘å¡Šçš„æ³¨æ„åŠ›åˆ†æ•¸é€²è¡Œæ±‚å’Œä¾†è¨ˆç®—ï¼š</p>
<p>|  | $$\texttt{QRScore}<em>{h}=\frac{1}{|Q|}\sum</em>{c_{i}\in G}\sum_{w_{q}\in Q}\sum_{w_{c}\in c_{i}}{A^{Q\rightarrow c_{i}}<em>{h}[w</em>{q},w_{c}]},$$ |  | (1) |
| --- | --- | --- | --- |</p>
<p>å…¶ä¸­ $w_{c}$ å’Œ $w_{Q}$ åˆ†åˆ¥æ˜¯é»ƒé‡‘å¡Š $c_{i}$ å’Œ $Q$ ä¸­çš„æ¨™è¨˜ã€‚QR score è¡¡é‡é ­éƒ¨ $h$ é—œæ³¨é»ƒé‡‘å¡Šçš„ç¨‹åº¦ã€‚æ›´é«˜çš„æ•¸å€¼è¡¨ç¤ºè©²é ­éƒ¨å…·æœ‰è­˜åˆ¥ $G$ çš„æ½›åŠ›ã€‚QR score å°‡åœ¨ç¨®å­è³‡æ–™é›†ä¸Šé‡å°æ¯å€‹é ­éƒ¨ $h\in H$ é€²è¡Œè¨ˆç®—å’Œå¹³å‡ï¼Œå°‡ $H$ æŒ‰é™åºæ’åºï¼Œç„¶å¾Œé¸æ“‡æ’åå‰ 16 çš„é ­éƒ¨ä½œç‚º QR é ­éƒ¨ï¼ˆ$h\in H_{QR}$ï¼‰ã€‚æˆ‘å€‘é€šéä¾†è‡ª NarrativeQA çš„ 1000 å€‹éš¨æ©Ÿæ¨£æœ¬ç‚º Qwen3-4B-Instruct-2507 Yang et al. ( 2025 ) é¸æ“‡ QR é ­éƒ¨ã€‚</p>
<p>QR é ­éƒ¨ä»¥é¡ä¼¼æ–¼å…¬å¼ 1 çš„æ–¹å¼è¨ˆç®—å¡Š $c_{i}$ çš„æª¢ç´¢åˆ†æ•¸ï¼Œå…¶ä¸­å°‡ $\sum_{c\in G}$ æ›¿æ›ç‚º $\sum_{h\in H_{QR}}$ã€‚Zhang et al. ( 2025a ) é€²ä¸€æ­¥æ·»åŠ äº†åˆ†æ•¸æ ¡æº–ä»¥æ¸›è¼•æ³¨æ„åŠ›æ¬Šé‡ä¸­çš„å…§åœ¨åå·®ï¼Œé€™æ¶‰åŠç·¨ç¢¼ä¸€å€‹ç©ºæŸ¥è©¢ $N$="N/A" ä¸¦æ¸›å»å…¶ $\frac{1}{|N|}\sum_{w_{q}\in N}A^{N\rightarrow c_{i}}$ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šéæˆ‘å€‘çš„ QR è¨“ç·´ï¼Œæ ¡æº–è®Šç‚ºå¯é¸çš„ã€‚</p>
<h2 id="4">4 æ–¹æ³•</h2>
<p>æˆ‘å€‘çš„ QRRanker æ˜¯ä¸€å€‹ Listwise æ–¹æ³•ï¼Œåœ¨å–®ä¸€æ¨è«–éç¨‹ä¸­é‡æ–°æ’åºæ‰€æœ‰æ–‡æª”ï¼Œéµå¾ªæ‰€è¬‚çš„ã€Œprompt-decodersã€Pradeep et al. ( 2023a )ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒQRRanker ä¸æ¶‰åŠä»»ä½•ç”Ÿæˆéç¨‹ï¼Œè€Œåªæ˜¯ç”¨å•é¡Œå’Œæ–‡æª”é å¡«å……æç¤ºï¼Œä¸¦å–å¾—æ³¨æ„åŠ›åˆ†æ•¸ï¼Œé€™æ¨£åœ¨æ™‚é–“å’Œè³‡æºæ–¹é¢æ›´åŠ å‹å–„ã€‚å„˜ç®¡åŸå§‹çš„ QR æª¢ç´¢å™¨ï¼ˆZhang et al. , 2025aï¼‰é…å‚™ä¸€çµ„é å…ˆè¨ˆç®—çš„ QR é ­å¯ä»¥é·ç§»åˆ°æ–°ä»»å‹™ï¼Œä½†æ•ˆèƒ½å¯èƒ½ä¸å¤ ç©©å®šï¼Œå› ç‚º QR é ­åœ¨æ–°ä»»å‹™ä¸Šå¯èƒ½æœƒæ”¹è®Šã€‚ç‚ºæ­¤ï¼Œæˆ‘å€‘æå‡ºäº† QRRanker çš„å°ˆé–€è¨“ç·´æµç¨‹ã€‚æˆ‘å€‘é¦–å…ˆæ§‹å»º Listwise è¨“ç·´å¯¦ä¾‹ï¼Œç„¶å¾Œä½¿ç”¨å°æ¯”æ’åºç›®æ¨™ä¾†æœ€ä½³åŒ–é å…ˆè¨ˆç®—çš„ QR é ­ã€‚</p>
<h3 id="41-qr">4.1 QR è¨“ç·´çš„è³‡æ–™æ§‹å»º</h3>
<p>æ¼”ç®—æ³• 1 åœ¨ NarrativeQA ä¸Šä½¿ç”¨å¯é¸æ‘˜è¦å‰ç¶´æ§‹å»ºåˆ—è¡¨å¼è¨“ç·´å¯¦ä¾‹</p>
<h4 id="411">4.1.1 åˆ—è¡¨å¼è¨“ç·´å¯¦ä¾‹</h4>
<p>æˆ‘å€‘é€éåˆä½µ MuSiQue Trivedi et al. ( 2022 ) å’Œ NarrativeQA KoÄiská»³ et al. ( 2018 ) ä¾†å»ºç«‹çµ±ä¸€çš„è¨“ç·´é›†ã€‚æˆ‘å€‘é¦–å…ˆç‚ºæ¯å€‹å•é¡Œç¢ºå®šè­‰æ“šå¡Šã€‚å°æ–¼ MuSiQueï¼Œæˆ‘å€‘ç›´æ¥ä½¿ç”¨åŸå§‹æ¨™è¨»ä¸­çš„å®˜æ–¹æ”¯æŒäº‹å¯¦ä½œç‚ºè­‰æ“šã€‚å°æ–¼ NarrativeQAï¼Œç”±æ–¼æœªæä¾›é»ƒé‡‘å¡Šï¼Œæˆ‘å€‘éµå¾ª Li et al. ( 2025a ) ä¾†æ§‹å»ºéŠ€è‰²è­‰æ“šå¡Šã€‚</p>
<p>å»ºç«‹è­‰æ“šå¾Œï¼Œæˆ‘å€‘ä½¿ç”¨ Qwen3-Embedding-8B ç‚ºæ¯å€‹å•é¡Œæª¢ç´¢å‰ 50 å€‹å€™é¸é›†ï¼Œä¸¦é€éå°‡èˆ‡é æ§‹å»ºè­‰æ“šç›¸ç¬¦çš„æª¢ç´¢å€™é¸æ¨™è¨˜ç‚ºæ­£æ¨£æœ¬ï¼ŒåŒæ™‚å°‡å‰©é¤˜çš„æª¢ç´¢å€™é¸è¦–ç‚ºè² æ¨£æœ¬ï¼Œä¾†å½¢æˆåˆ—è¡¨å¼å¯¦ä¾‹ã€‚</p>
<p>é¸æ“‡æ€§åœ°ï¼Œæˆ‘å€‘é€éå°‡æª¢ç´¢åˆ°çš„å¡Šå°æ‡‰åˆ°å…¶ç›¸æ‡‰çš„æ‘˜è¦ä¾†æ§‹å»ºæ‘˜è¦å‰ç¶´ï¼Œä¸¦å°‡é€™äº›æ‘˜è¦å‰ç½®æ–¼å¡Šåˆ—è¡¨ä¹‹å‰ï¼Œå³ $X = [M; C]$ã€‚æ¼”ç®—æ³• 1 ç¸½çµäº† NarrativeQA ä¸Šçš„æ­¤æ§‹å»ºï¼›MuSiQue éµå¾ªç›¸åŒçš„ç¨‹åºï¼Œåªä¸éç›¸é—œè­‰æ“šç›´æ¥ä¾†è‡ªå…¶å®˜æ–¹æ”¯æŒäº‹å¯¦ã€‚æˆ‘å€‘åœ¨ä¸‹ä¸€å°ç¯€ä¸­æè¿°æ‘˜è¦çš„æ§‹å»ºæ–¹å¼ã€‚</p>
<h4 id="412">4.1.2 æ‘˜è¦æ§‹å»º</h4>
<p>ç‚ºäº†æä¾›é«˜å±¤ç´šçš„èªæ„æŒ‡å°ä¸¦æ”¯æ´é•·æ–‡æœ¬æ•˜äº‹ç†è§£ï¼Œæˆ‘å€‘æ§‹å»ºæ‘˜è¦ä½œç‚ºè¼”åŠ©è¨˜æ†¶ä¸Šä¸‹æ–‡ã€‚ä½¿ç”¨æ™‚ï¼Œæ‘˜è¦è¢«å‰ç½®ç‚ºæª¢ç´¢å¡Šåˆ—è¡¨çš„å…¨åŸŸå‰ç¶´ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥åŒæ™‚åˆ©ç”¨ç²—ç²’åº¦ä¸Šä¸‹æ–‡å’Œç´°ç²’åº¦è­‰æ“šã€‚
æˆ‘å€‘æ¢ç´¢äº†å…©ç¨®äº’è£œçš„æ‘˜è¦æ§‹å»ºç­–ç•¥ã€‚</p>
<p><strong>åŸºæ–¼å¡Šçš„æ‘˜è¦ã€‚</strong></p>
<p>å°æ–¼å†—é•·çš„æ•˜äº‹æ›¸ç±ï¼Œæˆ‘å€‘æ§‹å»ºå°Šé‡æ•…äº‹è¬›è¿°é †åºæ€§è³ªçš„å¡Šç´šæ‘˜è¦ã€‚å…·é«”ä¾†èªªï¼Œæˆ‘å€‘å°‡æ¯æœ¬æ›¸åˆ†æˆè‹¥å¹²å¡Šï¼ˆæ¯å¡Š 20 å€‹é€£çºŒå¡Šï¼‰ï¼Œä¸¦ç‚ºæ¯å€‹å¡Šç”Ÿæˆä¸€å€‹æ‘˜è¦ã€‚ï¼ˆåƒè¦‹é™„éŒ„ A.1ï¼‰</p>
<p><strong>ä»¥äº‹ä»¶ç‚ºä¸­å¿ƒçš„æ‘˜è¦ã€‚</strong></p>
<p>å°æ–¼åŸºæ–¼å°è©±çš„è³‡æ–™ï¼Œæˆ‘å€‘å¾å°è©±ä¸­æå–çµæ§‹åŒ–äº‹ä»¶ï¼Œä¸¦å½¢æˆä»¥äº‹ä»¶ç‚ºä¸­å¿ƒçš„æ‘˜è¦ã€‚æ¯å€‹äº‹ä»¶ç”±ç°¡çŸ­æè¿°è¡¨ç¤ºï¼Œä¸¦é€£çµåˆ°å…¶ä¾†æºè©±èªï¼Œå¾è€Œèƒ½è¿½æº¯åˆ°åŸå§‹å°è©±ã€‚ï¼ˆåƒè¦‹é™„éŒ„ A.2ï¼‰</p>
<h3 id="42-qr">4.2 QR è¨“ç·´</h3>
<p>é€éç¬¬ 3 ç¯€ä¸­æåˆ°çš„ QR åˆ†æ•¸ç²å¾—é å…ˆè¨ˆç®—çš„ QR é ­ï¼Œæˆ‘å€‘çš„è¨“ç·´æ–¹æ¡ˆå°ˆæ³¨æ–¼è¨“ç·´é€™äº›é ­ã€‚å°æ–¼ä¸€å€‹å•é¡Œ Q Q å’Œç”±æª¢ç´¢å™¨ï¼ˆä¾‹å¦‚ Qwen3-Embedding ç­‰åµŒå…¥æ¨¡å‹ï¼‰æ’åºçš„å‰ 50 å€‹å€™é¸æ–‡ä»¶ C = [ c 1 , â€¦ , c 50 ] C=[c_{1},...,c_{50}]ï¼Œå…¶ä¸­é‡‘æ¨™ï¼ˆæ­£æ¨£æœ¬ï¼‰æ–‡ä»¶ç‚º G = [ c g â€‹ 0 , . . , c g â€‹ m ] G=[c_{g0},..,c_{gm}]ï¼ŒQRRanker çš„æç¤ºè¼¸å…¥æ˜¯é€éæŒ‰é †åºé€£æ¥ C C å’Œ Q Q ä»¥åŠä¸€äº›æŒ‡ä»¤ä¾†æ§‹é€ çš„ï¼šP = Inst â€‹ ( C , Q ) \texttt{P}=\text{Inst}(C,Q)ï¼Œå…¶ä¸­æŒ‡ä»¤ç¯„æœ¬åœ¨é™„éŒ„ A.3 ä¸­æä¾›ã€‚</p>
<p>æç¤º P è¢«é¥‹å…¥æ¨¡å‹ï¼Œåœ¨æ¯å€‹æ³¨æ„åŠ›é ­ä¸­ï¼Œæ³¨æ„åŠ›åˆ†æ•¸è¢«è¨ˆç®—ç‚º A h P â†’ P A^{\texttt{P}\rightarrow\texttt{P}}<em>{h}ã€‚æˆ‘å€‘å®šä½ Q Q å’Œ c i âˆˆ C c</em>{i}\in C çš„ä½ç½®ï¼Œä¸¦æå–ä»¥å•é¡Œç‚ºä¸­å¿ƒçš„éƒ¨åˆ† A h Q â†’ c i A^{Q\rightarrow c_{i}}<em>{h}ã€‚ç”± QR é ­ h âˆˆ H Q â€‹ R h\in H</em>{QR} è¨ˆç®—çš„æ®µè½ c i c_{i} çš„æª¢ç´¢åˆ†æ•¸ç‚ºï¼š</p>
<p>|  | s c i h = 1 | Q | â€‹ âˆ‘ i âˆˆ c i âˆ‘ j âˆˆ Q A h Q â†’ c i â€‹ [ i , j ] , s_{c_{i}}^{h}=\frac{1}{|Q|}\sum_{i\in c_{i}}\sum_{j\in Q}A^{Q\rightarrow c_{i}}_{h}[i,j], |  | (2) |
| --- | --- | --- | --- |</p>
<p>å…¶ä¸­åˆ†æ•¸è¨ˆç®—å¦‚åœ– 1 æ‰€ç¤ºã€‚ç„¶å¾Œï¼Œæœ€çµ‚æª¢ç´¢åˆ†æ•¸æ˜¯é€éå°‡æ‰€æœ‰ QR é ­æä¾›çš„åˆ†æ•¸ç›¸åŠ å¾—åˆ°çš„ï¼šs c i = âˆ‘ h âˆˆ H Q â€‹ R s c i h s_{c_{i}}=\sum_{h\in H_{QR}}{s_{c_{i}}^{h}}ã€‚æ­¤å¤–ï¼Œs c i h s_{c_{i}}^{h} ä¹Ÿå¯ä»¥é€éèšåˆæœ€å¤§æ³¨æ„åŠ›é …é€²è¡Œè¨ˆç®—ï¼Œå¦‚ ColBERT Khattab and Zaharia ( 2020 ) ç­‰æ–¹æ³•æ‰€ä½¿ç”¨çš„é‚£æ¨£ï¼Œé€™å¯ä»¥é”åˆ°é¡ä¼¼çš„æ•ˆèƒ½ï¼Œå› æ­¤æˆ‘å€‘åœ¨æ­¤ä¸è¨è«–ã€‚</p>
<p>æˆ‘å€‘éš¨å¾Œä½¿ç”¨æ¨£æœ¬ç´šå°æ¯”æå¤±å„ªåŒ–æ–‡ä»¶åˆ†æ•¸ S = [ s c 1 , â€¦ , s c 50 ] S=[s_{c_{1}},...,s_{c_{50}}]ã€‚åœ¨å‚³çµ±çš„å°æ¯”å ´æ™¯ä¸­ï¼Œåˆ†æ•¸ s c i s_{c_{i}} é€šå¸¸ç©©å®šåœ°ç¯„åœåœ¨ [0, 1]ï¼Œè€Œåœ¨æˆ‘å€‘çš„æƒ…æ³ä¸‹ï¼Œs c i s_{c_{i}} å¯èƒ½å—åˆ°æŒ‡ä»¤ä¸­çš„æ¨™è¨˜å½±éŸ¿ï¼ˆä¾‹å¦‚é ­å°æ³¨æ„åŠ›é™·é˜±çš„æ•æ„Ÿæ€§ï¼‰ï¼Œé€™å¯èƒ½å°è‡´æ¨£æœ¬çš„ç¯„åœä¸ç©©å®šã€‚å› æ­¤ï¼Œæº«åº¦åƒæ•¸å¯èƒ½ä¸é©åˆç”¨æ–¼ç¸®æ”¾åˆ†æ•¸ã€‚ç‚ºæ­¤ï¼Œæˆ‘å€‘ä½¿ç”¨æœ€å¤§æœ€å°æ­¸ä¸€åŒ–ä¾†æ­¸ä¸€åŒ–åˆ†æ•¸ï¼Œå…¶å½¢å¼å¦‚ä¸‹ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>S = s â€‹ c â€‹ a â€‹ l â€‹ e Ã— ( S âˆ’ min â€‹ ( S ) ) max â€‹ ( S ) âˆ’ min â€‹ ( S ) , S=\frac{scale\times(S-\text{min}(S))}{\text{max}(S)-\text{min}(S)},</th>
<th></th>
<th>(3)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ s â€‹ c â€‹ a â€‹ l â€‹ e scale æ˜¯ä¸€å€‹å› å­ï¼Œç”¨æ–¼å°‡ç¯„åœç¸®æ”¾åˆ° [0, s â€‹ c â€‹ a â€‹ l â€‹ e scale ] ä»¥ä¿æŒç©©å®šæ€§ã€‚</p>
<p>åŸå§‹çš„å°æ¯”æå¤±ä¸€æ¬¡æ¡æ¨£ä¸€å€‹æ­£æ¨£æœ¬æ–‡ä»¶ï¼›ç„¶è€Œï¼Œå‰ 50 å€‹æ–‡ä»¶å¯èƒ½åŒ…å«å¤šå€‹æ­£æ¨£æœ¬æ–‡ä»¶ã€‚å¦‚æœæŒ‰ç…§åŸå§‹è¨­å®šï¼Œé€™å¯èƒ½æ˜¯æ¬¡å„ªçš„ï¼Œå› ç‚ºæœªé¸ä¸­çš„æ­£æ¨£æœ¬æ–‡ä»¶æœƒè¢«å¿½ç•¥ã€‚æˆ‘å€‘æå‡ºäº†ä¸€å€‹å°æ¯”æå¤±çš„ç¾¤çµ„ç‰ˆæœ¬ï¼Œä»¥åŒæ™‚å„ªåŒ–å®ƒå€‘ï¼š</p>
<p>|  | L s â€‹ a â€‹ m â€‹ p â€‹ l â€‹ e = 1 | G | â€‹ âˆ‘ c p âˆˆ G log â€‹ Ï„ â€‹ ( s c p ) Ï„ â€‹ ( s c p ) + âˆ‘ c n âˆˆ C âˆ– G Ï„ â€‹ ( s c n ) , L_{sample}=\frac{1}{|G|}\sum_{c_{p}\in G}\text{log}\frac{\tau(s_{c_{p}})}{\tau(s_{c_{p}})+\sum_{c_{n}\in C\setminus G}\tau(s_{c_{n}})}, |  | (4) |
| --- | --- | --- | --- |</p>
<p>å…¶ä¸­ Ï„ \tau è¡¨ç¤ºæŒ‡æ•¸å‡½æ•¸ã€‚ä¸Šè¿°ç›®æ¨™å°‡æ¯å€‹æ­£æ¨£æœ¬æ–‡ä»¶è¦–ç‚ºç¨ç«‹çš„å­æ¨£æœ¬ï¼Œä¸¦åœ¨æ¨£æœ¬å…§è¨ˆç®—å¹³å‡æå¤±ã€‚å°æ–¼è³‡æ–™é›†ï¼Œè©²ç›®æ¨™èˆ‡å‚³çµ±å°æ¯”æå¤±ä¸€è‡´ã€‚</p>
<p>ç”±æ–¼æˆ‘å€‘çš„ QRRanker å¯ä»¥æœ‰æ„ŸçŸ¥è¨˜æ†¶èƒ½åŠ›ï¼Œä»¥æ•´åˆæ›´å»£æ³›çš„ä¸Šä¸‹æ–‡è¨Šæ¯ï¼Œåœ¨ QR è¨“ç·´æœŸé–“ï¼Œæˆ‘å€‘å¯ä»¥é¸æ“‡æ€§åœ°åœ¨å€™é¸æ–‡ä»¶åˆ—è¡¨ C C ä¹‹å‰é™„åŠ ä¸€å€‹è¨˜æ†¶å‰ç½®å­—é¦– M Mï¼ˆä¾‹å¦‚å¾æª¢ç´¢å¡Šæ˜ å°„å¾—åˆ°çš„æ‘˜è¦ï¼‰ã€‚ç”¢ç”Ÿçš„ QRRanker æç¤ºè¢«æ§‹é€ ç‚º P = Inst â€‹ ( M , C , Q ) \texttt{P}=\text{Inst}(M,C,Q)ã€‚</p>
<p>è¡¨ 1ï¼šæŒ‰ Recall@{k} æ¸¬é‡çš„æª¢ç´¢å’Œé‡æ’æ•ˆèƒ½ã€‚'â€“' è¡¨ç¤ºç›¸æ‡‰è«–æ–‡ä¸­æœªå ±å‘Šè©²æŒ‡æ¨™ã€‚å°æ–¼ Wikipedia QAï¼Œæˆ‘å€‘é‡æ’ç”± Qwen3-Embedding-8B æª¢ç´¢çš„å‰ 50 å€‹å€™é¸ï¼›å°æ–¼ Story QAï¼Œæˆ‘å€‘é‡æ’ç”± SFT-Embedding-8B æª¢ç´¢çš„å‰ 50 å€‹å€™é¸ã€‚DetectiveQA åˆ†æ•¸æ˜¯åœ¨è‹±æ–‡å’Œä¸­æ–‡é›†åˆä¸Šçš„å¹³å‡å€¼ã€‚Overall åˆ—å ±å‘Šåœ¨å››å€‹è³‡æ–™é›†ä¸Šå¹³å‡çš„ avg@3/avg@5/avg@10ã€‚ç²—é«”æ•¸å­—è¡¨ç¤ºæ¯ä¸€åˆ—çš„æœ€ä½³çµæœã€‚âˆ— ç‚ºäº†å…¬å¹³èµ·è¦‹ï¼Œæ‰€æœ‰é‡æ’å™¨éƒ½æ¡ç”¨å–®æ¬¡åŸ·è¡Œé€²è¡Œè©•ä¼°ã€‚</p>
<p>è¡¨ 2ï¼šLoCoMo ä¸Šçš„æª¢ç´¢å’Œé‡æ’æ•ˆèƒ½ã€‚</p>
<h2 id="5">5 å¯¦é©—è¨­ç½®</h2>
<h3 id="51">5.1 è³‡æ–™é›†</h3>
<p>ç‚ºäº†åœ¨å¤šç¨®æª¢ç´¢è¨­ç½®ä¸­è©•ä¼° QRRankerï¼Œæˆ‘å€‘åœ¨æ¶µè“‹ç¶­åŸºç™¾ç§‘å¤šè·³å•ç­”ã€é•·æ–‡æœ¬æ•…äº‹å•ç­”å’Œå°è©±è¨˜æ†¶çš„åŸºæº–ä¸Šé€²è¡Œå¯¦é©—ã€‚</p>
<p>ç¶­åŸºç™¾ç§‘å¤šè·³å•ç­”</p>
<p>å°æ–¼åŸºæ–¼äº‹å¯¦çš„å¤šè·³æª¢ç´¢ï¼Œæˆ‘å€‘åœ¨ HotpotQA Yang et al. ( 2018 ) å’Œ MuSiQue Trivedi et al. ( 2022 ) ä¸Šé€²è¡Œè©•ä¼°ã€‚ç‚ºç¢ºä¿å…¬å¹³æ¯”è¼ƒï¼Œæˆ‘å€‘æ¡ç”¨ HippoRAG Guti'errez et al. ( 2025 ) æä¾›çš„èªæ–™åº«å’Œæ¸¬è©¦åˆ†å‰²ï¼Œä¿æŒå€™é¸æ®µè½æ± çš„ä¸€è‡´æ€§ã€‚</p>
<p>é•·æ–‡æœ¬æ•…äº‹å•ç­”</p>
<p>æˆ‘å€‘åˆ©ç”¨éœ€è¦åœ¨æ“´å±•æ–‡æœ¬ä¸Šé€²è¡Œè¤‡é›œæ¨ç†çš„è³‡æ–™é›†ï¼Œå…·é«”åŒ…æ‹¬ï¼š
(1) HELMET åŸºæº–ä¸­çš„ NarrativeQA Yen et al. ( 2024 )ï¼ŒåŒ…å« 1,272 å€‹å•é¡Œï¼Œæœ€é•·æ–‡ä»¶é” 518k ä»¤ç‰Œã€‚
(2) DetectiveQA Xu et al. ( 2025b ) æ˜¯ä¸€å€‹é›™èªåµæ¢æ•…äº‹è³‡æ–™é›†ï¼Œå¹³å‡é•·åº¦è¶…é 100k ä»¤ç‰Œï¼Œéœ€è¦åœ¨åˆ†æ•£çš„æƒ…ç¯€é»ä¸Šé€²è¡Œç²¾ç¢ºè­‰æ“šå®šä½ã€‚</p>
<p>é•·æ–‡æœ¬å°è©±è¨˜æ†¶</p>
<p>æˆ‘å€‘åœ¨ LoCoMo Maharana et al. ( 2024 ) ä¸Šè©•ä¼°æˆ‘å€‘çš„æ¨¡å‹ï¼Œé€™æ˜¯ä¸€å€‹ç‚ºé•·æ–‡æœ¬å°è©±è¨˜æ†¶è€Œè¨­è¨ˆçš„å¤§è¦æ¨¡åŸºæº–ã€‚è©²è³‡æ–™é›†åŒ…å«è·¨ 10 å€‹ä¸åŒä½¿ç”¨è€…ç¾¤çµ„çš„ 50 å€‹å¤šå ´æ™¯å°è©±ï¼Œæ¯å€‹å°è©±å¹³å‡ç´„ 9,000 ä»¤ç‰Œã€‚éµå¾ªå…ˆå‰çš„å·¥ä½œï¼Œæˆ‘å€‘å ±å‘Šå››å€‹ç´°ç²’åº¦é¡åˆ¥çš„æ€§èƒ½è¡¨ç¾ï¼šå–®è·³ã€å¤šè·³ã€æ™‚é–“æ¨ç†å’Œé–‹æ”¾é ˜åŸŸã€‚</p>
<h3 id="52">5.2 åŸºæº–</h3>
<p>æˆ‘å€‘å°‡ QRRanker èˆ‡å»£æ³›çš„æª¢ç´¢å’Œè¨˜æ†¶æ¡†æ¶é€²è¡Œè©•ä¼°ã€‚</p>
<p>å°æ–¼ç¶­åŸºç™¾ç§‘å•ç­”å’Œé•·æ–‡æœ¬æ•…äº‹ä»»å‹™ä¸Šçš„é€šç”¨é‡æ’åºï¼Œæˆ‘å€‘å°‡ QRRanker èˆ‡å…©é¡æ¨¡å‹é€²è¡Œæ¯”è¼ƒï¼š
(1) åµŒå…¥æ¨¡å‹ï¼šQwen3-Embedding (4B/8B) Zhang et al. ( 2025b ) å’Œ SFT-Embedding-8Bï¼Œå¾Œè€…æ˜¯åœ¨æˆ‘å€‘æ§‹å»ºçš„è³‡æ–™ä¸Šå¾ Qwen3-Embedding-8B å¾®èª¿è€Œä¾†ã€‚
(2) é‡æ’åºæ–¹æ³•ï¼šHippoRAG Jimenez Gutierrez et al. ( 2024 ); Guti'errez et al. ( 2025 )ã€GroupRank-32B Sun et al. ( 2025 )ã€Qwen3-Reranker-4Bï¼ˆé–‹ç®±å³ç”¨ï¼‰Zhang et al. ( 2025b ) ä»¥åŠåœ¨èˆ‡æˆ‘å€‘çš„ QRRanker ç›¸åŒè³‡æ–™ä¸Šè¨“ç·´çš„ Qwen3-Reranker-4B è®Šé«”ã€‚æˆ‘å€‘ä¹Ÿå°‡æœªè¨“ç·´çš„ QRHead ç´å…¥ç‚ºåŸºæº–ã€‚</p>
<p>å°æ–¼ LoCoMo ä¸Šçš„é•·æ–‡æœ¬å°è©±ä»»å‹™ï¼Œæˆ‘å€‘å°‡ QRRanker èˆ‡ä¸€ç³»åˆ—å¼·å¤§çš„åŸºæº–é€²è¡Œæ¯”è¼ƒï¼ŒåŒ…æ‹¬ï¼šA-Mem Xu et al. ( 2025a )ã€MemoryOS Li et al. ( 2025b )ã€Zep Rasmussen et al. ( 2025 )ã€Mem0 Chhikara et al. ( 2025 )ã€Nemori Nan et al. ( 2025 ) å’Œ LightMem Fang et al. ( 2025 )ï¼›TiMem Li et al. ( 2026 )ã€Synapse Jiang et al. ( 2026 )ã€Membox Tao et al. ( 2026 )ã€CompassMem Hu et al. ( 2026b ) å’Œ ES-Mem Zou et al. ( 2026 )ï¼›SimpleMem Liu et al. ( 2026 )ã€‚è©³ç´°çš„åŸºæº–æè¿°è¦‹é™„éŒ„ Bã€‚</p>
<h3 id="53-implementation-details">5.3 Implementation Details</h3>
<p>Our QRRanker is trained on Qwen3 - 4B - Instruct - 2507 , with QR heads selected as described in Appendix C . In the training process, the s â€‹ c â€‹ a â€‹ l â€‹ e scale factor in the max-min norm is set to 8; the batch size is set to 1; the gradient accumulating step is set to 4; the learning rate is set to 1e-5. We utilize the DeepSpeed ZERO2 strategy and train QRRanker using 8 H20 GPUs.</p>
<p>For downstream QA evaluation, we use task-specific prompting for generation; the full prompt templates for NarrativeQA, DetectiveQA, and LoCoMo are provided in Appendix A .
We employ Qwen3-8B as the generator for NarrativeQA and DetectiveQA, where books are chunked into non-overlapping passages of âˆ¼ \sim 200 tokens.
For the LoCoMo benchmark, we utilize GPT-4o-mini and GPT-5-mini as the generators. We segment the dialogue history into small chunks, ensuring that utterance continuity is preserved, with an average chunk size of 258 tokens. When enabling the memory-aware setting, we prepend a summary prefix before the ranked chunk list. We cap the summary prefix at 512 tokens and select summaries based on their coverage of the retrieved/reranked chunks.</p>
<h2 id="6-results">6 Results</h2>
<p>Table 3: Comparison with SOTA Memory and Agent frameworks on the LoCoMo. Results marked with â€  \dagger are derived from ES-Mem Zou et al. ( 2026 ) . For QRRanker, we rerank the top-50 chunks retrieved by SFT-Emb-8B and utilize only the top-3 chunks as context for generation, without additional memory mechanisms. â€˜â€“â€™ indicates the metric is not reported in the corresponding paper.</p>
<h3 id="61">6.1 ä¸»è¦çµæœ</h3>
<p>æˆ‘å€‘é€²è¡Œäº†è·¨è¶Šä¸‰å€‹ä¸åŒé ˜åŸŸçš„å»£æ³›å¯¦é©—ï¼šWikipedia å¤šè·³å•ç­”ã€é•·æ–‡æœ¬æ•…äº‹å•ç­”å’Œå°è©±è¨˜æ†¶ã€‚é€™äº›å¯¦é©—æ¶µè“‹äº”å€‹è‹±æ–‡å’Œä¸­æ–‡è³‡æ–™é›†ã€‚è¡¨ 1 å’Œè¡¨ 2 å±•ç¤ºäº†æŒ‰ Recall@k æ¸¬é‡çš„æ•´é«”é‡æ’æ€§èƒ½ã€‚çµæœè¡¨æ˜ï¼Œæˆ‘å€‘æå‡ºçš„ QRRanker åœ¨æ‰€æœ‰è³‡æ–™é›†ä¸Šéƒ½èƒ½ä¸€è‡´åœ°é”åˆ°æœ€ä½³æ€§èƒ½ã€‚å®ƒå¤§å¹…å„ªæ–¼ç´”åµŒå…¥å¼æª¢ç´¢ã€å¼·åŸºç·šæ¨¡å‹å¦‚ Qwen-Reranker å’Œæ™®é€šç¾æˆ QRHeads è®Šé«”ã€‚æ­¤å¤–ï¼Œæˆ‘å€‘åœ¨è¡¨ 3 å’Œè¡¨ 4 ä¸­è©•ä¼°äº†åœ¨ Narrative QA å’Œå°è©±è¨˜æ†¶ä¸Šçš„ä¸‹æ¸¸ç”Ÿæˆä»»å‹™ã€‚åœ¨é€™äº›ä»»å‹™ä¸­ï¼Œæˆ‘å€‘çš„ QRRanker ç”¢ç”Ÿäº†æŒçºŒçš„æ”¹é€²ï¼Œä¸¦å±•ç¾äº†å¾æª¢ç´¢é‡æ’åˆ°çµ‚ç«¯ä»»å‹™çš„å¼·å¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p>é‡æ’æ€§èƒ½ã€‚</p>
<p>æˆ‘å€‘é¦–å…ˆåˆ†æ QRRanker åœ¨é‡æ’åµŒå…¥å¼æ–¹æ³•æª¢ç´¢çš„å‰ 50 å€‹å€™é¸çµæœæ™‚çš„æª¢ç´¢æ•ˆæœã€‚å¦‚è¡¨ 1 æ‰€ç¤ºï¼ŒQRRanker å»ºç«‹äº†æ–°çš„æœ€å…ˆé€²åŸºæº–ã€‚å®ƒå¤§å¹…è¶…è¶Šå¼·åŸºç·šæ¨¡å‹ Qwen-Reranker-4Bï¼Œä¸¦é¡¯è‘—æé«˜äº†å¹³å‡å¬å›ç‡ã€‚åœ¨ Musique å’Œ HotpotQA ç­‰ Wikipedia è³‡æ–™é›†ä¸Šï¼ŒQRRanker è¶…è¶Šäº†è¤‡é›œçš„åŸºæ–¼åœ–çš„æ–¹æ³•ï¼Œå¦‚ HippoRAGï¼ˆGutiÃ© rrez et al. (2025)ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå„˜ç®¡æ¨¡å‹è¼•é‡å¾—å¤šï¼Œå®ƒé‚„è¶…è¶Šäº† GroupRank-32B çš„æ€§èƒ½ã€‚é€™è¡¨æ˜æˆ‘å€‘çš„æ–¹æ³•æ¯”ç°¡å–®çš„åˆ†çµ„è©•åˆ†æˆ–åœ–éæ­·æ›´æœ‰æ•ˆåœ°æ•æ‰æ®µè½é–“çš„ä¾è³´é—œä¿‚ã€‚ç‰¹åˆ¥æ˜¯åœ¨æ•…äº‹é ˜åŸŸï¼Œæ€§èƒ½å·®è·å°¤å…¶æ˜é¡¯ï¼Œå…¶ä¸­ä¸Šä¸‹æ–‡è¿½è¹¤è‡³é—œé‡è¦ã€‚ä¾‹å¦‚ï¼ŒQRRanker åœ¨ NarrativeQA ä¸Šé”åˆ° 54.93 çš„ Recall@10ï¼Œè€Œ GroupRank ç‚º 48.83ï¼Œæ™®é€š QRHeads ç‚º 48.89ã€‚æœ€å¾Œï¼Œåœ¨ LoCoMoï¼ˆè¡¨ 2ï¼‰ä¸Šï¼ŒQRRanker ä¿æŒç›¸åŒçš„å„ªå‹¢ï¼Œè¡¨æ˜å…¶åœ¨å¾é•·å°è©±æ­·å²ä¸­æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡çš„æœ‰æ•ˆæ€§ã€‚</p>
<p>é•·æ–‡æœ¬æ•…äº‹å•ç­”æ€§èƒ½ã€‚</p>
<p>è¡¨ 4ï¼šNarrativeQA å’Œ DetectiveQA ä¸Šçš„å•ç­”æ€§èƒ½ã€‚æ‰€æœ‰æ–¹æ³•å‡ä½¿ç”¨ R@3 æª¢ç´¢çš„æ–‡æœ¬å¡Šä½œç‚ºç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼ˆQwen3-8B ä½œç‚ºç”Ÿæˆå™¨ï¼‰ã€‚</p>
<p>é«˜è³ªé‡çš„æª¢ç´¢æ‡‰è©²è½‰åŒ–ç‚ºæ”¹é€²çš„ç”Ÿæˆæº–ç¢ºåº¦ã€‚æˆ‘å€‘åœ¨æ•˜äº‹ç†è§£è³‡æ–™é›†ä¸Šé€²è¡Œäº†è©•ä¼°ã€‚å¦‚è¡¨ 4 æ‰€ç¤ºï¼ŒQRRanker é¡¯è‘—æ”¹é€²äº†ä¸‹æ¸¸å•ç­”æ€§èƒ½ã€‚åœ¨ NarrativeQA ä¸Šï¼Œå®ƒé”åˆ° 33.61 F1ï¼Œè¶…è¶Šäº†è¨“ç·´éçš„ Qwen3-Reranker-4Bï¼ˆ30.51ï¼‰ã€‚åœ¨ DetectiveQA ä¸Šï¼Œæº–ç¢ºåº¦å¾ SFT-Embedding-8B çš„ 62.85 æå‡åˆ° QRRanker çš„ 67.25ã€‚
é€™äº›çµæœè¡¨æ˜ QRRanker é¸æ“‡çš„è­‰æ“šä¸åƒ…åœ¨èªç¾©ä¸Šç›¸é—œï¼Œè€Œä¸”æ›´å¥½åœ°ç¬¦åˆç­”æ¡ˆç”Ÿæˆæ‰€éœ€çš„æ¨ç†ã€‚</p>
<p>å°è©±è¨˜æ†¶æ€§èƒ½</p>
<p>å¦‚è¡¨ 3 æ‰€ç¸½çµï¼ŒQRRanker åœ¨ LoCoMo ä¸Šå±•ç¤ºäº†å“è¶Šçš„æ•ˆç‡ï¼Œåœ¨è¼¸å…¥é ç®—æ¥µå…¶ç·Šæ¹Šçš„æƒ…æ³ä¸‹é”åˆ°äº†æœ€ä½³çš„æ•´é«” F1ã€‚é€šéç›´æ¥ä½¿ç”¨åŸå§‹å°è©±æ­·å²ä¸­å¹³å‡åƒ… 854 å€‹ä»¤ç‰Œï¼ˆå‰ 3 å€‹æ–‡æœ¬å¡Šï¼‰ï¼Œæˆ‘å€‘çš„æ–¹æ³•åœ¨ä½¿ç”¨ GPT-4o-mini æ™‚é”åˆ° 57.03 çš„æ•´é«” F1ï¼Œä½¿ç”¨ GPT-5-mini æ™‚é”åˆ° 57.32ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¨±å¤šè¨˜æ†¶å¢å¼·æ¡†æ¶éœ€è¦å¯¦è³ªä¸Šæ›´å¤§çš„é ç®—ä¾†ç¶­è­·é¡¯å¼è¨˜æ†¶å­˜å„²æˆ–åœ–ã€‚æˆ‘å€‘çš„æ–¹æ³•å‰‡æ˜¯å°åµŒå…¥å¼æª¢ç´¢å™¨æª¢ç´¢çš„å‰ 50 å€‹æ–‡æœ¬å¡Šé€²è¡Œé‡æ’ï¼Œä¸¦åƒ…å°‡å‰æ’åçš„å°‘é‡åŸå§‹å°è©±æ–‡æœ¬å¡Šä¾›çµ¦ç”Ÿæˆå™¨ã€‚é€™ç¨®è¼•é‡ç´šè¨­è¨ˆä¿æŒäº†é«˜æ¨ç†æ•ˆç‡å’Œä½ç³»çµ±è¤‡é›œæ€§ï¼ŒåŒæ™‚ä»èƒ½æ•æ‰é•·è·é›¢ä¾è³´é—œä¿‚ï¼Œåœ¨æˆ‘å€‘çš„æ¯”è¼ƒä¸­ç”¢ç”Ÿäº† LoCoMo ä¸Šä¹‹å‰å ±å‘Šçµæœä¸­æœ€é«˜çš„æ•´é«” F1ã€‚</p>
<h3 id="62-results-with-contextual-information">6.2 Results with Contextual Information</h3>
<p>Table 5: Recall@3 comparison of QRRanker with chunk-only inputs versus a summary prefix (+Sum) as contextual memory. Î” \Delta indicates the absolute change after adding the summary prefix.</p>
<p>As shown in Table 5 , equipping QRRanker with a summary prefix consistently improves ranking performance across long-dialogue and long-context story benchmarks. This suggests that the summary provides global contextual guidance, complementing the fine-grained evidence from retrieved chunks.
Moreover, we test summary-based memory on Wikipedia-based multi-hop QA. We build a hierarchical clustering tree over retrieved passages and use parent summaries as the prefix. However, this strategy brings no gains and can even degrade performance, suggesting that abstracted global summaries are less helpful when evidence is highly localized in Wikipedia passages.</p>
<h3 id="63-results-with-heads-from-different-layer-levels">6.3 Results with Heads from Different Layer- Levels</h3>
<p>QRRanker uses static preset heads, which invokes our curiosity about the heads from which level of layers are suitable as starters for QR training. We propose a variant that dynamically selects heads from a range of continuous layers for every sample. The variant totally picks up 16 heads from layer l s l_{s} to l e l_{e} with 16 / ( l e âˆ’ l s ) 16/(l_{e}-l_{s}) heads per layer, where l s l_{s} - l e l_{e} determines the level of layers ( i.e. , low, middle, high). Details of the variant are elaborated in Appendix D . We train and evaluate both QRRanker and its variants on the NarrativeQA dataset.</p>
<p>Table 6: Retrieval performance on NarrativeQA of QRRanker and its variants adapted on different levels of layers. l s âˆ’ l e l_{s}-l_{e} denotes the layers with head selection.</p>
<p>As shown in Tab. 6 , training models with lower layers 10-17 shows a significant performance drop, while middle layers 17-24 and top layer 28-35 almost keep the same performance as QRRanker. Intuitively, lower layers truncate too much knowledge from higher layers, and heads in the middle-to-top layers are more likely to be retrievers. The outcome aligns with the phenomenon that QR heads in QRRanker are all positioned in the middle layers (17-24). Interestingly, we compare QR heads in QRRanker with those selected by the variant (17-24), and the degree of overlap is pretty low. It indicates that, with QR training, such potential is activated, which shows that our method can utilize the robustness of heads, even not QR heads, from the middle to the top. This provides a way to only focus on heads in the middle and truncate the higher layers for a smaller and faster ranker. We quantify the inference efficiency benefits of this middle-layer truncation in Section 6.4 .</p>
<h3 id="64">6.4 æ¨ç†æ•ˆç‡</h3>
<p>æˆ‘å€‘é€²ä¸€æ­¥èª¿æŸ¥äº†æˆ‘å€‘çš„æ–¹æ³•åœ¨ä¸€çµ„ 20 å€‹æŸ¥è©¢ä¸Šç›¸æ¯”æ–¼åŸºç·šæ–¹æ³•çš„è¨ˆç®—æ•ˆç‡ã€‚
å¦‚è¡¨ 7 æ‰€ç¤ºï¼ŒQRRanker ç›¸æ¯”æ–¼ Qwen3-Reranker-4B å¯¦ç¾äº†æ›´ä½çš„ P50/P95 å»¶é²ï¼ŒåŒæ™‚ä¹Ÿæ¸›å°‘äº†è¨ˆç®—é‡ï¼ˆTFLOPsï¼‰å’Œå³°å€¼è¨˜æ†¶é«”ã€‚æ­¤å¤–ï¼ŒQRRanker(middle) é€šéåœ¨ç¬¬ 24 å±¤ä¹‹å¾Œæˆªæ–·æ¨¡å‹ä¸¦ä¸Ÿæ£„æ›´é«˜å±¤ï¼Œé€²ä¸€æ­¥æ”¹é€²äº†æ•ˆç‡ã€‚å®ƒä»¥é¡å¤–æ¸›å°‘è¨ˆç®—é‡å’Œè¨˜æ†¶é«”çš„æ–¹å¼å¯¦ç¾äº†æœ€ä½³çš„ P50/P95 å»¶é²ã€‚å°æ–¼ Qwen3-Reranker-4Bï¼Œæˆ‘å€‘å ±å‘Šäº†å…©ç¨®æ¨ç†è¨­ç½®ã€‚åœ¨ batch=50 æ™‚ï¼Œæ‰€æœ‰ 50 å€‹ chunkâ€“query å°åœ¨å–®æ¬¡å‰å‘å‚³æ’­ä¸­è¢«è™•ç†ã€‚åœ¨ batch=1 æ™‚ï¼Œ50 å€‹å°é€šé 50 æ¬¡ç¨ç«‹çš„å‰å‘å‚³æ’­è¢«è™•ç†ï¼Œé€™å¤§å¤§å¢åŠ äº†å»¶é²ã€‚ç¸½çš„ä¾†èªªï¼ŒQRRanker æä¾›äº†æ›´å¥½çš„æ€§èƒ½å’Œæˆæœ¬æ¬Šè¡¡ï¼Œè€Œæˆªæ–·çš„ä¸­å±¤è®Šé«”æä¾›äº†ä¸€å€‹ç‰¹åˆ¥è¼•é‡ä¸”å¿«é€Ÿçš„é¸é …ã€‚</p>
<p>è¡¨ 7ï¼šå»¶é²ï¼ˆP50/P95ï¼‰ã€è¨ˆç®—é‡ï¼ˆæ¯å€‹æŸ¥è©¢çš„ TFLOPsï¼‰å’Œå³°å€¼ GPU è¨˜æ†¶é«”ä¸­çš„æ¨ç†æ•ˆç‡æ¯”è¼ƒã€‚æ‰€æœ‰æ¨¡å‹åœ¨ç›¸åŒçš„ç¡¬é«”å’Œæ¨ç†è¨­ç½®ä¸‹å° 20 å€‹æŸ¥è©¢é€²è¡Œè©•ä¼°ã€‚å°æ–¼ Qwen3-Reranker-4Bï¼Œbatch=50 åœ¨å–®æ¬¡å‰å‘å‚³æ’­ä¸­è™•ç† 50 å€‹ chunkâ€“query å°ï¼Œè€Œ batch=1 é€šé 50 æ¬¡ç¨ç«‹çš„å‰å‘å‚³æ’­è™•ç† 50 å€‹å°ã€‚QRRanker(middle) åœ¨ç¬¬ 24 å±¤ä¹‹å¾Œæˆªæ–·æ¨¡å‹ã€‚</p>
<h2 id="7">7 çµè«–</h2>
<p>åœ¨æœ¬è«–æ–‡ä¸­ï¼Œæˆ‘å€‘å‘ˆç¾äº† QRRankerï¼Œä¸€å€‹å»ºç«‹åœ¨ LLM ä¸­ Query-focused Retrievalï¼ˆQRï¼‰é ­éƒ¨åŸºç¤ä¸Šçš„è¼•é‡ç´šä¸”é«˜æ•ˆçš„åˆ—è¡¨å¼é‡æ–°æ’åºæ¡†æ¶ã€‚é€šéé¡¯å¼è¨“ç·´é¸å®šçš„ QR é ­éƒ¨é€²è¡Œæ’åºï¼ŒQRRanker ç”¢ç”Ÿå¯¦å€¼é—œè¯æ€§åˆ†æ•¸ï¼Œä¸¦åœ¨æ¨ç†æ™‚åŸ·è¡Œé‡æ–°æ’åºè€Œç„¡éœ€ç”Ÿæˆã€‚
åœ¨è·¨è¶Šç¶­åŸºç™¾ç§‘å¤šè·³å•ç­”ã€é•·ä¸Šä¸‹æ–‡æ•…äº‹å•ç­”å’Œå°è©±è¨˜æ†¶çš„äº”å€‹è³‡æ–™é›†ä¸Šï¼ŒQRRanker æŒçºŒæ”¹é€²é‡æ–°æ’åºå“è³ªå’Œä¸‹æ¸¸å•ç­”æ€§èƒ½ã€‚QRRanker ä»ç„¶å¯¦ç”¨æ–¼å°å‹éª¨å¹¹ç¶²è·¯ï¼ˆä¾‹å¦‚ 4Bï¼‰ï¼Œä¸¦æä¾›æ¸…æ™°çš„æ¨ç†æ•ˆç‡å„ªå‹¢ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æ´ç°¡å–®çš„æ“´å±•ï¼Œä¾‹å¦‚ç”¨æ–¼å…¨å±€ä¸Šä¸‹æ–‡çš„å¯é¸æ‘˜è¦å‰ç¶´å’Œç”¨æ–¼é€²ä¸€æ­¥æé«˜æ•ˆç‡çš„ä¸­å±¤é ­éƒ¨é¸æ“‡ã€‚</p>
<h2 id="_2">åƒè€ƒæ–‡ç»</h2>
<ul>
<li>Y. Babakhin, R. Osmulski, R. Ak, G. Moreira, M. Xu, B. Schifferer, B. Liu, and E. Oldridge (2025) Llama-embed-nemotron-8b: a universal text embedding model for multilingual and cross-lingual tasks . External Links: 2511.07025 , Link Cited by: Â§1 .</li>
<li>J. Chen, S. Xiao, P. Zhang, K. Luo, D. Lian, and Z. Liu (2024) M3-embedding: multi-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge distillation . In Findings of the Association for Computational Linguistics, ACL 2024, æ›¼è°·ï¼Œæ³°åœ‹åŠè™›æ“¬æœƒè­°ï¼Œ2024å¹´8æœˆ11-16æ—¥ , L. Ku, A. Martins, and V. Srikumar (Eds.) , Findings of ACL , Vol. ACL 2024 , pp. 2318â€“2335 . External Links: Link , Document Cited by: Â§2 .</li>
<li>P. Chhikara, D. Khant, S. Aryan, T. Singh, and D. Yadav (2025) Mem0: building production-ready ai agents with scalable long-term memory . arXiv preprint arXiv:2504.19413 . Cited by: 4th item , Â§2 , Â§5.2 , Table 3 .</li>
<li>J. Fang, X. Deng, H. Xu, Z. Jiang, Y. Tang, Z. Xu, S. Deng, Y. Yao, M. Wang, S. Qiao, et al. (2025) Lightmem: lightweight and efficient memory-augmented generation . arXiv preprint arXiv:2510.18866 . Cited by: 8th item , Â§5.2 , Table 3 .</li>
<li>W. Fedus, B. Zoph, and N. Shazeer (2022) Switch transformers: scaling to trillion parameter models with simple and efficient sparsity . J. Mach. Learn. Res. 23 , pp. 120:1â€“120:39 . External Links: Link Cited by: Appendix D .</li>
<li>B. J. Gutierrez, Y. Shu, W. Qi, S. Zhou, and Y. Su (2025) From rag to memory: non-parametric continual learning for large language models . In arXiv.org , External Links: Document Cited by: Â§1 , Â§5.1 , Â§5.2 , Â§6.1 .</li>
<li>C. Hu, X. Gao, Z. Zhou, D. Xu, Y. Bai, X. Li, H. Zhang, T. Li, C. Zhang, L. Bing, et al. (2026a) EverMemOS: a self-organizing memory operating system for structured long-horizon reasoning . arXiv preprint arXiv:2601.02163 . Cited by: Â§1 , Â§2 .</li>
<li>Y. Hu, J. Liu, J. Tan, Y. Zhu, and Z. Dou (2026b) Memory matters more: event-centric memory as a logic map for agent searching and reasoning . arXiv preprint arXiv:2601.04726 . Cited by: 1st item , Â§2 , Â§5.2 , Table 3 .</li>
<li>H. Jiang, J. Chen, Y. Pan, L. Chen, W. You, Y. Zhou, R. Zhang, Y. Abate, and T. Liu (2026) SYNAPSE: empowering llm agents with episodic-semantic memory via spreading activation . arXiv preprint arXiv:2601.02744 . Cited by: 1st item , Â§2 , Â§5.2 , Table 3 .</li>
<li>B. Jimenez Gutierrez, Y. Shu, Y. Gu, M. Yasunaga, and Y. Su (2024) Hipporag: neurobiologically inspired long-term memory for large language models . Advances in Neural Information Processing Systems 37 , pp. 59532â€“59569 . Cited by: Â§5.2 .</li>
<li>O. Khattab and M. Zaharia (2020) Colbert: efficient and effective passage search via contextualized late interaction over bert . In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , pp. 39â€“48 . Cited by: Â§4.2 .</li>
<li>G. Koch, R. Zemel, R. Salakhutdinov, et al. (2015) Siamese neural networks for one-shot image recognition . In ICML deep learning workshop , Vol. 2 , pp. 1â€“30 . Cited by: Â§2 .</li>
<li>T. KoÄiská»³, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis, and E. Grefenstette (2018) The narrativeqa reading comprehension challenge . Transactions of the Association for Computational Linguistics 6 , pp. 317â€“328 . External Links: Link Cited by: Â§1 , Â§4.1.1 .</li>
<li>K. Li, X. Yu, Z. Ni, Y. Zeng, Y. Xu, Z. Zhang, X. Li, J. Sang, X. Duan, X. Wang, et al. (2026) TiMem: temporal-hierarchical memory consolidation for long-horizon conversational agents . arXiv preprint arXiv:2601.02845 . Cited by: 1st item , Â§2 , Â§5.2 , Table 3 .</li>
<li>Y. Li, J. Li, Z. Lin, Z. Zhou, J. Wu, W. Wang, J. Zhou, and M. Yu (2025a) Mindscape-aware retrieval augmented generation for improved long context understanding . arXiv preprint arXiv:2512.17220 . Cited by: Â§1 , Â§2 , Â§4.1.1 , 5 .</li>
<li>Z. Li, C. Xi, C. Li, D. Chen, B. Chen, S. Song, S. Niu, H. Wang, J. Yang, C. Tang, et al. (2025b) Memos: a memory os for ai system . arXiv preprint arXiv:2507.03724 . Cited by: 6th item , Â§1 , Â§2 , Â§5.2 , Table 3 .</li>
<li>L. Lin, J. Fu, P. Liu, Q. Li, Y. Gong, J. Wan, F. Zhang, Z. Wang, D. Zhang, and K. Gai (2024) Just ask one more time! self-agreement improves reasoning of language models in (almost) all scenarios . In Findings of the Association for Computational Linguistics: ACL 2024 , pp. 3829â€“3852 . Cited by: Â§1 .</li>
<li>J. Liu, Y. Su, P. Xia, S. Han, Z. Zheng, C. Xie, M. Ding, and H. Yao (2026) SimpleMem: efficient lifelong memory for llm agents . arXiv preprint arXiv:2601.02553 . Cited by: 1st item , Â§5.2 , Table 3 .</li>
<li>W. Liu, X. Ma, W. Sun, Y. Zhu, Y. Li, D. Yin, and Z. Dou (2025a) Reasonrank: empowering passage ranking with strong reasoning ability . arXiv preprint arXiv:2508.07050 . Cited by: Â§1 , Â§2 .</li>
<li>X. Liu, T. Chen, L. Da, C. Chen, Z. Lin, and H. Wei (2025b) Uncertainty quantification and confidence calibration in large language models: a survey . In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2 , pp. 6107â€“6117 . Cited by: Â§1 .</li>
<li>X. Ma, X. Zhang, R. Pradeep, and J. Lin (2023) Zero-shot listwise document reranking with a large language model . CoRR abs/2305.02156 . External Links: Link , Document , 2305.02156 Cited by: Â§2 .</li>
<li>A. Maharana, D. Lee, S. Tulyakov, M. Bansal, F. Barbieri, and Y. Fang (2024) Evaluating very long-term conversational memory of llm agents . In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 13851â€“13870 . Cited by: Â§1 , Â§5.1 .</li>
<li>J. Nan, W. Ma, W. Wu, and Y. Chen (2025) Nemori: self-organizing agent memory inspired by cognitive science . arXiv preprint arXiv:2508.03341 . Cited by: 5th item , Â§2 , Â§5.2 , Table 3 .</li>
<li>R. Pradeep, S. Sharifymoghaddam, and J. Lin (2023a) Rankvicuna: zero-shot listwise document reranking with open-source large language models . arXiv preprint arXiv:2309.15088 . Cited by: Â§2 , Â§4 .</li>
<li>R. Pradeep, S. Sharifymoghaddam, and J. Lin (2023b) RankZephyr: effective and robust zero-shot listwise reranking is a breeze! . arXiv preprint arXiv:2312.02724 . Cited by: Â§1 , Â§2 .</li>
<li>X. Qin, J. Bai, J. Li, Z. Jia, and Z. Zheng (2025) TongSearch-qr: reinforced query reasoning for retrieval . CoRR abs/2506.11603 . External Links: Link , Document , 2506.11603 Cited by: Â§2 .</li>
<li>Z. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, L. Yan, J. Shen, T. Liu, J. Liu, D. Metzler, X. Wang, and M. Bendersky (2024) Large language models are effective text rankers with pairwise ranking prompting . In Findings of the Association for Computational Linguistics: NAACL 2024ï¼Œå¢¨è¥¿å“¥åŸï¼Œå¢¨è¥¿å“¥ï¼Œ2024å¹´6æœˆ16-21æ—¥ , K. Duh, H. GÃ³mez-Adorno, and S. Bethard (Eds.) , Findings of ACL , Vol. NAACL 2024 , pp. 1504â€“1518 . External Links: Link , Document Cited by: Â§2 .</li>
<li>P. Rasmussen, P. Paliychuk, T. Beauvais, J. Ryan, and D. Chalef (2025) Zep: a temporal knowledge graph architecture for agent memory . arXiv preprint arXiv:2501.13956 . Cited by: 7th item , Â§1 , Â§2 , Â§5.2 , Table 3 .</li>
<li>D. Sun, M. Long, D. Yang, Y. Jiao, Z. Tan, J. Feng, J. Wang, Y. Shen, P. Wei, J. Wang, et al. (2025) GroupRank: a groupwise reranking paradigm driven by reinforcement learning . arXiv preprint arXiv:2511.11653 . Cited by: Â§1 , Â§2 , Â§5.2 .</li>
<li>W. Sun, L. Yan, X. Ma, S. Wang, P. Ren, Z. Chen, D. Yin, and Z. Ren (2023) Is chatgpt good at search? investigating large language models as re-ranking agents . In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023ï¼Œæ–°åŠ å¡ï¼Œ2023å¹´12æœˆ6-10æ—¥ , H. Bouamor, J. Pino, and K. Bali (Eds.) , pp. 14918â€“14937 . External Links: Link , Document Cited by: Â§2 .</li>
<li>D. Tao, G. Ma, Y. Huang, and M. Jiang (2026) Membox: weaving topic continuity into long-range memory for llm agents . arXiv preprint arXiv:2601.03785 . Cited by: 3rd item , Â§2 , Â§5.2 , Table 3 .</li>
<li>N. Thakur, N. Reimers, J. Daxenberger, and I. Gurevych (2021) Augmented SBERT: data augmentation method for improving bi-encoders for pairwise sentence scoring tasks . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021ï¼Œç·šä¸Šï¼Œ2021å¹´6æœˆ6-11æ—¥ , K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-TÃ¼r, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou (Eds.) , pp. 296â€“310 . External Links: Link , Document Cited by: Â§2 .</li>
<li>H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal (2022) â™« MuSiQue: multihop questions via single-hop question composition . Transactions of the Association for Computational Linguistics 10 , pp. 539â€“554 . Cited by: Â§1 , Â§4.1.1 , Â§5.1 .</li>
<li>O. Weller, M. Boratko, I. Naim, and J. Lee (2025) On the theoretical limitations of embedding-based retrieval . arXiv preprint arXiv:2508.21038 . Cited by: Â§1 .</li>
<li>W. Wu, Y. Wang, G. Xiao, H. Peng, and Y. Fu (2024) Retrieval head mechanistically explains long-context factuality . ArXiv abs/2404.15574 . External Links: Link Cited by: Â§1 , Â§2 , Â§3 .</li>
<li>W. Xu, Z. Liang, K. Mei, H. Gao, J. Tan, and Y. Zhang (2025a) A-mem: agentic memory for llm agents . arXiv preprint arXiv:2502.12110 . Cited by: Â§2 , Â§5.2 , Table 3 .</li>
<li>Z. Xu, J. Ye, X. Liu, X. Liu, T. Sun, Z. Liu, Q. Guo, L. Li, Q. Liu, X. Huang, and X. Qiu (2025b) DetectiveQA: evaluating long-context reasoning on detective novels . In Workshop on Reasoning and Planning for Large Language Models , External Links: Link Cited by: Â§1 , Â§5.1 .</li>
<li>A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, et al. (2025) Qwen3 technical report . arXiv preprint arXiv:2505.09388 . Cited by: Â§3 .</li>
<li>Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning (2018) HotpotQA: a dataset for diverse, explainable multi-hop question answering . In Proceedings of the 2018 conference on empirical methods in natural language processing , pp. 2369â€“2380 . Cited by: Â§1 , Â§5.1 .</li>
<li>H. Yen, T. Gao, M. Hou, K. Ding, D. Fleischer, P. Izsak, M. Wasserblat, and D. Chen (2024) Helmet: how to evaluate long-context language models effectively and thoroughly . arXiv preprint arXiv:2410.02694 . Cited by: Â§5.1 .</li>
<li>W. Zhang, F. Yin, H. Yen, D. Chen, and X. Ye (2025a) Query-focused retrieval heads improve long-context reasoning and re-ranking . arXiv preprint arXiv:2506.09944 . Cited by: Â§1 , Â§2 , Â§3 , Â§3 , Â§4 .</li>
<li>X. Zhang, Y. Zhang, D. Long, W. Xie, Z. Dai, J. Tang, H. Lin, B. Yang, P. Xie, F. Huang, M. Zhang, W. Li, and M. Zhang (2024) MGTE: generalized long-context text representation and reranking models for multilingual text retrieval . In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: EMNLP 2024 - Industry Trackï¼Œé‚é˜¿å¯†ï¼Œç¾åœ‹ä½›ç¾…é‡Œé”å·ï¼Œ2024å¹´11æœˆ12-16æ—¥ , F. Dernoncourt, D. Preotiuc-Pietro, and A. Shimorina (Eds.) , pp. 1393â€“1412 . External Links: Link , Document Cited by: Â§2 .</li>
<li>Y. Zhang, M. Li, D. Long, X. Zhang, H. Lin, B. Yang, P. Xie, A. Yang, D. Liu, J. Lin, et al. (2025b) Qwen3 embedding: advancing text embedding and reranking through foundation models . arXiv preprint arXiv:2506.05176 . Cited by: Â§1 , Â§1 , Â§2 , Â§2 , Â§5.2 .</li>
<li>X. Zhao, X. Hu, Z. Shan, S. Huang, Y. Zhou, X. Zhang, Z. Sun, Z. Liu, D. Li, X. Wei, et al. (2025) Kalm-embedding-v2: superior training techniques and data inspire a versatile embedding model . arXiv preprint arXiv:2506.20923 . Cited by: Â§1 .</li>
<li>S. Zhuang, X. Ma, B. Koopman, J. Lin, and G. Zuccon (2025) Rank-r1: enhancing reasoning in llm-based document rerankers via reinforcement learning . CoRR abs/2503.06034 . External Links: Link , Document , 2503.06034 Cited by: Â§2 .</li>
<li>H. Zou, T. Sun, C. He, Y. Tian, Z. Li, L. Jin, N. Liu, J. Zhong, and K. Wei (2026) ES-mem: event segmentation-based memory for long-term dialogue agents . arXiv preprint arXiv:2601.07582 . Cited by: 2nd item , Â§2 , Â§5.2 , Table 3 , Table 3 .</li>
</ul>
<h2 id="a">é™„éŒ„ A æç¤ºè©ç¯„æœ¬</h2>
<h3 id="a1">A.1 åŸºæ–¼å€å¡Šçš„æ‘˜è¦ç”Ÿæˆæç¤ºè©</h3>
<h3 id="a2">A.2 äº‹ä»¶ä¸­å¿ƒæ‘˜è¦ç”Ÿæˆ</h3>
<h3 id="a3-qrranker">A.3 QRRanker æŒ‡ä»¤ç¯„æœ¬</h3>
<h3 id="a4-locomo">A.4 LoCoMo å•ç­”æç¤ºè©</h3>
<h3 id="a5-narrativeqa">A.5 NarrativeQA æç¤ºè©</h3>
<h3 id="a6-detectiveqa">A.6 DetectiveQA æç¤ºè©</h3>
<h2 id="b-locomo">é™„éŒ„ B LoCoMo åŸºæº–æ–¹æ³•</h2>
<p>æˆ‘å€‘åœ¨ LoCoMo ä¸Šå°‡ QRRanker èˆ‡ä¸€çµ„è¨˜æ†¶å¢å¼·åŸºæº–æ–¹æ³•é€²è¡Œæ¯”è¼ƒã€‚ä»¥ä¸‹æ˜¯å„æ–¹æ³•çš„ç°¡è¦èªªæ˜ã€‚</p>
<ul>
<li>â€¢ TiMem Li et al. ( 2026 ) : ä½¿ç”¨æ™‚é–“éšå±¤çµæ§‹çµ„ç¹”è¨˜æ†¶ï¼Œä»¥æœ‰æ•ˆæª¢ç´¢é•·æœŸè³‡è¨Šã€‚</li>
</ul>
<p>TiMem Li et al. ( 2026 ) : ä½¿ç”¨æ™‚é–“éšå±¤çµæ§‹çµ„ç¹”è¨˜æ†¶ï¼Œä»¥æœ‰æ•ˆæª¢ç´¢é•·æœŸè³‡è¨Šã€‚</p>
<ul>
<li>â€¢ SimpleMem Liu et al. ( 2026 ) : å°‡å°è©±æ­·å²å£“ç¸®ç‚ºç·Šæ¹Šçš„èªç¾©è¨˜æ†¶ï¼Œä»¥æ¸›å°‘å†—é¤˜æ€§å’Œä¸Šæ–‡é•·åº¦ã€‚</li>
</ul>
<p>SimpleMem Liu et al. ( 2026 ) : å°‡å°è©±æ­·å²å£“ç¸®ç‚ºç·Šæ¹Šçš„èªç¾©è¨˜æ†¶ï¼Œä»¥æ¸›å°‘å†—é¤˜æ€§å’Œä¸Šæ–‡é•·åº¦ã€‚</p>
<ul>
<li>â€¢ SYNAPSE Jiang et al. ( 2026 ) : å°‡è¨˜æ†¶å»ºæ¨¡ç‚ºå‹•æ…‹åœ–ï¼Œä¸¦é€éæ“´æ•£æ¿€æ´»æª¢ç´¢ç›¸é—œé …ç›®ã€‚</li>
</ul>
<p>SYNAPSE Jiang et al. ( 2026 ) : å°‡è¨˜æ†¶å»ºæ¨¡ç‚ºå‹•æ…‹åœ–ï¼Œä¸¦é€éæ“´æ•£æ¿€æ´»æª¢ç´¢ç›¸é—œé …ç›®ã€‚</p>
<ul>
<li>â€¢ CompassMem Hu et al. ( 2026b ) : å°‡äº’å‹•åˆ†æ®µæˆäº‹ä»¶ï¼Œä¸¦æ§‹å»ºäº‹ä»¶ç´šçµæ§‹ä»¥æŒ‡å°æª¢ç´¢å’Œæ¨ç†ã€‚</li>
<li>â€¢ ES-Mem Zou et al. ( 2026 ) : ä½¿ç”¨äº‹ä»¶åˆ†æ®µç‚ºå°è©±ä»£ç†å»ºç«‹é€£è²«çš„é•·æœŸè¨˜æ†¶ã€‚</li>
<li>â€¢ Membox Tao et al. ( 2026 ) : å°‡å°è©±æ‰“åŒ…ç‚ºä¸»é¡Œä¸€è‡´çš„è¨˜æ†¶å–®å…ƒï¼Œä»¥åœ¨é•·ä¸Šæ–‡ä¸­ä¿æŒä¸»é¡Œé€£çºŒæ€§ã€‚</li>
<li>â€¢ Mem0 Chhikara et al. ( 2025 ) : ä¸€å€‹ã€Œä»¥è¨˜æ†¶ç‚ºä¸­å¿ƒã€çš„æ¶æ§‹ï¼Œå‹•æ…‹æå–ã€æ•´åˆå’Œæª¢ç´¢ä¾†è‡ªå°è©±çš„é‡è¦è³‡è¨Šï¼Œä»¥å»ºç«‹å’Œç¶­è­·å¯æ“´å±•çš„é•·æœŸè¨˜æ†¶ã€‚</li>
<li>â€¢ Nemori Nan et al. ( 2025 ) : æ¡ç”¨é›™æ­¥å°é½ŠåŸå‰‡å°‡å°è©±æµçµæ§‹åŒ–ç‚ºèªç¾©é€£è²«çš„äº‹ä»¶åˆ†æ®µï¼Œä¸¦åˆ©ç”¨é æ¸¬-æ ¡æº–åŸå‰‡ä¸»å‹•å¾é æ¸¬åå·®ä¸­å­¸ç¿’ï¼Œå¯¦ç¾çŸ¥è­˜çš„è‡ªé©æ‡‰æ¼”åŒ–ã€‚</li>
<li>â€¢ MemoryOS Li et al. ( 2025b ) : ä¸€å€‹å—ä½œæ¥­ç³»çµ±å•Ÿç™¼çš„ AI è¨˜æ†¶ç³»çµ±ï¼Œå…·æœ‰å„²å­˜ã€æ›´æ–°ã€æª¢ç´¢å’Œç”Ÿæˆæ¨¡çµ„çš„éšå±¤æ¶æ§‹ã€‚å®ƒé€é FIFO å°è©±éˆå’ŒåŸºæ–¼ç†±åº¦çš„åˆ†æ®µåˆ†é ä¾†å„ªåŒ–å‹•æ…‹æ›´æ–°ã€‚</li>
<li>â€¢ Zep Rasmussen et al. ( 2025 ) : åˆ©ç”¨å‹•æ…‹ä¸”æ™‚é–“æ„ŸçŸ¥çš„çŸ¥è­˜åœ–è­œå¼•æ“ï¼Œå°‡éçµæ§‹åŒ–å°è©±æ•¸æ“šèˆ‡çµæ§‹åŒ–æ¥­å‹™æ•¸æ“šæ•´åˆï¼ŒåŒæ™‚ä¿ç•™å…¶æ­·å²é—œä¿‚ã€‚</li>
<li>â€¢ LightMem Fang et al. ( 2025 ) : ä¸€å€‹å—èªçŸ¥å•Ÿç™¼çš„æ¶æ§‹ï¼Œå…·æœ‰æ„Ÿè¦ºå’ŒçŸ­æœŸæ¨¡çµ„ï¼Œç”¨æ–¼è¼•é‡ç´šå£“ç¸®å’Œæ•´åˆã€‚ç¨ç‰¹ä¹‹è™•åœ¨æ–¼å®ƒåœ¨ã€Œç¡çœ æ™‚é–“ã€æœŸé–“æ›´æ–°é•·æœŸè¨˜æ†¶ï¼Œä»¥å°‡éå›ºèˆ‡ç·šä¸Šæ¨ç†è§£è€¦ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é–“å–å¾—å¹³è¡¡ã€‚</li>
</ul>
<p>CompassMem Hu et al. ( 2026b ) : å°‡äº’å‹•åˆ†æ®µæˆäº‹ä»¶ï¼Œä¸¦æ§‹å»ºäº‹ä»¶ç´šçµæ§‹ä»¥æŒ‡å°æª¢ç´¢å’Œæ¨ç†ã€‚</p>
<p>ES-Mem Zou et al. ( 2026 ) : ä½¿ç”¨äº‹ä»¶åˆ†æ®µç‚ºå°è©±ä»£ç†å»ºç«‹é€£è²«çš„é•·æœŸè¨˜æ†¶ã€‚</p>
<p>Membox Tao et al. ( 2026 ) : å°‡å°è©±æ‰“åŒ…ç‚ºä¸»é¡Œä¸€è‡´çš„è¨˜æ†¶å–®å…ƒï¼Œä»¥åœ¨é•·ä¸Šæ–‡ä¸­ä¿æŒä¸»é¡Œé€£çºŒæ€§ã€‚</p>
<p>Mem0 Chhikara et al. ( 2025 ) : ä¸€å€‹ã€Œä»¥è¨˜æ†¶ç‚ºä¸­å¿ƒã€çš„æ¶æ§‹ï¼Œå‹•æ…‹æå–ã€æ•´åˆå’Œæª¢ç´¢ä¾†è‡ªå°è©±çš„é‡è¦è³‡è¨Šï¼Œä»¥å»ºç«‹å’Œç¶­è­·å¯æ“´å±•çš„é•·æœŸè¨˜æ†¶ã€‚</p>
<p>Nemori Nan et al. ( 2025 ) : æ¡ç”¨é›™æ­¥å°é½åŸå‰‡å°‡å°è©±æµçµæ§‹åŒ–ç‚ºèªç¾©é€£è²«çš„äº‹ä»¶åˆ†æ®µï¼Œä¸¦åˆ©ç”¨é æ¸¬-æ ¡æº–åŸå‰‡ä¸»å‹•å¾é æ¸¬åå·®ä¸­å­¸ç¿’ï¼Œå¯¦ç¾çŸ¥è­˜çš„è‡ªé©æ‡‰æ¼”åŒ–ã€‚</p>
<p>MemoryOS Li et al. ( 2025b ) : ä¸€å€‹å—ä½œæ¥­ç³»çµ±å•Ÿç™¼çš„ AI è¨˜æ†¶ç³»çµ±ï¼Œå…·æœ‰å„²å­˜ã€æ›´æ–°ã€æª¢ç´¢å’Œç”Ÿæˆæ¨¡çµ„çš„éšå±¤æ¶æ§‹ã€‚å®ƒé€é FIFO å°è©±éˆå’ŒåŸºæ–¼ç†±åº¦çš„åˆ†æ®µåˆ†é ä¾†å„ªåŒ–å‹•æ…‹æ›´æ–°ã€‚</p>
<p>Zep Rasmussen et al. ( 2025 ) : åˆ©ç”¨å‹•æ…‹ä¸”æ™‚é–“æ„ŸçŸ¥çš„çŸ¥è­˜åœ–è­œå¼•æ“ï¼Œå°‡éçµæ§‹åŒ–å°è©±æ•¸æ“šèˆ‡çµæ§‹åŒ–æ¥­å‹™æ•¸æ“šæ•´åˆï¼ŒåŒæ™‚ä¿ç•™å…¶æ­·å²é—œä¿‚ã€‚</p>
<p>LightMem Fang et al. ( 2025 ) : ä¸€å€‹å—èªçŸ¥å•Ÿç™¼çš„æ¶æ§‹ï¼Œå…·æœ‰æ„Ÿè¦ºå’ŒçŸ­æœŸæ¨¡çµ„ï¼Œç”¨æ–¼è¼•é‡ç´šå£“ç¸®å’Œæ•´åˆã€‚ç¨ç‰¹ä¹‹è™•åœ¨æ–¼å®ƒåœ¨ã€Œç¡çœ æ™‚é–“ã€æœŸé–“æ›´æ–°é•·æœŸè¨˜æ†¶ï¼Œä»¥å°‡éå›ºèˆ‡ç·šä¸Šæ¨ç†è§£è€¦ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é–“å–å¾—å¹³è¡¡ã€‚</p>
<h2 id="appendix-c-qr-heads-for-qwen3-4b-instruct-2507">Appendix C QR Heads for Qwen3-4B-Instruct-2507</h2>
<p>We compute the QR scores of all attention heads in Qwen3 - 4B - Instruct - 2507 using 1000 random samples from NarrativeQA. The top 16 heads with the largest QR scores are selected as QR heads for retrieval and further training. As Qwen3 - 4B - Instruct - 2507 contains 36 layers of 32-head self-attention, the QR heads (demonstrated as l l â€“ h h , where 0 â‰¤ l &lt; 36 0\leq l&lt;36 denotes the layer and 0 â‰¤ h &lt; 32 0\leq h&lt;32 denotes the head in this layer) are: 20-15 , 21-11 , 17-27 , 23-10 , 22-4 , 21-10 , 21-8 , 21-18 , 18-15 , 18-19 , 17-25 , 17-17 , 24-13 , 17-4 , 19-12 , 21-31 .</p>
<h2 id="appendix-d-variant-with-semi-auto-head-selection">Appendix D Variant with Semi-Auto Head Selection</h2>
<p>QRRanker statically trains and utilizes a group of precomputed QR heads. If we use a set of seed samples from another task to recompute QR scores, the QR heads may be different from the current ones. Our initial motivation for using the precomputed QR heads is that they provide a proper initialization. Along with training, heads will be forced to learn such a retrieval ability. We are curious about which part of heads are better suited to be a good starter, as QR heads do. Therefore, we propose a variant of QRRanker with semi-automatic head selection, which is limited to selecting heads from a local range of layers, but is free to choose heads from every layer for every sample.</p>
<p>We set layers for head selection ranged from l s l_{s} to l e l_{e} , where 0 &lt; l s &lt; l e â‰¤ 36 0&lt;l_{s}&lt;l_{e}\leq 36 . We restrict that the number of selected heads must equal 16 (the number of QR heads), and therefore, for simplified control, the model should select n = 16 / ( l e âˆ’ l s ) n=16/(l_{e}-l_{s}) heads per layer. To achieve selection, we follow the router technique of Mixture-of-Expert (Fedus et al. , 2022 ) and add a gate to these layers. Instead of choosing MLPs for every token, our gate chooses n n heads for a sample. For selecting heads, we concatenate a repeat question Q â€² = [ t h i n k ] Q [ / t h i n k ] Q^{{}^{\prime}}=[think]Q[/think] after the original question Q Q , where Q â€² Q^{{}^{\prime}} is used for head selection and Q Q is still for score computing. A gate of layer l i l_{i} is a linear map from the dimension 32 âˆ— d h 32*d_{h} to 32 32 , with the trainable parameter W l i âˆˆ â„ d Ã— 32 W_{l_{i}}\in\mathbb{R}^{d\times 32} . The head score is computed by:</p>
<table>
<thead>
<tr>
<th></th>
<th>S l i = q l i â‹… W l i , \displaystyle S_{l_{i}}=q_{l_{i}}\cdot W_{l_{i}},</th>
<th></th>
<th>(5)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>S l i = mean â€‹ ( softmax â€‹ ( S l i ) , d = 0 ) , \displaystyle S_{l_{i}}=\text{mean}(\text{softmax}(S_{l_{i}}),\text{d}=0),</td>
<td></td>
<td>(6)</td>
</tr>
</tbody>
</table>
<p>where q l i âˆˆ â„ | Q â€² | Ã— d q_{l_{i}}\in\mathbb{R}^{|Q^{{}^{\prime}}|\times d} is the hidden states of tokens in Q â€² Q^{{}^{\prime}} at layer l i l_{i} , d d is the dimension of the hidden state, cat ( â‹… \cdot ) is concatenating all query states along the head, mean( â‹… \cdot , d=0) is averaging the score along the number of tokens in Q â€² Q^{{}^{\prime}} , and S l i âˆˆ â„ 32 S_{l_{i}}\in\mathbb{R}^{32} is the head score. We then choose the top- n n highest head scores S l i Q = [ s h â€‹ 0 l i , â€¦ , s h â€‹ n l i ] S_{l_{i}}^{Q}=[s_{h0}^{l_{i}},...,s_{hn}^{l_{i}}] and the corresponding heads. Following MoE, S l i Q S_{l_{i}}^{Q} is normalized to 1. After picking up heads for all layers with gates, these heads participate in computing retrieval scores, and the retrieval score will be multiplied by its head score S l i Q â€‹ [ x ] , 0 &lt; x &lt; n S_{l_{i}}^{Q}[x],0&lt;x&lt;n for the purpose of backward gradients. These gates will learn to select heads for samples during the QR training.</p>
<p>åœ¨ç¬¬ 6.3 ç¯€ä¸­ï¼Œæˆ‘å€‘è¨“ç·´ QRRanker å’Œåƒ…ä½¿ç”¨ä¾†è‡ª NarrativeQA çš„è¨“ç·´è³‡æ–™çš„è®Šé«”ï¼Œä¸¦ä½¿ç”¨ NarrativeQA çš„è©•ä¼°é›†é€²è¡Œè©•ä¼°ã€‚è¨“ç·´è¶…åƒæ•¸è¨­å®šèˆ‡ç¬¬ 5.3 ç¯€ä¸­çš„ç›¸åŒã€‚æˆ‘å€‘æ¢ç´¢å¯ç”¨æ–¼é¸æ“‡å’Œè¨“ç·´é¡ä¼¼ QR çš„é ­éƒ¨çš„å±¤ã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>