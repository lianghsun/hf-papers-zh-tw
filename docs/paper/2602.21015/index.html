<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>å¾æ„ŸçŸ¥åˆ°è¡Œå‹•ï¼šè¦–è¦ºæ¨ç†çš„äº’å‹•å¼åŸºæº– â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">å¾æ„ŸçŸ¥åˆ°è¡Œå‹•ï¼šè¦–è¦ºæ¨ç†çš„äº’å‹•å¼åŸºæº–</h1>
  
  <div class="en-title">From Perception to Action: An Interactive Benchmark for Vision Reasoning</div>
  

  <div class="paper-meta">
    
    <span>Yuhao Wu, Maojia Song, Yihuai Lan, Lei Wang, Zhiqiang Hu, Yao Xiao, Heng Zhou, Weihua Zheng, Dylan Raharja, Soujanya Poria, Roy Ka-Wei Lee</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.21015" target="_blank">2602.21015</a></span>
    
    <span style="color:var(--text-muted);font-size:0.8rem;">
      ä¾†æºï¼šPDF + DotsOCR
    </span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.21015" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.21015" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.21015" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">Robotics</span><span class="tag domain">Multimodal</span><span class="tag domain">RL</span>
    <span class="tag method">Vision-Language Model</span><span class="tag method">Diffusion Model</span><span class="tag method">Physics Simulation</span>
    <span class="tag task">Vision Reasoning</span><span class="tag task">Action Planning</span><span class="tag task">Physical Constraint Understanding</span><span class="tag task">Long-horizon Manipulation</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

ç†è§£ç‰©ç†çµæ§‹å°æ–¼å…·èº«æ™ºèƒ½é«”ã€äº’å‹•å¼è¨­è¨ˆå’Œé•·è¦–é‡æ“æ§ç­‰å¯¦éš›æ‡‰ç”¨è‡³é—œé‡è¦ã€‚ç„¶è€Œï¼Œç¾æœ‰çš„è¦–è¦º-èªè¨€æ¨¡å‹ï¼ˆVLMï¼‰è©•ä¼°ä»ç„¶é›†ä¸­æ–¼çµæ§‹ç„¡é—œçš„å–®è¼ªè¨­ç½®ï¼ˆä¾‹å¦‚è¦–è¦ºå•ç­”ï¼‰ï¼Œç„¡æ³•è©•ä¼°æ™ºèƒ½é«”å°å¹¾ä½•ã€æ¥è§¸å’Œæ”¯æ’é—œä¿‚å¦‚ä½•è¯åˆåˆ¶ç´„å‹•æ…‹ç’°å¢ƒä¸­å¯èƒ½è¡Œç‚ºçš„æ¨ç†èƒ½åŠ›ã€‚ç‚ºäº†è§£æ±ºé€™ä¸€å·®è·ï¼Œæˆ‘å€‘å¼•å…¥äº†è¡Œç‚ºèˆ‡äº’å‹•å› æœå±¤ç´šï¼ˆCHAINï¼‰åŸºæº–æ¸¬è©¦ï¼Œé€™æ˜¯ä¸€å€‹äº’å‹•å¼çš„ä¸‰ç¶­ç‰©ç†é©…å‹•æ¸¬è©¦å¹³å°ï¼Œæ—¨åœ¨è©•ä¼°æ¨¡å‹æ˜¯å¦èƒ½å¤ ç†è§£ã€è¦åŠƒå’ŒåŸ·è¡ŒåŸºæ–¼ç‰©ç†ç´„æŸçš„çµæ§‹åŒ–è¡Œç‚ºåºåˆ—ã€‚CHAIN å°‡è©•ä¼°ç¯„ç–‡å¾è¢«å‹•æ„ŸçŸ¥è½‰è®Šç‚ºä¸»å‹•å•é¡Œæ±‚è§£ï¼Œæ¶µè“‹äº’é–æ©Ÿæ¢°è¬é¡Œã€ä¸‰ç¶­å †ç–Šå’ŒåŒ…è£ç­‰ä»»å‹™ã€‚æˆ‘å€‘åœ¨çµ±ä¸€çš„äº’å‹•å¼è¨­ç½®ä¸‹å°æœ€å…ˆé€²çš„ VLM å’ŒåŸºæ–¼æ“´æ•£çš„æ¨¡å‹é€²è¡Œäº†å…¨é¢ç ”ç©¶ã€‚æˆ‘å€‘çš„çµæœè¡¨æ˜ï¼Œè¡¨ç¾æœ€ä½³çš„æ¨¡å‹ä»ç„¶é›£ä»¥å…§åŒ–ç‰©ç†çµæ§‹å’Œå› æœç´„æŸï¼Œå¸¸å¸¸ç„¡æ³•ç”¢ç”Ÿå¯é çš„é•·è¦–é‡è¦åŠƒï¼Œä¸¦ä¸”ç„¡æ³•ç©©å¥åœ°å°‡æ„ŸçŸ¥åˆ°çš„çµæ§‹è½‰åŒ–ç‚ºæœ‰æ•ˆçš„è¡Œç‚ºã€‚è©²é …ç›®å¯æ–¼ https://social-ai-studio.github.io/CHAIN/ ç²å¾—ã€‚</p>
  
  <div class="abstract-en">Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents&#39; ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>