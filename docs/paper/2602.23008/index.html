<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>é€šéæ··åˆåœ¨ç­–ç•¥å’Œé›¢ç­–ç•¥æœ€å„ªåŒ–çš„æ¢ç´¢æ€§è¨˜æ†¶å¢å¼·LLMä»£ç† â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-27/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-27 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">é€šéæ··åˆåœ¨ç­–ç•¥å’Œé›¢ç­–ç•¥æœ€å„ªåŒ–çš„æ¢ç´¢æ€§è¨˜æ†¶å¢å¼·LLMä»£ç†</h1>
  
  <div class="en-title">Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization</div>
  

  <div class="paper-meta">
    
    <span>Zeyuan Liu, Jeonghye Kim, Xufang Luo, Dongsheng Li, Yuqing Yang</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.23008" target="_blank">2602.23008</a></span>
    
    <span style="color:var(--text-muted);font-size:0.8rem;">
      ä¾†æºï¼šarxiv HTML
    </span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.23008" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.23008" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.23008" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span><span class="tag domain">RL</span>
    <span class="tag method">Memory-Augmented LLM</span><span class="tag method">On-Policy Optimization</span><span class="tag method">Off-Policy Optimization</span><span class="tag method">Hybrid RL</span>
    <span class="tag task">Agent</span><span class="tag task">Exploration</span><span class="tag task">Decision Making</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

æ¢ç´¢ä»ç„¶æ˜¯ä½¿ç”¨å¼·åŒ–å­¸ç¿’è¨“ç·´çš„å¤§å‹èªè¨€æ¨¡å‹ä»£ç†çš„é—œéµç“¶é ¸ã€‚é›–ç„¶å…ˆå‰çš„æ–¹æ³•åˆ©ç”¨é è¨“ç·´çŸ¥è­˜ï¼Œä½†å®ƒå€‘åœ¨éœ€è¦ç™¼ç¾æ–°ç©ç‹€æ…‹çš„ç’°å¢ƒä¸­è¡¨ç¾ä¸ä½³ã€‚æˆ‘å€‘æå‡º Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPOÂ²)ï¼Œä¸€å€‹æ··åˆå¼·åŒ–å­¸ç¿’æ¡†æ¶ï¼Œåˆ©ç”¨è¨˜æ†¶ä¾†é€²è¡Œæ¢ç´¢ï¼Œä¸¦çµåˆåŒç­–ç•¥å’Œç•°ç­–ç•¥æ›´æ–°ï¼Œä½¿å¤§å‹èªè¨€æ¨¡å‹åœ¨å…·æœ‰è¨˜æ†¶çš„æƒ…æ³ä¸‹è¡¨ç¾è‰¯å¥½ï¼ŒåŒæ™‚ä¹Ÿç¢ºä¿åœ¨æ²’æœ‰è¨˜æ†¶æ™‚çš„ç©©å¥æ€§ã€‚åœ¨ ScienceWorld å’Œ WebShop ä¸Šï¼ŒEMPOÂ² åˆ†åˆ¥ç›¸å°æ–¼ GRPO é”åˆ° 128.6% å’Œ 11.3% çš„æ”¹é€²ã€‚æ­¤å¤–ï¼Œåœ¨åˆ†å¸ƒå¤–æ¸¬è©¦ä¸­ï¼ŒEMPOÂ² å±•ç¤ºäº†å°æ–°ä»»å‹™å“è¶Šçš„é©æ‡‰æ€§ï¼Œåƒ…éœ€å°‘æ•¸å¹¾æ¬¡å¸¶æœ‰è¨˜æ†¶çš„è©¦é©—ä¸”ç„¡éœ€åƒæ•¸æ›´æ–°ã€‚é€™äº›çµæœçªå‡ºäº† EMPOÂ² ä½œç‚ºæ§‹å»ºæ›´å…·æ¢ç´¢æ€§å’Œå¯æ³›åŒ–ä¹‹å¤§å‹èªè¨€æ¨¡å‹ä»£ç†çš„æœ‰å‰æ™¯æ¡†æ¶ã€‚</p>
  
  <div class="abstract-en">Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO^2), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO^2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO^2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO^2 as a promising framework for building more exploratory and generalizable LLM-based agents.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <h1 id="llm-agent">æ¢ç´¢æ€§è¨˜æ†¶å¢å¼·å‹ LLM Agentï¼šæ··åˆåœ¨ç·šèˆ‡é›¢ç·šç­–ç•¥å„ªåŒ–</h1>
<p>æ¢ç´¢ä»ç„¶æ˜¯ä½¿ç”¨å¼·åŒ–å­¸ç¿’è¨“ç·´çš„å¤§å‹èªè¨€æ¨¡å‹ Agent çš„é—œéµç“¶é ¸ã€‚é›–ç„¶å…ˆå‰çš„æ–¹æ³•åˆ©ç”¨é è¨“ç·´çŸ¥è­˜ï¼Œä½†åœ¨éœ€è¦ç™¼ç¾æ–°ç‹€æ…‹çš„ç’°å¢ƒä¸­å»è¡¨ç¾ä¸ä½³ã€‚æˆ‘å€‘æå‡ºæ¢ç´¢æ€§è¨˜æ†¶å¢å¼·å‹åœ¨ç·šèˆ‡é›¢ç·šç­–ç•¥å„ªåŒ–ï¼ˆEMPOÂ²ï¼‰ï¼Œä¸€å€‹æ··åˆ RL æ¡†æ¶ï¼Œåˆ©ç”¨è¨˜æ†¶é€²è¡Œæ¢ç´¢ï¼Œä¸¦çµåˆåœ¨ç·šå’Œé›¢ç·šç­–ç•¥æ›´æ–°ï¼Œä½¿ LLM åœ¨ä½¿ç”¨è¨˜æ†¶æ™‚è¡¨ç¾è‰¯å¥½ï¼ŒåŒæ™‚ä¹Ÿç¢ºä¿åœ¨æ²’æœ‰è¨˜æ†¶æƒ…æ³ä¸‹çš„é­¯æ£’æ€§ã€‚åœ¨ ScienceWorld å’Œ WebShop ä¸Šï¼ŒEMPOÂ² åˆ†åˆ¥ç›¸å°æ–¼ GRPO é”åˆ°äº† 128.6% å’Œ 11.3% çš„æ”¹é€²ã€‚æ­¤å¤–ï¼Œåœ¨åˆ†ä½ˆå¤–æ¸¬è©¦ä¸­ï¼ŒEMPOÂ² å±•ç¤ºäº†å°æ–°ä»»å‹™çš„å„ªè¶Šé©æ‡‰æ€§ï¼Œåªéœ€å°‘é‡è¨˜æ†¶è©¦é©—ä¸”ç„¡éœ€åƒæ•¸æ›´æ–°ã€‚é€™äº›çµæœçªé¡¯ EMPOÂ² æ˜¯æ§‹å»ºæ›´å…·æ¢ç´¢æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„åŸºæ–¼ LLM Agent çš„æœ‰å‰æ™¯æ¡†æ¶ã€‚</p>
<h2 id="1">1 ä»‹ç´¹</h2>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23008/25_graph.png" loading="lazy"></figure> (a)</p>
<p>å¤§èªè¨€æ¨¡å‹ (LLMs) æœ€è¿‘å·²æˆç‚ºå…·æœ‰æ¨ç†ã€è¦åŠƒä»¥åŠèˆ‡å¤–éƒ¨ç’°å¢ƒäº’å‹•èƒ½åŠ›çš„å¼·å¤§ä»£ç† (Achiam et al., 2023; Park et al., 2023; Yao et al., 2023; Kim et al., 2025)ã€‚ç•¶çµåˆå¼·åŒ–å­¸ç¿’ (RL) æ™‚ï¼Œæ­¤é¡ä»£ç†å¯æ ¹æ“šç¶“é©—å’Œåé¥‹èª¿æ•´å…¶è¡Œç‚ºï¼Œä½¿å…¶è¶…è¶Šéœæ…‹æç¤ºæˆ–ç›£ç£å¾®èª¿ (Guo et al., 2025; Tan et al., 2024)ã€‚æ­¤ç¯„ä¾‹é©…å‹•äº†äº’å‹•å¼æ±ºç­–ã€å·¥å…·ä½¿ç”¨å’Œå…·èº«äººå·¥æ™ºæ…§ç­‰é ˜åŸŸçš„æœ€è¿‘é€²å±• (Feng et al., 2025b; Lu et al., 2025b; Feng et al., 2025a; Dong et al., 2025; Luo et al., 2025)ã€‚</p>
<p>ç„¶è€Œï¼Œç•¶å‰åŸºæ–¼ LLM çš„ä»£ç†çš„ä¸€å€‹é—œéµé™åˆ¶åœ¨æ–¼å®ƒå€‘ä¾è³´æ–¼åˆ©ç”¨å…ˆé©—çŸ¥è­˜ï¼Œè€Œéé€²è¡Œç³»çµ±æ€§çš„æ¢ç´¢ã€‚é›–ç„¶ RL æ¡†æ¶å¼·èª¿å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨ï¼Œä½†è¨±å¤š LLM ä»£ç†ç³»çµ±ä¸»è¦åˆ©ç”¨é è¨“ç·´çŸ¥è­˜ï¼Œä¸¦ä¸”åªåœ¨ç†Ÿæ‚‰çš„åˆ†ä½ˆå…§é€²è¡Œæœ‰é™æœç´¢ã€‚å› æ­¤ï¼Œé€™äº›ä»£ç†åœ¨éœ€è¦ç™¼ç¾æ–°ç‹€æ…‹æˆ–ä¸»å‹•ç²å–æ–°ä¿¡æ¯çš„ç’°å¢ƒä¸­å¾€å¾€è¡¨ç¾ä¸ä½³ï¼Œè€Œä¸æ˜¯é‡è¤‡ä½¿ç”¨å·²çŸ¥çš„å…§å®¹ã€‚</p>
<p>ç‚ºäº†æ‡‰å°é€™ä¸€æŒ‘æˆ°ï¼Œæœ€è¿‘çš„ç ”ç©¶å·²å°‡å¤–éƒ¨è¨˜æ†¶æ¨¡çµ„ç´å…¥ LLMsï¼Œä½œç‚ºé•·æœŸè¨˜æ†¶çš„å½¢å¼ã€‚é€™ä½¿æ¨¡å‹èƒ½å¤ åˆ©ç”¨éå»çš„ç¶“é©—ä¾†ä¿®æ­£å¤±æ•—çš„å˜—è©¦ï¼Œå¾è€Œåœ¨å¾ŒçºŒè©¦é©—ä¸­æ”¹é€²æ±ºç­–ï¼Œè€Œç„¡éœ€åƒæ•¸æ›´æ–° (Shinn et al., 2023; Zhang et al., 2023)ã€‚ç„¶è€Œï¼Œå¦‚ Zhang et al. (2023) æ‰€æŒ‡å‡ºçš„ï¼Œæ­¤é¡æ–¹æ³•çš„æ€§èƒ½å¾€å¾€æœƒè¿…é€Ÿé£½å’Œï¼Œå› ç‚ºä½¿ç”¨éœæ…‹åƒæ•¸æ”¶é›†ç¶“é©—ç„¡æ³•å®Œå…¨æ•æ‰é€£çºŒæ”¹é€²æ‰€éœ€çš„å¤šæ¨£æ€§ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23008/concept.png" loading="lazy"></figure> åœ– 2ï¼šéåƒæ•¸åŒ–æ›´æ–°å¯ä»¥é¼“å‹µæ¢ç´¢ï¼Œå¼•å°åƒæ•¸åŒ–æ›´æ–°ã€‚</p>
<p>åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹çµ±ä¸€æ¡†æ¶ï¼Œä½¿ LLM ä»£ç†èƒ½å¤ é€šéå»£æ³›æ¢ç´¢æ›´æœ‰æ•ˆåœ°å­¸ç¿’ï¼Œæ–¹æ³•æ˜¯ä½¿ç”¨ RL è¯åˆæ›´æ–°å…¶åƒæ•¸åŒ–ç­–ç•¥åƒæ•¸ï¼Œä¸¦é€šéäº’å‹•æ›´æ–°å…¶éåƒæ•¸åŒ–è¨˜æ†¶æ¨¡çµ„ã€‚è‡³é—œé‡è¦çš„æ˜¯ï¼Œéåƒæ•¸åŒ–æ›´æ–°ä¸åƒ…è£œå……ï¼Œè€Œä¸”å¢å¼·äº†åƒæ•¸åŒ–å­¸ç¿’çš„æ•ˆç‡ï¼Œå¾è€Œå¯¦ç¾æ›´æœ‰æ•ˆçš„æ¢ç´¢å’Œé©æ‡‰ã€‚æ­¤é›™æ›´æ–°ç¯„ä¾‹å……ç•¶äº†åƒæ•¸ç´šå„ªåŒ–å’Œè¨˜æ†¶æ“´å¢æ¨ç†ä¹‹é–“çš„æ©‹æ¨‘ã€‚é›–ç„¶åœ¨å­¸ç¿’æœŸé–“åˆ©ç”¨è¨˜æ†¶ï¼Œä½†è¦å¯¦ç¾æ›´å¯æ¦‚æ‹¬çš„æ™ºæ…§ï¼Œéœ€è¦æ¸›å°‘å°å¤–éƒ¨è¨˜æ†¶çš„ä¾è³´ï¼Œè€Œæ˜¯å°‡å…¶å¥½è™•ç›´æ¥åµŒå…¥åˆ°æ¨¡å‹çš„åƒæ•¸ä¸­ã€‚ç‚ºæ­¤ï¼Œæˆ‘å€‘æå‡º Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPOÂ²)ï¼Œä¸€ç¨®æ–°çš„æ··åˆ RL ç®—æ³•ï¼Œåœ¨æ¨å»£éšæ®µçµåˆäº†å…©ç¨®æ¨¡å¼â€”â€”å–æ±ºæ–¼æ˜¯å¦ä½¿ç”¨è¨˜æ†¶â€”â€”ä»¥åŠåœ¨æ›´æ–°éšæ®µçš„å…©ç¨®æ¨¡å¼â€”â€”åœ¨ç·šå’Œé›¢ç·šå­¸ç¿’â€”â€”å¾è€Œä½¿ä»£ç†èƒ½å¤ åœ¨æœ‰è¨˜æ†¶æ™‚åˆ©ç”¨è¨˜æ†¶ï¼ŒåŒæ™‚åœ¨æ²’æœ‰è¨˜æ†¶æ™‚ä¿æŒç©©å¥ã€‚</p>
<p>åœ¨æˆ‘å€‘çš„å¯¦é©—ä¸­ï¼Œæˆ‘å€‘åœ¨å…©å€‹å»£æ³›ä½¿ç”¨çš„å¤šæ­¥å…·èº«æ¨ç†ç’°å¢ƒä¸Šè©•ä¼° EMPOÂ²ï¼Œé€™äº›ç’°å¢ƒéœ€è¦æ¢ç´¢ä¾†è§£æ±ºè¤‡é›œä»»å‹™ï¼šScienceWorld (Wang et al., 2022) å’Œ WebShop (Yao et al., 2022)ã€‚æˆ‘å€‘å°‡å…¶æ€§èƒ½èˆ‡ä¸€ç³»åˆ—éåƒæ•¸åŒ–å’Œåƒæ•¸åŒ–ï¼ˆé›¢ç·šå’Œåœ¨ç·šï¼‰RL æ–¹æ³•é€²è¡Œæ¯”è¼ƒã€‚å¦‚åœ– 1 æ‰€ç¸½çµçš„ï¼ŒEMPOÂ² å¤§å¹…è¶…è¶Šå…ˆå‰çš„ç®—æ³•ï¼Œåœ¨ ScienceWorld ä¸Šç›¸è¼ƒå¼·å¤§çš„åœ¨ç·š RL åŸºç·š GRPO å¯¦ç¾äº† 128.6% çš„æ”¹é€²ï¼Œåœ¨ WebShop ä¸Šå¯¦ç¾äº† 11.3% çš„æ”¹é€²ã€‚åœ– 1 (a) ä¸­çš„è¨“ç·´æ›²ç·šé€²ä¸€æ­¥é¡¯ç¤ºï¼Œèˆ‡æ”¶æ–‚åˆ°æ¬¡å„ªè§£çš„ GRPO ä¸åŒï¼ŒEMPOÂ² åˆ©ç”¨æŒçºŒæ¢ç´¢ä¸¦æˆåŠŸè§£æ±ºä»»å‹™ã€‚æ­¤å¤–ï¼Œé‡å° OOD å¯¦é©—ï¼ˆåœ– 1ï¼Œæœ€å³å´ï¼‰ï¼Œæ¨¡å‹ä¹Ÿèƒ½ä»¥åƒ…æœ‰çš„å¹¾æ¬¡è©¦é©—å’Œç„¡æ¬Šé‡æ›´æ–°çš„æƒ…æ³ä¸‹é”åˆ°è‰¯å¥½åˆ†æ•¸ï¼Œè¡¨æ˜æ›´æ–°å¾Œçš„æ¨¡å‹å·²ç²å¾—ä½¿ç”¨è¨˜æ†¶æ¢ç´¢æœªè¦‹æˆ–ä¸ç†Ÿæ‚‰ç’°å¢ƒçš„èƒ½åŠ›ã€‚é€™äº›çµæœçªå‡ºäº† EMPOÂ² æ˜¯æ§‹å»ºæ›´é©æ‡‰æ€§å’Œæ›´å¯æ¦‚æ‹¬çš„å…·èº«ä»£ç†çš„æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<h2 id="2">2 é å‚™çŸ¥è­˜</h2>
<p>åœ¨ç·š RL åŒ…æ‹¬åœ¨ rollout éšæ®µå’Œæ›´æ–°éšæ®µä¹‹é–“äº¤æ›¿é€²è¡Œï¼Œåœ¨ rollout éšæ®µä¸­ä½¿ç”¨ç”± Î¸ \theta åƒæ•¸åŒ–çš„ç•¶å‰ç­–ç•¥ Ï€ \pi ç”Ÿæˆè»Œè·¡ï¼Œåœ¨æ›´æ–°éšæ®µä¸­æ ¹æ“šé€™äº› rollout å„ªåŒ–ç­–ç•¥ã€‚</p>
<p><strong>ç­–ç•¥ Rolloutã€‚</strong> æˆ‘å€‘è€ƒæ…®ä¸€å€‹è¨­ç½®ï¼Œå…¶ä¸­çµ¦å®šå¾ p â€‹ ( ğ’° ) p(\mathcal{U}) æ¡æ¨£çš„ä»»å‹™ u u ï¼Œä¸€å€‹ LLM agent é€šéèˆ‡ç’°å¢ƒçš„å¤šæ­¥äº¤äº’ä¾†è§£æ±ºä»»å‹™ã€‚å¾ä»»å‹™ u u é–‹å§‹ï¼ŒLLM Ï€ Î¸ \pi_{\theta} ç”Ÿæˆç¬¬ä¸€å€‹è‡ªç„¶èªè¨€å‹•ä½œ a 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ u ) âˆˆ ğ’œ a_{1}\sim\pi_{\theta}(\cdot\mid u)\in\mathcal{A} ã€‚åŸ·è¡Œæ­¤å‹•ä½œå¾Œï¼Œç’°å¢ƒè¿”å›çå‹µ r 1 r_{1} å’Œä¸‹ä¸€å€‹ç‹€æ…‹ s 1 s_{1} ã€‚åœ¨ä¸€èˆ¬æ™‚åˆ» t t ï¼Œä»¥ç•¶å‰ç‹€æ…‹ s t s_{t} å’Œä»»å‹™ u u ç‚ºæ¢ä»¶ï¼Œç­–ç•¥ç”¢ç”Ÿä¸‹ä¸€å€‹å‹•ä½œ a t + 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ s t , u ) a_{t+1}\sim\pi_{\theta}(\cdot\mid s_{t},u) ã€‚æ­¤äº¤äº’å¾ªç’°æŒçºŒé€²è¡Œï¼Œç›´åˆ°ä»»å‹™å®Œæˆæˆ–é”åˆ°æœ€å¤§æ­¥æ•¸ã€‚rollout è»Œè·¡å› æ­¤å®šç¾©ç‚ºç‹€æ…‹ã€å‹•ä½œå’Œçå‹µçš„åºåˆ—ï¼ŒÏ„ = ( u , a 1 , r 1 , s 1 , a 2 , r 2 , â€¦ , s T ) . \tau=\big(u,a_{1},r_{1},s_{1},a_{2},r_{2},\ldots,s_{T}\big).</p>
<p><strong>ç¾¤çµ„ç›¸å°ç­–ç•¥å„ªåŒ–ã€‚</strong> ç¾¤çµ„ç›¸å°ç­–ç•¥å„ªåŒ– (Group Relative Policy Optimization, GRPO) (Shao et al. , 2024 ) é€šéæ¯”è¼ƒåŒä¸€ä»»å‹™ u u çš„å¤šå€‹ rollout ä¾†æ›´æ–°ç­–ç•¥ï¼Œæ¶ˆé™¤äº† PPO (Schulman et al. , 2017 ) ä¸­å°åƒ¹å€¼å‡½æ•¸çš„éœ€æ±‚ã€‚çµ¦å®šä»»å‹™ u u ï¼Œç­–ç•¥ Ï€ Î¸ \pi_{\theta} ç”Ÿæˆ N N æ¢ rollout è»Œè·¡ { Ï„ ( 1 ) , â€¦ , Ï„ ( N ) } {\tau^{(1)},\ldots,\tau^{(N)}} ã€‚æ¯æ¢è»Œè·¡ç²å¾—å›å ± { R ( 1 ) , â€¦ , R ( N ) } {R^{(1)},\ldots,R^{(N)}} ï¼Œå®šç¾©ç‚ºæ²¿è‘—è»Œè·¡çš„çå‹µä¹‹å’Œï¼šR ( i ) = âˆ‘ t = 1 T r t ( i ) . R^{(i)}=\sum_{t=1}^{T}r_{t}^{(i)}. ã€‚å°æ–¼åœ¨è»Œè·¡ Ï„ ( i ) \tau^{(i)} ä¸­æ¡å–çš„æ¯å€‹å‹•ä½œ a t ( i ) a_{t}^{(i)} ï¼Œæˆ‘å€‘å°‡å…¶ç›¸å°å„ªå‹¢å®šç¾©ç‚ºï¼šA â€‹ ( a t ( i ) ) = R ( i ) âˆ’ 1 N â€‹ âˆ‘ j = 1 N R ( j ) Ïƒ â€‹ ( R ) , A(a_{t}^{(i)})=\frac{R^{(i)}-\frac{1}{N}\sum_{j=1}^{N}R^{(j)}}{\sigma(R)}, å…¶ä¸­ä¾†è‡ªçå‹µé«˜æ–¼å¹³å‡å€¼è»Œè·¡çš„å‹•ä½œç²å¾—æ­£å„ªå‹¢ï¼Œè€Œä¾†è‡ªè¡¨ç¾è¼ƒå·®è»Œè·¡çš„å‹•ä½œç²å¾—è² å„ªå‹¢ã€‚GRPO æå¤±å‰‡ç‚ºï¼š</p>
<p>|  | ğ”¼ u âˆ¼ p â€‹ ( ğ’° ) { Ï„ ( i ) } i = 1 N âˆ¼ Ï€ Î¸ old \displaystyle\mathbb{E}<em>{\begin{subarray}{c}u\sim p(\mathcal{U})\
{\tau^{(i)}}</em>{i=1}^{N}\sim\pi_{\theta_{\text{old}}}\end{subarray}} | [ 1 N â€‹ T â€‹ âˆ‘ i = 1 N âˆ‘ t = 1 T min â¡ ( Ï Î¸ â€‹ ( a t ( i ) ) â€‹ A â€‹ ( a t ( i ) ) , clip â€‹ ( Ï Î¸ â€‹ ( a t ( i ) ) , 1 âˆ’ Ïµ , 1 + Ïµ ) â€‹ A â€‹ ( a t ( i ) ) ) ] \displaystyle\Bigg[\frac{1}{NT}\sum_{i=1}^{N}\sum_{t=1}^{T}\min\Big(\rho_{\theta}(a_{t}^{(i)})A(a_{t}^{(i)}),\text{clip}\big(\rho_{\theta}(a_{t}^{(i)}),1-\epsilon,1+\epsilon\big)A(a_{t}^{(i)})\Big)\Bigg] |  |
| --- | --- | --- | --- |
|  |  | âˆ’ Î² D KL ( Ï€ Î¸ ( â‹… | u ) âˆ¥ Ï€ ref ( â‹… | u ) ) , \displaystyle\quad-\beta\,D_{\text{KL}}!\big(\pi_{\theta}(\cdot|u)\,|\;\pi_{\text{ref}}(\cdot|u)\big), |  | (1) |</p>
<p>å…¶ä¸­ Ï Î¸ â€‹ ( a t ( i ) ) = Ï€ Î¸ â€‹ ( a t ( i ) | s t ( i ) , u ) Ï€ Î¸ old â€‹ ( a t ( i ) | s t ( i ) , u ) , \rho_{\theta}(a_{t}^{(i)})=\frac{\pi_{\theta}(a_{t}^{(i)}|s_{t}^{(i)},u)}{\pi_{\theta_{\text{old}}}(a_{t}^{(i)}|s_{t}^{(i)},u)}, å…¶ä¸­ Î² â‰¥ 0 \beta\geq 0 æ§åˆ¶æœå‘åƒè€ƒç­–ç•¥ Ï€ ref \pi_{\text{ref}} çš„æ­£å‰‡åŒ–å¼·åº¦ã€‚</p>
<h2 id="3-the-exploration-problem-of-llm-agents">3 The Exploration Problem of LLM Agents</h2>
<p>LLMs encode rich prior knowledge, but such priors often fail to reflect the actual rules or dynamics of a given environment. Blind reliance on these priors can lead to erroneous behaviors, making it necessary for agents to adapt through direct interaction and trial-and-error. A key requirement for such adaptation is exploration , which involves seeking information beyond pre-training, sometimes by taking atypical or counterintuitive actions. However, current LLM-based agents struggle with this (Qiao et al. , 2024 ; Zhou et al. , 2024 ) , as it demands stepping outside the distribution of behaviors where the model feels most confident.</p>
<p>Consequently, many prior studies have sought to align agents with new environments through warm-start supervised fine-tuning (SFT) using numerous golden trajectories (Song et al. , 2024 ; Qiao et al. , 2024 ; Xiang et al. , 2024 ) , leveraging large-scale models such as GPT-4 (Tang et al. , 2024 ; Lin et al. , 2023 ) , or employing human engineering or well-established simulation information (Choudhury and Sodhi, 2025 ) . While these methods achieve strong results in constrained settings, their effectiveness is limited to cases where such external support is available, and they generalize poorly to unseen scenarios without it.</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23008/exploration_problem.png" loading="lazy"></figure> Figure 3: When training LLM with GRPO in ScienceWorld, the agent struggles because of insufficient exploration. For instance, in the task â€œturn on the red light bulb,â€ the agent must first find the red light bulb before activating it. However, the agent fails to locate it and, as a result, cannot complete the task. Rather than analyzing the cause of failure and exploring alternative actions, the agent proceeds unchanged, so its score stagnates even as additional training steps are taken.</p>
<p>Therefore, we focus on how to efficiently train agents in online RL through trial and error, without any prior embedding of the environmentâ€™s rules. The key challenge is that, without intrinsic exploration capability, online RL struggles to optimize effectively. As illustrated in Figure 3 , in ScienceWorld (Wang et al. , 2022 ) environment the agent is given the mission â€œturn on the red light bulb.â€ The instructions specify that the agent should first focus on the light bulb and then build a circuit to activate it, based on the current room observation. However, since no red light bulb is present in the observation, the agent must search the environment to locate it. Instead, the agent follows the instruction literally, attempts to focus on the red light bulb, and fails because it does not exist in the room. Ideally, when an agent fails to reach its goal, it should analyze the reasons for failure and broaden its action space to discover successful strategies. Yet in representative online RL algorithms GRPO (Shao et al. , 2024 ) , prior trajectory rollouts provide no continuity beyond a scalar reward signal, thereby restricting exploration and ultimately limiting learning.</p>
<h2 id="4-method">4 Method</h2>
<p>In this section, we present Exploratory Memory-augmented On- and Off-Policy Optimization (EMPO 2 ), a novel algorithm aimed at tackling the exploration challenges in online RL. EMPO 2 operates in two modes for both rollout phase and update phase. During rollout, actions can be generated either through (1) prompting without memory , where no retrieved information is used, or (2) memory-augmented prompting , conditioned on tips retrieved from memory. In the update phase, rollouts with memory-augmented prompting are used in two ways: (a) on-policy , where tips are retained and the update is performed with the original prompt, and (b) off-policy , where tips are removed during update. Notably, tips are generated not by a separate model but by the policy Ï€ Î¸ \pi_{\theta} itself, which is continually updated during training. The full algorithm is provided in Appendix A .</p>
<h3 id="41">4.1 ä½¿ç”¨è‡ªç”Ÿæˆè¨˜æ†¶ä¿ƒé€²æ¢ç´¢</h3>
<p>EMPO 2 çš„ä¸€å€‹é—œéµçµ„ä»¶æ˜¯å…¶ä½¿ç”¨è¨˜æ†¶ä¾†ç¶­æŒè·¨æ»¾å‹•è»Œè·¡ï¼ˆrolloutsï¼‰çš„é€£çºŒæ€§ã€‚å¾æ™ºèƒ½é«”çš„äº’å‹•ä¸­ç²å¾—çš„ä¿¡æ¯å¯ä»¥é€šéç­–ç•¥å„ªåŒ–ç·¨ç¢¼åˆ°åƒæ•¸ä¸­ï¼Œä½†ä¹Ÿå¯ä»¥è¨˜éŒ„åœ¨æ™ºèƒ½é«”æŒçºŒæŸ¥é–±çš„å¤–éƒ¨è¨˜æ†¶ä¸­ã€‚ç”±æ–¼æˆ‘å€‘çš„ç­–ç•¥å¾é è¨“ç·´çš„ LLM åˆå§‹åŒ–ï¼Œè©² LLM å…·æœ‰å›ºæœ‰çš„ç¸½çµå’Œåæ€èƒ½åŠ›ï¼Œé€™äº›èƒ½åŠ›å¯ä»¥ä½œç‚ºè¼”åŠ©ä¿¡è™Ÿåˆ©ç”¨ï¼Œé™¤äº†æ¨™é‡çå‹µå¤–ï¼Œå¾è€Œæ›´æœ‰æ•ˆåœ°å¼•å°æ¢ç´¢ã€‚ç‚ºäº†å¯¦ç¾é€™ä¸€é»ï¼ŒEMPO 2 æ•´åˆäº†åƒæ•¸åŒ–ï¼ˆLLM å…§çš„åƒæ•¸æ›´æ–°ï¼‰å’Œéåƒæ•¸åŒ–ï¼ˆå¤–éƒ¨è¨˜æ†¶ï¼‰æ›´æ–°ï¼Œå¼·åŒ–äº†æ»¾å‹•è»Œè·¡ä¹‹é–“çš„è¯ç¹«ä¸¦ä¿ƒé€²æ¢ç´¢ï¼Œæ‰€æœ‰æ•¸æ“šå’ŒæŒ‡å°éƒ½ç”±æ™ºèƒ½é«”è‡ªä¸»ç”Ÿæˆã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23008/motivation3.png" loading="lazy"></figure> åœ– 4ï¼šåœ¨ EMPO 2 ä¸­ï¼Œç•¶å‰çš„ç­–ç•¥åƒæ•¸ $\pi_{\theta}$ ç”¨æ–¼å¯©æŸ¥éå»çš„æ»¾å‹•è»Œè·¡ï¼Œæ‰€ç”¢ç”Ÿçš„è¦‹è§£è¢«æ·»åŠ åˆ°è¨˜æ†¶ä¸­ã€‚é€™å€‹æ›´æ–°çš„è¨˜æ†¶å½±éŸ¿å¾ŒçºŒçš„æ»¾å‹•è»Œè·¡ä¸¦ä¿ƒé€²æ¢ç´¢ã€‚</p>
<p>åœ¨éåƒæ•¸åŒ–æ›´æ–°ä¸­ï¼Œé¡ä¼¼æ–¼ Reflexionï¼ˆShinn et al., 2023ï¼‰ï¼Œæ™ºèƒ½é«”å¯©æŸ¥éå»çš„æ»¾å‹•è»Œè·¡ï¼Œç”Ÿæˆè‡ªæˆ‘æŒ‡å°æç¤ºï¼Œä¸¦å°‡å…¶å­˜å„²åœ¨è¨˜æ†¶ä¸­ã€‚é€™äº›æç¤ºå¹«åŠ©æ™ºèƒ½é«”é¿å…é‡è¤‡çŠ¯éŒ¯ä¸¦æ¢ç´¢æ–°ç­–ç•¥ã€‚èˆ‡ Reflexion ä¸åŒï¼ŒReflexion èšç„¦æ–¼è¿­ä»£çš„èªè¨€æŒ‡å°ä»¥åœ¨ä¸‹ä¸€æ¬¡è©¦é©—ä¸­ç²å¾—æ›´é«˜çš„çå‹µï¼Œè€Œæˆ‘å€‘çš„æ–¹æ³•æ—¨åœ¨è®“é€™äº›æç¤ºå°è‡´å¢å¼·çš„æ¢ç´¢ï¼Œæœ€çµ‚é€šéåƒæ•¸åŒ–æ›´æ–°ä¾†éå›ºã€‚</p>
<p>è‡ªç”Ÿæˆè¨˜æ†¶å’Œæç¤ºã€‚æˆ‘å€‘å®šç¾©ä¸€å€‹è¨˜æ†¶ç·©è¡å€ $\mathcal{M}={\text{tip}<em>{1},\text{tip}</em>{2},\ldots}$ï¼Œå…¶å­˜å„²ç”±ç­–ç•¥ $\pi_{\theta}$ åœ¨è»Œè·¡åæ€æœŸé–“ç”Ÿæˆçš„åæ€æç¤ºã€‚æ­£å¼åœ°èªªï¼Œç•¶ä»»å‹™ $u$ çš„ç¬¬ $i$ å€‹ç–‡ï¼ˆepisodeï¼‰åœ¨æ™‚é–“æ­¥ $t$ çµ‚æ­¢æ™‚ï¼Œç­–ç•¥ä»¥æœ€çµ‚ç‹€æ…‹ $s_{t}$ å’Œæç¤ºç”Ÿæˆæç¤ºè©ï¼ˆtip-generation promptï¼‰ä½œç‚ºè¼¸å…¥ï¼Œä¸¦ç”Ÿæˆä¸€å€‹æç¤ºï¼Œå…¶ä¸­ $\text{tip}<em>{i}\sim\pi</em>{\theta}(s_{t},u,\text{tip-generation prompt})$ã€‚ä¸‹é¢æä¾›äº†ä¸€çµ„èªªæ˜æ€§ç¤ºä¾‹ï¼Œè€Œæç¤ºç”Ÿæˆæç¤ºè©å‘ˆç¾åœ¨é™„éŒ„ B ä¸­ï¼Œå…¶ä»–ç¤ºä¾‹åŒ…å«åœ¨é™„éŒ„ E.1 ä¸­ã€‚</p>
<h3 id="42-parameterize-non-parametric-updates-via-hybrid-policy-optimization">4.2 Parameterize non-parametric updates via hybrid policy optimization</h3>
<p>Agents can use memory to improve exploration and learning efficiency, but the acquired knowledge needs be internalized into model parameters to enhance inherent capabilities. To this end, we propose two modes for the rollout and update phases, whose combinations yield three hybrid learning modes (Figure 5 ).</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23008/EMPO.png" loading="lazy"></figure> Figure 5: EMPO 2 mode combinations. By combining the two rollout modes and update modes, three EMPO mode configurations are possible: on-policy learning without memory, on-policy learning with memory and off-policy learning.</p>
<p>Rollout Modes. During rollouts, the agent samples between the two modes, selecting one mode at each step: mode (2) with memory rollout probability p p and mode (1) with probability 1 âˆ’ p 1-p . The ablation study of p p can be found in Appendix F.1 .</p>
<ul>
<li>(1) Prompting Without Memory. For each task u u , at each timestep t t , the policy Ï€ Î¸ \pi_{\theta} generates actions conditioned only on the current state s t s_{t} and the task u u : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ s t , u ) . a_{t+1}\sim\pi_{\theta}(\cdot\mid s_{t},u).</li>
<li>(2) Memory-Augmented Prompting. For each task u u , at each timestep t t , a retrieval operator Retr â€‹ ( o t ; â„³ ) âŠ† â„³ \mathrm{Retr}(o_{t};\mathcal{M})\subseteq\mathcal{M} selects tips most relevant to the current state s t s_{t} , e.g., via similarity search in the embedding space. We denote the retrieved set as tips t \text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}<em>{t} . In memory-augmented prompting, the policy conditions its action on both s t s</em>{t} and tips t \text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}<em>{t} : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… | s t , u , tips t ) . a</em>{t+1}\sim\pi_{\theta}!\left(\cdot\,\middle|\,s_{t},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t}\right). We limit the number of retrieved tips at 10.</li>
</ul>
<p>Prompting Without Memory. For each task u u , at each timestep t t , the policy Ï€ Î¸ \pi_{\theta} generates actions conditioned only on the current state s t s_{t} and the task u u : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ s t , u ) . a_{t+1}\sim\pi_{\theta}(\cdot\mid s_{t},u).</p>
<p>Memory-Augmented Prompting. For each task u u , at each timestep t t , a retrieval operator Retr â€‹ ( o t ; â„³ ) âŠ† â„³ \mathrm{Retr}(o_{t};\mathcal{M})\subseteq\mathcal{M} selects tips most relevant to the current state s t s_{t} , e.g., via similarity search in the embedding space. We denote the retrieved set as tips t \text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}<em>{t} . In memory-augmented prompting, the policy conditions its action on both s t s</em>{t} and tips t \text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}<em>{t} : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… | s t , u , tips t ) . a</em>{t+1}\sim\pi_{\theta}!\left(\cdot\,\middle|\,s_{t},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t}\right). We limit the number of retrieved tips at 10.</p>
<p>Update Modes. Trajectories generated under rollout mode (1) are directly used for updates, whereas those generated under rollout mode (2)â€”memory-augmented promptingâ€”follow one of two update modes chosen at random during the update phase. Mode (b) is selected with off-policy update probability q q , and mode (a) with probability 1 âˆ’ q 1-q . The ablation study of q q can be found in Appendix F.1 .</p>
<ul>
<li>(a) On-Policy Updates. On-policy update uses the same prompt as in the rollout, and Ï Î¸ â€‹ ( a t ( i ) ) \rho_{\theta}(a_{t}^{(i)}) in eq. 1 becomes Ï Î¸ â€‹ ( a t ( i ) ) = Ï€ Î¸ â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) Ï€ Î¸ old â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) . \rho_{\theta}(a_{t}^{(i)})=\frac{\pi_{\theta}(a_{t}^{(i)}\mid s_{t}^{(i)},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}<em>{t})}{\pi</em>{\theta_{\text{old}}}(a_{t}^{(i)}\mid s_{t}^{(i)},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t})}.</li>
<li>(b) Off-Policy Updates. In this mode, the stored log-probabilities â„“ t tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u , tips t ) \ell^{\text{tips}}<em>{t}=\log\pi</em>{\theta}(a_{t}\mid s_{t},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}<em>{t}) are replaced with the log-probabilities assigned by the same policy Ï€ Î¸ \pi</em>{\theta} when conditioned only on ( s t , u ) (s_{t},u) , namely â„“ t no-tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u ) \ell^{\text{no-tips}}<em>{t}=\log\pi</em>{\theta}(a_{t}\mid s_{t},u) . In this formulation, the advantage update is performed based on how natural the action appears under the distribution without tips. This construction can be interpreted as a form of reward-guided knowledge distillation . Trajectories sampled under the tips-conditioned policy act as teacher demonstrations, while the student policy Ï€ Î¸ ( â‹… âˆ£ s , u ) \pi_{\theta}(\cdot\mid s,u) is updated to reproduce those trajectories in proportion to their advantage. High-reward trajectories ( A ^ t &gt; 0 \hat{A}<em>{t}&gt;0 ) are reinforced, while low-reward trajectories ( A ^ t &lt; 0 \hat{A}</em>{t}&lt;0 ) are suppressed, resulting in selective distillation that emphasizes beneficial behaviors. In this way, tips serve as an intermediate scaffolding mechanism that improves exploration and trajectory quality, while the reward signal ensures that only advantageous behaviors are ultimately retained. Consequently, the final policy learns to internalize the benefits of tip conditioning without requiring tips at inference time. Appendix C provides an illustrative breakdown and a summary table for the calculation of the importance sampling ratio.</li>
</ul>
<p>On-Policy Updates. On-policy update uses the same prompt as in the rollout, and Ï Î¸ â€‹ ( a t ( i ) ) \rho_{\theta}(a_{t}^{(i)}) in eq. 1 becomes Ï Î¸ â€‹ ( a t ( i ) ) = Ï€ Î¸ â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) Ï€ Î¸ old â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) . \rho_{\theta}(a_{t}^{(i)})=\frac{\pi_{\theta}(a_{t}^{(i)}\mid s_{t}^{(i)},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}<em>{t})}{\pi</em>{\theta_{\text{old}}}(a_{t}^{(i)}\mid s_{t}^{(i)},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t})}.</p>
<p>Off-Policy Updates. In this mode, the stored log-probabilities â„“ t tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u , tips t ) \ell^{\text{tips}}<em>{t}=\log\pi</em>{\theta}(a_{t}\mid s_{t},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}<em>{t}) are replaced with the log-probabilities assigned by the same policy Ï€ Î¸ \pi</em>{\theta} when conditioned only on ( s t , u ) (s_{t},u) , namely â„“ t no-tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u ) \ell^{\text{no-tips}}<em>{t}=\log\pi</em>{\theta}(a_{t}\mid s_{t},u) . In this formulation, the advantage update is performed based on how natural the action appears under the distribution without tips.</p>
<p>This construction can be interpreted as a form of reward-guided knowledge distillation . Trajectories sampled under the tips-conditioned policy act as teacher demonstrations, while the student policy Ï€ Î¸ ( â‹… âˆ£ s , u ) \pi_{\theta}(\cdot\mid s,u) is updated to reproduce those trajectories in proportion to their advantage. High-reward trajectories ( A ^ t &gt; 0 \hat{A}<em>{t}&gt;0 ) are reinforced, while low-reward trajectories ( A ^ t &lt; 0 \hat{A}</em>{t}&lt;0 ) are suppressed, resulting in selective distillation that emphasizes beneficial behaviors. In this way, tips serve as an intermediate scaffolding mechanism that improves exploration and trajectory quality, while the reward signal ensures that only advantageous behaviors are ultimately retained. Consequently, the final policy learns to internalize the benefits of tip conditioning without requiring tips at inference time. Appendix C provides an illustrative breakdown and a summary table for the calculation of the importance sampling ratio.</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23008/low_filtering.png" loading="lazy"></figure> Figure 6: Masking tokens stabilizes training.</p>
<p>Stabilizing Off-Policy Training. Off-policy training is prone to instability and may collapse (see Figure 6 ). In such cases, gradient normalization, entropy loss, KL loss, and policy gradient loss can all diverge to NaN. Prior work, Yang et al. ( 2025 ) shows that low-probability tokens destabilize training by amplifying gradient magnitudes through unbounded likelihood ratios. Motivated by this, we introduce a masking mechanism that suppresses the advantage term for tokens whose probability under Ï€ Î¸ \pi_{\theta} falls below a threshold Î´ \delta . Finally, the loss in Eq. 1 is modified as</p>
<p>|  | ğ”¼ u âˆ¼ p â€‹ ( ğ’° ) { Ï„ ( i ) } âˆ¼ Ï€ Î¸ old [ 1 N â€‹ T âˆ‘ i = 1 N âˆ‘ t = 1 T \displaystyle\mathbb{E}<em>{\begin{subarray}{c}u\sim p(\mathcal{U})\
{\tau^{(i)}}\sim\pi</em>{\theta_{\text{old}}}\end{subarray}}\Bigg[\frac{1}{NT}\sum_{i=1}^{N}\sum_{t=1}^{T} | min ( Ï Î¸ ( i , t ) A ( a t ( i ) ) , clip ( Ï Î¸ ( i , t ) , 1 âˆ’ Ïµ , 1 + Ïµ ) A ( a t ( i ) ) ) â‹… ğŸ Ï€ Î¸ â€‹ ( a t ( i ) | s t ( i ) , u ) â‰¥ Î´ ] \displaystyle\min\Big(\rho_{\theta}^{(i,t)}A(a_{t}^{(i)}),\;\text{clip}\big(\rho_{\theta}^{(i,t)},1-\epsilon,1+\epsilon\big)A(a_{t}^{(i)})\Big)\cdot\mathbf{1}<em>{\pi</em>{\theta}(a_{t}^{(i)}|s_{t}^{(i)},u)\geq\delta}\Bigg] |  |
| --- | --- | --- | --- |
|  |  | âˆ’ Î² D KL ( Ï€ Î¸ ( â‹… | u ) âˆ¥ Ï€ ref ( â‹… | u ) ) . \displaystyle\quad-\beta D_{\text{KL}}!\big(\pi_{\theta}(\cdot|u)\,|\;\pi_{\text{ref}}(\cdot|u)\big). |  | (2) |</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23008/entropy.png" loading="lazy"></figure> Figure 7: Policy entropy comparison with vs. without intrinsic rewards.</p>
<p>Intrinsic Rewards for Exploration. To further encourage exploration, and inspired by prior work on exploration-targeted online RL (Burda et al. , 2018b ; Bellemare et al. , 2016 ; Ecoffet et al. , 2019 ) , we introduce an intrinsic reward based on the novelty of the current state. A memory list stores distinct states, and for each new state we compute its cosine similarity with existing entries. If the similarity falls below a threshold, the state is added to memory and assigned a reward. The intrinsic reward is defined as r intrinsic = 1 n r_{\text{intrinsic}}=\frac{1}{n} , where n n denotes the number of similar past states. This mechanism encourages the agent to explore novel states even when no extrinsic reward is provided by the environment and maintains policy entropy, as shown in Figure 7 .</p>
<h2 id="5">5 ç›¸é—œå·¥ä½œ</h2>
<p><strong>LLM Agents åœ¨å¤šæ­¥å…·èº«ä»»å‹™ä¸­çš„æ‡‰ç”¨</strong>ã€‚LLM agents åœ¨å¤šæ­¥å…·èº«ä»»å‹™ä¸­å·²åœ¨ä¸åŒçš„ç¯„å¼ä¸‹é€²è¡Œéç ”ç©¶ã€‚è³‡æ–™é©…å‹•çš„æ–¹æ³•ï¼ˆSong et al.ï¼Œ2024ï¼›Xiong et al.ï¼Œ2024ï¼›Qiao et al.ï¼Œ2025ï¼›2024ï¼›Tajwar et al.ï¼Œ2025ï¼‰é€éæœ‰æ•ˆçš„è³‡æ–™æ”¶é›†æ–¹æ³•å’Œæ¨¡ä»¿å­¸ç¿’ä¾†å¢å¼·æ±ºç­–èƒ½åŠ›ã€‚åŸºæ–¼æ¨¡å‹çš„ agentsï¼ˆTang et al.ï¼Œ2024ï¼›Zhou et al.ï¼Œ2024ï¼‰å»ºç«‹ä¸–ç•Œæ¨¡å‹ï¼Œé€šå¸¸é€éä½¿ç”¨ GPT-4 ç­‰å¤§å‹é–‰æºç³»çµ±ç”Ÿæˆç¨‹å¼ç¢¼ã€‚å…¶ä»–æ–¹æ³•ï¼ˆLin et al.ï¼Œ2023ï¼›Choudhury and Sodhiï¼Œ2025ï¼‰é€éæ¨¡å‹è½‰ç§»æˆ–åˆ©ç”¨æ¨¡æ“¬ç’°å¢ƒæä¾›çš„ç‰¹æ®Šè³‡è¨Šä¾†åŠ å¼·æ¨ç†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘å€‘çš„æ–¹æ³•æ¸›å°‘äº†å°æ­¤é¡å¤–éƒ¨è³‡æºçš„ä¾è³´ï¼Œä¸¦å¼·èª¿é€é agent è‡ªèº«çš„æ¢ç´¢å’Œè‡ªæˆ‘æ”¹é€²ä¾†å¯¦ç¾è‡ªä¸»æˆé•·ã€‚</p>
<p><strong>LLM Agents çš„è¨˜æ†¶æ©Ÿåˆ¶</strong>ã€‚ç‚ºäº†å¯¦ç¾å¾éå¾€ç¶“é©—çš„æ¼¸é€²å¼æ”¹é€²ï¼ŒReflexionï¼ˆShinn et al.ï¼Œ2023ï¼‰å’Œ REMEMBERERï¼ˆZhang et al.ï¼Œ2023ï¼‰åˆ©ç”¨å¤–éƒ¨è¨˜æ†¶ã€‚Reflexion å„²å­˜å£é ­åæ€ä¾›å¾ŒçºŒæç¤ºä½¿ç”¨ï¼Œè€Œ REMEMBERER è¨˜éŒ„è§€å¯Ÿã€è¡Œå‹•ã€çå‹µå’Œ Q å€¼ï¼Œæª¢ç´¢é¡ä¼¼æ¡ˆä¾‹ä½œç‚ºå°‘æ¨£æœ¬ç¯„ä¾‹ã€‚é€™äº›æ–¹æ³•è¡¨æ˜ LLMs å¯ä»¥åœ¨ä¸é€²è¡Œåƒæ•¸æ›´æ–°çš„æƒ…æ³ä¸‹é€²è¡Œæ”¹é€²ã€‚ç„¶è€Œï¼Œç”±æ–¼åƒæ•¸å›ºå®šï¼Œå®ƒå€‘ç„¡æ³•æ“´å±•å…§åœ¨çŸ¥è­˜ï¼Œæ‰€ä»¥é©æ‡‰åªæ˜¯çŸ­æœŸçš„ï¼ˆZhang et al.ï¼Œ2023ï¼‰ï¼Œä¾è³´å¤–éƒ¨è¨˜æ†¶è€Œä¸æ˜¯å¯¦ç¾é•·æœŸæ¼”é€²å’Œæ³›åŒ–ã€‚</p>
<p><strong>é€éçŸ¥è­˜è’¸é¤¾é€²è¡Œå­¸ç¿’</strong>ã€‚æˆ‘å€‘çš„æ··åˆéç­–ç•¥æ›´æ–°å‡½æ•¸åœ¨ç·šä¸Šè¨“ç·´æœŸé–“å……ç•¶çå‹µå¼•å°çš„çŸ¥è­˜è’¸é¤¾ã€‚Snell et al.ï¼ˆ2022ï¼‰å¼•å…¥äº†ä¸Šä¸‹æ–‡è’¸é¤¾ï¼Œå…¶ä¸­æ¨¡å‹é¦–å…ˆä½¿ç”¨ Teacher æç¤ºï¼ˆåŒ…å«æŒ‡ç¤ºã€ç¯„ä¾‹ã€è§£é‡‹å’Œæš«å­˜æ¿æ¨ç†ï¼‰è§£æ±ºä»»å‹™ï¼Œç„¶å¾Œå­¸ç¿’é€éé›¢ç·šçš„ã€åŸºæ–¼ SFT çš„è’¸é¤¾å¾æœ€å° Student æç¤ºç”¢ç”Ÿæœ€çµ‚ç­”æ¡ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘å€‘å°‡çŸ¥è­˜è’¸é¤¾æ•´åˆåˆ°ç·šä¸Š RL ä¸­ï¼Œåˆ©ç”¨ç·šä¸Šé©æ‡‰æ€§åŒæ™‚å¢å¼·æ¢ç´¢ï¼Œä»¥å¯¦ç¾æ›´é«˜æ•ˆçš„è¨“ç·´ã€‚</p>
<p><strong>LLM Agents çš„ RL</strong>ã€‚RL ç‚ºé€éç’°å¢ƒäº¤äº’çš„è§€å¯Ÿå’Œçå‹µä¿¡è™Ÿä¾†æœ€ä½³åŒ– LLM åƒæ•¸æä¾›äº†å¥å…¨çš„æ¡†æ¶ã€‚å…ˆå‰çš„å·¥ä½œ Retrospexï¼ˆXiang et al.ï¼Œ2024ï¼‰è¡¨æ˜é›¢ç·š RLï¼ˆå¾å¤§å‹è¨˜éŒ„è³‡æ–™é›†å­¸ç¿’æœ€å„ªç­–ç•¥ï¼‰å¯ä»¥æ”¹é€² LLM agent çš„æ€§èƒ½ã€‚æœ€è¿‘çš„ç ”ç©¶å°ˆæ³¨æ–¼ç·šä¸Š RLï¼ˆShao et al.ï¼Œ2024ï¼›Feng et al.ï¼Œ2025bï¼›Wang et al.ï¼Œ2025ï¼‰ï¼Œå…¶ä¸­ agents é€²è¡Œå¯¦æ™‚å­¸ç¿’ã€‚GiGPOï¼ˆFeng et al.ï¼Œ2025bï¼‰é€éå°å…·æœ‰ç›¸ä¼¼è§€å¯Ÿçš„ rollouts é€²è¡Œåˆ†çµ„ä¾†æ¨é€² GRPOï¼Œå¯¦ç¾æ›´ç´°ç²’åº¦çš„ä¿¡ç”¨åˆ†é…å’Œæ›´å¼·çš„æ€§èƒ½ã€‚æˆ‘å€‘çš„å·¥ä½œé€éå°‡éåƒæ•¸è¨˜æ†¶æ›´æ–°æ•´åˆåˆ°ç­–ç•¥å…§å’Œç­–ç•¥å¤–å­¸ç¿’ä¸­ä¾†æ¨é€²é€™ä¸€ç·šä¸Š RL æ–¹å‘ï¼Œå¾è€Œç”¢ç”Ÿé¡¯è‘—æ›´é«˜çš„æ¨£æœ¬æ•ˆç‡ã€‚</p>
<p><strong>å¢å¼·ç·šä¸Š RL çš„æ¢ç´¢</strong>ã€‚ç·šä¸Š RL ä¸­çš„æ ¸å¿ƒæŒ‘æˆ°æ˜¯æœ‰æ•ˆçš„æ¢ç´¢ã€‚å¤å…¸æ–¹æ³•ï¼Œå¦‚è¨ˆæ•¸å‹æ¢ç´¢ï¼ˆBellemare et al.ï¼Œ2016ï¼‰å’Œéš¨æ©Ÿç¶²è·¯è’¸é¤¾ Random Network Distillationï¼ˆBurda et al.ï¼Œ2018bï¼‰ä½¿ç”¨å…§åœ¨çå‹µä¾†é¼“å‹µæ–°ç©æ€§ã€‚Go-Exploreï¼ˆEcoffet et al.ï¼Œ2019ï¼‰å„²å­˜é—œéµç‹€æ…‹ä¸¦å¾ä¸­é‡æ–°æ¢ç´¢ï¼Œè§£æ±ºäº† Atari éŠæˆ²ç­‰é›£ä»¥æ¢ç´¢çš„ä»»å‹™ã€‚å…¶ LLM æ“´å±• Intelligent Go-Exploreï¼ˆLu et al.ï¼Œ2025aï¼‰åœ¨ TextWorldï¼ˆCÃ´tÃ© et al.ï¼Œ2018ï¼‰ç­‰ç’°å¢ƒä¸­å–å¾—äº†å¼·å‹çš„æˆæœï¼Œä½†ä¾è³´æ–¼å¤§å‹é–‰æºæ¨¡å‹ä¸”ä¸åŸ·è¡Œåƒæ•¸æ›´æ–°ã€‚åœ¨æˆ‘å€‘çš„ä¸¦è¡Œå·¥ä½œä¸­ï¼ŒRLVMRï¼ˆZhang et al.ï¼Œ2025ï¼‰æ¡ç”¨æš–å•Ÿå‹• SFT ä¾†å¼•ç™¼å¤šæ¨£åŒ–çš„æ¨ç†é¡å‹ï¼ˆè¦åŠƒã€æ¢ç´¢å’Œåæ€ï¼‰ï¼Œä¸¦åœ¨ç·šä¸Š RL æœŸé–“ç‚ºæ¯ç¨®æ¨ç†é¡å‹æä¾›å¯†é›†çš„éç¨‹ç´šçå‹µï¼Œå¢å¼·æ¢ç´¢å’Œä¿¡ç”¨åˆ†é…ã€‚é€™äº›ç ”ç©¶å…±åŒå¼·èª¿äº†çµæ§‹åŒ–æ¢ç´¢å°æ–¼å°‡ RL æ“´å±•åˆ°è¤‡é›œç’°å¢ƒçš„é‡è¦æ€§ã€‚</p>
<h2 id="6">6 å¯¦é©—</h2>
<p>ç‚ºäº†æª¢é©— EMPO 2 çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘å€‘åœ¨å…©å€‹å»£æ³›ä½¿ç”¨çš„ LLM agent åŸºæº–ä¸Šé€²è¡Œäº†å¤§é‡å¯¦é©—ï¼šScienceWorldï¼ˆWang et al., 2022ï¼‰å’Œ WebShopï¼ˆYao et al., 2022ï¼‰ï¼Œä½¿ç”¨ Qwen2.5-7B-Instructï¼ˆQwen et al., 2025ï¼‰ä½œç‚ºåŸºç¤æ¨¡å‹ã€‚æˆ‘å€‘è©•ä¼°çš„ EMPO 2 æ•ˆèƒ½æ˜¯è¨“ç·´æ¨¡å‹åœ¨æ¸¬è©¦æ™‚ä¸å¸¶è¨˜æ†¶çš„æ•ˆèƒ½ã€‚</p>
<h3 id="61-scienceworld">6.1 ScienceWorld</h3>
<p>ScienceWorldï¼ˆWang et al., 2022ï¼‰æ˜¯ä¸€å€‹äº’å‹•å¼æ–‡å­—åŸºæº–ï¼Œå…¶ä¸­ agent åœ¨å°å­¸æ°´æº–åŸ·è¡Œç§‘å­¸å¯¦é©—ã€‚æˆåŠŸå®Œæˆé€™äº›å¯¦é©—éœ€è¦é•·æœŸçš„å¤šæ­¥é©Ÿè¦åŠƒã€å‡è¨­æª¢é©—å’Œçµæœè§£é‡‹ï¼Œä»¥åŠå……åˆ†çš„æ¢ç´¢ä»¥ç¢ºå®šå¿…è¦å·¥å…·çš„ä½ç½®å’Œæ‡‰æ¡å–çš„é©ç•¶è¡Œå‹•ã€‚ScienceWorld åŒ…å«ä¾†è‡ªä¸åŒä¸»é¡Œçš„ä»»å‹™ï¼Œåœ¨æˆ‘å€‘çš„å¯¦é©—ä¸­ï¼Œæˆ‘å€‘æ¶µè“‹äº† 19 é …ä»»å‹™ï¼Œè·¨è¶ŠåŒ–å­¸ã€åˆ†é¡ã€ç”Ÿç‰©å­¸ã€é›»å­¸å’Œæ¸¬é‡ã€‚</p>
<p>åŸºç·šã€‚æˆ‘å€‘å°‡ EMPO 2 èˆ‡å¹¾ç¨® RL æ–¹æ³•é€²è¡Œæ¯”è¼ƒã€‚å°æ–¼éåƒæ•¸ RLï¼ŒReflexionï¼ˆShinn et al., 2023ï¼‰ä»¥éåƒæ•¸æ–¹å¼æ›´æ–°è¨˜æ†¶ï¼Œé€éç´å…¥ä¾†è‡ªå‰åºè»Œè·¡çš„ LLM åæ€ï¼Œä¸¦åœ¨å¾ŒçºŒè©¦é©—çš„æç¤ºä¸­ä½¿ç”¨å®ƒå€‘ã€‚å°æ–¼é›¢ç·š RLï¼ŒRetrospexï¼ˆXiang et al., 2024ï¼‰åˆ©ç”¨ SFT è¨“ç·´çš„æ¨¡å‹ï¼Œä¸¦ä½¿ç”¨é€ééš±å¼ Q å­¸ç¿’ï¼ˆImplicit Q-learningï¼‰ï¼ˆKostrikov et al., 2022ï¼‰å­¸ç¿’çš„ Q å‡½æ•¸å‹•æ…‹é‡æ–°è©•åˆ†è¡Œå‹•ã€‚å®˜æ–¹ Retrospex è«–æ–‡ä½¿ç”¨äº†è¼ƒå°çš„ Flan-T5-Largeï¼ˆChung et al., 2024ï¼‰ï¼ˆ770Mï¼‰ï¼Œä¸¦ç´å…¥äº†äººå·¥è¨­è¨ˆçš„å•Ÿç™¼å¼æ–¹æ³•ä¾†åœ¨è©•ä¼°æœŸé–“å”åŠ© agentã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç‚ºäº†ç¢ºä¿æˆ‘å€‘å¯¦é©—è¨­ç½®çš„ä¸€è‡´æ€§ï¼Œæˆ‘å€‘å°‡ Retrospex çš„åŸºç¤æ¨¡å‹æ¨™æº–åŒ–ç‚º Qwen2.5-7B-Instructï¼Œä¸¦æ’é™¤é€™äº›å•Ÿç™¼å¼æ–¹æ³•ã€‚æœ€å¾Œï¼Œå°æ–¼ç·šä¸Š RLï¼Œæˆ‘å€‘å°‡ GRPOï¼ˆShao et al., 2024ï¼‰ç´å…¥ä½œç‚ºä»£è¡¨æ€§åŸºç·šã€‚æ›´å¤šç´°ç¯€åœ¨é™„éŒ„ D ä¸­æä¾›ã€‚</p>
<p>P[1]Â¿ \arraybackslash p#1</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23008/13.png" loading="lazy"></figure> è¡¨ 1ï¼šScienceWorld çš„æ¯”è¼ƒçµæœã€‚ScienceWorld ä¸­çš„æ¯é …ä»»å‹™åŒ…å«å¤šå€‹è®Šé«”ã€‚æˆ‘å€‘ä½¿ç”¨å‰äº”å€‹è®Šé«”é€²è¡Œè¨“ç·´ï¼Œä¸¦åœ¨ 20 å€‹æœªè¦‹éçš„æ¸¬è©¦è®Šé«”ä¸Šé€²è¡Œè©•ä¼°ã€‚ç²—é«”è¡¨ç¤ºæ¯é …ä»»å‹™çš„æœ€ä½³æ•ˆèƒ½ï¼Œè€Œç´…è‰²é™°å½±æ¨™è¨˜äº†åƒæ•¸æ›´æ–°å¾—åˆ†ä½æ–¼éåƒæ•¸æ›´æ–°çš„æƒ…æ³ã€‚æˆ‘å€‘è©•ä¼°çš„ EMPO 2 æ•ˆèƒ½æ˜¯è¨“ç·´æ¨¡å‹åœ¨æ¸¬è©¦æ™‚ä¸å¸¶è¨˜æ†¶çš„æ•ˆèƒ½ã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>