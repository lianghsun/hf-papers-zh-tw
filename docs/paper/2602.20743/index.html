<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>é©æ‡‰æ€§æ–‡æœ¬åŒ¿ååŒ–ï¼šé€šéæç¤ºè©å„ªåŒ–å­¸ç¿’éš±ç§-æ•ˆç”¨æ¬Šè¡¡ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">é©æ‡‰æ€§æ–‡æœ¬åŒ¿ååŒ–ï¼šé€šéæç¤ºè©å„ªåŒ–å­¸ç¿’éš±ç§-æ•ˆç”¨æ¬Šè¡¡</h1>
  
  <div class="en-title">Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization</div>
  

  <div class="paper-meta">
    
    <span>Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.20743" target="_blank">2602.20743</a></span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.20743" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.20743" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.20743" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span>
    <span class="tag method">Prompt Optimization</span><span class="tag method">Language Models</span>
    <span class="tag task">Text Anonymization</span><span class="tag task">Privacy Protection</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

æ–‡æœ¬æ–‡ä»¶çš„åŒ¿ååŒ–æ˜¯ä¸€å€‹é«˜åº¦å…·æœ‰ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§çš„å•é¡Œï¼šéš±ç§ä¿è­·èˆ‡æ•ˆç”¨ä¿å­˜ä¹‹é–“çš„é©ç•¶å¹³è¡¡å› è³‡æ–™é ˜åŸŸã€éš±ç§ç›®æ¨™å’Œä¸‹éŠæ‡‰ç”¨è€Œç•°ã€‚ç„¶è€Œï¼Œç¾æœ‰çš„åŒ¿ååŒ–æ–¹æ³•ä¾è³´æ–¼éœæ…‹çš„ã€äººå·¥è¨­è¨ˆçš„ç­–ç•¥ï¼Œç¼ºä¹éˆæ´»æ€§ä»¥é©æ‡‰å¤šæ¨£åŒ–çš„éœ€æ±‚ï¼Œä¸”å¾€å¾€ç„¡æ³•è·¨é ˜åŸŸé€²è¡Œæ¨å»£ã€‚æˆ‘å€‘å¼•å…¥äº†è‡ªé©æ‡‰æ–‡æœ¬åŒ¿ååŒ–ï¼Œé€™æ˜¯ä¸€å€‹æ–°çš„ä»»å‹™åˆ¶å®šæ–¹å¼ï¼Œå…¶ä¸­åŒ¿ååŒ–ç­–ç•¥è‡ªå‹•é©æ‡‰ç‰¹å®šçš„éš±ç§-æ•ˆç”¨éœ€æ±‚ã€‚æˆ‘å€‘æå‡ºäº†ä¸€å€‹ä»»å‹™ç‰¹å®šçš„æç¤ºæœ€ä½³åŒ–æ¡†æ¶ï¼Œè©²æ¡†æ¶è‡ªå‹•ç‚ºèªè¨€æ¨¡å‹æ§‹é€ åŒ¿ååŒ–æŒ‡ä»¤ï¼Œä½¿å…¶èƒ½å¤ é©æ‡‰ä¸åŒçš„éš±ç§ç›®æ¨™ã€é ˜åŸŸå’Œä¸‹éŠä½¿ç”¨æ¨¡å¼ã€‚ç‚ºäº†è©•ä¼°æˆ‘å€‘çš„æ–¹æ³•ï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹æ¶µè“‹äº”å€‹è³‡æ–™é›†çš„åŸºæº–æ¸¬è©¦ï¼Œé€™äº›è³‡æ–™é›†å…·æœ‰å¤šæ¨£åŒ–çš„é ˜åŸŸã€éš±ç§é™åˆ¶å’Œæ•ˆç”¨ç›®æ¨™ã€‚åœ¨æ‰€æœ‰è©•ä¼°çš„è¨­ç½®ä¸­ï¼Œæˆ‘å€‘çš„æ¡†æ¶ç›¸æ¯”æ–¼ç¾æœ‰çš„åŸºç·šæ–¹æ³•å§‹çµ‚èƒ½å¯¦ç¾æ›´å„ªçš„éš±ç§-æ•ˆç”¨æ¬Šè¡¡ï¼ŒåŒæ™‚ä¿æŒè¨ˆç®—æ•ˆç‡é«˜ï¼Œä¸¦åœ¨é–‹æºèªè¨€æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œå…¶æ€§èƒ½å¯èˆ‡æ›´å¤§å‹çš„é–‰æºæ¨¡å‹ç›¸åª²ç¾ã€‚æ­¤å¤–ï¼Œæˆ‘å€‘å±•ç¤ºäº†æˆ‘å€‘çš„æ–¹æ³•èƒ½å¤ ç™¼ç¾æ–°ç©çš„åŒ¿ååŒ–ç­–ç•¥ï¼Œé€™äº›ç­–ç•¥èƒ½å¤ æ¢ç´¢éš±ç§-æ•ˆç”¨æ¬Šè¡¡å‰æ²¿ä¸Šçš„ä¸åŒé»ã€‚</p>
  
  <div class="abstract-en">Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>