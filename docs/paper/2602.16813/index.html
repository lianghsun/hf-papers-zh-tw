<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ä¸€æ­¥å¼èªè¨€æ¨¡å‹åŒ–é€éé€£çºŒå»å™ª â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">ä¸€æ­¥å¼èªè¨€æ¨¡å‹åŒ–é€éé€£çºŒå»å™ª</h1>
  
  <div class="en-title">One-step Language Modeling via Continuous Denoising</div>
  

  <div class="paper-meta">
    
    <span>Chanhyuk Lee, Jaehoon Yoo, Manan Agarwal, Sheel Shah, Jerry Huang, Aditi Raghunathan, Seunghoon Hong, Nicholas M. Boffi, Jinwoo Kim</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.16813" target="_blank">2602.16813</a></span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.16813" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.16813" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.16813" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span>
    <span class="tag method">Flow-based Diffusion</span><span class="tag method">Continuous Denoising</span><span class="tag method">Distillation</span>
    <span class="tag task">Text Generation</span><span class="tag task">Language Modeling</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

åŸºæ–¼é›¢æ•£æ“´æ•£çš„èªè¨€æ¨¡å‹å› å…¶ç›¸æ¯”è‡ªè¿´æ­¸æ¨¡å‹æä¾›æ›´å¿«ç”Ÿæˆé€Ÿåº¦çš„æ½›åŠ›è€Œå—åˆ°å»£æ³›é—œæ³¨ã€‚ç„¶è€Œåœ¨å¯¦è¸ä¸­ï¼Œé€™äº›æ¨¡å‹åœ¨å°‘æ­¥é©Ÿï¼ˆfew-stepï¼‰æƒ…æ™¯ä¸‹è¡¨ç¾å‡ºæ¨£æœ¬å“è³ªçš„æ€¥åŠ‡ä¸‹é™ï¼Œæœªèƒ½å¯¦ç¾é€™ä¸€æ‰¿è«¾ã€‚æœ¬æ–‡é¡¯ç¤ºåˆ©ç”¨åŸºæ–¼æµçš„é€£çºŒå»å™ªçš„èªè¨€æ¨¡å‹å¯ä»¥åœ¨å“è³ªå’Œé€Ÿåº¦ä¸Šéƒ½å„ªæ–¼é›¢æ•£æ“´æ•£ã€‚é€šéé‡æ–°å¯©è¦–é›¢æ•£æ¨¡æ…‹ä¸Šçš„æµçš„åŸºç¤ç†è«–ï¼Œæˆ‘å€‘æ§‹å»ºäº†ä¸€å€‹æµåŸºèªè¨€æ¨¡å‹ï¼ˆFLMï¼‰ï¼Œå®ƒåœ¨ç¨ç†±ç·¨ç¢¼ä»¤ç‰Œä¸ŠåŸ·è¡Œæ­å¹¾é‡Œå¾—å»å™ªã€‚æˆ‘å€‘è­‰æ˜è©²æ¨¡å‹å¯ä»¥é€šéäº¤å‰ç†µç›®æ¨™é æ¸¬ä¹¾æ·¨æ•¸æ“šé€²è¡Œè¨“ç·´ï¼Œå…¶ä¸­æˆ‘å€‘å¼•å…¥äº†ç°¡å–®çš„æ™‚é–“é‡åƒæ•¸åŒ–ï¼Œå¤§å¹…æ”¹é€²äº†è¨“ç·´ç©©å®šæ€§å’Œç”Ÿæˆå“è³ªã€‚é€šéå°‡ FLM è’¸é¤¾åˆ°å…¶ç›¸é—œçš„æµæ˜ å°„ä¸­ï¼Œæˆ‘å€‘ç²å¾—äº†ä¸€å€‹è’¸é¤¾æµæ˜ å°„èªè¨€æ¨¡å‹ï¼ˆFMLMï¼‰ï¼Œèƒ½å¤ åŸ·è¡Œå°‘æ­¥é©Ÿç”Ÿæˆã€‚åœ¨ LM1B å’Œ OWT èªè¨€æ•¸æ“šé›†ä¸Šï¼ŒFLM å¯¦ç¾äº†èˆ‡æœ€å…ˆé€²é›¢æ•£æ“´æ•£æ¨¡å‹ç›¸åŒ¹é…çš„ç”Ÿæˆå“è³ªã€‚ä½¿ç”¨ FMLMï¼Œæˆ‘å€‘çš„æ–¹æ³•åœ¨å„æ–¹é¢éƒ½å„ªæ–¼æœ€è¿‘çš„å°‘æ­¥é©Ÿèªè¨€æ¨¡å‹ï¼Œå…¶å–®æ­¥ç”Ÿæˆè¶…éäº†å®ƒå€‘çš„å…«æ­¥å“è³ªã€‚æˆ‘å€‘çš„å·¥ä½œè³ªç–‘äº†å»£æ³›æŒæœ‰çš„å‡è¨­â€”â€”å³é›¢æ•£æ“´æ•£éç¨‹å°æ–¼é›¢æ•£æ¨¡æ…‹ä¸Šçš„ç”Ÿæˆå»ºæ¨¡æ˜¯å¿…è¦çš„ï¼Œä¸¦ç‚ºå¤§è¦æ¨¡åŠ é€ŸæµåŸºèªè¨€æ¨¡å‹é–‹é—¢äº†é“è·¯ã€‚ä»£ç¢¼å¯åœ¨ https://github.com/david3684/flm ç²å–ã€‚</p>
  
  <div class="abstract-en">Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>