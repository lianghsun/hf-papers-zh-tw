<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>è§£é–‹çš„å°¤åˆ©è¥¿æ–¯ï¼šé€šéé ­å‘åˆ†å¡Šå¯¦ç¾è¨˜æ†¶é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ä¸¦è¡Œ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem; color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">è§£é–‹çš„å°¤åˆ©è¥¿æ–¯ï¼šé€šéé ­å‘åˆ†å¡Šå¯¦ç¾è¨˜æ†¶é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ä¸¦è¡Œ</h1>
  
  <div class="en-title">Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking</div>
  

  <div class="paper-meta">
    
    <span>Ravi Ghadia, Maksim Abraham, Sergei Vorobyov, Max Ryabinin</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.21196" target="_blank">2602.21196</a></span>
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.21196" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.21196" target="_blank" rel="noopener" class="btn btn-outline">arXiv é é¢</a>
    <a href="https://huggingface.co/papers/2602.21196" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span>
    <span class="tag method">Transformer</span><span class="tag method">Context Parallelism</span><span class="tag method">Attention Mechanism</span><span class="tag method">Memory Optimization</span>
    <span class="tag task">Long Sequence Processing</span><span class="tag task">Model Training</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

ä½¿ç”¨ Transformer æ¨¡å‹é«˜æ•ˆè™•ç†é•·åºåˆ—é€šå¸¸éœ€è¦é€šéä¸Šä¸‹æ–‡ä¸¦è¡ŒåŒ–åœ¨åŠ é€Ÿå™¨ä¹‹é–“åˆ†å‰²è¨ˆç®—ã€‚é€™é¡æ–¹æ³•ä¸­çš„ä¸»æµæ–¹æ³•ï¼Œä¾‹å¦‚ Ring Attention æˆ– DeepSpeed Ulyssesï¼Œèƒ½å¤ åœ¨ä¸Šä¸‹æ–‡ç¶­åº¦ä¸Šé€²è¡Œæ“´å±•ï¼Œä½†ä¸è‘—é‡æ–¼è¨˜æ†¶é«”æ•ˆç‡ï¼Œé€™é™åˆ¶äº†å®ƒå€‘èƒ½æ”¯æ´çš„åºåˆ—é•·åº¦ã€‚æ›´å…ˆé€²çš„æŠ€è¡“ï¼Œä¾‹å¦‚ Fully Pipelined Distributed Transformer æˆ–å•Ÿå‹•å€¼å¸è¼‰ï¼Œå¯ä»¥é€²ä¸€æ­¥å»¶ä¼¸å¯èƒ½çš„ä¸Šä¸‹æ–‡é•·åº¦ï¼Œä½†ä»£åƒ¹æ˜¯è¨“ç·´ååé‡çš„é™ä½ã€‚åœ¨æœ¬è«–æ–‡ä¸­ï¼Œæˆ‘å€‘æå‡º UPipeï¼Œä¸€ç¨®ç°¡å–®ä½†æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ä¸¦è¡ŒåŒ–æŠ€è¡“ï¼Œåœ¨æ³¨æ„åŠ›é ­ç´šåˆ¥é€²è¡Œç´°ç²’åº¦åˆ†å¡Šã€‚è©²æŠ€è¡“é¡¯è‘—æ¸›å°‘äº†è‡ªæ³¨æ„åŠ›çš„å•Ÿå‹•å€¼è¨˜æ†¶é«”ä½¿ç”¨ï¼Œçªç ´äº†å•Ÿå‹•å€¼è¨˜æ†¶é«”çš„ç“¶é ¸ï¼Œé‡‹æ”¾äº†æ›´é•·çš„ä¸Šä¸‹æ–‡é•·åº¦ã€‚æˆ‘å€‘çš„æ–¹æ³•å°‡ 32B Transformer ä¸­æ³¨æ„åŠ›å±¤çš„ä¸­é–“å¼µé‡è¨˜æ†¶é«”ä½¿ç”¨é‡æ¸›å°‘äº†å¤šé” 87.5%ï¼ŒåŒæ™‚åœ¨è¨“ç·´é€Ÿåº¦æ–¹é¢èˆ‡å…ˆå‰çš„ä¸Šä¸‹æ–‡ä¸¦è¡ŒåŒ–æŠ€è¡“ç›¸ç•¶ã€‚UPipe åœ¨å–®å€‹ 8Ã—H100 ç¯€é»ä¸Šè¨“ç·´ Llama3-8B æ™‚å¯æ”¯æ´ 5M tokens çš„ä¸Šä¸‹æ–‡é•·åº¦ï¼Œç›¸æ¯”ä¹‹å‰çš„æ–¹æ³•æ”¹é€²è¶…é 25%ã€‚</p>
  
  <div class="abstract-en">Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8timesH100 node, improving upon prior methods by over 25%.</div>
  
</div>

<!-- Full paper content -->
<div class="paper-body">
  
</div>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>