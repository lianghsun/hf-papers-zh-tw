<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>OmniOCRï¼šå°‘æ•¸æ°‘æ—èªè¨€é€šç”¨å…‰å­¸å­—ç¬¦è­˜åˆ¥ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">OmniOCRï¼šå°‘æ•¸æ°‘æ—èªè¨€é€šç”¨å…‰å­¸å­—ç¬¦è­˜åˆ¥</h1>
  
  <div class="en-title">OmniOCR: Generalist OCR for Ethnic Minority Languages</div>
  

  <div class="paper-meta">
    
    <span>Bonan Liu, Zeyu Zhang, Bingbing Meng, Han Wang, Hanshuo Zhang, Chengping Wang, Daji Ergu, Ying Cai</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.21042" target="_blank">2602.21042</a></span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.21042" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.21042" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.21042" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">NLP</span><span class="tag domain">Multimodal</span>
    <span class="tag method">Dynamic LoRA</span><span class="tag method">Sparsity Regularization</span>
    <span class="tag task">Optical Character Recognition</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

å…‰å­¸å­—ç¬¦è­˜åˆ¥ï¼ˆOCRï¼‰åœ¨æ·±åº¦å­¸ç¿’å’Œå¤šæ¨¡æ…‹æ¨¡å‹çš„æ¨å‹•ä¸‹å·²å¿«é€Ÿç™¼å±•ï¼Œç„¶è€Œå¤§å¤šæ•¸æ–¹æ³•ä¸»è¦èšç„¦æ–¼è³‡æºå……è¶³çš„æ–‡å­—ç³»çµ±ï¼Œå¦‚æ‹‰ä¸å­—å’Œæ¼¢å­—ã€‚å°‘æ•¸æ°‘æ—èªè¨€ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œå› ç‚ºå…¶è¤‡é›œçš„æ›¸å¯«ç³»çµ±ã€ç¨€ç¼ºçš„æ¨™è¨»è³‡æ–™ï¼Œä»¥åŠå¤šæ¨£åŒ–çš„æ­·å²å’Œç¾ä»£æ–‡å­—å½¢å¼ï¼Œä½¿å¾—åœ¨ä½è³‡æºæˆ–é›¶æ¨£æœ¬è¨­å®šä¸‹çš„æ³›åŒ–é¢è‡¨æŒ‘æˆ°ã€‚ç‚ºè§£æ±ºé€™äº›å•é¡Œï¼Œæˆ‘å€‘æå‡º OmniOCRï¼Œä¸€å€‹é‡å°å°‘æ•¸æ°‘æ—æ–‡å­—çš„é€šç”¨æ¡†æ¶ã€‚OmniOCR å¼•å…¥å‹•æ…‹ä½ç§©é©æ‡‰ï¼ˆDynamic LoRAï¼‰æŠ€è¡“ï¼Œåœ¨å±¤å’Œæ–‡å­—ä¹‹é–“åˆ†é…æ¨¡å‹å®¹é‡ï¼Œå¯¦ç¾é«˜æ•ˆçš„é©æ‡‰åŒæ™‚ä¿ç•™çŸ¥è­˜ã€‚ç¨€ç–æ€§æ­£å‰‡åŒ–å°å†—é¤˜æ›´æ–°é€²è¡Œå‰ªæï¼Œç¢ºä¿ç·Šæ¹Šé«˜æ•ˆçš„é©æ‡‰è€Œç„¡éœ€é¡å¤–æ¨ç†æˆæœ¬ã€‚åœ¨è—æ–‡ MNISTã€æ°´èªã€å¤ç¾©å­—å’Œæ±å·´è±¡å½¢æ–‡çš„è©•ä¼°è¡¨æ˜ï¼ŒOmniOCR å„ªæ–¼é›¶æ¨£æœ¬åŸºç¤æ¨¡å‹å’Œæ¨™æº–å¾®èª¿æ–¹æ³•ï¼Œä»¥å„ªç•°çš„åƒæ•¸æ•ˆç‡é”åˆ°æœ€å…ˆé€²çš„æº–ç¢ºåº¦ï¼Œèˆ‡ç¾æœ‰æœ€ä½³åŸºç·šæ¨¡å‹ç›¸æ¯”ï¼Œåœ¨é€™å››å€‹è³‡æ–™é›†ä¸Šçš„æº–ç¢ºåº¦æå‡äº† 39%-66%ã€‚ç¨‹å¼ç¢¼ï¼šhttps://github.com/AIGeeksGroup/OmniOCRã€‚</p>
  
  <div class="abstract-en">Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>