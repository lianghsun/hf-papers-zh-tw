<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>PyVision-RLï¼šé€šéå¼·åŒ–å­¸ç¿’æ‰“é€ é–‹æ”¾å¼æ™ºèƒ½è¦–è¦ºæ¨¡å‹ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem; color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">PyVision-RLï¼šé€šéå¼·åŒ–å­¸ç¿’æ‰“é€ é–‹æ”¾å¼æ™ºèƒ½è¦–è¦ºæ¨¡å‹</h1>
  
  <div class="en-title">PyVision-RL: Forging Open Agentic Vision Models via RL</div>
  

  <div class="paper-meta">
    
    <span>Shitian Zhao, Shaoheng Lin, Ming Li, Haoquan Zhang, Wenshuo Peng, Kaipeng Zhang, Chen Wei</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.20739" target="_blank">2602.20739</a></span>
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.20739" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.20739" target="_blank" rel="noopener" class="btn btn-outline">arXiv é é¢</a>
    <a href="https://huggingface.co/papers/2602.20739" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">RL</span><span class="tag domain">Multimodal</span>
    <span class="tag method">Reinforcement Learning</span><span class="tag method">Tool Use</span><span class="tag method">Rollout Strategy</span><span class="tag method">Reward Shaping</span>
    <span class="tag task">Image Understanding</span><span class="tag task">Video Understanding</span><span class="tag task">Visual Reasoning</span><span class="tag task">Multi-turn Reasoning</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

ä»£ç†å¤šæ¨¡æ…‹æ¨¡å‹çš„å¼·åŒ–å­¸ç¿’ç¶“å¸¸é­é‡äº¤äº’å´©æ½°å•é¡Œï¼Œå³æ¨¡å‹å­¸æœƒæ¸›å°‘å·¥å…·ä½¿ç”¨å’Œå¤šæ­¥æ¨ç†ï¼Œé™åˆ¶äº†ä»£ç†è¡Œç‚ºçš„å„ªå‹¢ã€‚æˆ‘å€‘å¼•å…¥ PyVision-RLï¼Œä¸€å€‹ç‚ºé–‹æºæ¬Šé‡å¤šæ¨¡æ…‹æ¨¡å‹è¨­è¨ˆçš„å¼·åŒ–å­¸ç¿’æ¡†æ¶ï¼Œå¯ä»¥ç©©å®šè¨“ç·´ä¸¦ç¶­æŒäº¤äº’ã€‚æˆ‘å€‘çš„æ–¹æ³•çµåˆéåº¦æ¡æ¨£-éæ¿¾-æ’åºå±•é–‹ç­–ç•¥èˆ‡ç´¯ç©å·¥å…·çå‹µï¼Œä»¥é˜²æ­¢å´©æ½°ä¸¦é¼“å‹µå¤šæ­¥å·¥å…·ä½¿ç”¨ã€‚ä½¿ç”¨çµ±ä¸€çš„è¨“ç·´æµç¨‹ï¼Œæˆ‘å€‘é–‹ç™¼äº†ç”¨æ–¼å½±åƒå’Œè¦–è¨Šç†è§£çš„ PyVision-Image å’Œ PyVision-Videoã€‚å°æ–¼è¦–è¨Šæ¨ç†ï¼ŒPyVision-Video æ¡ç”¨æŒ‰éœ€ä¸Šä¸‹æ–‡æ§‹å»ºï¼Œåœ¨æ¨ç†éç¨‹ä¸­æœ‰é¸æ“‡åœ°æ¡æ¨£ä»»å‹™ç›¸é—œçš„å¹€ï¼Œä»¥é¡¯è‘—æ¸›å°‘è¦–è¦ºæ¨™è¨˜ä½¿ç”¨ã€‚å¯¦é©—çµæœå±•ç¤ºäº†å¼·å¤§çš„æ€§èƒ½å’Œæ”¹é€²çš„æ•ˆç‡ï¼Œè­‰æ˜äº†æŒçºŒäº¤äº’å’ŒæŒ‰éœ€è¦–è¦ºè™•ç†å°æ–¼å¯æ“´å±•å¤šæ¨¡æ…‹ä»£ç†è‡³é—œé‡è¦ã€‚</p>
  
  <div class="abstract-en">Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.</div>
  
</div>

<!-- Full paper content -->
<div class="paper-body">
  
    
      <figure class="paper-figure">
        <img src="../../figures/2026-02-25/2602.20739/p24_fig1.png"
             alt="Figure 1" loading="lazy">
      </figure>
    
  
    
      <p><em>**åœ– 20.** VSI-Bench ä¸Šç‰©ä»¶è¨ˆæ•¸çš„æ¡ˆä¾‹ç ”ç©¶ã€‚æ­¤ä»»å‹™è¦æ±‚ PyVision-Video è¨ˆæ•¸çµ¦å®šå½±ç‰‡ä¸­çš„ç‰¹å®šç‰©ä»¶ã€‚åœ¨æœ¬æ¡ˆä¾‹ä¸­ï¼Œé¦–å…ˆï¼ŒPyVision-Video å¾å½±ç‰‡ä¸­å‡å‹»æ¡æ¨£ 15 å¹€ã€‚éš¨å¾Œï¼Œå®ƒåœ¨ç¬¬ 700 å¹€å’Œç¬¬ 1100 å¹€è­˜åˆ¥å‡º 2 å¼µä¸åŒçš„æ¡Œå­ã€‚ç‚ºäº†æŸ¥çœ‹æ˜¯å¦æœ‰é¡å¤–çš„æ¡Œå­æˆ–ç›¸åŒçš„æ¡Œå­æ˜¯å¦å¾ä¸åŒè§’åº¦é¡¯ç¤ºï¼Œè©²æ¨¡å‹åœ¨ç¬¬ 600 åˆ°ç¬¬ 1200 å¹€ä¹‹é–“çš„å½±ç‰‡ç‰‡æ®µä¸­æ¡æ¨£æ›´å¤šå¹€ã€‚æœ€å¾Œï¼ŒåŸºæ–¼æ§‹å»ºçš„ä¸Šä¸‹æ–‡ï¼ŒPyVision-Video è­˜åˆ¥å‡ºå…©å¼µä¸åŒçš„æ¡Œå­ï¼Œä¸€å¼µæœ¨æ¡Œé…æœ‰èŠ±ç“¶å’Œæ¤…å­ï¼Œå¦ä¸€å¼µé…æœ‰ç´…è‰²æ”¯æ¶å’Œé›»è¦–ã€‚</em></p>
    
  
</div>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>