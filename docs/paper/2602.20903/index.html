<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TextPeckerï¼šçå‹µçµæ§‹ç•°å¸¸é‡åŒ–ä»¥å¢å¼·è¦–è¦ºæ–‡æœ¬æ¸²æŸ“ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem; color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">TextPeckerï¼šçå‹µçµæ§‹ç•°å¸¸é‡åŒ–ä»¥å¢å¼·è¦–è¦ºæ–‡æœ¬æ¸²æŸ“</h1>
  
  <div class="en-title">TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering</div>
  

  <div class="paper-meta">
    
    <span>Hanshen Zhu, Yuliang Liu, Xuecheng Wu, An-Lan Wang, Hao Feng, Dingkang Yang, Chao Feng, Can Huang, Jingqun Tang, Xiang Bai</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.20903" target="_blank">2602.20903</a></span>
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.20903" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.20903" target="_blank" rel="noopener" class="btn btn-outline">arXiv é é¢</a>
    <a href="https://huggingface.co/papers/2602.20903" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">Multimodal</span><span class="tag domain">RL</span>
    <span class="tag method">Reinforcement Learning</span><span class="tag method">OCR</span><span class="tag method">Diffusion Models</span><span class="tag method">Reward Model</span>
    <span class="tag task">Text-to-Image Generation</span><span class="tag task">Visual Text Rendering</span><span class="tag task">Structural Anomaly Detection</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è¦–è¦ºæ–‡å­—æ¸²æŸ“çš„çµæ§‹ç•°å¸¸æ„ŸçŸ¥å¼·åŒ–å­¸ç¿’ç­–ç•¥

è¦–è¦ºæ–‡å­—æ¸²æŸ“ï¼ˆVTRï¼‰åœ¨æ–‡å­—è½‰åœ–åƒç”Ÿæˆä¸­ä»ç„¶æ˜¯ä¸€é …é—œéµæŒ‘æˆ°ï¼Œå³ä½¿æ˜¯å…ˆé€²çš„æ¨¡å‹ä¹Ÿç¶“å¸¸ç”¢ç”Ÿå…·æœ‰çµæ§‹ç•°å¸¸çš„æ–‡å­—ï¼Œä¾‹å¦‚æ‰­æ›²ã€æ¨¡ç³Šå’Œä¸å°é½Šã€‚ç„¶è€Œï¼Œæˆ‘å€‘ç™¼ç¾é ˜å…ˆçš„ MLLM å’Œå°ˆæ¥­ OCR æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç„¡æ³•æ„ŸçŸ¥é€™äº›çµæ§‹ç•°å¸¸ï¼Œç‚º VTR è©•ä¼°å’ŒåŸºæ–¼å¼·åŒ–å­¸ç¿’çš„å„ªåŒ–éƒ½é€ æˆäº†é—œéµç“¶é ¸ã€‚å› æ­¤ï¼Œå³ä½¿æ˜¯æœ€å…ˆé€²çš„ç”Ÿæˆå™¨ï¼ˆä¾‹å¦‚ SeedDream4.0ã€Qwen-Imageï¼‰ä»ç„¶é›£ä»¥æ¸²æŸ“çµæ§‹å¿ å¯¦çš„æ–‡å­—ã€‚ç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œï¼Œæˆ‘å€‘æå‡ºäº† TextPeckerï¼Œä¸€ç¨®å³æ’å³ç”¨çš„çµæ§‹ç•°å¸¸æ„ŸçŸ¥å¼·åŒ–å­¸ç¿’ç­–ç•¥ï¼Œå¯ä»¥æ¸›è¼•å˜ˆé›œçš„çå‹µä¿¡è™Ÿä¸¦èˆ‡ä»»ä½•æ–‡å­—è½‰åœ–åƒç”Ÿæˆå™¨ç›¸é…åˆã€‚ç‚ºäº†å¯¦ç¾é€™ä¸€èƒ½åŠ›ï¼Œæˆ‘å€‘æ§‹å»ºäº†å…·æœ‰å­—ç¬¦ç´šçµæ§‹ç•°å¸¸æ¨™è¨»çš„è­˜åˆ¥æ•¸æ“šé›†ï¼Œä¸¦é–‹ç™¼äº†ç­†åŠƒç·¨è¼¯åˆæˆå¼•æ“ä»¥æ“´å±•çµæ§‹éŒ¯èª¤è¦†è“‹ç¯„åœã€‚å¯¦é©—è¡¨æ˜ TextPecker æŒçºŒæ”¹é€²å¤šç¨®æ–‡å­—è½‰åœ–åƒæ¨¡å‹ï¼›å³ä½¿åœ¨ç¶“éå……åˆ†å„ªåŒ–çš„ Qwen-Image ä¸Šï¼Œå®ƒä¹Ÿèƒ½ç‚ºä¸­æ–‡æ–‡å­—æ¸²æŸ“é¡¯è‘—æå‡å¹³å‡ 4% çš„çµæ§‹ä¿çœŸåº¦å’Œ 8.7% çš„èªç¾©å°é½Šï¼Œç¢ºç«‹äº†é«˜ä¿çœŸ VTR çš„æ–°æŠ€è¡“æ°´å¹³ã€‚æœ¬å·¥ä½œå¡«è£œäº† VTR å„ªåŒ–çš„ç©ºç™½ï¼Œç‚ºå¯¦ç¾å¯é ä¸”çµæ§‹å¿ å¯¦çš„è¦–è¦ºæ–‡å­—ç”Ÿæˆæä¾›äº†åŸºç¤æ€§æ­¥é©Ÿã€‚</p>
  
  <div class="abstract-en">Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.</div>
  
</div>

<!-- Full paper content -->
<div class="paper-body">
  
    
      <h2># åƒè€ƒæ–‡ç»</h2>
    
  
    
      <p>[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl æŠ€è¡“å ±å‘Šã€‚arXiv preprint arXiv:2502.13923, 2025ã€‚1, 2, 3, 5
[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. é€éæ›´å„ªè³ªçš„æ¨™é¡Œæ”¹é€²å½±åƒç”Ÿæˆã€‚è¨ˆç®—æ©Ÿç§‘å­¸ã€‚https://cdn.openai.com/papers/dall-e-3.pdf, 2(3):8, 2023ã€‚1
[3] ByteDanceã€‚Seedream4.0ã€‚https://seed.bytedance.com/en/seedream4_0, 2025ã€‚è¨ªå•æ™‚é–“ï¼š2025-09-22ã€‚1, 2, 3
[4] ByteDanceã€‚Doubao-seed-1.6ã€‚https://seed.bytedance.com/en/seed1_6, 2025ã€‚è¨ªå•æ™‚é–“ï¼š2025-09-22ã€‚5, 6
[5] ByteDanceã€‚Doubao-seed-1.6-thinkingã€‚https://seed.bytedance.com/en/seed1_6, 2025ã€‚è¨ªå•æ™‚é–“ï¼š2025-09-22ã€‚5, 6
[6] Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chenã€‚OneIG-Benchï¼šå½±åƒç”Ÿæˆçš„å…¨ç¶­åº¦ç´°ç·»è©•ä¼°ã€‚arXiv preprint arXiv:2506.07977, 2025ã€‚1, 2, 5, 6, 12, 14, 16, 17
[7] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Weiã€‚TextDiffuserï¼šä½œç‚ºæ–‡å­—ç¹ªè£½è€…çš„æ“´æ•£æ¨¡å‹ã€‚NIPS, 36:9353-9387, 2023ã€‚2
[8] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Weiã€‚TextDiffuser-2ï¼šé‡‹æ”¾èªè¨€æ¨¡å‹åœ¨æ–‡å­—æ¸²æŸ“ä¸­çš„åŠ›é‡ã€‚åœ¨ ECCV ä¸­ï¼Œé ç¢¼ 386-402ã€‚Springer, 2024ã€‚2
[9] Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, et alã€‚BLIP-30 Nextï¼šåŸç”Ÿå½±åƒç”Ÿæˆçš„ä¸‹ä¸€å€‹å‰æ²¿ã€‚arXiv preprint arXiv:2510.15857, 2025ã€‚1, 3
[10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et alã€‚Gemini 2.5ï¼šé€éé€²éšæ¨ç†ã€å¤šæ¨¡æ…‹ã€é•·ä¸Šä¸‹æ–‡å’Œä¸‹ä¸€ä»£ä»£ç†èƒ½åŠ›æ¨é€²å‰æ²¿ã€‚arXiv preprint arXiv:2507.06261, 2025ã€‚1, 18
[11] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et alã€‚Gemini 2.5ï¼šé€éé€²éšæ¨ç†ã€å¤šæ¨¡æ…‹ã€é•·ä¸Šä¸‹æ–‡å’Œä¸‹ä¸€ä»£ä»£ç†èƒ½åŠ›æ¨é€²å‰æ²¿ã€‚arXiv preprint arXiv:2507.06261, 2025ã€‚5, 6, 12, 13
[12] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et alã€‚çµ±ä¸€å¤šæ¨¡æ…‹é è¨“ç·´ä¸­çš„æ–°èˆˆç‰¹æ€§ã€‚arXiv preprint arXiv:2505.14683, 2025ã€‚1, 2
[13] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et alã€‚CogViewï¼šé€é Transformer æŒæ¡æ–‡å­—è½‰å½±åƒç”Ÿæˆã€‚NIPS, 34:19822-19835, 2021ã€‚4, 15</p>
    
  
    
      <p>[14] Nikai Du, Zhennan Chen, Shan Gao, Zhizhou Chen, Xi Chen, Zhengkai Jiang, Jian Yang, and Ying Tai. Textcrafter: Accurately rendering multiple texts in complex visual scenes. arXiv preprint arXiv:2503.23461, 2025. 1, 2, 5, 6, 14, 16, 17</p>
    
  
</div>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>