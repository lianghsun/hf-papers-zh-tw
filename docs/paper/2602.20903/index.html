<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TextPeckerï¼šçå‹µçµæ§‹ç•°å¸¸é‡åŒ–ä»¥å¢å¼·è¦–è¦ºæ–‡æœ¬æ¸²æŸ“ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">TextPeckerï¼šçå‹µçµæ§‹ç•°å¸¸é‡åŒ–ä»¥å¢å¼·è¦–è¦ºæ–‡æœ¬æ¸²æŸ“</h1>
  
  <div class="en-title">TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering</div>
  

  <div class="paper-meta">
    
    <span>Hanshen Zhu, Yuliang Liu, Xuecheng Wu, An-Lan Wang, Hao Feng, Dingkang Yang, Chao Feng, Can Huang, Jingqun Tang, Xiang Bai</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.20903" target="_blank">2602.20903</a></span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.20903" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.20903" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.20903" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">Multimodal</span><span class="tag domain">RL</span>
    <span class="tag method">Reinforcement Learning</span><span class="tag method">OCR</span><span class="tag method">Diffusion Models</span><span class="tag method">Reward Model</span>
    <span class="tag task">Text-to-Image Generation</span><span class="tag task">Visual Text Rendering</span><span class="tag task">Structural Anomaly Detection</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è¦–è¦ºæ–‡å­—æ¸²æŸ“çš„çµæ§‹ç•°å¸¸æ„ŸçŸ¥å¼·åŒ–å­¸ç¿’ç­–ç•¥

è¦–è¦ºæ–‡å­—æ¸²æŸ“ï¼ˆVTRï¼‰åœ¨æ–‡å­—è½‰åœ–åƒç”Ÿæˆä¸­ä»ç„¶æ˜¯ä¸€é …é—œéµæŒ‘æˆ°ï¼Œå³ä½¿æ˜¯å…ˆé€²çš„æ¨¡å‹ä¹Ÿç¶“å¸¸ç”¢ç”Ÿå…·æœ‰çµæ§‹ç•°å¸¸çš„æ–‡å­—ï¼Œä¾‹å¦‚æ‰­æ›²ã€æ¨¡ç³Šå’Œä¸å°é½Šã€‚ç„¶è€Œï¼Œæˆ‘å€‘ç™¼ç¾é ˜å…ˆçš„ MLLM å’Œå°ˆæ¥­ OCR æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç„¡æ³•æ„ŸçŸ¥é€™äº›çµæ§‹ç•°å¸¸ï¼Œç‚º VTR è©•ä¼°å’ŒåŸºæ–¼å¼·åŒ–å­¸ç¿’çš„å„ªåŒ–éƒ½é€ æˆäº†é—œéµç“¶é ¸ã€‚å› æ­¤ï¼Œå³ä½¿æ˜¯æœ€å…ˆé€²çš„ç”Ÿæˆå™¨ï¼ˆä¾‹å¦‚ SeedDream4.0ã€Qwen-Imageï¼‰ä»ç„¶é›£ä»¥æ¸²æŸ“çµæ§‹å¿ å¯¦çš„æ–‡å­—ã€‚ç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œï¼Œæˆ‘å€‘æå‡ºäº† TextPeckerï¼Œä¸€ç¨®å³æ’å³ç”¨çš„çµæ§‹ç•°å¸¸æ„ŸçŸ¥å¼·åŒ–å­¸ç¿’ç­–ç•¥ï¼Œå¯ä»¥æ¸›è¼•å˜ˆé›œçš„çå‹µä¿¡è™Ÿä¸¦èˆ‡ä»»ä½•æ–‡å­—è½‰åœ–åƒç”Ÿæˆå™¨ç›¸é…åˆã€‚ç‚ºäº†å¯¦ç¾é€™ä¸€èƒ½åŠ›ï¼Œæˆ‘å€‘æ§‹å»ºäº†å…·æœ‰å­—ç¬¦ç´šçµæ§‹ç•°å¸¸æ¨™è¨»çš„è­˜åˆ¥æ•¸æ“šé›†ï¼Œä¸¦é–‹ç™¼äº†ç­†åŠƒç·¨è¼¯åˆæˆå¼•æ“ä»¥æ“´å±•çµæ§‹éŒ¯èª¤è¦†è“‹ç¯„åœã€‚å¯¦é©—è¡¨æ˜ TextPecker æŒçºŒæ”¹é€²å¤šç¨®æ–‡å­—è½‰åœ–åƒæ¨¡å‹ï¼›å³ä½¿åœ¨ç¶“éå……åˆ†å„ªåŒ–çš„ Qwen-Image ä¸Šï¼Œå®ƒä¹Ÿèƒ½ç‚ºä¸­æ–‡æ–‡å­—æ¸²æŸ“é¡¯è‘—æå‡å¹³å‡ 4% çš„çµæ§‹ä¿çœŸåº¦å’Œ 8.7% çš„èªç¾©å°é½Šï¼Œç¢ºç«‹äº†é«˜ä¿çœŸ VTR çš„æ–°æŠ€è¡“æ°´å¹³ã€‚æœ¬å·¥ä½œå¡«è£œäº† VTR å„ªåŒ–çš„ç©ºç™½ï¼Œç‚ºå¯¦ç¾å¯é ä¸”çµæ§‹å¿ å¯¦çš„è¦–è¦ºæ–‡å­—ç”Ÿæˆæä¾›äº†åŸºç¤æ€§æ­¥é©Ÿã€‚</p>
  
  <div class="abstract-en">Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>