<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>é€šç”¨ LLM ä»£ç†çš„åŸºæº–æ¸¬è©¦æ™‚é–“ç¸®æ”¾ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem; color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">é€šç”¨ LLM ä»£ç†çš„åŸºæº–æ¸¬è©¦æ™‚é–“ç¸®æ”¾</h1>
  
  <div class="en-title">Benchmark Test-Time Scaling of General LLM Agents</div>
  

  <div class="paper-meta">
    
    <span>Xiaochuan Li, Ryan Ming, Pranav Setlur, Abhijay Paladugu, Andy Tang, Hao Kang, Shuai Shao, Rong Jin, Chenyan Xiong</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.18998" target="_blank">2602.18998</a></span>
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.18998" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.18998" target="_blank" rel="noopener" class="btn btn-outline">arXiv é é¢</a>
    <a href="https://huggingface.co/papers/2602.18998" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span><span class="tag domain">Code</span>
    <span class="tag method">LLM</span><span class="tag method">Agent</span><span class="tag method">Test-time Scaling</span><span class="tag method">Sequential Scaling</span><span class="tag method">Parallel Scaling</span>
    <span class="tag task">Agent Evaluation</span><span class="tag task">Tool Use</span><span class="tag task">Reasoning</span><span class="tag task">Code Generation</span><span class="tag task">Search</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># LLM æ™ºèƒ½é«”é€šç”¨åŸºæº–æ¸¬è©¦

LLM æ™ºèƒ½é«”è¶Šä¾†è¶Šéœ€è¦ä½œç‚ºé€šç”¨ç³»çµ±ä¾†é‹ä½œï¼Œä»¥è§£æ±ºé–‹æ”¾æ€§çš„ç”¨æˆ¶è«‹æ±‚ã€‚é›–ç„¶ç¾æœ‰åŸºæº–æ¸¬è©¦å´é‡æ–¼é ˜åŸŸæ„ŸçŸ¥çš„ç’°å¢ƒä¾†é–‹ç™¼å°ˆé–€åŒ–æ™ºèƒ½é«”ï¼Œä½†è©•ä¼°é€šç”¨æ™ºèƒ½é«”éœ€è¦æ›´åŠ çœŸå¯¦çš„è¨­ç½®ï¼Œä½¿å…¶åœ¨çµ±ä¸€ç’°å¢ƒä¸­è·¨è¶Šå¤šé …æŠ€èƒ½å’Œå·¥å…·é€²è¡Œé‹ä½œã€‚æˆ‘å€‘å¼•å…¥ General AgentBenchï¼Œä¸€å€‹æä¾›çµ±ä¸€æ¡†æ¶çš„åŸºæº–æ¸¬è©¦ï¼Œç”¨æ–¼åœ¨æœå°‹ã€ç·¨ç¢¼ã€æ¨ç†å’Œå·¥å…·ä½¿ç”¨ç­‰é ˜åŸŸè©•ä¼°é€šç”¨ LLM æ™ºèƒ½é«”ã€‚åˆ©ç”¨ General AgentBenchï¼Œæˆ‘å€‘ç³»çµ±åœ°ç ”ç©¶äº†åœ¨é †åºæ“´å±•ï¼ˆè¿­ä»£äº¤äº’ï¼‰å’Œå¹³è¡Œæ“´å±•ï¼ˆæ¡æ¨£å¤šå€‹è»Œè·¡ï¼‰ä¸‹çš„æ¸¬è©¦æ™‚é–“æ“´å±•è¡Œç‚ºã€‚å°åå€‹é ˜å…ˆ LLM æ™ºèƒ½é«”çš„è©•ä¼°æ­ç¤ºäº†å¾é ˜åŸŸç‰¹å®šè©•ä¼°è½‰ç§»åˆ°æ­¤é€šç”¨æ™ºèƒ½é«”è¨­ç½®æ™‚çš„é¡¯è‘—æ€§èƒ½è¡°é€€ã€‚æ­¤å¤–ï¼Œæˆ‘å€‘ç™¼ç¾ç”±æ–¼å…©å€‹æ ¹æœ¬é™åˆ¶â€”â€”é †åºæ“´å±•ä¸­çš„ä¸Šä¸‹æ–‡å¤©èŠ±æ¿å’Œå¹³è¡Œæ“´å±•ä¸­çš„é©—è­‰å·®è·â€”â€”å…©ç¨®æ“´å±•æ–¹æ³•åœ¨å¯¦è¸ä¸­éƒ½ç„¡æ³•ç”¢ç”Ÿæœ‰æ•ˆçš„æ€§èƒ½æ”¹é€²ã€‚ä»£ç¢¼å·²åœ¨ https://github.com/cxcscmu/General-AgentBench å…¬é–‹ç™¼ä½ˆã€‚</p>
  
  <div class="abstract-en">LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.</div>
  
</div>

<!-- Full paper content -->
<div class="paper-body">
  
</div>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>