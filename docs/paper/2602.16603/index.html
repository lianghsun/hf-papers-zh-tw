<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>FlowPrefillï¼šå°‡æ¶ä½”èˆ‡é å¡«å……èª¿åº¦ç²’åº¦è§£è€¦ä»¥ç·©è§£ LLM æœå‹™ä¸­çš„è¡Œé ­é˜»å¡å•é¡Œ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem; color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">FlowPrefillï¼šå°‡æ¶ä½”èˆ‡é å¡«å……èª¿åº¦ç²’åº¦è§£è€¦ä»¥ç·©è§£ LLM æœå‹™ä¸­çš„è¡Œé ­é˜»å¡å•é¡Œ</h1>
  
  <div class="en-title">FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving</div>
  

  <div class="paper-meta">
    
    <span>Chia-chi Hsieh, Zan Zong, Xinyang Chen, Jianjiang Li, Jidong Zhai, Lijie Wen</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.16603" target="_blank">2602.16603</a></span>
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.16603" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.16603" target="_blank" rel="noopener" class="btn btn-outline">arXiv é é¢</a>
    <a href="https://huggingface.co/papers/2602.16603" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span>
    <span class="tag method">Scheduling</span><span class="tag method">Preemption</span><span class="tag method">Operator-Level Optimization</span>
    <span class="tag task">LLM Serving</span><span class="tag task">Request Scheduling</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„éœ€æ±‚ä¸æ–·å¢é•·ï¼Œè¦æ±‚æœå‹™ç³»çµ±èƒ½å¤ è™•ç†å¤šå€‹ä¸¦è¡Œè«‹æ±‚ä¸¦æ»¿è¶³ä¸åŒçš„æœå‹™ç­‰ç´šç›®æ¨™ï¼ˆSLOsï¼‰ã€‚é€™åŠ åŠ‡äº†è¨ˆç®—å¯†é›†å‹é å¡«å……ï¼ˆprefillï¼‰éšæ®µçš„éšŠé¦–é˜»å¡ï¼ˆHoL blockingï¼‰å•é¡Œï¼Œå…¶ä¸­é•·æ™‚é–“é‹è¡Œçš„è«‹æ±‚æœƒä½”ç”¨è³‡æºä¸¦å»¶é²é«˜å„ªå…ˆç´šè«‹æ±‚ï¼Œå°è‡´å»£æ³›çš„é¦–ä»¤ç‰Œæ™‚é–“ï¼ˆTTFTï¼‰SLO é•è¦ã€‚é›–ç„¶åˆ†å¡Šé å¡«å……ï¼ˆchunked prefillï¼‰å¯¦ç¾äº†å¯ä¸­æ–·æ€§ï¼Œä½†å®ƒåœ¨éŸ¿æ‡‰æ€§å’Œååé‡ä¹‹é–“å¼•å…¥äº†å›ºæœ‰çš„æ¬Šè¡¡ï¼šæ¸›å°å¡Šå¤§å°å¯æ”¹å–„éŸ¿æ‡‰å»¶é²ä½†é™ä½è¨ˆç®—æ•ˆç‡ï¼Œè€Œå¢åŠ å¡Šå¤§å°å‰‡æœ€å¤§åŒ–ååé‡ä½†åŠ åŠ‡é˜»å¡ã€‚é€™è¦æ±‚æ¡ç”¨è‡ªé©æ‡‰æ¶ä½”æ©Ÿåˆ¶ã€‚ç„¶è€Œï¼Œå‹•æ…‹å¹³è¡¡åŸ·è¡Œç²’åº¦èˆ‡èª¿åº¦é–‹éŠ·ä»ç„¶æ˜¯ä¸€å€‹é—œéµæŒ‘æˆ°ã€‚

æœ¬æ–‡æå‡º FlowPrefillï¼Œä¸€å€‹é‡å° TTFT å’Œæœ‰æ•ˆååé‡ï¼ˆgoodputï¼‰å„ªåŒ–çš„æœå‹™ç³»çµ±ï¼Œé€šéå°‡æ¶ä½”ç²’åº¦èˆ‡èª¿åº¦é »ç‡è§£è€¦ä¾†è§£æ±ºé€™ä¸€è¡çªã€‚ç‚ºå¯¦ç¾è‡ªé©æ‡‰é å¡«å……èª¿åº¦ï¼ŒFlowPrefill å¼•å…¥äº†å…©é …é—œéµå‰µæ–°ï¼š1) **ç®—å­ç´šæ¶ä½”**ï¼ˆOperator-Level Preemptionï¼‰ï¼Œåˆ©ç”¨ç®—å­é‚Šç•Œå¯¦ç¾ç´°ç²’åº¦åŸ·è¡Œä¸­æ–·ï¼Œè€Œç„¡éœ€èˆ‡å›ºå®šå°å¡Šç›¸é—œçš„æ•ˆç‡æå¤±ï¼›2) **äº‹ä»¶é©…å‹•èª¿åº¦**ï¼ˆEvent-Driven Schedulingï¼‰ï¼Œåƒ…åœ¨è«‹æ±‚åˆ°é”æˆ–å®Œæˆäº‹ä»¶æ™‚è§¸ç™¼èª¿åº¦æ±ºç­–ï¼Œå¾è€Œæ”¯æŒé«˜æ•ˆçš„æ¶ä½”éŸ¿æ‡‰æ€§ï¼ŒåŒæ™‚æœ€å°åŒ–æ§åˆ¶å¹³é¢é–‹éŠ·ã€‚åœ¨çœŸå¯¦ç”Ÿç”¢è·¡è·¡ä¸Šçš„è©•ä¼°è¡¨æ˜ï¼ŒFlowPrefill ç›¸æ¯”æœ€å…ˆé€²çš„ç³»çµ±å°‡æœ€å¤§æœ‰æ•ˆååé‡æé«˜äº†æœ€å¤š 5.6 å€ï¼ŒåŒæ™‚æ»¿è¶³ç•°æ§‹ SLOsã€‚</p>
  
  <div class="abstract-en">The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.
  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6times compared to state-of-the-art systems while satisfying heterogeneous SLOs.</div>
  
</div>

<!-- Full paper content -->
<div class="paper-body">
  
    
      <p>[41] RuØ§Ù„ÙŠn Qin, Zheming Li, Weiran He, Jialei Cui, Heyi Tang, Feng Ren, Teng Ma, Shangming Cai, Yineng Zhang, Mingxing Zhang ç­‰äººã€‚2024ã€‚Mooncakeï¼šä¸€ç¨®ç”¨æ–¼ LLM æœå‹™çš„ KVCache ä¸­å¿ƒåˆ†æ•£å¼æ¶æ§‹ã€‚ACM å„²å­˜äº¤æ˜“ (2024)ã€‚</p>
    
  
</div>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>