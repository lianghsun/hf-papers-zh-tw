<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>æ“´æ•£å°å¶æ€§ï¼Œç¬¬äºŒç« ï¼šÎ¨-æ¡æ¨£å™¨èˆ‡é«˜æ•ˆèª²ç¨‹ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">æ“´æ•£å°å¶æ€§ï¼Œç¬¬äºŒç« ï¼šÎ¨-æ¡æ¨£å™¨èˆ‡é«˜æ•ˆèª²ç¨‹</h1>
  
  <div class="en-title">The Diffusion Duality, Chapter II: Î¨-Samplers and Efficient Curriculum</div>
  

  <div class="paper-meta">
    
    <span>Justin Deschenaux, Caglar Gulcehre, Subham Sekhar Sahoo</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.21185" target="_blank">2602.21185</a></span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.21185" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.21185" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.21185" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span><span class="tag domain">CV</span>
    <span class="tag method">Diffusion</span><span class="tag method">Predictor-Corrector Sampling</span>
    <span class="tag task">Text Generation</span><span class="tag task">Image Generation</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

å‡å‹»æ…‹é›¢æ•£æ“´æ•£æ¨¡å‹ï¼ˆUniform-state discrete diffusion modelsï¼‰å› å…¶è‡ªæˆ‘ä¿®æ­£çš„èƒ½åŠ›è€Œåœ¨å°‘æ­¥é©Ÿç”Ÿæˆå’Œå¼•å°ä¸­è¡¨ç¾å“è¶Šï¼Œç›¸æ¯”æ–¼è‡ªè¿´æ­¸æˆ– Masked æ“´æ•£æ¨¡å‹åœ¨é€™äº›è¨­å®šä¸­æ›´å—é’çã€‚ç„¶è€Œï¼Œéš¨è‘—æ­¥é©Ÿæ•¸é‡å¢åŠ ï¼Œå®ƒå€‘çš„æ¡æ¨£å“è³ªåœ¨ç¥–å…ˆæ¡æ¨£å™¨ï¼ˆancestral samplersï¼‰ä¸Šé”åˆ°å¹³å°æœŸã€‚æˆ‘å€‘å¼•å…¥äº†ä¸€å€‹é æ¸¬å™¨-æ ¡æ­£å™¨ï¼ˆPredictor-Corrector, PCï¼‰æ¡æ¨£å™¨æ—ç¾¤ç”¨æ–¼é›¢æ•£æ“´æ•£ï¼Œè©²æ—ç¾¤æ³›åŒ–äº†å…ˆå‰çš„æ–¹æ³•ä¸¦é©ç”¨æ–¼ä»»æ„å™ªè²éç¨‹ã€‚ç•¶èˆ‡å‡å‹»æ…‹æ“´æ•£é…å°æ™‚ï¼Œæˆ‘å€‘çš„æ¡æ¨£å™¨åœ¨èªè¨€å’Œåœ–åƒå»ºæ¨¡ä¸Šéƒ½è¶…è¶Šäº†ç¥–å…ˆæ¡æ¨£ï¼Œåœ¨ OpenWebText ä¸Šé”åˆ°äº†æ›´ä½çš„ç”Ÿæˆå›°æƒ‘åº¦ï¼ˆmatched unigram entropyï¼‰ï¼Œåœ¨ CIFAR10 ä¸Šé”åˆ°äº†æ›´å¥½çš„ FID/IS åˆ†æ•¸ã€‚è‡³é—œé‡è¦çš„æ˜¯ï¼Œä¸åŒæ–¼å‚³çµ±æ¡æ¨£å™¨ï¼Œæˆ‘å€‘çš„ PC æ–¹æ³•æœƒéš¨è‘—æ¡æ¨£æ­¥é©Ÿå¢åŠ è€ŒæŒçºŒæ”¹é€²ã€‚ç¶œåˆé€™äº›ç™¼ç¾ï¼Œæˆ‘å€‘å° Masked æ“´æ•£æ˜¯åŸºæ–¼æ“´æ•£çš„èªè¨€å»ºæ¨¡å¿…ç„¶æœªä¾†é€™ä¸€å‡è¨­æå‡ºäº†è³ªç–‘ã€‚é™¤äº†æ¡æ¨£å¤–ï¼Œæˆ‘å€‘ç‚º Gaussian é¬†å¼›è¨“ç·´éšæ®µé–‹ç™¼äº†ä¸€ç¨®è¨˜æ†¶é«˜æ•ˆçš„èª²ç¨‹ï¼ˆmemory-efficient curriculumï¼‰ï¼Œç›¸æ¯”æ–¼ Duo æ¸›å°‘äº† 25% çš„è¨“ç·´æ™‚é–“å’Œ 33% çš„è¨˜æ†¶ä½”ç”¨ï¼ŒåŒæ™‚åœ¨ OpenWebText å’Œ LM1B ä¸Šä¿æŒäº†ç›¸ç•¶çš„å›°æƒ‘åº¦ä»¥åŠå¼·å¤§çš„ä¸‹æ¸¸ä»»å‹™æ€§èƒ½ã€‚æˆ‘å€‘åœ¨ä»¥ä¸‹ç¶²å€ç™¼å¸ƒäº†ç¨‹å¼ç¢¼ã€æª¢æŸ¥é»å’Œå½±ç‰‡æ•™ç¨‹ï¼šhttps://s-sahoo.com/duo-ch2</p>
  
  <div class="abstract-en">Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>