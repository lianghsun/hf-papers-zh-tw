<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>æ¸¬è©¦æ™‚è¨“ç·´èˆ‡ KV Binding æœ¬è³ªä¸Šæ˜¯ç·šæ€§æ³¨æ„åŠ›æ©Ÿåˆ¶ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">æ¸¬è©¦æ™‚è¨“ç·´èˆ‡ KV Binding æœ¬è³ªä¸Šæ˜¯ç·šæ€§æ³¨æ„åŠ›æ©Ÿåˆ¶</h1>
  
  <div class="en-title">Test-Time Training with KV Binding Is Secretly Linear Attention</div>
  

  <div class="paper-meta">
    
    <span>Junchen Liu, Sven Elflein, Or Litany, Zan Gojcic, Ruilong Li</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.21204" target="_blank">2602.21204</a></span>
    
    <span style="color:var(--text-muted);font-size:0.8rem;">
      ä¾†æºï¼šarxiv HTML
    </span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.21204" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.21204" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.21204" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span><span class="tag domain">Theory</span>
    <span class="tag method">Transformer</span><span class="tag method">Linear Attention</span><span class="tag method">Sequence Modeling</span>
    <span class="tag task">Sequence Modeling</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

æ¸¬è©¦æ™‚è¨“ç·´ï¼ˆTest-time training, TTTï¼‰æ­é… KV binding ä½œç‚ºåºåˆ—å»ºæ¨¡å±¤ï¼Œé€šå¸¸è¢«è§£é‡‹ç‚ºä¸€ç¨®ç·šä¸Šå¾Œè¨­å­¸ç¿’ï¼ˆonline meta-learningï¼‰ï¼Œåœ¨æ¸¬è©¦æ™‚è¨˜æ†¶åŒ–éµå€¼å°æ‡‰ã€‚ç„¶è€Œï¼Œæˆ‘å€‘çš„åˆ†ææ­ç¤ºäº†å¤šå€‹ç¾è±¡èˆ‡é€™ç¨®åŸºæ–¼è¨˜æ†¶åŒ–çš„è§£é‡‹ç›¸æ‚–ã€‚åŸºæ–¼é€™äº›ç™¼ç¾ï¼Œæˆ‘å€‘é‡æ–°å¯©è¦– TTT çš„å…¬å¼åŒ–ï¼Œè­‰æ˜ä¸€å¤§é¡ TTT æ¶æ§‹å¯ä»¥è¡¨ç¤ºç‚ºå­¸ç¿’ç·šæ€§æ³¨æ„åŠ›ç®—å­ï¼ˆlearned linear attention operatorï¼‰çš„å½¢å¼ã€‚é™¤äº†è§£é‡‹å…ˆå‰ä»¤äººå›°æƒ‘çš„æ¨¡å‹è¡Œç‚ºå¤–ï¼Œé€™å€‹è¦–è§’é‚„å¸¶ä¾†å¤šé …å¯¦éš›çš„å¥½è™•ï¼šå®ƒä½¿åŸç†æ€§çš„æ¶æ§‹ç°¡åŒ–æˆç‚ºå¯èƒ½ï¼Œå…è¨±å®Œå…¨ä¸¦è¡Œçš„å…¬å¼åŒ–åŒæ™‚ä¿æŒæ•ˆèƒ½ä¸¦æ”¹é€²æ•ˆç‡ï¼Œä¸¦æä¾›å°‡å¤šç¨® TTT è®Šé«”ç³»çµ±åŒ–åœ°åŒ–ç´„ç‚ºæ¨™æº–ç·šæ€§æ³¨æ„åŠ›å½¢å¼çš„æ–¹æ³•ã€‚ç¸½é«”è€Œè¨€ï¼Œæˆ‘å€‘çš„ç ”ç©¶çµæœå°‡ TTT é‡æ–°å®šç¾©ä¸æ˜¯æ¸¬è©¦æ™‚è¨˜æ†¶åŒ–ï¼Œè€Œæ˜¯å…·æœ‰å¢å¼·è¡¨ç¤ºèƒ½åŠ›çš„å­¸ç¿’ç·šæ€§æ³¨æ„åŠ›ã€‚</p>
  
  <div class="abstract-en">Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>