<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>é€šä¿¡å•Ÿç™¼çš„çµæ§‹åŒ–å½±åƒè¡¨ç¤ºæ¨™è¨˜åŒ– â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem; color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">é€šä¿¡å•Ÿç™¼çš„çµæ§‹åŒ–å½±åƒè¡¨ç¤ºæ¨™è¨˜åŒ–</h1>
  
  <div class="en-title">Communication-Inspired Tokenization for Structured Image Representations</div>
  

  <div class="paper-meta">
    
    <span>Aram Davtyan, Yusuf Sahin, Yasaman Haghighi, Sebastian Stapf, Pablo Acuaviva, Alexandre Alahi, Paolo Favaro</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.20731" target="_blank">2602.20731</a></span>
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.20731" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.20731" target="_blank" rel="noopener" class="btn btn-outline">arXiv é é¢</a>
    <a href="https://huggingface.co/papers/2602.20731" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">Multimodal</span>
    <span class="tag method">Transformer</span><span class="tag method">Flow Matching</span><span class="tag method">Discrete Tokenization</span><span class="tag method">Recurrent Encoding</span>
    <span class="tag task">Image Tokenization</span><span class="tag task">Image Reconstruction</span><span class="tag task">Semantic Representation Learning</span><span class="tag task">Compositional Generalization</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

é›¢æ•£å½±åƒ tokenizer å·²æˆç‚ºç¾ä»£è¦–è¦ºå’Œå¤šæ¨¡æ…‹ç³»çµ±çš„é—œéµå…ƒä»¶ï¼Œç‚ºåŸºæ–¼ transformer çš„æ¶æ§‹æä¾›äº†åºåˆ—åŒ–ä»‹é¢ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•¸ç¾æœ‰æ–¹æ³•ä¸»è¦ä»å„ªåŒ–æ–¼é‡å»ºå’Œå£“ç¸®ï¼Œé€šå¸¸ç”¢ç”Ÿæ•æ‰å±€éƒ¨ç´‹ç†è€Œéç‰©é«”ç´šèªç¾©çµæ§‹çš„ tokenã€‚å—äººé¡æºé€šçš„å¢é‡å’Œçµ„æˆç‰¹æ€§å•Ÿç™¼ï¼Œæˆ‘å€‘å¼•å…¥ COMmunication å•Ÿç™¼çš„ Tokenizationï¼ˆCOMiTï¼‰ï¼Œä¸€å€‹ç”¨æ–¼å­¸ç¿’çµæ§‹åŒ–é›¢æ•£è¦–è¦º token åºåˆ—çš„æ¡†æ¶ã€‚COMiT é€šéè¿­ä»£è§€å¯Ÿå±€éƒ¨å½±åƒè£å‰ªå’Œå¾ªç’°æ›´æ–°å…¶é›¢æ•£è¡¨ç¤ºï¼Œåœ¨å›ºå®š token é ç®—å…§æ§‹é€ æ½›åœ¨è¨Šæ¯ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæ¨¡å‹æ•´åˆæ–°çš„è¦–è¦ºè³‡è¨Šï¼ŒåŒæ™‚ç²¾ç…‰å’Œé‡æ–°çµ„ç¹”ç¾æœ‰çš„ token åºåˆ—ã€‚ç¶“éæ•¸å€‹ç·¨ç¢¼è¿­ä»£å¾Œï¼Œæœ€çµ‚è¨Šæ¯æ¢ä»¶åŒ–ä¸€å€‹ flow-matching è§£ç¢¼å™¨ä¾†é‡å»ºå®Œæ•´å½±åƒã€‚ç·¨ç¢¼å’Œè§£ç¢¼éƒ½åœ¨å–®ä¸€ transformer æ¨¡å‹å…§å¯¦ç¾ï¼Œä¸¦ä½¿ç”¨ flow-matching é‡å»ºå’Œèªç¾©è¡¨ç¤ºå°é½æå¤±çµ„åˆçš„ç«¯åˆ°ç«¯è¨“ç·´ã€‚æˆ‘å€‘çš„å¯¦é©—è­‰æ˜ï¼Œé›–ç„¶èªç¾©å°é½æä¾›äº†åŸºç¤ï¼Œä½†å°ˆæ³¨çš„åºåˆ— tokenization å°æ–¼å¼•ç™¼å¯è§£é‡‹çš„ç‰©é«”ä¸­å¿ƒ token çµæ§‹ï¼Œä»¥åŠé¡¯è‘—æ”¹å–„ç›¸æ¯”å…ˆå‰æ–¹æ³•çš„çµ„åˆæ³›åŒ–å’Œé—œä¿‚æ¨ç†è‡³é—œé‡è¦ã€‚</p>
  
  <div class="abstract-en">Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.</div>
  
</div>

<!-- Full paper content -->
<div class="paper-body">
  
</div>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>