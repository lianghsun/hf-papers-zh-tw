<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>é¢¨éšªæ„ŸçŸ¥å‹ä¸–ç•Œæ¨¡å‹é æ¸¬æ§åˆ¶ç”¨æ–¼å¯æ³›åŒ–ç«¯åˆ°ç«¯è‡ªå‹•é§•é§› â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-27/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-27 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">é¢¨éšªæ„ŸçŸ¥å‹ä¸–ç•Œæ¨¡å‹é æ¸¬æ§åˆ¶ç”¨æ–¼å¯æ³›åŒ–ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›</h1>
  
  <div class="en-title">Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving</div>
  

  <div class="paper-meta">
    
    <span>Jiangxin Sun, Feng Xue, Teng Long, Chang Liu, Jian-Fang Hu, Wei-Shi Zheng, Nicu Sebe</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.23259" target="_blank">2602.23259</a></span>
    
    <span style="color:var(--text-muted);font-size:0.8rem;">
      ä¾†æºï¼šPDF + DotsOCR
    </span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.23259" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.23259" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.23259" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">RL</span><span class="tag domain">Robotics</span>
    <span class="tag method">World Model</span><span class="tag method">Model Predictive Control</span><span class="tag method">Imitation Learning</span><span class="tag method">Risk Evaluation</span><span class="tag method">Distillation</span>
    <span class="tag task">Autonomous Driving</span><span class="tag task">End-to-End Learning</span><span class="tag task">Action Prediction</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p>éš¨è‘—æ¨¡ä»¿å­¸ç¿’ï¼ˆILï¼‰å’Œå¤§è¦æ¨¡é§•é§›è³‡æ–™é›†çš„é€²æ­¥ï¼Œç«¯åˆ°ç«¯è‡ªå‹•é§•é§›ï¼ˆE2E-ADï¼‰è¿‘æœŸå–å¾—äº†é‡å¤§é€²å±•ã€‚ç›®å‰ï¼ŒåŸºæ–¼ IL çš„æ–¹æ³•å·²æˆç‚ºä¸»æµç¯„å¼ï¼šæ¨¡å‹ä¾è³´æ–¼å°ˆå®¶æä¾›çš„æ¨™æº–é§•é§›è¡Œç‚ºï¼Œä¸¦å­¸ç¿’æœ€å°åŒ–å…¶è¡Œå‹•èˆ‡å°ˆå®¶è¡Œå‹•ä¹‹é–“çš„å·®ç•°ã€‚ç„¶è€Œï¼Œé€™å€‹ã€Œåƒ…ä¾ç…§å°ˆå®¶è¡Œé§›ã€çš„ç›®æ¨™å­˜åœ¨æ³›åŒ–èƒ½åŠ›æœ‰é™çš„å•é¡Œï¼šç•¶é‡åˆ°è¶…å‡ºå°ˆå®¶ç¤ºç¯„åˆ†ä½ˆçš„ç½•è¦‹æˆ–æœªè¦‹éçš„é•·å°¾å ´æ™¯æ™‚ï¼Œåœ¨ç¼ºä¹å…ˆå‰ç¶“é©—çš„æƒ…æ³ä¸‹ï¼Œæ¨¡å‹å‚¾å‘æ–¼åšå‡ºä¸å®‰å…¨çš„æ±ºç­–ã€‚é€™å¼•ç™¼äº†ä¸€å€‹æ ¹æœ¬æ€§å•é¡Œï¼šE2E-AD ç³»çµ±èƒ½å¦åœ¨æ²’æœ‰ä»»ä½•å°ˆå®¶è¡Œå‹•ç›£ç£çš„æƒ…æ³ä¸‹åšå‡ºå¯é çš„æ±ºç­–ï¼Ÿå—æ­¤å•Ÿç™¼ï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹åç‚º Risk-aware World Model Predictive Controlï¼ˆRaWMPCï¼‰çš„çµ±ä¸€æ¡†æ¶ï¼Œé€éé­¯æ£’æ§åˆ¶ä¾†è§£æ±ºé€™å€‹æ³›åŒ–å›°å¢ƒï¼Œè€Œä¸ä¾è³´æ–¼å°ˆå®¶ç¤ºç¯„ã€‚åœ¨å¯¦è¸ä¸­ï¼ŒRaWMPC åˆ©ç”¨ä¸–ç•Œæ¨¡å‹ä¾†é æ¸¬å¤šå€‹å€™é¸è¡Œå‹•çš„å¾Œæœï¼Œä¸¦é€éæ˜ç¢ºçš„é¢¨éšªè©•ä¼°é¸æ“‡ä½é¢¨éšªè¡Œå‹•ã€‚ç‚ºäº†è³¦äºˆä¸–ç•Œæ¨¡å‹é æ¸¬é¢¨éšªé§•é§›è¡Œç‚ºçµæœçš„èƒ½åŠ›ï¼Œæˆ‘å€‘è¨­è¨ˆäº†ä¸€ç¨®é¢¨éšªæ„ŸçŸ¥çš„äº¤äº’ç­–ç•¥ï¼Œç³»çµ±æ€§åœ°å°‡ä¸–ç•Œæ¨¡å‹æš´éœ²æ–¼å±éšªè¡Œç‚ºï¼Œä½¿å¾—ç½é›£æ€§å¾Œæœå¯é æ¸¬ï¼Œé€²è€Œå¯ä»¥é¿å…ã€‚æ­¤å¤–ï¼Œç‚ºäº†åœ¨æ¸¬è©¦æ™‚ç”Ÿæˆä½é¢¨éšªçš„å€™é¸è¡Œå‹•ï¼Œæˆ‘å€‘å¼•å…¥äº†è‡ªè©•ä¼°è’¸é¤¾æ–¹æ³•ï¼Œå°‡ç¶“éå……åˆ†è¨“ç·´çš„ä¸–ç•Œæ¨¡å‹ä¸­çš„é¿é¢¨éšªèƒ½åŠ›è’¸é¤¾åˆ°ä¸€å€‹ç”Ÿæˆå¼è¡Œå‹•ææ¡ˆç¶²çµ¡ä¸­ï¼Œç„¡éœ€ä»»ä½•å°ˆå®¶ç¤ºç¯„ã€‚å»£æ³›çš„å¯¦é©—è¡¨æ˜ï¼ŒRaWMPC åœ¨åˆ†ä½ˆå…§å’Œåˆ†ä½ˆå¤–å ´æ™¯ä¸­éƒ½å„ªæ–¼æœ€å…ˆé€²çš„æ–¹æ³•ï¼ŒåŒæ™‚æä¾›äº†å„ªè¶Šçš„æ±ºç­–å¯è§£é‡‹æ€§ã€‚</p>
  
  <div class="abstract-en">With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of &#34;only driving like the expert&#34; suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p>arXiv:2602.23229v1 [cs.CV] 26 Sep 2022</p>
<h1 id="_1">é¢¨éšªæ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹é æ¸¬æ§åˆ¶ç”¨æ–¼å¯æ¨å»£çš„ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›</h1>
<p>Jiangxin SunÂ¹, Feng XueÂ¹, Teng LongÂ¹, Chang LiuÂ¹, Jian-Fang HuÂ², Wei-Shi ZhengÂ², Nicu SebeÂ¹</p>
<p>Â¹University of Trento, Trento, Italy, Â²Sun Yat-sen University, Guangzhou, China</p>
<p>jiangxin.sun@unitn.it, feng.xue@unitn.it, teng.long@unitn.it, chang.liu@unitn.it, hujianf@mail.sysu.edu.cn, wszheng@ieee.org, nicu.sebe@unitn.it</p>
<h2 id="_2">æ‘˜è¦</h2>
<p>éš¨è‘—æ¨¡ä»¿å­¸ç¿’ï¼ˆILï¼‰å’Œå¤§è¦æ¨¡é§•é§›è³‡æ–™é›†çš„é€²å±•ï¼Œç«¯åˆ°ç«¯è‡ªå‹•é§•é§›ï¼ˆE2E-ADï¼‰æœ€è¿‘å–å¾—äº†é‡å¤§é€²å±•ã€‚ç›®å‰ï¼ŒåŸºæ–¼ IL çš„æ–¹æ³•å·²æˆç‚ºä¸»æµç¯„ä¾‹ï¼šæ¨¡å‹ä¾è³´æ–¼å°ˆå®¶æä¾›çš„æ¨™æº–é§•é§›è¡Œç‚ºï¼Œä¸¦å­¸ç¿’æœ€å°åŒ–å…¶å‹•ä½œèˆ‡å°ˆå®¶å‹•ä½œä¹‹é–“çš„å·®ç•°ã€‚ç„¶è€Œï¼Œã€Œåƒ…åƒå°ˆå®¶é‚£æ¨£é§•é§›ã€é€™ä¸€ç›®æ¨™å­˜åœ¨æ³›åŒ–èƒ½åŠ›æœ‰é™çš„å•é¡Œï¼šç•¶é­é‡è¶…å‡ºå°ˆå®¶ç¤ºç¯„åˆ†ä½ˆä¹‹å¤–çš„ç½•è¦‹æˆ–æœªè¦‹é•·å°¾æƒ…æ™¯æ™‚ï¼Œæ¨¡å‹åœ¨ç¼ºä¹å…ˆé©—ç¶“é©—çš„æƒ…æ³ä¸‹å¾€å¾€æœƒç”¢ç”Ÿä¸å®‰å…¨çš„æ±ºç­–ã€‚é€™å¼•ç™¼äº†ä¸€å€‹æ ¹æœ¬å•é¡Œï¼šE2E-AD ç³»çµ±èƒ½å¦åœ¨æ²’æœ‰ä»»ä½•å°ˆå®¶å‹•ä½œç›£ç£çš„æƒ…æ³ä¸‹åšå‡ºå¯é çš„æ±ºç­–ï¼Ÿå—æ­¤å•Ÿç™¼ï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹åç‚ºé¢¨éšªæ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹é æ¸¬æ§åˆ¶ï¼ˆRaWMPCï¼‰çš„çµ±ä¸€æ¡†æ¶ï¼Œä»¥é€éé­¯æ£’æ§åˆ¶ä¾†è§£æ±ºé€™ä¸€æ³›åŒ–å›°å¢ƒï¼Œç„¡éœ€ä¾è³´å°ˆå®¶ç¤ºç¯„ã€‚åœ¨å¯¦è¸ä¸­ï¼ŒRaWMPC åˆ©ç”¨ä¸–ç•Œæ¨¡å‹é æ¸¬å¤šå€‹å€™é¸å‹•ä½œçš„å¾Œæœï¼Œä¸¦é€éæ˜ç¢ºçš„é¢¨éšªè©•ä¼°é¸æ“‡ä½é¢¨éšªå‹•ä½œã€‚ç‚ºäº†è³¦äºˆä¸–ç•Œæ¨¡å‹é æ¸¬å±éšªé§•é§›è¡Œç‚ºçµæœçš„èƒ½åŠ›ï¼Œæˆ‘å€‘è¨­è¨ˆäº†ä¸€ç¨®é¢¨éšªæ„ŸçŸ¥äº¤äº’ç­–ç•¥ï¼Œç³»çµ±åœ°å°‡ä¸–ç•Œæ¨¡å‹æš´éœ²æ–¼å±éšªè¡Œç‚ºä¸­ï¼Œä½¿ç½é›£æ€§çµæœå¯é æ¸¬ï¼Œé€²è€Œå¯é¿å…ã€‚æ­¤å¤–ï¼Œç‚ºäº†åœ¨æ¸¬è©¦æ™‚ç”Ÿæˆä½é¢¨éšªå€™é¸å‹•ä½œï¼Œæˆ‘å€‘å¼•å…¥äº†è‡ªè©•ä¼°è’¸é¤¾æ–¹æ³•ï¼Œä»¥å°‡é¢¨éšªè¦é¿èƒ½åŠ›å¾è¨“ç·´è‰¯å¥½çš„ä¸–ç•Œæ¨¡å‹è’¸é¤¾åˆ°ç”Ÿæˆå‹•ä½œææ¡ˆç¶²çµ¡ä¸­ï¼Œç„¡éœ€ä»»ä½•å°ˆå®¶ç¤ºç¯„ã€‚å»£æ³›çš„å¯¦é©—è¡¨æ˜ï¼ŒRaWMPC åœ¨åˆ†ä½ˆå…§å’Œåˆ†ä½ˆå¤–æƒ…æ™¯ä¸­éƒ½è¶…è¶Šäº†æœ€å…ˆé€²çš„æ–¹æ³•ï¼ŒåŒæ™‚æä¾›äº†å“è¶Šçš„æ±ºç­–å¯è§£é‡‹æ€§ã€‚</p>
<p>æ—¥æœŸï¼š2026 å¹´ 2 æœˆ 27 æ—¥
å°æ‡‰ä½œè€…ï¼šFeng Xueï¼Œé›»å­éƒµä»¶ feng.xue@unitn.it</p>
<h1 id="1">1 ç°¡ä»‹</h1>
<p>ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›ï¼ˆE2E-ADï¼‰[20, 100, 44, 102] æ—¨åœ¨å°‡æ„Ÿæ¸¬å™¨è§€æ¸¬æ˜ å°„åˆ°æ§åˆ¶å‹•ä½œï¼Œä¾‹å¦‚è½‰å‘ã€æ²¹é–€å’Œç…è»Šï¼Œè€Œä¸ä¾è³´æ–¼æ‰‹å·¥è£½ä½œçš„æ„ŸçŸ¥ã€é æ¸¬æˆ–è¦åŠƒæ¨¡çµ„ã€‚èˆ‡å‚³çµ±çš„æ¨¡çµ„åŒ–</p>
<p>ç®¡é“ [48, 51, 53, 74, 76, 101] ç›¸æ¯”ï¼ŒE2E-AD æä¾›äº†é§•é§›ä»»å‹™çš„æ›´çµ±ä¸€çš„è¡¨ç¤ºï¼Œä½¿ç­–ç•¥èƒ½å¤ æ¨ç†è‡ªè»Šèˆ‡å‹•æ…‹ç’°å¢ƒä¹‹é–“çš„è¤‡é›œäº’å‹•ã€‚å› æ­¤ï¼ŒE2E-AD ç”±æ–¼å…¶ç°¡åŒ–ç³»çµ±è¨­è¨ˆã€è¯åˆæœ€ä½³åŒ–å’Œå³æ™‚æ±ºç­–çš„æ½›åŠ›è€Œå¸å¼•äº†è¶Šä¾†è¶Šå¤šçš„é—œæ³¨ã€‚</p>
<hr />
<p>[FIGURE: 1] ç¾æœ‰ E2E-AD æ–¹æ³•èˆ‡ RaWMPC çš„æ¯”è¼ƒã€‚ç¬¬ä¸€åˆ—é¡¯ç¤ºé æ¸¬è»Œè·¡ï¼Œç¬¬äºŒåˆ—æ¯”è¼ƒæ ¸å¿ƒå·¥ä½œæµç¨‹ã€‚é»‘è‰²ç®­é ­è¡¨ç¤ºæ¸¬è©¦æ™‚åŸ·è¡Œï¼Œç²‰ç´…è‰²ç®­é ­è¡¨ç¤ºåƒ…è¨“ç·´æ­¥é©Ÿã€‚æ¯”è¼ƒé¡¯ç¤ºå…ˆå‰çš„æ–¹æ³•é€šå¸¸å¿½ç•¥é¡¯å¼å±éšªå»ºæ¨¡ä¸¦å¯èƒ½è§¸ç™¼äº¤é€šé•è¦ï¼Œè€Œ RaWMPC ä½¿ç”¨é¢¨éšªæ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹ä¾†è©•ä¼°å‹•ä½œå¾Œæœä¸¦åœ¨é—œéµå ´æ™¯ä¸­é¸æ“‡å®‰å…¨ã€åˆè¦çš„å‹•ä½œã€‚</p>
<p>é—œæ–¼ E2E-AD çš„æ—©æœŸç ”ç©¶ [4, 6, 75] ä¸»è¦é—œæ³¨é€šéç·šä¸Šæ¢ç´¢çš„å¼·åŒ–å­¸ç¿’ï¼ˆRLï¼‰ä¾†å­¸ç¿’é§•é§›ç­–ç•¥ã€‚æ›´è¿‘æœŸçš„ç ”ç©¶ [5, 95] è­‰æ˜äº†åˆ©ç”¨ç‰¹æ¬Šè³‡è¨Šï¼ˆä¾‹å¦‚é³¥ç°åˆ†å‰²å’Œé«˜æ¸…åœ°åœ–ï¼‰çš„ <strong>åŸºæ–¼è¦å‰‡</strong> æˆ– <strong>åŸºæ–¼RL</strong> çš„æ™ºèƒ½é«”å¯ä»¥ç”¢ç”Ÿæ›´å„ªè¶Šçš„é§•é§›æ±ºç­–ã€‚åŸºæ–¼é€™äº›è¦‹è§£ï¼Œæœ€å…ˆé€²çš„æ–¹æ³• [28, 62, 63, 81, 90] é€šå¸¸éµå¾ª <strong>æ¨¡ä»¿å­¸ç¿’ï¼ˆILï¼‰æ¡†æ¶</strong>ï¼Œå¦‚åœ–1æ‰€ç¤ºï¼Œå…¶ä¸­åƒ…ä½¿ç”¨æ„Ÿæ¸¬å™¨è¼¸å…¥ï¼ˆä¾‹å¦‚ RGB å½±åƒå’Œ LiDARï¼‰çš„æ™ºèƒ½é«”è¢«è¨“ç·´ç‚ºé€šéé§•é§›ç­–ç•¥å’Œæ½›åœ¨ç‰¹å¾µçš„çŸ¥è­˜è’¸é¤¾ä¾†è¤‡è£½ç‰¹æ¬Šå°ˆå®¶çš„è¡Œç‚ºã€‚å„˜ç®¡ä¸€äº›å·¥ä½œè©¦åœ–é€šéæœªä¾†é‹å‹•å»ºæ¨¡ [55, 62, 63, 73]ã€å‹•ä½œæ„ŸçŸ¥çš„æœªä¾†é æ¸¬ [23, 28, 38, 39] å’Œå¤§å‹èªè¨€æ¨¡å‹çš„æ•´åˆ [14, 21, 35, 57] ä¾†å¢å¼·é§•é§›æ€§èƒ½ï¼Œä½†é€™äº›æ–¹æ³•ä»ç„¶éµå¾ªã€Œ<em>åƒå°ˆå®¶ä¸€æ¨£é§•é§›</em>ã€çš„å­¸ç¿’ç›®æ¨™ï¼Œå¦‚åœ–1ï¼ˆaï¼‰æ‰€ç¤ºã€‚å®ƒå€‘ç„¡æ³•å¾æ ¹æœ¬ä¸Šè§£æ±ºæ¨¡ä»¿å­¸ç¿’å›ºæœ‰çš„æ³›åŒ–å›°å¢ƒï¼š<em>ç”±æ–¼å°ˆå®¶ç¤ºç¯„ç„¡æ³•æ¶µè“‹æ‰€æœ‰æƒ…æ™¯å’Œç‹€æ³ï¼ŒåŸºæ–¼æ¨¡ä»¿çš„ç­–ç•¥åœ¨é‡åˆ°å°ˆå®¶ç¤ºç¯„ä¹‹å¤–çš„æœªè¦‹éçš„æƒ…æ™¯æ™‚å¾€å¾€æœƒç”¢ç”Ÿä¸å¯é æ¸¬ä¸”é€šå¸¸ä¸å®‰å…¨çš„é§•é§›è¡Œç‚º</em>ã€‚æœ€è¿‘ï¼Œ<strong>åŸºæ–¼æ¨¡å‹çš„ RL æ–¹æ³•</strong> [37, 85] å·²ç¶“å‡ºç¾ä¸¦è©¦åœ–é€šéå­¸ç¿’ç’°å¢ƒå‹•æ…‹ä¸¦åœ¨å…¶ä¸Šé€²è¡Œè¦åŠƒä¾†æ”¹é€²æ³›åŒ–ã€‚ç„¶è€Œï¼Œå¦‚åœ–1ï¼ˆbï¼‰æ‰€ç¤ºï¼Œå…¶ä¸­å¤§å¤šæ•¸ä»ç„¶æ—¨åœ¨æœ€å¤§åŒ–é æœŸçå‹µï¼Œä¸¦ç¼ºä¹å°ç½•è¦‹ä½†é«˜é¢¨éšªæƒ…æ³çš„é¡¯å¼å»ºæ¨¡å’Œæ¡æ¨£ï¼Œå› æ­¤åœ¨é€™äº›å ´æ™¯ä¸­ç¹¼çºŒé›£ä»¥ä¿è­‰å®‰å…¨æ€§ã€‚</p>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å€‘ä¸»å¼µã€Œ<em>ä½¿ E2E-AD ç³»çµ±èƒ½å¤ å­¸ç¿’å’Œä¸»å‹•é¿å…é¢¨éšªå‹•ä½œæ¯”é€å­—è¤‡è£½å°ˆå®¶é§•é§›è¡Œç‚ºæ›´é‡è¦</em>ã€ã€‚å—åˆ°é€™ä¸€è§€é»çš„å•Ÿç™¼ï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹ E2E-AD æ¡†æ¶ï¼Œä¸éœ€è¦ä»»ä½•å°ˆå®¶å‹•ä½œç›£ç£ï¼Œç¨±ç‚º <strong>é¢¨éšªæ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹é æ¸¬æ§åˆ¶ï¼ˆRaWMPCï¼‰</strong>ï¼Œå¦‚åœ–1ï¼ˆcï¼‰æ‰€ç¤ºã€‚è©²æ¡†æ¶æ”¾æ£„äº†å°ˆå®¶ç¤ºç¯„ï¼Œè€Œæ˜¯åˆ©ç”¨é¢¨éšªæ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹ä¾†é©…å‹•é æ¸¬æ§åˆ¶ä»¥å…‹æœæ³›åŒ–æŒ‘æˆ°ã€‚èˆ‡è¨“ç·´æ¼”å“¡å¾ä¸–ç•Œæ¨¡å‹çš„æƒ³åƒæ¨å»£ä¸­æœ€å¤§åŒ–çå‹µçš„åŸºæ–¼æ¨¡å‹çš„ RL ä¸åŒï¼ŒRaWMPC ä¸­çš„ä¸–ç•Œæ¨¡å‹ç‚ºä¸€çµ„ã€Œå€™é¸ã€é§•é§›è¡Œç‚ºé æ¸¬è¿‘æœŸç‹€æ…‹ä¸¦æ˜ç¢ºè©•ä¼°å…¶é¢¨éšªï¼Œä»¥ä¾¿é¸æ“‡æœ€ä½é¢¨éšªçš„å€™é¸ã€‚ç‚ºäº†è³¦äºˆæˆ‘å€‘çš„ä¸–ç•Œæ¨¡å‹é¢¨éšªæ„ŸçŸ¥èƒ½åŠ›ï¼Œæˆ‘å€‘å¼•å…¥äº† <strong>é¢¨éšªæ„ŸçŸ¥äº’å‹•</strong> ç­–ç•¥ï¼šå¾é›¶é–‹å§‹ï¼Œæ¨¡å‹é¸æ“‡è‡ªæˆ‘è­˜åˆ¥çš„é«˜é¢¨éšªå‹•ä½œèˆ‡ç’°å¢ƒäº’å‹•ï¼Œæˆ‘å€‘çš„ä¸–ç•Œæ¨¡å‹å¾ä¸­å­¸ç¿’é æ¸¬å¤šæ¨£åŒ–é¢¨éšªè¡Œç‚ºçš„å¾Œæœã€‚æ²’æœ‰ä»»ä½•å°ˆå®¶ç¤ºç¯„ï¼Œæˆ‘å€‘çš„ä¸–ç•Œæ¨¡å‹å¯ä»¥å¾é›¶é–‹å§‹é”åˆ°å¼·å¤§çš„æ€§èƒ½ï¼Œä¸¦ä¸”å¦‚æœåŸºæº–æä¾›çš„å°‘é‡è¦–é »ç‰‡æ®µé€²è¡Œè¼•åº¦é ç†±ï¼Œå¯ä»¥é€²ä¸€æ­¥åŠ é€Ÿ</p>
<hr />
<p>æœ€å¾Œï¼Œç‚ºäº†åœ¨æ¸¬è©¦æ™‚æœ‰æ•ˆåœ°æä¾›ä½é¢¨éšªå€™é¸ï¼Œæˆ‘å€‘é€²ä¸€æ­¥æå‡ºäº†é§•é§›ç­–ç•¥å­¸ç¿’çš„ <strong>è‡ªæˆ‘è©•ä¼°è’¸é¤¾</strong>ã€‚è¨“ç·´è‰¯å¥½çš„ä¸–ç•Œæ¨¡å‹è¢«åˆ©ç”¨ä¾†è­˜åˆ¥æ¡æ¨£å‹•ä½œç©ºé–“ä¸­çš„å®‰å…¨å’Œé¢¨éšªè¡Œç‚ºï¼Œä¸¦é€šéå®‰å…¨é¢¨éšªå°æ¯”å­¸ç¿’å°‡é€™ä¸€çŸ¥è­˜è’¸é¤¾åˆ°ç”Ÿæˆå‹•ä½œæè­°ç¶²è·¯ä¸­ã€‚åœ¨ Bench2Drive å’Œ NAVSIM ä¸Šçš„å¯¦é©—è¡¨æ˜ï¼ŒRaWMPC å³ä½¿æ²’æœ‰å°ˆå®¶ç¤ºç¯„ï¼Œä¹Ÿè¶…éäº†ä»¥å‰çš„æœ€å…ˆé€²æ–¹æ³•ï¼Œè€Œå¯é¸çš„è¼•åº¦é ç†±é€²ä¸€æ­¥æœ‰åŠ©æ–¼åŠ é€Ÿæ”¶æ–‚å’Œæ”¹é€²æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç”±æ–¼ RaWMPC å¾äº’å‹•è€Œä¸æ˜¯å°ˆå®¶æ¨™ç±¤ä¸­å­¸ç¿’é¢¨éšªæ„ŸçŸ¥ï¼Œå®ƒåœ¨ä»¥å‰æœªè¦‹éçš„å ´æ™¯ä¸­å¯¦ç¾äº†å¤§å¹…æ›´é«˜çš„é§•é§›æ€§èƒ½ã€‚æˆ‘å€‘çš„ä¸»è¦è²¢ç»å¯ä»¥ç¸½çµå¦‚ä¸‹ï¼š</p>
<ul>
<li>æˆ‘å€‘æå‡º RaWMPCï¼Œä¸€å€‹é›¶å°ˆå®¶è¦æ±‚çš„ E2E-AD æ¡†æ¶ã€‚èˆ‡ IL å’Œ MBRL æ–¹æ³•ä¸åŒï¼ŒRaWMPC ä½¿ç”¨ä¸–ç•Œæ¨¡å‹å¾å¤šå€‹å€™é¸å‹•ä½œä¸­é¸æ“‡ä½é¢¨éšªè¡Œç‚ºï¼Œé€™è‡ªç„¶æ”¹é€²äº†å…¶æ±ºç­–çš„å¯è§£é‡‹æ€§å’Œå¯é æ€§ã€‚</li>
<li>åœ¨ RaWMPC ä¸­ï¼Œæˆ‘å€‘è¨­è¨ˆäº†ä¸€å€‹é¢¨éšªæ„ŸçŸ¥äº’å‹•å­¸ç¿’ç­–ç•¥ï¼Œä½¿ä¸–ç•Œæ¨¡å‹èƒ½å¤ ç´”ç²¹å¾ç’°å¢ƒäº’å‹•ä¸­ç²å¾—é¢¨éšªæ„ŸçŸ¥ï¼Œç„¡éœ€ä»»ä½•å°ˆå®¶ç¤ºç¯„ã€‚</li>
<li>æˆ‘å€‘ç‚ºé§•é§›ç­–ç•¥å­¸ç¿’å¼•å…¥äº†è‡ªæˆ‘è©•ä¼°è’¸é¤¾æ–¹æ¡ˆï¼Œå³ä½¿åœ¨æ¸¬è©¦æ™‚æä¾›é«˜å“è³ªå€™é¸å‹•ä½œï¼Œç”šè‡³è¶…éç›´æ¥å¾å°ˆå®¶ç¤ºç¯„å­¸ç¿’çš„ç­–ç•¥ã€‚</li>
</ul>
<h1 id="2-related-work">2 Related Work</h1>
<h2 id="21-end-to-end-learning-in-autonomous-driving">2.1 End-to-End Learning in Autonomous Driving</h2>
<p>Learning-based autonomous driving approaches typically follow two main paradigms: imitation learning (IL) and reinforcement learning (RL) [7, 9]. Early studies explored RL extensively [1, 4, 6, 20, 32, 51, 58, 59, 75, 93] for its natural capability to refine driving strategies through interactive feedback. Subsequent works [37, 95] demonstrated that training sensor-based agents to imitate RL experts' behavior could lead to superior performance, prompting a shift toward leveraging privileged information (e.g., BEV segmentation and HD maps) to train stronger RL experts. More recently, model-based RL has revisited this line by learning explicit dynamics/world</p>
<p>models and performing look-ahead rollouts [85], improving consequence-aware evaluation and sample efficiency. Nevertheless, these RL approaches are commonly driven by maximizing expected return, and they rarely provide an explicit mechanism to systematically discover and model rare-but-catastrophic outcomes, making reliable decisions in long-tail, high-risk scenarios still challenging.</p>
<p>With large-scale driving datasets, imitation learning has attracted increased attention and achieved state-of-the-art performance in closed-loop autonomous driving [40, 56, 72, 83]. Most IL approaches rely on collecting trajectory data and features from RL-based [37, 95] or rule-based [10, 65] experts using privileged information, and train sensor-based IL agents to replicate expert behaviors through knowledge distillation. A variety of research studies have explored ways to improve IL performance, such as multi-modal information fusion [10, 26, 54], object motion modeling [30, 55, 62, 63], action-aware future prediction [23, 28, 38, 39], feature alignment [27, 90], and the integration of large language models [14, 45, 56, 57, 64, 65, 83, 86]. Despite impressive results, the core objective of â€œdriving like the expertâ€ inherently limits generalization: expert demonstrations cannot cover all long-tail situations, and experts typically avoid dangerous behaviors, leaving IL agents with limited supervision on how to recognize and proactively avoid high-risk actions. Moreover, pure imitation often provides limited interpretability because it outputs a single action without explicitly comparing alternative actions via consequence evaluation. Motivated by these insights, we propose RaWMPC, a unified framework that replaces expert action supervision with risk-aware predictive control: it learns a risk-aware world model via a risk-aware interaction strategy that deliberately exposes the model to risky behaviors, and uses the learned model to predict and evaluate the consequences of multiple candidate actions, selecting low-risk behaviors with explicit risk evaluation.</p>
<h2 id="22-world-models">2.2 World Models</h2>
<p>World models è¿‘ä¼¼ Markov Decision Process ä¸‹çš„ç’°å¢ƒè½‰æ›ï¼Œé€éå¾ç•¶å‰è§€å¯Ÿå’Œå‹•ä½œé æ¸¬æœªä¾†ç‹€æ…‹å’Œçå‹µï¼Œåœ¨å¼·åŒ–å­¸ç¿’ä¸­å–å¾—äº†æˆåŠŸ [17-20, 34, 46, 75, 77, 79]ã€‚ç„¶è€Œï¼Œå°‡é€™äº›æ¨¡å‹æ‡‰ç”¨æ–¼è‡ªå‹•é§•é§›ç­‰è¤‡é›œä»»å‹™ä»ç„¶å…·æœ‰æŒ‘æˆ°æ€§ã€‚å…ˆå‰çš„ç ”ç©¶ [1, 12, 15, 24, 36, 49, 50, 70-72, 78, 92, 96-98, 102] ä¸»è¦åˆ©ç”¨ world models ç”Ÿæˆå¯æ§çš„æœªä¾†é§•é§›è»Œè·¡ï¼ˆä¾‹å¦‚ RGB å½±åƒå’Œ 3D/4D è¡¨ç¤ºï¼‰ï¼Œä»¥ç‰¹å®šå‹•ä½œå’Œå ´æ™¯æè¿°ç‚ºæ¢ä»¶ã€‚é€™é¡é æ¸¬æ¨¡å‹å¯ä»¥æ“´å¤§è¨“ç·´è³‡æ–™ä¸¦å¢åŠ å¤šæ¨£æ€§ï¼Œå¯èƒ½æœ‰ç›Šæ–¼ä¸‹æ¸¸çš„æ¨¡ä»¿å­¸ç¿’ï¼Œç‰¹åˆ¥æ˜¯å°æ–¼äº¤é€šäº‹æ•…ç­‰ç½•è¦‹å ´æ™¯ã€‚</p>
<p>é™¤äº†é æ¸¬ä¹‹å¤–ï¼Œä¸€äº›æœ€è¿‘çš„å˜—è©¦å·²åˆ©ç”¨ world models æ”¹é€²é–‰ç’°è‡ªå‹•é§•é§›æ•ˆèƒ½ã€‚ç‰¹åˆ¥æ˜¯ï¼Œä¸€äº›ç ”ç©¶é–‹å§‹å°‡ world modeling èˆ‡é§•é§›è¨­å®šä¸­çš„è¦åŠƒå’Œç·šä¸Šè©•ä¼°ç›¸é€£æ¥ [8, 39, 91]ï¼Œè€Œå…¶ä»–ç ”ç©¶å‰‡æä¾›é«˜ä¿çœŸç”Ÿæˆå¹³å°ä»¥æ”¯æ´é–‰ç’°è©•ä¼° [3, 11, 84]ã€‚ç‰¹åˆ¥æ˜¯ï¼ŒåŸºæ–¼æ¨¡å‹çš„ IL æ–¹æ³• [23, 38, 39, 61] æ¡ç”¨ world models æ”¯æ´æ›´å¥½çš„æ¨¡ä»¿ï¼šLAW [38] é€éåœ¨æ½›åœ¨ world model ä¸­é æ¸¬æœªä¾†è³‡è¨Šä¾†å¢å¼·ç«¯å°ç«¯é§•é§›ä»¥å”åŠ©ç­–ç•¥å­¸ç¿’ï¼ŒWoTE [39] é€é BEV world model åŸ·è¡Œç·šä¸Šè»Œè·¡è©•ä¼°ä¾†å°å€™é¸è»Œè·¡è©•åˆ†ã€‚æœ€è¿‘ï¼ŒåŸºæ–¼æ¨¡å‹çš„ RL æ–¹æ³•å¼•èµ·äº†è¶Šä¾†è¶Šå¤šçš„é—œæ³¨ã€‚Think2Drive [37] è¨­è¨ˆäº†åŸºæ–¼æ¨¡å‹çš„ RL å°ˆå®¶ä¾†é æ¸¬å‹•ä½œæ¢ä»¶çš„æœªä¾†çå‹µä»¥è¨“ç·´æ›´æœ‰æ•ˆçš„æ‰¹è©•å®¶ç¶²è·¯ï¼ŒRaw2Drive [85] åˆ©ç”¨å¾ç‰¹æ¬Šå°ˆå®¶é è¨“ç·´çš„ world model ä¾†æŒ‡å°æ„Ÿæ¸¬å™¨ä»£ç†çš„å­¸ç¿’ã€‚å„˜ç®¡æœ‰é€™äº›é€²å±•ï¼Œå¤§å¤šæ•¸ç¾æœ‰å·¥ä½œä»ç„¶å¾å°ˆå®¶æˆ–çå‹µç¹¼æ‰¿ç›£ç£ï¼Œå®ƒå€‘ä¸»è¦é—œæ³¨æ¨¡ä»¿ä¿çœŸåº¦æˆ–é æœŸå›å ±æœ€å¤§åŒ–ï¼Œç¼ºä¹ç³»çµ±åœ°ç™¼ç¾ã€å»ºæ¨¡å’Œé¿å…ç½•è¦‹ä½†é«˜é¢¨éšªçµæœçš„æ˜ç¢ºæ©Ÿåˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRaWMPC ä½¿ç”¨ world model ä½œç‚ºé æ¸¬æ§åˆ¶ä¸­çš„é¢¨éšªè©•ä¼°å™¨ï¼šæˆ‘å€‘å¼•å…¥é¢¨éšªæ„ŸçŸ¥äº¤äº’ç­–ç•¥ä¾†æœ‰æ„æ¢ç´¢é¢¨éšªè¡Œç‚ºï¼Œä½¿å¾—ç½é›£æ€§å¾Œæœè®Šå¾—å¯é æ¸¬å’Œå¯é¿å…ï¼Œæˆ‘å€‘é€éæ˜ç¢ºåœ°åœ¨å¤šå€‹å€™é¸ä¸­æœ€å°åŒ–é¢¨éšªä¾†é¸æ“‡å‹•ä½œï¼Œå¢å¼·æ±ºç­–éç¨‹çš„å¯è§£é‡‹æ€§ã€å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p>3 Method</p>
<p>ç‚ºäº†å±•ç¤ºæˆ‘å€‘çš„è§£æ±ºæ–¹æ¡ˆï¼Œç¬¬ 3.1 ç¯€ä»‹ç´¹äº† RaWMPC çš„æ•´é«”ç¶²è·¯çµæ§‹å’Œç®¡é“ã€‚ç¬¬ 3.2 ç¯€å‘ˆç¾æˆ‘å€‘çš„è¨“ç·´æ–¹æ¡ˆï¼Œå³é¢¨éšªæ„ŸçŸ¥äº’å‹•è¨“ç·´ï¼Œç”¨æ–¼æœ‰æ•ˆå„ªåŒ–ã€‚ç‚ºç¢ºä¿æ•´å€‹ E2E-AD ç³»çµ±åœ¨æ¸¬è©¦æœŸé–“é«˜æ•ˆé‹è¡Œï¼Œç¬¬ 3.3 ç¯€èªªæ˜äº†è‡ªè©•ä¼°è’¸é¤¾æ–¹æ³•ä¾†è¨“ç·´å‹•ä½œæè­°ç¶²è·¯ã€‚</p>
<p>3.1 RaWMPC çš„ç¶²è·¯çµæ§‹</p>
<p>3.1.1 å•é¡Œè¨­ç½®</p>
<p>æˆ‘å€‘è€ƒæ…®é–‰ç’°ç«¯å°ç«¯è‡ªå‹•é§•é§›ï¼ˆE2E-ADï¼‰è¨­å®šã€‚åœ¨æ¯å€‹æ™‚é–“æ­¥ $t$ï¼Œä»£ç†æ¥æ”¶ä¸‰å€‹è¼¸å…¥ï¼šè¦–è¦ºè¼¸å…¥ $I_t$ï¼ˆå¤šè¦–è§’ RGB å½±åƒï¼‰ã€è‡ªæˆ‘ä¸­å¿ƒæ¸¬é‡ $M_t$ï¼ˆé€Ÿåº¦å’Œä½ç½®ï¼‰ä»¥åŠä¸€çµ„å€™é¸é§•é§›è¡Œç‚º {$A_{t:t+H-1}^n$}$<em>{n=1}^N$ï¼Œå…¶ä¸­ $N$ æ˜¯å€™é¸æ•¸é‡ï¼Œ$H$ æ˜¯è¦åŠƒè¦–é‡ã€‚æ¯å€‹å‹•ä½œæ­¥é©Ÿç”±ä¸‰å€‹å€¼çµ„æˆï¼š$\mathbf{A} = (\text{steer} \in [-1, 1], \text{throttle} \in [0, 1], \text{brake} \in [0, 1])$ã€‚åŸºæ–¼é§•é§›æ­·å²ï¼ˆ$I</em>{1:t}, M_{1:t}, A_{1:t-1}$ï¼‰ï¼ŒRaWMPC æ—¨åœ¨å¾å€™é¸ä¸­é¸æ“‡æœ€ä½³çš„ $A_{t:t+H-1}^n$ï¼Œä»¥ä¾¿è»Šè¼›èƒ½å¤ å‘ç›®çš„åœ°ç§»å‹•ï¼ŒåŒæ™‚ç¢ºä¿å®‰å…¨æ€§ä¸¦éµå®ˆäº¤é€šè¦å‰‡ã€‚</p>
<p>3.1.2 æ¦‚è¿°</p>
<p>å¦‚åœ– 2 æ‰€ç¤ºï¼ŒRaWMPC å¾è¼¸å…¥ç·¨ç¢¼é–‹å§‹ã€‚è¦–è¦ºç·¨ç¢¼å™¨ã€å‹•ä½œç·¨ç¢¼å™¨å’Œè‡ªæˆ‘ç‹€æ…‹ç·¨ç¢¼å™¨ç”¨ä¾†åˆ†åˆ¥å°‡ $\mathbf{I}<em>t$ã€{$\mathbf{A}</em>{t:t+H-1}^n$}$<em>{n=1}^N$ å’Œ $\mathbf{M}_t$ æ˜ å°„åˆ°åµŒå…¥ $\mathbf{r}_t$ã€{$\mathbf{a}</em>{t:t+H-1}^n$}$<em>{n=1}^N$ å’Œ $\mathbf{m}_t$ ä¸­ã€‚éš¨å¾Œï¼ŒåŸºæ–¼è§€å¯Ÿçš„ç‹€æ…‹ $s</em>{1:t} = (\mathbf{i}<em>{1:t}, \mathbf{m}</em>{1:t})$ï¼Œæˆ‘å€‘ä½¿ç”¨ world model ä¾†ä¼°è¨ˆå…¶æœªä¾†ç‹€æ…‹ $\hat{\mathbf{s}}<em>{t+1:t+H}^n$ï¼Œä»¥æ¯å€‹å‹•ä½œåµŒå…¥ $\mathbf{a}</em>{t:t+H-1}^n$ ç‚ºæ¢ä»¶ã€‚æœ€å¾Œï¼Œæˆ‘å€‘é€éå° $\hat{\mathbf{s}}_{t+1:t+H}^n$ é€²è¡Œè§£ç¢¼ä¸¦è¨ˆç®—æˆæœ¬å€¼ï¼Œé¸æ“‡ä½¿è‡ªæˆ‘è»Šè¼›èƒ½å¤ å®‰å…¨å‰é€²åŒæ™‚é¿å…äº¤é€šé•è¦çš„å‹•ä½œï¼š</p>
<p>$$ A_{t:t+H-1}^<em> = A_{t:t+H-1}^{n</em>}, \ \text{where } n^* = \underset{n \in {1, \dots, N}}{\arg \min} C(\hat{\mathbf{s}}_{t+1:t+H}^n). \quad (1) $$</p>
<p>å…¶ä¸­ $C(\cdot)$ è¡¨ç¤ºè§£ç¢¼éç¨‹ä¸­çš„æˆæœ¬å‡½æ•¸ï¼ˆå°‡åœ¨å…¬å¼ (6) ä¸­è©³ç´°èªªæ˜ï¼‰ï¼Œ$n^*$ æ˜¯æœ€å„ªå‹•ä½œçš„ç´¢å¼•ã€‚èˆ‡æ¨¡ä»¿å­¸ç¿’æ–¹æ¡ˆç›¸æ¯”ï¼ŒRaWMPC æä¾›äº†æ”¹é€²çš„å¯è§£é‡‹æ€§ï¼Œä¸¦ç‚ºæ±ºç­–é©—è­‰å’Œé¢¨éšªç·©è§£å¼•å…¥äº†æ˜ç¢ºæ©Ÿåˆ¶ã€‚</p>
<p>3.1.3 World Model</p>
<p>åœ¨æˆ‘å€‘çš„ç®¡é“ä¸­ï¼Œworld model è¨˜ç‚º $\mathcal{M}$ï¼Œç”¨æ–¼åœ¨çµ¦å®šå‹•ä½œçš„æƒ…æ³ä¸‹é æ¸¬æœªä¾†ç‹€æ…‹ã€‚å…·é«”ä¾†èªªï¼Œä»¥è§€å¯Ÿçš„ç‹€æ…‹ $s_{1:t} = (\mathbf{i}<em>{1:t}, \mathbf{m}</em>{1:t})$ å’Œæ½›åœ¨ä¸‹ä¸€å‹•ä½œ $\mathbf{a}<em>t^n$ ç‚ºæ¢ä»¶ï¼Œworld model $\mathcal{M}$ é æ¸¬è¿‘æœŸæœªä¾†ç‹€æ…‹ $\hat{\mathbf{s}}</em>{t+1}^n$ã€‚éš¨å¾Œï¼Œå°æ–¼é€²ä¸€æ­¥çš„æœªä¾†æ™‚é–“æ­¥ {$2, \dots, H$}ï¼Œworld model éè¿´å±•é–‹ï¼Œå¯ä»¥å½¢å¼åŒ–ç‚ºè‡ªå›æ­¸å› å¼åˆ†è§£ï¼š</p>
<p>$$ p_M(\hat{\mathbf{s}}<em>{t+1:t+H}^n | s</em>{1:t}, \mathbf{a}<em>{1:t+H-1}^n) = \prod</em>{k=1}^{H} p_M(\hat{\mathbf{s}}<em>{t+k}^n | (s</em>{1:t}, \hat{\mathbf{s}}<em>{t+1:t+k-1}^n), \mathbf{a}</em>{1:t+k-1}^n) \quad (2) $$</p>
<hr />
<figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig2.jpeg" loading="lazy" alt="Figure 2"></figure>
<p><strong>åœ– 2</strong> RaWMPC æ¦‚è¿°ã€‚å¤šè¦–è§’å½±åƒ <strong>I</strong><sub>t</sub>ã€è‡ªæˆ‘ç‹€æ…‹ <strong>M</strong><sub>t</sub> å’Œå€™é¸å‹•ä½œåºåˆ— {A<sup>n</sup><sub>t:t+Hâˆ’1</sub>}<sup>N</sup><sub>n=1</sub> è¢«ç·¨ç¢¼ä¸¦é€é world model åœ¨è¦–é‡ H ä¸Šå±•é–‹ã€‚ä¸‰å€‹è§£ç¢¼å™¨é æ¸¬èªç¾©åˆ†å‰²ã€èªç¾©å¼•å°çš„äº¤é€šäº‹ä»¶å’Œæœªä¾†è‡ªæˆ‘ç‹€æ…‹ï¼Œæ”¯æ´é æ¸¬æ§åˆ¶çš„å‹•ä½œè©•ä¼°ã€‚è¨“ç·´çµåˆäº†åœ¨è¨˜éŒ„è»Œè·¡ä¸Šçš„é›¢ç·šé ç†±å’Œä½¿ç”¨ world model å¼•å°æ¢ç´¢çš„ç·šä¸Šæ¨¡æ“¬å™¨äº¤äº’ã€‚</p>
<p>å…¶ä¸­ $p_M$ è¡¨ç¤ºç”± world model $M$ å®šç¾©çš„æ¢ä»¶åˆ†ä½ˆã€‚$\mathbf{a}<em>{1:t+k-1}^n = [\mathbf{a}</em>{1:t-1}, \mathbf{a}_{t:t+k-1}^n]$ è¡¨ç¤ºåŒ…å«å‹•ä½œæ­·å²çš„å€™é¸å‹•ä½œåºåˆ—ã€‚</p>
<h3 id="314"><strong>3.1.4 èªç¾©å¼•å°è§£ç¢¼</strong></h3>
<p>ä¸€æ—¦ä¸–ç•Œæ¨¡å‹é æ¸¬äº†ä¸€ç³»åˆ—æœªä¾†ç‹€æ…‹ $\hat{\mathbf{s}}<em>{t+1:t+H}^n$ï¼Œä¸‰å€‹ transformer è§£ç¢¼å™¨åˆ†åˆ¥å°‡é€™äº›ç‹€æ…‹æ˜ å°„åˆ°ç‰¹å®šä»»å‹™çš„è¼¸å‡ºï¼šèªç¾©åˆ†å‰²ã€æ½›åœ¨äº¤é€šäº‹ä»¶ï¼ˆä¾‹å¦‚ç¢°æ’ï¼‰å’Œæœªä¾†è‡ªè»Šç‹€æ…‹ï¼ˆä¾‹å¦‚ä½ç½®ï¼‰ã€‚åœ¨æ¥ä¸‹ä¾†çš„æè¿°ä¸­ï¼Œæˆ‘å€‘ä½¿ç”¨æ™‚é–“æ­¥ $t+k$ è™•çš„é æ¸¬ç‹€æ…‹ï¼Œå³ $\hat{\mathbf{s}}</em>{t+k}^n$ï¼Œå…¶ä¸­ $k \in {1, \dots, H}$ã€‚</p>
<p>ç‚ºäº†å¯¦ç¾å°é§•é§›å ´æ™¯çš„æ›´é«˜å±¤æ¬¡ç†è§£ï¼Œä¸¦ç‚ºé æ¸¬çš„äº¤é€šäº‹ä»¶æä¾›è¦–è¦ºè§£é‡‹ï¼Œæˆ‘å€‘å°‡ä¾†è‡ªåˆ†å‰²è§£ç¢¼å™¨çš„èªç¾©æ³¨æ„åŠ›æ³¨å…¥åˆ°äº‹ä»¶è§£ç¢¼å™¨ä¸­ã€‚<strong>åˆ†å‰²</strong>è§£ç¢¼å™¨æ¡ç”¨æ¨™æº– transformer æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼š</p>
<p>$$ \text{Att}_{\text{seg}}(\mathbf{Q}_c, \mathbf{K}_c, V_c) = \text{softmax}(\text{sim}(\mathbf{Q}_c, \mathbf{K}_c)) \cdot V_c, \quad (3) $$</p>
<p>å…¶ä¸­ $\mathbf{Q}<em>c$ æ˜¯å¯å­¸ç¿’çš„é¡åˆ¥æŸ¥è©¢ï¼Œ$\mathbf{K}_c, V_c$ è¡ç”Ÿè‡ªè¦–è¦ºä»¤ç‰Œ $\mathbf{\dot{i}}</em>{t+k}^n \subset \hat{\mathbf{s}}<em>{t+k}^n$ã€‚åœ¨æœ€å¾Œä¸€å±¤ï¼Œæˆ‘å€‘éµå¾ª SegViT [88] ç‚ºæ¯å€‹è¼¸å…¥é¡åˆ¥æŸ¥è©¢é æ¸¬å–®ç†±ç·¨ç¢¼çš„èªç¾©åˆ†å‰²åœ– $\mathbf{Y}</em>{t+k}^n$ã€‚éš¨å¾Œï¼Œæˆ‘å€‘é€éå°‡<strong>äº‹ä»¶</strong>è§£ç¢¼å™¨çš„æ³¨æ„åŠ›æ—¥èªŒèˆ‡ä¾†è‡ªæœ€å¾Œåˆ†å‰²å±¤çš„å°æ‡‰èªç¾©æ³¨æ„åŠ›æ—¥èªŒèåˆä¾†å¢å¼·å®ƒï¼š</p>
<p>$$ \mathbf{Z}_e = \mathbf{Q}_e \mathbf{K}_e^{\top}, \quad \mathbf{Z}_c = \text{pad}(\mathbf{Q}_c \mathbf{K}_c^{\top}) $$</p>
<p>$$ \hat{E}_{t+k}^n = \text{sigmoid}(\text{softmax}(\mathbf{W}_e * [\mathbf{Z}_e, \mathbf{Z}_c])\mathbf{V}_e). \quad (4) $$</p>
<p>å…¶ä¸­ $\mathbf{Q}<em>e$ æ˜¯å¯å­¸ç¿’çš„äº‹ä»¶æŸ¥è©¢ï¼Œ$\mathbf{K}_e, V_e$ ç”±é æ¸¬çš„æœªä¾†ç‹€æ…‹ $\hat{\mathbf{s}}</em>{t+k}^n$ è¨ˆç®—å¾—å‡ºã€‚$\text{pad}(\mathbf{\cdot})$ å° $\mathbf{Z}<em>c$ é€²è¡Œé›¶å¡«å……ä»¥åŒ¹é… $V_e$ çš„å¤§å°ï¼Œ$\mathbf{W}_e$ æ˜¯ä¸€å€‹ $1 \times 1$ å·ç©ï¼Œç”¨æ–¼èåˆä¸²æ¥çš„ç‰¹å¾µã€‚äº‹ä»¶è§£ç¢¼å™¨çš„è¼¸å‡º $\hat{E}</em>{t+k}^n \in [0, 1]^\alpha$ è¡¨ç¤º $\alpha$ ç¨®äº‹ä»¶é¡å‹çš„æ¦‚ç‡ã€‚æœ€å¾Œï¼Œå°æ–¼æœªä¾†è‡ªè»Šç‹€æ…‹é æ¸¬ï¼Œæˆ‘å€‘è§£ç¢¼è‡ªè»Šä»¤ç‰Œ $\dot{\mathbf{m}}<em>{t+k}^n \subset \hat{\mathbf{s}}</em>{t+k}^n$ ä»¥ç²å¾—é€Ÿåº¦å’Œä½ç½® $\hat{\mathbf{M}}_{t+k}^n$ã€‚</p>
<p>åœ¨èªç¾©æ³¨æ„åŠ›åœ–çš„å¼•å°ä¸‹ï¼Œäº‹ä»¶è§£ç¢¼å™¨å°‡æ›´å¤šæ³¨æ„åŠ›é›†ä¸­åœ¨å°ç‰¹å®šäº‹ä»¶è‡³é—œé‡è¦çš„å€åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨è­˜åˆ¥è»Šè¼›ç¢°æ’äº‹ä»¶æ™‚ï¼Œæ¨¡å‹æ›´å°ˆæ³¨æ–¼è»Šè¼›å€åŸŸï¼Œæé«˜äº†äº‹ä»¶é æ¸¬çš„æº–ç¢ºæ€§å’Œå¯é æ€§ã€‚</p>
<h3 id="315-action-selection-and-predictive-control"><strong>3.1.5 Action Selection and Predictive Control</strong></h3>
<p>Given the decoder outputs, we perform predictive control by evaluating each candidate action sequence over the planning horizon <em>H</em> and selecting the one with the minimum predicted cost.</p>
<p>Specifically, for the <em>n</em>-th candidate $A_{t:t+H-1}^n$, we consider (i) progress toward the target and (ii) the risk of traffic-violation events. Let <strong>p$$^\star$$</strong> be the target 3D position and $\hat{\mathbf{p}}<em>{t+k}^n \subset \hat{\mathbf{M}}</em>{t+k}^n$ the predicted ego position at step $t+k$. We define the progress as the reduction in target distance:</p>
<p>$$ \hat{D}<em>{t+k}^{n} = | \mathbf{p}^{\star} - \hat{\mathbf{p}}</em>{t+k-1}^{n} |<em>{2} - | \mathbf{p}^{\star} - \hat{\mathbf{p}}</em>{t+k}^{n} |_{2}, \quad (5) $$</p>
<hr />
<p>We then define the predictive-control objective as:</p>
<p>$$ 
\begin{equation} 
\begin{split}
C(\hat{\mathbf{s}}<em>{t+1:t+H}) &amp;= \sum</em>{k=1}^{H} \eta_k (-\hat{D}<em>{t+k}^{n} + \sum</em>{j=1}^{\alpha} \lambda_j \hat{E}_{t+k,j}^{n})
\end{split}
\tag{6}
\end{equation} 
$$</p>
<p>where $\eta_k = \max(2^{-k+1}, 1/8)$ down-weights distant predictions to account for increasing uncertainty. We floor $\eta_k$ at $1/8$ to avoid vanishing contributions from distant steps, which stabilizes planning when $H$ is moderately large. $\lambda_j &gt; 0$ reflects the severity of violation type $j$ (e.g., pedestrian/vehicle collisions receive larger weights). Finally, we select the action sequence that minimizes the horizon cost in Eq. (6), <em>i.e.</em>, a model-predictive control policy that favors faster progress while proactively reducing the probability of predicted violations.</p>
<h3 id="316-rawmpc">3.1.6 RaWMPC çš„æ•´é«”æå¤±å‡½æ•¸</h3>
<p>RaWMPC æ¡†æ¶ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼é€²è¡Œè¨“ç·´ï¼ŒåŒ…å«ä¸–ç•Œæ¨¡å‹æå¤± $\mathcal{L}<em>{\text{world}}$ã€ä¾†è‡ª SegViT [88] çš„åˆ†å‰²æå¤± $\mathcal{L}</em>{\text{seg}}$ã€è‡ªè»Šç‹€æ…‹æå¤± $\mathcal{L}<em>{\text{ego}}$ å’Œäº‹ä»¶æå¤± $\mathcal{L}</em>{\text{event}}$ï¼š</p>
<p>$$ \mathcal{L} = \mathcal{L}<em>{\text{world}} + \mathcal{L}</em>{\text{seg}} + \mathcal{L}<em>{\text{ego}} + \mathcal{L}</em>{\text{event}} \tag{7} $$</p>
<p>ä¾ç…§ SegViT [88]ï¼Œåˆ†å‰²é …åŒ…å«åˆ†é¡æå¤± $\mathcal{L}<em>{\text{cls}}$ï¼ˆäº¤å‰ç†µï¼‰å’ŒäºŒå…ƒé®ç½©æå¤±ã€‚é®ç½©æå¤±ç”±ç„¦é»æå¤± $\mathcal{L}</em>{\text{focal}}$ [43] å’Œ Dice æå¤± $\mathcal{L}_{\text{dice}}$ [47] çµ„æˆï¼Œç”¨æ–¼å„ªåŒ–åˆ†å‰²æº–ç¢ºåº¦ï¼š</p>
<p>$$ \mathcal{L}<em>{\text{seg}} = \mathcal{L}</em>{\text{cls}} + \mathcal{L}<em>{\text{focal}} + \mathcal{L}</em>{\text{dice}} \tag{8} $$</p>
<p>å°æ–¼å…¶ä»–ä¸‰å€‹æå¤±å‡½æ•¸ï¼Œæˆ‘å€‘ä½¿ç”¨å‡æ–¹èª¤å·®ï¼ˆMSEï¼‰ä¾†ç›£ç£è‡ªè»Šç‹€æ…‹å’Œä¸–ç•Œæ¨¡å‹ï¼Œä¸¦å°äº‹ä»¶è§£ç¢¼å™¨æ¡ç”¨äºŒå…ƒäº¤å‰ç†µï¼ˆBCEï¼‰æå¤±ï¼š</p>
<p>$$ \begin{align}
\mathcal{L}<em>{\text{world}} &amp;= \frac{1}{H} \sum</em>{k=1}^{H} |\hat{\mathbf{s}}<em>{t+k} - \mathbf{s}</em>{t+k}|<em>{2}^{2}, \
\mathcal{L}</em>{\text{ego}} &amp;= \frac{1}{H} \sum_{k=1}^{H} |\hat{\mathbf{M}}<em>{t+k} - \mathbf{M}</em>{t+k}|_{2}^{2}, 
\end{align} \tag{9} $$</p>
<p>$$ \mathcal{L}<em>{\text{event}} = \frac{1}{H} \sum</em>{k=1}^{H} BCE(\hat{\mathbf{E}}<em>{t+k}, \mathbf{E}</em>{t+k}), $$</p>
<p>å…¶ä¸­ BCE(Â·) è¡¨ç¤º BCE æå¤±ã€‚æ‰€æœ‰è¨»è§£ $\mathbf{s}<em>{t+k}$ã€$\mathbf{M}</em>{t+k}$ã€$\mathbf{E}<em>{t+k}$ æ²¿è‘—åœ¨ $\mathbf{A}</em>{t:t+H-1}$ ä¸‹åŸ·è¡Œçš„å±•é–‹éç¨‹éƒ½å¾æ¨¡æ“¬å™¨ï¼ˆä¾‹å¦‚ CARLAï¼‰ç²å¾—ã€‚</p>
<h3 id="32-risk-aware-interactive-training">3.2 Risk-aware Interactive Training</h3>
<p>To enable RaWMPC to evaluate diverse actions and identify risky scenarios, we propose a two-stage risk-aware interactive training scheme, as shown in Fig. 2. We first warm-start the world model from logged driving trajectories. Then, we refine it via online simulator interaction, intentionally collecting both good</p>
<figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig3.jpeg" loading="lazy" alt="Figure 3"></figure>
<p>Figure 3 Different action-selection ranges under three driving modes in online simulator interaction.</p>
<p>Red denotes high cost and green denotes low cost. <strong>rand</strong> samples uniformly from all candidates, <strong>bad</strong> samples from the high-cost region, and <strong>good</strong> samples from the low-cost one.</p>
<p>(safe, goal-directed) and <strong>bad</strong> (hazardous) rollouts to improve generalization under out-of-distribution controls and to learn rare but safety-critical events. Notably, RaWMPC does not rely on expert action labels for policy learning. The optional warm-up stage, when used, serves solely to initialize the predictive world model from observed state transitions, rather than to imitate expert actions.</p>
<h4 id="321-offline-world-model-warm-up">3.2.1 Offline World Model Warm-up</h4>
<p>We bootstrap RaWMPC using a small set of logged trajectories to achieve simple and basic state-forecasting capability. Given state-action sequences ${(\mathbf{s}<em>t, \mathbf{a}_t)}</em>{t=1}^T$ from NAVSIM or CARLA, the world model predicts the next state $\hat{\mathbf{s}}<em>{t+1}$ and is trained with $\mathcal{L}</em>{\text{world}}$ to match the ground-truth $\mathbf{s}_{t+1}$. We supervise the segmentation and ego-state decoders using the simulator-provided annotations. Since the warm-up trajectories contain no traffic violations, we train the event decoder with an all-zero target. In this way, only a small subset of training data (10%) is typically sufficient for warm-up, providing a reliable initialization for long-horizon rollouts and stabilizing subsequent world model optimization.</p>
<h4 id="322-online-simulator-interactive-training">3.2.2 Online Simulator Interactive Training</h4>
<p>Offline warm-up data are mostly concentrated around human-like safe behaviors and thus provide limited coverage of hazardous or unconventional actions. To learn the consequences of risky behaviors, we perform world-model-guided exploration: selected simulator rollouts are fed back to refine the same world model, progressively improving prediction fidelity and risk sensitivity.</p>
<p>Specifically, to ensure temporal continuity and avoid unrealistic control jitter, we sample horizon-$H$ action sequences (segment-wise) rather than single-step actions (step-wise). The segment-wise sampling allows</p>
<hr />
<p>sustained safe or risky behaviors to unfold and reveals
their long-term consequences. At each training step,
we sample N<sub>s</sub> horizon-<i>H</i> candidate action sequences
{A<sub>t:t+<i>H</i>-1</sub><sup>n</sup>}<sub>n=1</sub><sup>N<sub>s</sub></sup>, roll out future states s&#x0302;<sub>t+1:t+<i>H</i></sub><sup>n</sup> with
the current world model, evaluate their costs {C<sup>n</sup>}</sup>
using Eq. (6), and rank candidates by costs.</p>
<p><strong>Modes for Interaction.</strong> We define three patterns
for our risk-aware sampling strategy to select one
candidate from {$\mathbf{A}<em>{t:t+H-1}^n$}$</em>{n=1}^{N_s}$ to execute, as shown
in Fig. 3:</p>
<ul>
<li>Rand samples uniformly from all candidates;</li>
</ul>
<p>â€¢ Bad samples from high-cost candidates;</p>
<p>â€¢ Good samples from low-cost candidates.</p>
<p>At the start of segment <em>r</em>, the practical control mode
is sampled according to probability:</p>
<p>$$
m_r = \begin{cases}
\text{rand, } &amp; \text{w.p. } \epsilon_1, \
\text{bad, } &amp; \text{w.p. } (1 - \epsilon_1)\epsilon_2, \
\text{good, } &amp; \text{w.p. } (1 - \epsilon_1)(1 - \epsilon_2),
\end{cases} \qquad (10)
$$</p>
<p>where â€œw.p.â€ means â€œwith probabilityâ€. $\epsilon_1$ controls broad action-space exploration, and $\epsilon_2$ controls the fraction of risk-seeking interaction within model-guided sampling.</p>
<p>Soft <strong>Candidates</strong> Selection in Three Modes. Given
sorted candidate actions with costs {$C^n$}, we con-
struct two cost-quantile sets: $\mathcal{N}<em>{\text{good}}$ as the bottom-$K$-
candidates and $\mathcal{N}</em>{\text{bad}}$ as the top-$K$ candidates. To
avoid low-information trajectories, we filter $\mathcal{N}<em>{\text{bad}}$ by
removing degenerate rollouts (e.g., those caused by
unrealistic excessive control jumps), yielding $\tilde{\mathcal{N}}</em>{\text{bad}}$.
In the <strong>rand</strong> mode, we randomly select the executing
action sequence from all candidates.</p>
<p>In the good mode, rather than <strong>deterministically</strong> selecting the minimum-cost candidate, we sample from $\mathcal{N}_{\text{good}}$ using a <strong>soft distribution</strong> to preserve diversity among low-cost plans and mitigate bias from imperfect model predictions:</p>
<p>$$
P(n | \mathbf{good}) \propto \exp(-C^n / \tau_g), \quad n \in \mathcal{N}_{good}. \quad (11)
$$</p>
<p>where Ï„<sub>g</sub> is a temperature hyper-parameter that con-
trols the softness of the sampling distributions in
the <strong>good</strong> mode, trading off greediness for diversity.
This stochastic selection avoids repeatedly executing
a single estimated optimum and encourages broader
coverage of nominal behaviors.</p>
<p>In the <strong>bad</strong> mode, instead of always executing the
maximum-cost trajectory, we sample from high-cost</p>
<figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig4.jpeg" loading="lazy" alt="Figure 4"></figure>
<p>Figure 4 Self-Evaluation Distillation for Policy Learn-
ing. A cVAE is trained with RaWMPC-scored actions in
a contrastive manner, pulling the condition prior toward
positives and pushing it away from negatives. The well-
trained decoder serves as the test-time action proposer.</p>
<p>candidates to deliberately expose the model to a spec-
trum of risky outcomes that are under-represented
in safe logs:</p>
<p>$$
P(n \mid \text{bad}) \propto \exp(C^n / \tau_b), \quad n \in \tilde{\mathcal{N}}_{\text{bad}}. \quad (12)
$$</p>
<p>where Ï„_b is a temperature hyper-parameter. Compared to argmax selection, this soft sampling strategy prevents over-concentration on extreme or degenerate failures while still biasing interaction toward high-risk regions.</p>
<p>In this way, segment-wise interaction and soft cost-
based sampling bias exploration toward temporally
coherent and informative safe and hazardous tra-
jectories, enabling the world model to learn both
reasonable dynamics and safety-critical consequences
for risk-aware decision making.</p>
<hr />
<p><strong>3.3 Self-Evaluation Distillation for Policy</strong>
<strong>Learning</strong></p>
<p>After risk-aware interactive training, RaWMPC can
reliably <em>score</em> candidate action sequences by predict-
ing their long-horizon consequences. To reduce the
cost of online optimization at test time, we distill
this evaluation capability into a lightweight <em>action</em>
proposal network, enabling efficient inference <em>without</em>
<em>expert demonstrations</em>. The action proposal network
corresponds to the â€œGuidanceâ€ module illustrated in
Fig. 1(c), and is used to generate candidate action
sequences for predictive control. Our key idea is
to use RaWMPC as a self-evaluator to pseudo-label
sampled actions and train a generative policy via
contrastive learning.</p>
<p><strong>3.3.1 Action Sampling and Pseudo-labeling</strong></p>
<p>Given a state history $\mathbf{s}<em>{1:t}$ (simplified as $\mathbf{s}$), we
randomly sample $N_s$ horizon-$H$ action sequences
${ \mathbf{A}</em>{t:t+H-1}^n }<em>{n=1}^{N_s}$ (simplified as ${ \mathbf{A}^n }$) and compute
their costs ${ C^n }$ with the pretrained RaWMPC. We
then form pseudo labels by ranking costs: the lowest-
cost sequence is treated as a positive example $\mathbf{A}^+$,
and the top-$K$ highest-cost sequences are treated
as negatives ${ \bar{\mathbf{A}}_j^K }</em>{j=1}^K$. This construction transfers
RaWMPC's knowledge (low-risk / high-quality ac-
tions) to the proposal network while avoiding any
external supervision.</p>
<p>3.3.2 Action Proposal Network</p>
<p>Following [60, 87], we adopt a conditional VAE (cVAE) with an action encoder $q_{\theta}(z|\mathbf{A}, \mathbf{s})$, a conditional prior $p_{\gamma}(z|\mathbf{s})$, and a decoder $p_{\psi}(\mathbf{A}|z, \mathbf{s})$. The decoder serves as the proposal policy at inference.</p>
<p>For the positive action, we obtain a Gaussian pos-
terior $q^+ = q_{\theta}(z|\mathbf{A}^{+}, \mathbf{s}) = \mathcal{N}(\mu^{+}, \text{diag}((\sigma^{+})^{2}))$ and
train the decoder to reconstruct $\mathbf{A}^{+}$. For each neg-
ative action $\bar{\mathbf{A}}<em>{j}$, we compute $q</em>{j} = q_{\theta}(z|\bar{\mathbf{A}}<em>{j}, \mathbf{s}) =$
$\mathcal{N}(\bar{\mu}</em>{j}, \text{diag}((\sigma_{j}^{+})^{2}))$, but <strong>do not</strong> reconstruct nega-
tives to prevent the generator from imitating unsafe
behaviors. The conditional prior is $p^{c} = p_{\gamma}(z|\mathbf{s}) =$
$\mathcal{N}(\bar{\mu}^{c}, \text{diag}((\sigma^{c})^{2}))$.</p>
<p>3.3.3 Contrastive Training Objective</p>
<p>To address the lack of expert supervision in policy
learning, we use an InfoNCE objective to make the
conditional prior predictive of high-quality actions.
Concretely, there are two potential contrastive for-
mulations:</p>
<p>â€¢ Using $p^c$ as the anchor, pulling $p^c$ toward $q^+$ while pushing it away from ${q^-}<em>{j=1}^K$.
â€¢ Using $q^+$ as the anchor, pulling $q^+$ toward $p^c$ while pushing it away from ${q^-}</em>{j=1}^K$.</p>
<p>We empirically found that the former often produces under-optimized trajectories. One possible reason is that negative samples are far more numerous and broadly cover the latent space, therefore $p^c$ is easily driven to a region that is far from most negatives yet not sufficiently close to the positive. In contrast, the latter explicitly pulls $p^c$ toward the statistical center of the positive posterior and, via $q^+$, indirectly separates it from the negatives, leading to more stable learning and higher-quality trajectories. Therefore, we adopt the latter design choice to define our InfoNCE objective as:</p>
<p>$$
\begin{align<em>}
\mathcal{L}<em>c &amp;= -\log \frac{\exp(\ell^+)}{\exp(\ell^+) + \sum</em>{j=1}^K \exp(\ell_j^-)}, \
\ell^+ &amp;= -\mathcal{D}(q^+, p^c)/\tau, \
\ell_j^- &amp;= -\mathcal{D}(q^+, q_j^-)/\tau,
\end{align</em>}
\tag{13}
$$</p>
<p>where $D(\cdot, \cdot)$ is the Wasserstein-$2$ distance between Gaussians and $\tau$ is a temperature.</p>
<p>3.3.4 Overall Loss of Action Proposal Network</p>
<p>The total loss of our cVAE combines reconstruction,
KL regularization, and contrastive loss:</p>
<p>$$
\mathcal{L}<em>{\text{total}} = E</em>{z \sim q^{+}} [-\log p_{\psi}(\mathbf{A}^{+} | z, \mathbf{s})] + \beta D_{\text{KL}}(q^{+} | p^{c}) + \lambda \mathcal{L}_{c}. \quad (14)
$$</p>
<p>This self-evaluation distillation trains a fast proposal
policy that generates candidate action sequences con-
sistent with RaWMPCâ€™s evaluations, eliminating the
need for expert demonstrations during policy learn-
ing.</p>
<p>4 Experiments</p>
<p>In this section, we present a comprehensive perfor-
mance comparison between the proposed framework
and state-of-the-art methods. We also conduct ex-
tensive ablation studies to assess the effectiveness of
our predictive control approach.</p>
<p>4.1 Benchmarks</p>
<p>Following prior works [26, 40, 41], we evalu-
ate RaWMPC on two widely used benchmarks:
Bench2Drive [29] and Navsim [11]. They are com-
plementary: Bench2Drive provides fully interactive</p>
<hr />
<p><strong>Table 1</strong> Comparison with SOTA approaches on the closed-loop Bench2Drive benchmark on CARLA simulator. â†‘ means the higher the better. DS is taken as the primary metric in comparison and we rank all the methods accordingly, with bold indicating best performance.</p>
<table><thead><tr><th>Method</th><th>Venue</th><th>Scheme</th><th>DSâ†‘</th><th>SR(%)â†‘</th><th>Efficiencyâ†‘</th><th>Comfortnessâ†‘</th></tr></thead><tbody><tr><td>VAD [31]</td><td>ICCV 2023</td><td>IL</td><td>42.35</td><td>15.00</td><td>157.94</td><td>46.01</td></tr><tr><td>SparseDrive [69]</td><td>ICRA 2025</td><td>IL</td><td>44.54</td><td>16.71</td><td>170.21</td><td>48.63</td></tr><tr><td>GenAD [99]</td><td>ECCV 2024</td><td>IL</td><td>44.81</td><td>15.90</td><td>-</td><td>-</td></tr><tr><td>UniAD [25]</td><td>CVPR 2023</td><td>IL</td><td>45.81</td><td>16.36</td><td>129.21</td><td>43.58</td></tr><tr><td>MomAD [68]</td><td>CVPR 2025</td><td>IL</td><td>47.91</td><td>18.11</td><td>174.91</td><td><u>51.20</u></td></tr><tr><td>UAD [16]</td><td>T-PAMI 2025</td><td>IL</td><td>49.22</td><td>20.45</td><td>189.53</td><td><b>52.71</b></td></tr><tr><td>BridgeAD [89]</td><td>CVPR 2025</td><td>IL</td><td>50.06</td><td>22.73</td><td>-</td><td>-</td></tr><tr><td>TCP [81]</td><td>NeurIPS 2022</td><td>IL</td><td>59.90</td><td>30.00</td><td>76.54</td><td>18.08</td></tr><tr><td>WoTE [39]</td><td>ICCV 2025</td><td>IL</td><td>61.71</td><td>31.36</td><td>-</td><td>-</td></tr><tr><td>DriveDPO [61]</td><td>NeurIPS 2025</td><td>IL &amp; RL</td><td>62.02</td><td>30.62</td><td>166.80</td><td>26.79</td></tr><tr><td>ThinkTwice [28]</td><td>CVPR 2023</td><td>IL</td><td>62.44</td><td>31.23</td><td>69.33</td><td>16.22</td></tr><tr><td>DriveTransformer [30]</td><td>ICLR 2025</td><td>IL</td><td>63.46</td><td>35.01</td><td>100.64</td><td>20.78</td></tr><tr><td>DriveAdapter [27]</td><td>ICCV 2023</td><td>IL</td><td>64.22</td><td>33.08</td><td>70.22</td><td>16.01</td></tr><tr><td>Raw2Drive [85]</td><td>NeurIPS 2025</td><td>RL</td><td>71.36</td><td>50.24</td><td><u>214.17</u></td><td>22.42</td></tr><tr><td>Hydra-NeXt [40]</td><td>ICCV 2025</td><td>IL</td><td>73.86</td><td>50.00</td><td>197.76</td><td>20.68</td></tr><tr><td>HiP-AD [73]</td><td>ICCV 2025</td><td>IL</td><td>86.77</td><td>69.09</td><td>203.12</td><td>19.36</td></tr><tr><td><b>RaWMPC w/o Warm-up</b></td><td></td><td>PC</td><td><u>87.34</u></td><td><u>69.62</u></td><td>203.25</td><td>30.95</td></tr><tr><td><b>RaWMPC</b></td><td></td><td>PC</td><td><b>88.31</b></td><td><b>70.48</b></td><td>206.85</td><td>32.65</td></tr><tr><td colspan="7"><b>Pretrained VLM-based Approach</b></td></tr><tr><td>ReAL-AD [44]</td><td>ICCV 2025</td><td>IL</td><td>41.17</td><td>11.36</td><td>-</td><td>-</td></tr><tr><td>Dual-AEB [94]</td><td>ICRA 2025</td><td>IL</td><td>45.23</td><td>10.00</td><td>-</td><td>-</td></tr><tr><td>ETA [21]</td><td>ICCV 2025</td><td>IL</td><td>74.33</td><td>48.33</td><td>186.04</td><td>25.77</td></tr><tr><td>VLR-Drive [35]</td><td>ICCV 2025</td><td>IL</td><td>75.01</td><td>50.00</td><td>122.52</td><td>0.59</td></tr><tr><td>ORION [14]</td><td>ICCV 2025</td><td>IL</td><td>77.74</td><td>54.62</td><td>151.48</td><td>17.38</td></tr><tr><td>SimLingo [57]</td><td>CVPR 2025</td><td>IL</td><td>85.94</td><td>66.82</td><td><b>244.18</b></td><td>30.76</td></tr></tbody></table>

<p>closed-loop evaluation in CARLA [12] with dense annotations, while NAVSIM evaluates large-scale real-world planning via a data-driven, non-reactive simulation-based short-horizon rollout with safety- and progress-aware metrics.</p>
<p><strong>Bench2Drive.</strong> Bench2Drive is a CARLA Leadboard v2 closed-loop benchmark for multi-ability stress testing under complex interactions (e.g., cut-ins, overtakes, detours, emergency braking, and give-way). Its official dataset contains ~2M fully annotated frames from short clips spanning 44 scenarios, 23 weather conditions, and 12 towns; the commonly used <em>Base</em> training set contains 1K clips. Closed-loop evaluation is performed on 220 short routes (each focused on a single scenario), enabling stable and fine-grained comparison. We report four official metrics: <em>Driving Score</em> (<em>DS</em>), <em>Success Rate</em> (<em>SR</em>), <em>Efficiency</em>, and <em>Comfortness</em>. <em>DS</em> is the primary aggregate score with penalties for safety and rule violations; <em>SR</em> measures successful completion; <em>Efficiency</em> reflects progress; and <em>Comfortness</em> captures motion smoothness.</p>
<p><strong>NAVSIM.</strong> NAVSIM benchmarks sensor-based planning on large-scale real-world data built on Open-Scene (a planning-oriented reprocessing of nuPlan logs). The task predicts a 4-second future ego trajectory (typically 8 waypoints) given a short history (e.g., 1.5 seconds) of observations. Following prior works [39, 61], we use the official splits: NAvtrain (~103K samples) and Navtest (~12K samples). We report NAVSIM metrics including <em>NC</em>, <em>DAC</em>, <em>EP</em>, <em>TTC</em>, <em>C</em>, and the primary score <em>PDMS</em>, where $\text{PDMS} = \text{NC} \cdot \text{DAC} \cdot \frac{(5\text{EP} + 5\text{TTC} + 2\text{C})}{12}$. These metrics jointly capture safety, compliance, progress, and motion quality. All results are computed with the official toolkits and recommended splits.</p>
<p>Our experiments combine offline warm-up with online simulator interaction, leveraging RaWMPC to learn predictive dynamics and risk-aware decision making. We adopt Bench2Drive for interactive closed-loop evaluation and NAVSIM for large-scale real-world generalization. We further conduct ablations to analyze key training components and strategies.</p>
<hr />
<p><strong>Table 2</strong> Comparison with the SOTA approaches on NAVSIM test set. â†‘ means the higher the better. PDMS is taken as the primary metric in comparison and we rank all the methods accordingly, with bold indicating best performance.</p>
<table><thead><tr><th>Method</th><th>Venue</th><th>Scheme</th><th>NCâ†‘</th><th>DACâ†‘</th><th>EPâ†‘</th><th>TTCâ†‘</th><th>Câ†‘</th><th>PDMSâ†‘</th></tr></thead><tbody><tr><td>Human</td><td>-</td><td>-</td><td>100</td><td>100</td><td>87.5</td><td>100</td><td>99.9</td><td>94.8</td></tr><tr><td>DrivingGPT [8]</td><td>ICCV 2025</td><td>IL</td><td>98.9</td><td>90.7</td><td>79.7</td><td>94.9</td><td>100.0</td><td>82.4</td></tr><tr><td>UniAD [25]</td><td>CVPR 2023</td><td>IL</td><td>97.8</td><td>91.9</td><td>78.8</td><td>92.9</td><td>100.0</td><td>83.4</td></tr><tr><td>Latent TransFuser [10]</td><td>T-PAMI 2023</td><td>IL</td><td>97.4</td><td>92.8</td><td>79.0</td><td>92.4</td><td>100.0</td><td>83.8</td></tr><tr><td>PARA-Drive [80]</td><td>CVPR 2024</td><td>IL</td><td>97.9</td><td>92.4</td><td>79.3</td><td>93.0</td><td>99.8</td><td>84.0</td></tr><tr><td>TransFuser [10]</td><td>T-PAMI 2023</td><td>IL</td><td>97.7</td><td>92.7</td><td>79.8</td><td>92.7</td><td>100.0</td><td>84.5</td></tr><tr><td>LAW [38]</td><td>ICLR 2025</td><td>IL</td><td>96.4</td><td>95.4</td><td>81.7</td><td>88.7</td><td>99.9</td><td>84.6</td></tr><tr><td>World4Drive [100]</td><td>ICCV 2025</td><td>IL</td><td>97.4</td><td>94.3</td><td>79.9</td><td>92.8</td><td>100.0</td><td>85.1</td></tr><tr><td>DiffusionDrive [42]</td><td>CVPR 2025</td><td>IL</td><td>98.2</td><td>96.2</td><td><b>88.2</b></td><td>94.7</td><td>100.0</td><td>88.1</td></tr><tr><td>WoTE [39]</td><td>ICCV 2025</td><td>IL</td><td>98.5</td><td>96.8</td><td>81.9</td><td>94.9</td><td>99.9</td><td>88.3</td></tr><tr><td>Hydra-NeXt [40]</td><td>ICCV 2025</td><td>IL</td><td>98.1</td><td>97.7</td><td>81.8</td><td>94.6</td><td>100.0</td><td>88.6</td></tr><tr><td>UAD [16]</td><td>T-PAMI 2025</td><td>IL</td><td><b>99.5</b></td><td>96.9</td><td>78.8</td><td><b>97.5</b></td><td>100.0</td><td>89.3</td></tr><tr><td>DriveDPO [61]</td><td>NeurIPS 2025</td><td>IL & RL</td><td>98.5</td><td>98.1</td><td>84.3</td><td>94.8</td><td>99.9</td><td>90.0</td></tr><tr><td>GoalFlow [82]</td><td>CVPR 2025</td><td>IL</td><td>98.4</td><td><b>98.3</b></td><td>85.0</td><td>94.6</td><td>100.0</td><td>90.3</td></tr><tr><td><b>RaWMPC w/o Warm-up</b></td><td></td><td>PC</td><td>98.3</td><td><u>98.2</u></td><td>85.3</td><td>94.5</td><td>99.9</td><td>90.5</td></tr><tr><td><b>RaWMPC</b></td><td></td><td>PC</td><td><u>98.9</u></td><td><b><u>98.3</u></b></td><td><u>86.1</u></td><td><u>95.6</u></td><td>99.9</td><td><b><u>91.3</u></b></td></tr></tbody></table>

<h2 id="42">4.2 å¯¦ç¾ç´°ç¯€</h2>
<p><strong>ç¶²è·¯æ¶æ§‹ã€‚</strong> æˆ‘å€‘æ¡ç”¨é è¨“ç·´çš„ ViT [66] ä½œç‚ºè¦–è¦ºç·¨ç¢¼å™¨ï¼Œä¸¦ä½¿ç”¨ SegViT åˆ†å‰²é ­ [88] ä½œç‚ºåˆ†å‰²è§£ç¢¼å™¨ã€‚BEV ç‰¹å¾µé€šéæŸ¥è©¢å¼è¦–åœ–è½‰æ›å™¨ [41] å¾å¤šè¦–åœ–å½±åƒä¸­æå–ã€‚éµå¾ªå…ˆå‰çš„å·¥ä½œ [10, 39]ï¼Œè¼¸å…¥å½±åƒè§£æåº¦ç‚º $1024 \times 256$ï¼ŒBEV ç‰¹å¾µåœ–è§£æåº¦ç‚º $256 \times 256$ã€‚æˆ‘å€‘ä½¿ç”¨ä¸‹æ¡æ¨£ä¿‚æ•¸ 32ï¼ˆå‰è¦–åœ–ï¼‰å’Œ 16ï¼ˆBEVï¼‰ï¼Œç”¢ç”Ÿ 512 å€‹è¦–è¦º token $i_t$ï¼ˆæ¯åˆ†æ”¯ 256 å€‹ï¼‰ã€‚æ¸¬é‡è¼¸å…¥é€šéåŸºæ–¼ MLP çš„ç·¨ç¢¼å™¨ç·¨ç¢¼ç‚º 4 å€‹æ¸¬é‡ token $m_t$ï¼Œé§•é§›å‹•ä½œè¡¨ç¤ºç‚ºä¸‰å€‹æ¨™é‡å€¼ï¼ˆè½‰å‘ã€æ²¹é–€ã€åˆ¶å‹•ï¼‰ï¼Œé€šéç·šæ€§å±¤è½‰æ›ç‚º 3 å€‹å‹•ä½œ token $a_t$ã€‚æˆ‘å€‘å°‡ä¸–ç•Œæ¨¡å‹å¯¦ç¾ç‚ºå…·æœ‰ 4 å±¤å’Œ 8 å€‹æ³¨æ„åŠ›é ­çš„ transformerã€‚RaWMPC åœ¨éå» 5 å€‹è§€æ¸¬æ­¥é©Ÿçš„æ¢ä»¶ä¸‹é æ¸¬ $H=10$ å€‹æœªä¾†æ­¥é©Ÿï¼Œä¸¦åœ¨æ¨è«–æœŸé–“ä½¿ç”¨å¼ (6) ä¸­çš„æˆæœ¬è©•ä¼°ç”±è’¸é¤¾å‹•ä½œææ¡ˆç¶²è·¯ï¼ˆåœ¨ç¬¬ 3.3 ç¯€ä¸­æè¿°ï¼‰æå‡ºçš„ $N=10$ å€‹å€™é¸å‹•ä½œåºåˆ—ã€‚åœ¨å‹•ä½œé¸æ“‡æœŸé–“ï¼Œæˆ‘å€‘ä½¿ç”¨ $\eta_k = \max(2^{-k+1}, 1/8)$ ä¾†é™ä½é è™•é æ¸¬çš„æ¬Šé‡ï¼Œä¸¦å°‡äº‹ä»¶åš´é‡æ€§æ¬Šé‡è¨­å®šç‚º $\lambda_j = 10, 15, 30$ï¼Œåˆ†åˆ¥ç”¨æ–¼é§›å‡ºè»Šé“ã€äº¤é€šæ¨™èªŒé•è¦å’Œç¢°æ’ã€‚</p>
<p><strong>è¨“ç·´ã€‚</strong> RaWMPC ä½¿ç”¨ç¬¬ 3.2 ç¯€ä¸­æè¿°çš„å…©éšæ®µé¢¨éšªæ„ŸçŸ¥äº¤äº’è¨“ç·´ç­–ç•¥é€²è¡Œè¨“ç·´ã€‚æˆ‘å€‘é¦–å…ˆä½¿ç”¨è¨“ç·´è³‡æ–™çš„ 10%ï¼ˆBench2Drive çš„ 100 å€‹ç‰‡æ®µå’Œ NAVSIM çš„ 10K å€‹æ¨£æœ¬ï¼‰é€²è¡Œé›¢ç·šé ç†±ï¼Œç„¶å¾Œé€šéä½¿ç”¨æå‡ºçš„é¢¨éšªæ„ŸçŸ¥è¨“ç·´æ–¹æ¡ˆé€²è¡Œç·šä¸Šäº¤äº’ä¾†æ”¹é€²æ¨¡å‹ã€‚éš¨æ©ŸæŠ½æ¨£æ©Ÿç‡ $\epsilon_1$ å¾ 1 ç·šæ€§é€€ç«è‡³ 0ï¼Œè€Œé¢¨éšªæŠ½æ¨£æ©Ÿç‡ $\epsilon_2$ å¾ 0 ç·šæ€§å¢åŠ è‡³ 0.3ã€‚æˆ‘å€‘ç¶­è­·ä¸€å€‹å¤§å°ç‚º 10K å¹€çš„é‡æ’­ç·©è¡å€ï¼Œä»¥å„²å­˜æœ€è¿‘çš„äº¤äº’è³‡æ–™ï¼ˆä¾‹å¦‚ï¼ŒRGB å½±åƒã€èªç¾©åˆ†å‰²ã€è‡ªè»Šæ¸¬é‡å’Œäº¤é€šäº‹ä»¶æ¨™è¨»ï¼‰ã€‚æˆ‘å€‘ä½¿ç”¨åœ°å¹³ç·š $H=10$ å‹•ä½œåºåˆ—çš„é€æ®µæŠ½æ¨£ä»¥ç¢ºä¿æ™‚é–“é€£çºŒæ€§ã€‚åœ¨è‰¯å¥½/ä¸è‰¯æ¨¡å¼ä¸­ï¼Œæˆ‘å€‘æŒ‰æˆæœ¬æ’åº $N_s=50$ å€‹å€™é¸é …ï¼Œä¸¦ä½¿ç”¨æº«åº¦ $\tau_g = 0.5$ å’Œ $\tau_b = 1.0$ å¾åº•éƒ¨/é ‚éƒ¨ $K=5$ é›†åˆé€²è¡ŒæŠ½æ¨£ã€‚ç•¶æ»¿è¶³ä»¥ä¸‹ä»»ä½•æ¢ä»¶æ™‚ï¼Œä¸€å€‹å›åˆçµ‚æ­¢ï¼š(1) è‡ªè»Šé­å— 3 æ¬¡ç¢°æ’ï¼Œ(2) è‡ªè»Šé§›å‡ºé“è·¯æˆ–ä¿æŒå¡ä½è¶…é 100 å€‹é€£çºŒæ­¥é©Ÿï¼Œæˆ– (3) è‡ªè»ŠæˆåŠŸå®Œæˆè·¯ç·šã€‚åœ¨é›¢ç·šé ç†±å’Œç·šä¸Šäº¤äº’æœŸé–“ï¼Œæˆ‘å€‘åœ¨å››å€‹ NVIDIA A100 GPU ä¸Šä½¿ç”¨ç¸½å…± 1K å€‹ Bench2Drive ç‰‡æ®µå’Œ 100K å€‹ NAVSIM æ¨£æœ¬é€²è¡Œ RaWMPC è¨“ç·´ï¼ˆè¦æ¨¡èˆ‡å®˜æ–¹è¨“ç·´é›†ç›¸ç•¶ï¼Œä»¥é€²è¡Œå…¬å¹³æ¯”è¼ƒï¼‰ã€‚æˆ‘å€‘ä½¿ç”¨ Adam [33]ï¼Œåˆå§‹å­¸ç¿’ç‡ç‚º $10^{-4}$ è¡°æ¸›è‡³ $10^{-5}$ï¼Œæ‰¹æ¬¡å¤§å°ç‚º 16ã€‚</p>
<p><strong>è‡ªè©•ä¼°è’¸é¤¾ã€‚</strong> å°æ–¼è‡ªè©•ä¼°è’¸é¤¾ï¼ˆç¬¬ 3.3 ç¯€ï¼‰ï¼Œå‹•ä½œææ¡ˆç¶²è·¯å¯¦ç¾ç‚ºå…·æœ‰ 32 ç¶­éš±ç©ºé–“çš„ cVAE [67]ã€‚åœ¨å°æ¯”è¨“ç·´æœŸé–“ï¼Œæˆ‘å€‘æŠ½æ¨£ $N_s=50$ å€‹å‹•ä½œåºåˆ—ï¼Œå°‡æœ€å°æˆæœ¬åºåˆ—è¦–ç‚ºæ­£ä¾‹ $\boldsymbol{A}^{+}$ï¼Œä¸¦ä½¿ç”¨å‰ $K=5$ å€‹æœ€é«˜æˆæœ¬åºåˆ—ä½œç‚ºè² ä¾‹ã€‚æˆ‘å€‘ä½¿ç”¨ç¬¬ 3.3 ç¯€ä¸­çš„ç›®æ¨™å„ªåŒ–ææ¡ˆç¶²è·¯ï¼Œä½¿ç”¨æº«åº¦ $\tau=0.3$ã€KL æ¬Šé‡æ’ç¨‹ $\beta: 0 \to 0.1$ å’Œå°æ¯”æ¬Šé‡ $\lambda=0.1$ã€‚åœ¨æ¨è«–æ™‚ï¼Œæˆ‘å€‘å¾ cVAE è§£ç¢¼å™¨ä¸­æŠ½æ¨£ $N=10$ å€‹å€™é¸é …ï¼Œä¸¦é€šéåœ¨ä¸–ç•Œæ¨¡å‹æ¨å°ä¸‹æœ€å°åŒ–é æ¸¬æˆæœ¬ï¼ˆå¼ (6)ï¼‰ä¾†é¸æ“‡æœ€çµ‚å‹•ä½œã€‚</p>
<h2 id="43">4.3 èˆ‡æœ€å…ˆé€²æ–¹æ³•çš„æ¯”è¼ƒ</h2>
<p>åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘åœ¨ CARLA çš„é–‰ç’° Bench2Drive åŸºæº–æ¸¬è©¦å’Œ NAVSIM æ¸¬è©¦é›†ä¸Šæ¯”è¼ƒ RaWMPC èˆ‡æœ€å…ˆé€²çš„ç«¯åˆ°ç«¯é§•é§›æ–¹æ³•ã€‚ç‚ºäº†åæ˜ æˆ‘å€‘çš„ä¸»è¦ä¸»å¼µï¼Œæˆ‘å€‘å ±å‘Šå…©ç¨®è¨“ç·´è¨­ç½®ï¼š(i) <strong>ä¸é€²è¡Œæš–å•Ÿå‹•</strong>ï¼Œå…¶ä¸­ RaWMPC åœ¨ä¸ä½¿ç”¨ä»»ä½•é›¢ç·šè¨˜éŒ„å½±ç‰‡çš„æƒ…æ³ä¸‹é€²è¡Œè¨“ç·´ï¼Œä»¥åŠ (ii) <strong>å®Œæ•´</strong>è¨­ç½®ï¼Œå…¶ä¸­ä¸€å°çµ„è¨˜éŒ„çš„é§•é§›è»Œè·¡è¢«ç”¨ä½œå¯é¸çš„æš–å•Ÿå‹•ã€‚æš–å•Ÿå‹•åœ¨ç¶“é©—ä¸Šå¯ä»¥åŠ é€Ÿæ”¶æ–‚ä¸¦é€²ä¸€æ­¥æ”¹å–„æœ€çµ‚æ€§èƒ½ï¼Œè€Œ RaWMPC å³ä½¿åœ¨æ²’æœ‰æš–å•Ÿå‹•çš„æƒ…æ³ä¸‹ä¹Ÿå·²ç¶“è¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆé€²æ–¹æ³•ã€‚</p>
<h3 id="431-bench2drive">4.3.1 åœ¨ Bench2Drive ä¸Šçš„è©•ä¼°</h3>
<p>è¡¨ 1 èªªæ˜äº† Bench2Drive ä¸Šçš„é–‰ç’°çµæœã€‚RaWMPC åœ¨æ‰€æœ‰æ¯”è¼ƒæ–¹æ³•ä¸­é”åˆ°äº†æœ€ä½³çš„æ•´é«”æ€§èƒ½ï¼Œåœ¨å®Œæ•´è¨­ç½®ä¸­é”åˆ° 88.31 DS å’Œ 70.48% SRã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå³ä½¿<strong>æ²’æœ‰æš–å•Ÿå‹•</strong>ï¼ˆå³æ²’æœ‰è¨˜éŒ„è»Œè·¡ï¼‰ï¼ŒRaWMPC ä»ç„¶é”åˆ° 87.34 DS å’Œ 69.62% SRï¼Œè¶…è¶Šäº†å¼·å¤§çš„è¿‘æœŸåŸºç·šå¦‚ HiP-AD [73]ï¼ˆ86.77 DS / 69.09% SRï¼‰å’Œé è¨“ç·´ VLM æ–¹æ³• SimLingo [57]ï¼ˆ85.94 DS / 66.82% SRï¼‰ã€‚æ­¤å¤–ï¼ŒRaWMPC ä¿æŒäº†å…·æœ‰ç«¶çˆ­åŠ›çš„æ•ˆç‡ï¼Œä¸¦æ¯”å¤§å¤šæ•¸é«˜æ€§èƒ½é–‰ç’°æ™ºèƒ½é«”é”åˆ°äº†æ›´é«˜çš„èˆ’é©åº¦ï¼Œé€™è¡¨æ˜æ‰€ç²å¾—çš„æ”¹é€²ä¸¦éä¾†è‡ªæ–¼æ¿€é€²çš„æ“ä½œï¼Œè€Œæ˜¯ä¾†è‡ªæ–¼æ›´å¯é çš„æ±ºç­–åˆ¶å®šã€‚</p>
<h3 id="432-navsim">4.3.2 åœ¨ NAVSIM ä¸Šçš„è©•ä¼°</h3>
<p>è¡¨ 2 ç¸½çµäº† NAVSIM ä¸Šçš„çµæœã€‚RaWMPC åœ¨æ‰€æœ‰åŸºæ–¼å­¸ç¿’çš„æ–¹æ³•ä¸­é”åˆ°äº†æœ€é«˜çš„ <strong>91.3 PDMS</strong>ã€‚åœ¨æ²’æœ‰æš–å•Ÿå‹•çš„æƒ…æ³ä¸‹ï¼ŒRaWMPC ä»ç„¶é”åˆ° 90.5 PDMSï¼Œå·²ç¶“è¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ–¹æ³•ï¼ˆä¾‹å¦‚ GoalFlow [82]ï¼š90.3ï¼‰ã€‚æš–å•Ÿå‹•é€²ä¸€æ­¥æ”¹å–„äº† PDMSï¼ˆ90.5â†’91.3ï¼‰ï¼Œé€™èˆ‡è§€å¯Ÿçµæœä¸€è‡´ï¼Œå³å°‘é‡è¨˜éŒ„è»Œè·¡å¯ä»¥åŠ é€Ÿæ”¶æ–‚ä¸¦æ”¹å–„æ€§èƒ½ï¼Œä½†ä¸æ˜¯é”åˆ°æœ€å…ˆé€²çµæœæ‰€å¿…éœ€çš„ã€‚</p>
<h3 id="433">4.3.3 å¤©æ°£å¼•èµ·çš„åŸŸè½‰ç§»ä¸‹çš„æ³›åŒ–æ€§èƒ½</h3>
<p>ç‚ºäº†è©•ä¼°è¶…å‡ºè¨“ç·´åˆ†å¸ƒçš„ç©©å¥æ€§ï¼Œæˆ‘å€‘é€²è¡Œäº†å¤©æ°£è½‰ç§»ç ”ç©¶ï¼Œå…¶ä¸­æ‰€æœ‰æ–¹æ³•åƒ…åœ¨ <em>Rainy</em> å ´æ™¯ä¸Šé€²è¡Œè©•ä¼°ï¼ŒåŒæ™‚åœ¨ <em>Sunny only</em> æˆ– <em>Sunny &amp; Rainy</em> è³‡æ–™ä¸Šé€²è¡Œè¨“ç·´ï¼ˆè¡¨ 3ï¼‰ã€‚ä¸€å€‹é—œéµè§€å¯Ÿæ˜¯ï¼Œæ¨¡ä»¿å­¸ç¿’æ–¹æ³•å°è¨“ç·´åŸŸè¦†è“‹ç¯„åœå¾ˆæ•æ„Ÿï¼šç•¶è¨“ç·´ä¸­ä¸å­˜åœ¨é›¨å¤©æ¢ä»¶ï¼ˆSunny-onlyï¼‰æ™‚ï¼Œå…¶æ€§èƒ½æœƒé¡¯è‘—ä¸‹é™ï¼Œåæ˜ äº†å°å…ˆå‰æœªè¦‹éçš„ç’°å¢ƒçš„è½‰ç§»èƒ½åŠ›æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRaWMPC åœ¨å…©ç¨®è¨“ç·´æ–¹æ¡ˆä¸‹éƒ½é”åˆ°æœ€ä½³çš„ DS å’Œ SRï¼Œä¸¦ä¸”å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨ <em>Sunny only</em> ä¸Šè¨“ç·´æ™‚ï¼ˆå³é¢å°æœªè¦‹éçš„é›¨å¤©ç›®æ¨™åŸŸï¼‰ï¼Œä»é¡¯è‘—å„ªæ–¼å¼·å¤§çš„ IL åŸºç·šæ–¹æ³•ï¼ˆLAWã€WoTEï¼‰å’Œ SimLingoã€‚æ­¤å¤–ï¼Œèˆ‡ SimLingo ç›¸æ¯”ï¼ŒRaWMPC åœ¨å¾è¨“ç·´ä¸­ç§»é™¤é›¨å¤©è³‡æ–™æ™‚è¡¨ç¾å‡ºæ˜é¡¯æ›´å°çš„æ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜å°å…ˆå‰æœªè¦‹éçš„æ¢ä»¶æœ‰æ›´å¼·çš„ç©©å¥æ€§ã€‚</p>
<figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig5.jpeg" loading="lazy" alt="Figure 5"></figure>
<p>åœ– 5 é¡¯ç¤ºäº†é€™ç¨® <em>Sunny-only</em> â†’ <em>Rainy</em> è½‰ç§»çš„å®šæ€§ç¤ºä¾‹ã€‚LAW åœ¨æ”¹è®Šçš„è¦–è¦ºæ¢ä»¶ä¸‹æœªèƒ½è­˜åˆ¥å‰æ–¹è»Šè¼›ï¼Œå°è‡´é«˜åš´é‡ç¨‹åº¦çš„æ­£é¢ç¢°æ’ã€‚WoTE å’Œ SimLingo å˜—è©¦è¦é¿å‹•ä½œä¾†æ¸›å°‘ç›¸å°æ–¼ç›´æ¥æ­£é¢ç¢°æ’çš„æ’æ“Šï¼Œä½†å ´æ™¯ä»ä»¥å´æ»‘/è¿½å°¾ç¢°æ’çµæŸã€‚ä¸€å€‹åˆç†çš„è§£é‡‹æ˜¯ï¼Œé›¨å¤©è½‰ç§»é™ä½äº†æ„ŸçŸ¥-æ±ºç­–æ£§çš„å¯é æ€§ï¼Œè€Œä¸‹æ¸¸ç­–ç•¥ä¸¦æœªæ˜ç¢ºå„ªåŒ–ä¸ç¢ºå®šæ€§ä¸‹çš„æœ€å°é¢¨éšªé–“éš”ï¼Œå°è‡´åœ¨è¿‘è·é›¢è¦é¿æœŸé–“å®‰å…¨é‚Šéš›ä¸è¶³ï¼ˆä¾‹å¦‚ï¼Œä¸æº–ç¢ºçš„å‹•ä½œé æ¸¬æˆ–æ¿•æ»‘è·¯é¢ä¸Šè‡ªè»Šåæ‡‰ä¸åŒ¹é…ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRaWMPC ä½¿ç”¨é¢¨éšªæ„ŸçŸ¥çš„ä¸–ç•Œæ¨¡å‹æ˜ç¢ºè©•ä¼°å€™é¸å‹•ä½œåºåˆ—çš„é æ¸¬çµæœï¼Œä¸¦é¸æ“‡æœ€å°é¢¨éšªçš„é æ¸¬æ§åˆ¶è¡Œç‚ºï¼Œåœ¨ä¸ç¢ºå®šæ€§ä¸‹ä¿æŒå®‰å…¨é–“éš”ã€‚</p>
<p>æˆ‘å€‘å°‡é€™ä¸€å„ªå‹¢æ­¸å› æ–¼ RaWMPC çš„é¢¨éšªæ„ŸçŸ¥é æ¸¬æ§åˆ¶å…¬å¼ï¼šRaWMPC ä¸¦éé‡ç¾å°ˆå®¶å‹•ä½œï¼Œè€Œæ˜¯å­¸ç¿’<em>å¾äº¤äº’ä¸­çš„é¢¨éšªæ„ŸçŸ¥</em>ï¼Œä¸¦é€šéå­¸ç¿’çš„ä¸–ç•Œæ¨¡å‹æœ€å°åŒ–é æ¸¬é¢¨éšªä¾†é¸æ“‡å‹•ä½œã€‚é€™æ¨£çš„ç›®æ¨™é¼“å‹µå¯è½‰ç§»çš„æ±ºç­–åŸå‰‡ï¼ˆä¾‹å¦‚ï¼Œåœ¨ä¸ç¢ºå®šæ€§ä¸‹ä¿æŒå®‰å…¨é‚Šéš›å’Œä¿å®ˆè¡Œå‹•ï¼‰ï¼Œé€™äº›åŸå‰‡åœ¨å¤–è§€å’Œå‹•åŠ›å­¸è·¨åŸŸè®ŠåŒ–æ™‚ä»ç„¶æœ‰æ•ˆã€‚å› æ­¤ï¼ŒRaWMPC å°é‚Šç•Œæƒ…æ³çš„è©³ç›¡å°ˆå®¶å‹•ä½œè¦†è“‹çš„ä¾è³´ç¨‹åº¦è¼ƒä½ï¼Œé€™æ›´å¥½åœ°ç¬¦åˆçœŸå¯¦ä¸–ç•Œéƒ¨ç½²çš„é•·å°¾ç‰¹æ€§ï¼Œå…¶ä¸­æœªè¦‹éçš„å ´æ™¯æ˜¯ä¸å¯é¿å…çš„ã€‚</p>
<hr />
<p>å¤©æ°£å¼•èµ·çš„åŸŸè½‰ç§»ä¸‹çš„å®šæ€§æ¯”è¼ƒï¼ˆSunny-only â†’ Rainyï¼‰ã€‚æ‰€æœ‰æ–¹æ³•å‡åœ¨ <em>Sunny-only</em> è³‡æ–™ä¸Šè¨“ç·´ï¼Œåœ¨ <em>Rainy</em> æ¢ä»¶ä¸‹è©•ä¼°ã€‚LAW [38] éŒ¯éäº†å‰æ–¹è»Šè¼›ï¼Œå°è‡´åš´é‡çš„æ­£é¢ç¢°æ’ã€‚WoTE [39] å’Œ SimLingo [57] é€šéè¦é¿å‹•ä½œé™ä½äº†åš´é‡æ€§ï¼Œä½†ç”±æ–¼æ„ŸçŸ¥-æ±ºç­–å¯é æ€§ä¸‹é™å’Œå®‰å…¨é‚Šéš›å¯¦æ–½ä¸åŠ›è€Œä»ç„¶ç™¼ç”Ÿç¢°æ’ã€‚RaWMPC é€šéåœ¨ä¸ç¢ºå®šæ€§ä¸‹é¸æ“‡æœ€å°é¢¨éšªçš„é æ¸¬æ§åˆ¶å‹•ä½œä¾†é¿å…ç¢°æ’ã€‚</p>
<p><strong>è¡¨ 3</strong> åŸŸè½‰ç§»ä¸‹çš„æ€§èƒ½æ¯”è¼ƒã€‚æ‰€æœ‰æ–¹æ³•å‡åœ¨ <em>Sunny only</em> æˆ– <em>Sunny &amp; Rainy</em> è³‡æ–™ä¸Šè¨“ç·´ï¼Œä¸¦å°ˆé–€åœ¨ <em>Rainy</em> å ´æ™¯ä¸Šè©•ä¼°ã€‚â†‘ è¡¨ç¤ºè¶Šé«˜è¶Šå¥½ã€‚</p>
<table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>å ´åœ°</th>
<th>æ–¹æ¡ˆ</th>
<th>è¨“ç·´è³‡æ–™</th>
<th>åœ¨ Rainy ä¸Šæ¸¬è©¦</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>DSâ†‘</td>
<td>SR(%)â†‘</td>
</tr>
<tr>
<td>LAW</td>
<td>ICLR 2025</td>
<td>IL</td>
<td>Sunny &amp; Rainy</td>
<td>34.54</td>
<td>7.09</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Sunny only</td>
<td>23.58</td>
<td>3.56</td>
</tr>
<tr>
<td>WoTE</td>
<td>ICCV 2025</td>
<td>IL</td>
<td>Sunny &amp; Rainy</td>
<td>36.54</td>
<td>7.85</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Sunny only</td>
<td>28.65</td>
<td>5.21</td>
</tr>
<tr>
<td>SimLingo (Pretrained-VLM)</td>
<td>CVPR 2025</td>
<td>IL</td>
<td>Sunny &amp; Rainy</td>
<td>51.69</td>
<td>13.68</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Sunny only</td>
<td>33.49</td>
<td>8.97</td>
</tr>
<tr>
<td>RaWMPC</td>
<td>â€“</td>
<td>PC</td>
<td>Sunny &amp; Rainy</td>
<td><strong>53.67</strong></td>
<td><strong>14.96</strong></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Sunny only</td>
<td><strong>41.36</strong></td>
<td><strong>10.83</strong></td>
</tr>
</tbody>
</table>
<h3 id="434-qualitative-visualization-of-predictive-control">4.3.4 Qualitative visualization of predictive control</h3>
<p>We provide some visualization results of the predictive control procedure of RaWMPC in Fig. 6. Given RGB observations and high-level navigation commands (e.g., keep going straight or merge left), our generative policy proposes a small set of candidate action sequences (e.g., keep going straight, detour, brake, and lane change). For each candidate, the risk-aware world model predicts the near-future semantic traffic state and the decoding module evaluates its consequence from both task progress and safety perspectives, including collision risk, off-lane/sidewalk intrusion risk, and progress-related penalties such as getting stuck in traffic. The final action is selected by comparing these predicted consequences and choosing the minimum-cost one under the navigation goal.</p>
<p>In the first case, going straight collides with a crossing</p>
<p>pedestrian, while detours either collide with an oncoming vehicle or drive onto the sidewalk; RaWMPC chooses <em>slow down, straight briefly, then stop</em> to safely stop in front of the pedestrian (instead of an overly conservative early brake). In the second case, going straight or merging left immediately causes a collision, steering right hits a parked vehicle, and stopping leads to a deadlock; thus RaWMPC selects <em>pause briefly, then merge left</em> for a collision-free merge. These cases demonstrate that RaWMPC can proactively avoid risky behaviors by explicitly forecasting and comparing action consequences, rather than merely following a single command or relying on a fixed fallback maneuver.</p>
<h2 id="44">4.4 æ¶ˆèç ”ç©¶</h2>
<p>æœ¬ç« ç¯€æˆ‘å€‘ä½¿ç”¨ Bench2Drive è³‡æ–™é›†ï¼Œå°æ‰€æå‡ºçš„æ–¹æ³•é€²è¡Œå…¨é¢çš„æ¶ˆèç ”ç©¶ã€‚</p>
<hr />
<figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig6.jpeg" loading="lazy" alt="Figure 6"></figure>
<p><strong>åœ– 6</strong> é æ¸¬æ§åˆ¶ç¨‹åºçš„è¦–è¦ºåŒ–ã€‚åœ¨æ™‚åˆ» tï¼Œæˆ‘å€‘å±•ç¤ºå‰è¦–è¦–è§’å’Œ BEV å½±åƒï¼ˆå«èªæ„åˆ†å‰²ï¼‰ã€‚è™›ç·šæ›²ç·šè¡¨ç¤ºå€™é¸å‹•ä½œï¼Œçªå‡ºé¡¯ç¤ºçš„æ™ºèƒ½é«”è¡¨ç¤ºä¸»è¦é¢¨éšªã€‚å¾ t + 1 åˆ° t + 5 çš„æ¨æ¼”èªªæ˜é æ¸¬çš„å¾Œæœï¼Œä¸‹æ–¹é¢æ¿å ±å‘Šå„å€‹å‹•ä½œçš„çµæœå’Œæˆæœ¬ï¼ˆä¾‹å¦‚ç¢°æ’ã€äººè¡Œé“å…¥ä¾µã€åœæ­¢è·é›¢ï¼‰ã€‚<strong>æƒ…æ™¯ 1ï¼š</strong> RaWMPC æ¸›é€Ÿã€çŸ­æš«å‰é€²ï¼Œç„¶å¾Œç‚ºè¡Œäººåœæ­¢ã€‚<strong>æƒ…æ™¯ 2ï¼š</strong> RaWMPC çŸ­æš«ç­‰å¾…ï¼Œç„¶å¾Œå·¦è½‰ä»¥é¿é–‹å‰å·¦æ–¹çš„è»Šè¼›å’Œåœæ”¾çš„è»Šè¼›ã€‚</p>
<p>4.4.1 æ¡†æ¶åˆ†æ</p>
<p>è¡¨ 4 åˆ†æäº†èˆ‡æˆ‘å€‘æ¨¡å‹è¨­è¨ˆï¼ˆç¬¬ 3.1.1 ç¯€ï¼‰ç›¸ç¬¦çš„æ ¸å¿ƒçµ„ä»¶ã€‚<em>ä¸å«èªæ„å¼•å°</em> ç§»é™¤èªæ„å¼•å°äº‹ä»¶è§£ç¢¼ï¼Œå³ä¾†è‡ªèªæ„åˆ†å‰²è§£ç¢¼å™¨çš„æ³¨æ„åŠ›èåˆåˆ°äº‹ä»¶è§£ç¢¼å™¨ä¸­ã€‚é€™å°è‡´æ˜é¡¯çš„æ•ˆèƒ½ä¸‹é™ï¼ˆDS 88.31â†’82.36ï¼ŒSR 70.48%â†’62.69%ï¼‰ï¼Œè¡¨æ˜æº–ç¢ºçš„å®‰å…¨äº‹ä»¶é æ¸¬å°é¢¨éšªæ„ŸçŸ¥æˆæœ¬è©•ä¼°è‡³é—œé‡è¦ã€‚<em>ä¸å«åˆ†å‰²è§£ç¢¼å™¨</em></p>
<p><strong>è¡¨ 4</strong> æ‰€ææ¡†æ¶çš„æ¶ˆèç ”ç©¶ã€‚</p>
<table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>è©•é‡æŒ‡æ¨™</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>DSâ†‘</td>
<td>SR(%)â†‘</td>
</tr>
<tr>
<td>å®Œæ•´çš„ RaWMPCï¼ˆæˆ‘å€‘çš„æ–¹æ³•ï¼‰</td>
<td>88.31</td>
<td>70.48</td>
</tr>
<tr>
<td>w/o èªæ„å¼•å°</td>
<td>82.36 -5.95</td>
<td>62.69 -7.79</td>
</tr>
<tr>
<td>w/o åˆ†å‰²è§£ç¢¼å™¨</td>
<td>70.85 -17.46</td>
<td>48.95 -21.53</td>
</tr>
<tr>
<td>w/o å‹•ä½œé¸æ“‡</td>
<td>61.35 -26.96</td>
<td>30.98 -39.50</td>
</tr>
</tbody>
</table>
<p>é€²ä¸€æ­¥ç§»é™¤åˆ†å‰²è§£ç¢¼åˆ†æ”¯åŠå…¶ç›£ç£ï¼Œå°è‡´å¤§å¹…æ•ˆèƒ½ä¸‹é™</p>
<hr />
<p><strong>è¡¨ 5</strong> é¢¨éšªæ„ŸçŸ¥è¨“ç·´çš„æ¶ˆèç ”ç©¶ã€‚</p>
<table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>è©•é‡æŒ‡æ¨™</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>DSâ†‘</td>
<td>SR(%)â†‘</td>
</tr>
<tr>
<td>é¢¨éšªæ„ŸçŸ¥æ¡æ¨£ï¼ˆ<strong>æˆ‘å€‘çš„æ–¹æ³•</strong>ï¼‰</td>
<td><strong>88.31</strong></td>
<td><strong>70.48</strong></td>
</tr>
<tr>
<td>Îµ-è²ªå¿ƒæ¡æ¨£</td>
<td>83.86 -4.45</td>
<td>61.74 -8.74</td>
</tr>
<tr>
<td>éš¨æ©Ÿæ¡æ¨£</td>
<td>70.41 -17.90</td>
<td>46.82 -23.66</td>
</tr>
</tbody>
</table>
<p><strong>è¡¨ 6</strong> æ”¿ç­–å­¸ç¿’ä¸­ä½¿ç”¨ä¸åŒå‹•ä½œç›£ç£æ‰€ç²å¾—çš„çµæœã€‚</p>
<table>
<thead>
<tr>
<th>æ”¿ç­–å­¸ç¿’è³‡æ–™</th>
<th>è©•é‡æŒ‡æ¨™</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>DSâ†‘</td>
<td>SR(%)â†‘</td>
</tr>
<tr>
<td>æ­£å‘èˆ‡è² å‘å‹•ä½œï¼ˆ<strong>æˆ‘å€‘çš„æ–¹æ³•</strong>ï¼‰</td>
<td><strong>88.31</strong></td>
<td><strong>70.48</strong></td>
</tr>
<tr>
<td>å°ˆå®¶å‹•ä½œ</td>
<td>86.75 -1.56</td>
<td>68.25 -2.23</td>
</tr>
<tr>
<td>åƒ…é™æ­£å‘å‹•ä½œ</td>
<td>83.65 -4.66</td>
<td>66.52 -3.96</td>
</tr>
</tbody>
</table>
<p>ï¼ˆDS 70.85 / SR 48.95%ï¼‰ï¼Œé€™è¡¨æ˜é æ¸¬é«˜éšèªæ„å°å¯é çš„é•·è¦–é‡æ¨æ¼”è‡³é—œé‡è¦ã€‚<em>w/o å‹•ä½œé¸æ“‡</em> åœç”¨æ–¹ç¨‹å¼ (1) ä¸­çš„é æ¸¬æ§åˆ¶ï¼ˆç¹éæ–¹ç¨‹å¼ (6) ä¸­åŸºæ–¼æˆæœ¬çš„æ’åï¼‰ï¼Œç›´æ¥åŸ·è¡Œææ¡ˆ/å¼•å°è¼¸å‡ºï¼Œå°è‡´æœ€åš´é‡çš„æ•ˆèƒ½å´©æ½°ï¼ˆDS 61.35 / SR 30.98%ï¼‰ã€‚é€™è­‰å¯¦äº†é€šéæ˜ç¢ºè©•ä¼°é æ¸¬çš„é•·è¦–é‡å¾Œæœä¾†é¸æ“‡å‹•ä½œæ˜¯ RaWMPC çš„é—œéµã€‚</p>
<h3 id="442-analysis-of-risk-aware-training"><strong>4.4.2 Analysis of Risk-Aware Training</strong></h3>
<p>Table 5 evaluates the risk-aware interaction training strategy used to refine the world model. Our risk-aware sampling follows the design in Sec. 3.2: besides random exploration, it uses the current RaWMPC to score candidate action sequences and deliberately collects both good (low-cost) and bad (high-cost) rollouts, improving coverage of rare safety-critical outcomes. Replacing it with Ïµ-greedy sampling (random with probability Ïµâ‚, otherwise only selecting low-cost rollouts) reduces performance (DS 83.86 / SR 61.74%), showing that excluding high-cost failures weakens learning of risky consequences. Pure random sampling further degrades results (DS 70.41 / SR 46.82%), indicating that unguided data collection is substantially less efficient for learning long-horizon consequences.</p>
<h3 id="443"><strong>4.4.3 è‡ªè©•ä¼°è’¸é¤¾åˆ†æ</strong></h3>
<p>è¡¨ 6 ç ”ç©¶äº†ç¬¬ 3.3 ç¯€ä¸­å‹•ä½œæè­°ç¶²è·¯çš„è¨“ç·´æ–¹å¼ã€‚æˆ‘å€‘çš„é è¨­è¨­ç½®ä½¿ç”¨ RaWMPC ä½œç‚ºè‡ªè©•ä¼°å™¨ä¾†å½æ¨™ç±¤å‹•ä½œï¼šæœ€ä½æˆæœ¬åºåˆ—è¢«è¦–ç‚ºæ­£æ¨£æœ¬ï¼Œè€Œé«˜æˆæœ¬åºåˆ—ä½œç‚ºè² æ¨£æœ¬ï¼Œé€™ç”¢ç”Ÿäº†æœ€ä½³</p>
<p><strong>è¡¨ 7</strong> ä½¿ç”¨ä¸åŒé æ¸¬è¦–é‡é‡æ‰€ç²å¾—çš„çµæœã€‚</p>
<table>
<thead>
<tr>
<th>é æ¸¬è¦–é‡</th>
<th>æŒ‡æ¨™</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>DSâ†‘</td>
<td>SR(%)â†‘</td>
</tr>
<tr>
<td>H=1</td>
<td>57.85 <span style="color:red;">-30.46</span></td>
<td>28.64 <span style="color:red;">-41.84</span></td>
</tr>
<tr>
<td>H=5</td>
<td>74.98 <span style="color:red;">-13.33</span></td>
<td>49.52 <span style="color:red;">-20.96</span></td>
</tr>
<tr>
<td>H=10 (<strong>æˆ‘å€‘çš„</strong>)</td>
<td><strong>88.31</strong></td>
<td><strong>70.48</strong></td>
</tr>
<tr>
<td>H=15</td>
<td>82.34 <span style="color:red;">-5.97</span></td>
<td>62.38 <span style="color:red;">-8.10</span></td>
</tr>
</tbody>
</table>
<p><strong>è¡¨ 8</strong> ä½¿ç”¨ä¸åŒé ç†±é›¢ç·šå­¸ç¿’è³‡æ–™é‡æ‰€ç²å¾—çš„çµæœã€‚</p>
<table>
<thead>
<tr>
<th>é ç†±è³‡æ–™</th>
<th>æŒ‡æ¨™</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>DSâ†‘</td>
<td>SR(%)â†‘</td>
</tr>
<tr>
<td>0%</td>
<td>87.34 <span style="color:red;">-0.97</span></td>
<td>69.62 <span style="color:red;">-0.86</span></td>
</tr>
<tr>
<td>10%</td>
<td><strong>88.31</strong></td>
<td><strong>70.48</strong></td>
</tr>
<tr>
<td>20%</td>
<td>88.09 <span style="color:red;">-0.22</span></td>
<td>70.32 <span style="color:red;">-0.16</span></td>
</tr>
<tr>
<td>30%</td>
<td>86.95 <span style="color:red;">-1.36</span></td>
<td>68.52 <span style="color:red;">-1.96</span></td>
</tr>
</tbody>
</table>
<p>æ€§èƒ½ï¼ˆDS 88.31 / SR 70.48%ï¼‰ã€‚åƒ…ä½¿ç”¨å°ˆå®¶å‹•ä½œè¨“ç·´æè­°ç¶²è·¯æœƒç•¥å¾®é™ä½æ€§èƒ½ï¼ˆDS 86.75 / SR 68.25%ï¼‰ï¼Œé€™è¡¨æ˜è‡ªè©•ä¼°çš„ç›®æ¨™æ¯”ç›´æ¥æ¨¡ä»¿ç›®æ¨™æ›´ç¬¦åˆé æ¸¬æ§åˆ¶ç›®æ¨™ã€‚åƒ…ä½¿ç”¨æ­£å‹•ä½œé€²ä¸€æ­¥ä¸‹é™æ€§èƒ½ï¼ˆDS 83.65 / SR 66.52%ï¼‰ï¼Œé€™è¡¨æ˜æ˜ç¢ºå°æ¯”é«˜é¢¨éšªè² æ¨£æœ¬å°æ–¼é˜²æ­¢ä¸å®‰å…¨å€™é¸é …å’Œæ”¹é€²ä¸‹æ¸¸é¸æ“‡è‡³é—œé‡è¦ã€‚</p>
<h3 id="444"><strong>4.4.4 é æ¸¬è¦–é‡è¨è«–</strong></h3>
<p>è¡¨ 7 å°ä¸–ç•Œæ¨¡å‹è¿´æº¯å’Œæˆæœ¬è©•ä¼°ä¸­ä½¿ç”¨çš„è¦åŠƒè¦–é‡ <em>H</em> é€²è¡Œäº†æ¶ˆèå¯¦é©—ï¼ˆæ–¹ç¨‹å¼ (6)ï¼‰ã€‚çŸ­è¦–é‡ç„¡æ³•æ•æ‰å»¶é²å¾Œæœï¼Œå°è‡´æ€§èƒ½ä¸ä½³ï¼ˆH=1ï¼šDS 57.85 / SR 28.64%ï¼›H=5ï¼šDS 74.98 / SR 49.52%ï¼‰ã€‚å¢åŠ åˆ° H=10 ç”¢ç”Ÿæœ€ä½³çµæœï¼ˆDS 88.31 / SR 70.48%ï¼‰ï¼Œå› ç‚ºå®ƒæä¾›äº†è¶³å¤ çš„å‰ç»æ€§ä»¥é€²è¡Œé¢¨éšªè©•ä¼°ï¼ŒåŒæ™‚ä¿æŒé æ¸¬ä¸ç¢ºå®šæ€§å¯æ§ã€‚é€²ä¸€æ­¥å¢åŠ åˆ° H=15 æœƒé™ä½æ€§èƒ½ï¼ˆDS 82.34 / SR 62.38%ï¼‰ï¼Œå¯èƒ½æ˜¯ç”±æ–¼ç´¯ç©çš„è¿´æº¯èª¤å·®å½±éŸ¿äº†åŸºæ–¼æˆæœ¬çš„æ’åºã€‚</p>
<h3 id="445-warm-up"><strong>4.4.5 Warm-up çš„è¨è«–</strong></h3>
<p>è¡¨ 8 é€²è¡Œæ¶ˆèç ”ç©¶ï¼Œæ¯”è¼ƒäº’å‹•å¼è¨“ç·´å‰ç”¨æ–¼ warm-up çš„é›¢ç·šè¨˜éŒ„è»Œè·¡æ¯”ä¾‹ã€‚åœ¨æ­¤ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘ä¿æŒè¨“ç·´æ¨£æœ¬ç¸½æ•¸å›ºå®šï¼Œåªæ”¹è®Šåˆ†é…çµ¦é›¢ç·š warm-up è³‡æ–™çš„æ¯”ä¾‹ã€‚åœ¨æ²’æœ‰ warm-up çš„æƒ…æ³ä¸‹ï¼ˆ0%ï¼‰ï¼Œ</p>
<hr />
<p><strong>è¡¨ 9</strong> ä½¿ç”¨å­¸ç¿’ä¸–ç•Œæ¨¡å‹çš„æ§åˆ¶æ¨¡å¼æ¶ˆèç ”ç©¶ã€‚</p>
<table><thead><tr><th rowspan="2">æ–¹æ³•</th><th colspan="2">æŒ‡æ¨™</th></tr><tr><th>DSâ†‘</th><th>SR(%)â†‘</th></tr></thead><tbody><tr><td>é æ¸¬æ§åˆ¶ï¼ˆæˆ‘å€‘çš„æ–¹æ³•ï¼‰</td><td>88.31</td><td>70.48</td></tr><tr><td>å¼·åŒ–å­¸ç¿’</td><td>73.58 <sup>-14.73</sup></td><td>51.85 <sup>-18.63</sup></td></tr></tbody></table>

<table><tfoot><tr><th colspan="5" style="text-align: center;"> ä¸éœ€è¦</th></tr></tfoot></table>

<p>æ€§èƒ½ä¸‹é™ï¼ˆDS 87.34 / SR 69.62%ï¼‰ï¼Œè¡¨æ˜å¾é ­é–‹å§‹è¨“ç·´æœƒå°è‡´æ¨ç§»å“è³ªä¸å¤ å¯é å’Œæ—©æœŸæœ€ä½³åŒ–éšæ®µä¸å¤ ç©©å®šã€‚ä½¿ç”¨å°‘é‡è¨˜éŒ„è»Œè·¡ï¼ˆ10%ï¼‰ç”¢ç”Ÿæœ€ä½³çµæœï¼ˆDS 88.31 / SR 70.48%ï¼‰ï¼Œè¡¨æ˜è¼•é‡åŒ–çš„ warm-up æä¾›äº†æœ‰ç”¨çš„é æ¸¬å…ˆé©—ï¼ˆä¾‹å¦‚åŸºæœ¬å‹•åŠ›å­¸å»ºæ¨¡å’Œæ„ŸçŸ¥è§£ç¢¼ï¼‰ï¼Œæ”¹å–„äº†é•·è¦–é‡æ¨ç§»å“è³ªå’Œä¸‹æ¸¸æ§åˆ¶ã€‚ç„¶è€Œï¼Œé€²ä¸€æ­¥å¢åŠ  warm-up æ¯”ä¾‹é–‹å§‹é™ä½æ€§èƒ½ï¼ˆ20%ï¼šDS 88.09 / SR 70.32%ï¼›30%ï¼šDS 86.95 / SR 68.52%ï¼‰ã€‚æˆ‘å€‘å°‡é€™ä¸€è¶¨å‹¢æ­¸å› æ–¼é›¢ç·šè¨˜éŒ„è»Œè·¡å¼·çƒˆåå‘æ–¼å®‰å…¨ã€é¡äººçš„è¡Œç‚ºï¼ŒåŒ…å«å¾ˆå°‘å±éšªäº‹ä»¶ã€‚å› æ­¤ï¼Œå°‡éå¤šè³‡æ–™åˆ†é…çµ¦é›¢ç·š warm-up æœƒæ¸›å°‘å¾ŒçºŒç·šä¸Šäº’å‹•æ¢ç´¢éå¸¸è¦å‹•ä½œå’Œæ”¶é›†å®‰å…¨é—œéµå¤±æ•—çš„æ©Ÿæœƒâ€”ç‰¹åˆ¥æ˜¯åœ¨å±éšªå ´æ™¯ä¸­â€”é€™äº›å°æ–¼å­¸ç¿’ç©©å¥çš„é¢¨éšªæ„è­˜è‡³é—œé‡è¦ã€‚</p>
<p><strong>4.4.6 æ§åˆ¶æ¨¡å¼çš„è¨è«–</strong></p>
<p>è¡¨ 9 æ¯”è¼ƒé æ¸¬æ§åˆ¶èˆ‡ç›´æ¥ä½¿ç”¨åŸºæ–¼æ¨¡å‹çš„å¼·åŒ–å­¸ç¿’ï¼ˆRLï¼‰æœ€ä½³åŒ–ç­–ç•¥ã€‚é æ¸¬æ§åˆ¶é”åˆ°äº†æ˜é¡¯æ›´é«˜çš„æ€§èƒ½ï¼ˆDS 88.31 / SR 70.48%ï¼‰ï¼Œç›¸æ¯”åŸºæ–¼æ¨¡å‹çš„ RLï¼ˆDS 73.58 / SR 51.85%ï¼‰ã€‚é€™é©—è­‰äº†é€éè§£ç¢¼çš„æœªä¾†çµæœï¼ˆåˆ†å‰²ã€äº‹ä»¶ã€è‡ªè»Šç‹€æ…‹ï¼‰è©•ä¼°å€™é¸å‹•ä½œåºåˆ—ä¸¦é¸æ“‡æœ€å°æˆæœ¬åºåˆ—çš„å„ªå‹¢ï¼Œè€Œä¸æ˜¯åƒ…ä¾è³´ç«¯åˆ°ç«¯ç­–ç•¥æœ€ä½³åŒ–ã€‚</p>
<p><strong>4.4.7 ä¸–ç•Œæ¨¡å‹é æ¸¬æº–ç¢ºç‡</strong></p>
<p>è¡¨ 10 å ±å‘Šäº†å¼ï¼ˆ6ï¼‰æˆæœ¬å‡½æ•¸ä½¿ç”¨çš„äº‹ä»¶é æ¸¬å“è³ªã€‚é æ¸¬å™¨åœ¨å„ç¨®äº‹ä»¶é¡å‹ä¸Šé”åˆ°é«˜æº–ç¢ºç‡ï¼ˆ0.91â€“0.96ï¼‰å’Œåœ¨ç¢°æ’ç›¸é—œäº‹ä»¶ä¸Šçš„å¼·å¬å›ç‡ï¼ˆä¾‹å¦‚ï¼Œè¡Œäººç¢°æ’å¬å›ç‡ 0.99ï¼‰ï¼Œç‚ºé¢¨éšªæ„è­˜è©•ä¼°æä¾›å¯é ä¿¡è™Ÿã€‚æˆ‘å€‘è§€å¯Ÿåˆ°æŸäº›ç¨€æœ‰äº‹ä»¶çš„ç²¾ç¢ºç‡è¼ƒä½ï¼ˆä¾‹å¦‚ï¼Œè¡Œäººç¢°æ’ç²¾ç¢ºç‡ 0.52ï¼‰ï¼Œåæ˜ å‡ºè¼ƒç‚ºä¿å®ˆçš„å‚¾å‘ï¼Œç”¢ç”Ÿæ›´å¤šå‡é™½æ€§ï¼›åœ¨å®‰å…¨é—œéµçš„é§•é§›ä¸­ï¼Œå„ªå…ˆ</p>
<p><strong>è¡¨ 10</strong> ä½¿ç”¨å­¸ç¿’ä¸–ç•Œæ¨¡å‹é æ¸¬æœªä¾†äº¤é€šäº‹ä»¶çš„æº–ç¢ºç‡ã€‚</p>
<p>| | ç¢°æ’ | | | åŸ·è¡Œä¸­äº¤é€šè™ŸèªŒ |
|---|---|---|---|---|---|
| | è¡Œäºº | è»Šè¼› | éœæ…‹ | | |
| æº–ç¢ºåº¦ | 0.96 | 0.91 | 0.93 | 0.91 | |
| å¬å›ç‡ | 0.99 | 0.84 | 0.89 | 0.84 | |
| ç²¾æº–åº¦ | 0.52 | 0.62 | 0.63 | 0.68 | |</p>
<p>å„ªå…ˆè€ƒæ…®å¬å›ç‡å¯å„ªæ–¼éºæ¼å±å®³ã€‚</p>
<p><strong>5 çµè«–</strong></p>
<p>åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘æå‡ºäº† RaWMPCï¼Œä¸€å€‹é‡å°ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›çš„é¢¨éšªæ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹é æ¸¬æ§åˆ¶æ¡†æ¶ï¼Œä¸éœ€è¦å°ˆå®¶å‹•ä½œç›£ç£ã€‚RaWMPC å­¸ç¿’å‹•ä½œæ¢ä»¶åŒ–çš„ä¸–ç•Œæ¨¡å‹ä»¥æ¨è¡Œå¤šå€‹å€™é¸è¡Œç‚ºï¼Œé æ¸¬æœªä¾†èªæ„å’Œå®‰å…¨é—œéµäº‹ä»¶ï¼Œä¸¦é€šéæ˜ç¢ºæœ€å°åŒ–é¢¨éšªæ„ŸçŸ¥æˆæœ¬ä¾†é¸æ“‡å‹•ä½œã€‚ç‚ºäº†ä½¿ç½•è¦‹ä½†ç½é›£æ€§çš„çµæœå¯é æ¸¬ä¸”å¯é¿å…ï¼Œæˆ‘å€‘å¼•å…¥äº†é¢¨éšªæ„ŸçŸ¥äº’å‹•ç­–ç•¥ï¼Œæ•…æ„è’é›†å®‰å…¨å’Œå±éšªçš„æ¨è¡Œçµæœï¼Œä¸¦é€²ä¸€æ­¥æå‡ºè‡ªæˆ‘è©•ä¼°è’¸é¤¾ï¼Œä»¥ä½¿ç”¨ RaWMPC ä½œç‚ºè‡ªæˆ‘è©•ä¼°å™¨è¨“ç·´æœ‰æ•ˆçš„å‹•ä½œææ¡ˆç­–ç•¥ã€‚åœ¨ Bench2Drive å’Œ NAVSIM ä¸Šçš„å»£æ³›å¯¦é©—è¡¨æ˜ï¼ŒRaWMPC åœ¨æ²’æœ‰é›¢ç·šé ç†±çš„æƒ…æ³ä¸‹ä»é”åˆ°æœ€å…ˆé€²çš„æ€§èƒ½å’Œæ›´å¼·å¤§çš„é ˜åŸŸè½‰ç§»ç©©å¥æ€§ï¼Œå±•ç¤ºäº†é¡¯è‘—æ¸›å°‘å°æ˜‚è²´çš„çœŸå¯¦ä¸–ç•Œå°ˆå®¶ç¤ºç¯„ä¾è³´çš„æ½›åŠ›ã€‚æœªä¾†å·¥ä½œä¸­ï¼Œæˆ‘å€‘å°‡æ¢ç´¢é ˜åŸŸé©æ‡‰å’Œæ›´é«˜æ•ˆçš„è¦åŠƒï¼Œä»¥æ›´å¥½åœ°æ”¯æ´å¯¦éš›éƒ¨ç½²å’Œæ¨¡æ“¬è½‰å¯¦é©—è½‰ç§»ã€‚</p>
<p><strong>è²æ˜å’Œå®£è¨€</strong></p>
<ul>
<li>åˆ©ç›Šè¡çªã€‚ä½œè€…å®£å¸ƒä»–å€‘æ²’æœ‰å·²çŸ¥çš„ç«¶çˆ­æ€§è²¡å‹™åˆ©ç›Šæˆ–å€‹äººé—œä¿‚ï¼Œå¯èƒ½å°æœ¬è«–æ–‡å ±å‘Šçš„å·¥ä½œç”¢ç”Ÿå½±éŸ¿ã€‚</li>
<li>è³‡æ–™å¯ç”¨æ€§ã€‚æœ¬å·¥ä½œä¸æå‡ºä»»ä½•æ–°è³‡æ–™é›†ã€‚æ”¯æ´æœ¬ç ”ç©¶çµè«–çš„è³‡æ–™é›†ï¼ˆBench2Drive [29] å’Œ NAVSIM [11]ï¼‰å¯åœ¨ä»¥ä¸‹ç¶²å€é–‹æ”¾å–å¾—ï¼šBench2Drive å’Œ NAVSIMã€‚</li>
</ul>
<hr />
<p><strong>åƒè€ƒæ–‡ç»</strong></p>
<p>[1] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, ç­‰äººã€‚Genie: Generative interactive environmentsã€‚åœ¨ <em>Forty-first International Conference on Machine Learning</em>ï¼Œ2024ã€‚</p>
<p>[2] Jinkun Cao, Xin Wang, Trevor Darrell, å’Œ Fisher Yuã€‚Instance-aware predictive navigation in multi-agent environmentsã€‚åœ¨ <em>IEEE International Conference on Robotics and Automation</em>ï¼Œé ç¢¼ 5096-5102ï¼Œ2021ã€‚</p>
<p>[3] Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, ç­‰äººã€‚Pseudo-simulation for autonomous drivingã€‚<em>arXiv preprint arXiv:2506.04218</em>ï¼Œ2025ã€‚</p>
<p>[4] Raphael Chekroun, Marin Toromanoff, Sascha Hornauer, å’Œ Fabien Moutardeã€‚Gri: General reinforced imitation and its application to vision-based autonomous drivingã€‚åœ¨ <em>NeurIPS 2021, Machine Learning for Autonomous Driving Workshop</em>ï¼Œ2021ã€‚</p>
<p>[5] Dian Chen å’Œ Philipp KrÃ¤henbÃ¼hlã€‚Learning from all vehiclesã€‚åœ¨ <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>ï¼Œé ç¢¼ 17222-17231ï¼Œ2022ã€‚</p>
<p>[6] Dian Chen, Vladlen Koltun, å’Œ Philipp KrÃ¤henbÃ¼hlã€‚Learning to drive from a world on railsã€‚åœ¨ <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>ï¼Œé ç¢¼ 15590-15599ï¼Œ2021ã€‚</p>
<p>[7] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, å’Œ Hongyang Liã€‚End-to-end autonomous driving: Challenges and frontiersã€‚<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>ï¼Œ2024ã€‚</p>
<p>[8] Yuntao Chen, Yuqi Wang, å’Œ Zhaoxiang Zhangã€‚Drivinggpt: Unifying driving world modeling and planning with multi-modal autoregressive transformersã€‚åœ¨ <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>ï¼Œé ç¢¼ 26890-26900ï¼Œ2025 å¹´ 10 æœˆã€‚</p>
<p>[9] Pranav Singh Chib å’Œ Pravendra Singhã€‚Recent advancements in end-to-end autonomous driving using deep learning: A surveyã€‚<em>IEEE Transactions on Intelligent Vehicles</em>ï¼Œ9(1):103-118ï¼Œ2023ã€‚</p>
<p>[10] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, å’Œ Andreas Geigerã€‚Transfuser: Imitation with transformer-based sensor fusion for autonomous drivingã€‚<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>ï¼Œ2022ã€‚</p>
<p>[11] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, et al. Navsim: è³‡æ–™é©…å‹•çš„éåæ‡‰å¼è‡ªå‹•é§•é§›è»Šè¼›æ¨¡æ“¬èˆ‡åŸºæº–æ¸¬è©¦ã€‚<em>Advances in Neural Information Processing Systems</em>ï¼Œ37:28706â€“28719ï¼Œ2025ã€‚</p>
<p>[12] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, å’Œ Vladlen Koltunã€‚Carla: é–‹æ”¾åŸå¸‚é§•é§›æ¨¡æ“¬å™¨ã€‚è¦‹æ–¼ <em>Conference on Robot Learning</em>ï¼Œç¬¬ 1â€“16 é ï¼Œ2017ã€‚</p>
<p>[13] Lan Feng, Quanyi Li, Zhenghao Peng, Shuhan Tan, å’Œ Bolei Zhouã€‚Trafficgen: å­¸ç¿’ç”Ÿæˆå¤šæ¨£åŒ–èˆ‡é€¼çœŸçš„äº¤é€šæƒ…æ™¯ã€‚è¦‹æ–¼ <em>2023 IEEE åœ‹éš›æ©Ÿå™¨äººèˆ‡è‡ªå‹•åŒ–æœƒè­°ï¼ˆICRAï¼‰</em>ï¼Œç¬¬ 3567â€“3575 é ã€‚IEEEï¼Œ2023ã€‚</p>
<p>[14] Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Dingkang Liang, Chong Zhang, Dingyuan Zhang, Hongwei Xie, Bing Wang, å’Œ Xiang Baiã€‚Orion: é€šéè¦–è¦ºèªè¨€æŒ‡å°å‹•ä½œç”Ÿæˆçš„æ•´é«”ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›æ¡†æ¶ã€‚<em>arXiv preprint arXiv:2503.19755</em>ï¼Œ2025ã€‚</p>
<p>[15] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, å’Œ Hongyang Liã€‚Vista: å…·æœ‰é«˜ä¿çœŸåº¦èˆ‡å¤šæ¨£åŒ–å¯æ§æ€§çš„é€šç”¨é§•é§›ä¸–ç•Œæ¨¡å‹ã€‚è¦‹æ–¼ <em>ç¬¬ä¸‰åå…«å±†ç¥ç¶“è³‡è¨Šè™•ç†ç³»çµ±å¹´åº¦æœƒè­°</em>ï¼Œ2024ã€‚</p>
<p>[16] Mingzhe Guo, Zhipeng Zhang, Yuan He, Ke Wang, Liping Jing, å’Œ Haibin Lingã€‚ç„¡éœ€æ˜‚è²´æ¨¡çµ„åŒ–èˆ‡ 3D äººå·¥æ¨™è¨»çš„ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›ã€‚<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>ï¼Œ2025ã€‚</p>
<p>[17] David Ha å’Œ JÃ¼rgen Schmidhuberã€‚éè¿´ä¸–ç•Œæ¨¡å‹ä¿ƒé€²ç­–ç•¥é€²åŒ–ã€‚<em>Advances in neural information processing systems</em>ï¼Œ31ï¼Œ2018ã€‚</p>
<p>[18] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, å’Œ Mohammad Norouziã€‚å¤¢æƒ³é©…å‹•æ§åˆ¶: é€šéæ½›åœ¨æƒ³åƒå­¸ç¿’è¡Œç‚ºã€‚è¦‹æ–¼ <em>åœ‹éš›å­¸ç¿’è¡¨å¾µæœƒè­°</em>ï¼Œ2019ã€‚</p>
<p>[19] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, å’Œ James Davidsonã€‚å¾åƒç´ å­¸ç¿’æ½›åœ¨å‹•åŠ›å­¸ä»¥é€²è¡Œè¦åŠƒã€‚è¦‹æ–¼ <em>åœ‹éš›æ©Ÿå™¨å­¸ç¿’æœƒè­°</em>ï¼Œç¬¬ 2555â€“2565 é ã€‚PMLRï¼Œ2019ã€‚</p>
<p>[20] Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, å’Œ Jimmy Baã€‚ç”¨é›¢æ•£ä¸–ç•Œæ¨¡å‹æŒæ¡ Atariã€‚è¦‹æ–¼ <em>åœ‹éš›å­¸ç¿’è¡¨å¾µæœƒè­°</em>ï¼Œ2021ã€‚</p>
<p>[21] Shadi Hamdan, Chonghao Sima, Zetong Yang, Hongyang Li, å’Œ Fatma GÃ¼neyã€‚Eta: é€šéæå‰æ€è€ƒçš„æ•ˆç‡ï¼Œè‡ªå‹•é§•é§›çš„é›™é‡æ–¹æ³•èˆ‡å¤§å‹æ¨¡å‹ã€‚è¦‹æ–¼ <em>IEEE/CVF åœ‹éš›é›»è…¦è¦–è¦ºæœƒè­°è«–æ–‡é›†</em>ï¼Œ2025ã€‚</p>
<p>[22] Mikael Henaff, Alfredo Canziani, å’Œ Yann LeCunã€‚å…·æœ‰ä¸ç¢ºå®šæ€§æ­£å‰‡åŒ–çš„æ¨¡å‹é æ¸¬ç­–ç•¥å­¸ç¿’ç”¨æ–¼å¯†é›†äº¤é€šé§•é§›ã€‚è¦‹æ–¼åœ‹éš›å­¸ç¿’è¡¨å¾µæœƒè­°ï¼Œ2018ã€‚</p>
<p>[23] Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary Mrez, Corina Gurau, Hudson Yeo, Alex Kendall, Roberto Cipolla, and Jamie Shotton. Model-based imitation learning for urban driving. Advances in Neural Information Processing Systems, 35:20703â€“20716, 2022.</p>
<p>[24] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Mrez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023.</p>
<p>[25] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.</p>
<p>[26] Bernhard Jaeger, Kashyap Chitta, and Andreas Geiger. Hidden biases of end-to-end driving models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8240â€“8249, 2023.</p>
<p>[27] Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, and Hongyang Li. Driveadapter: Breaking the coupling barrier of perception and planning in end-to-end autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7953â€“7963, 2023.</p>
<p>[28] Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, and Hongyang Li. Think twice before driving: Towards scalable decoders for end-to-end autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21983â€“21994, 2023.</p>
<p>[29] Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, and Junchi Yan. Bench2drive: Towards multi-ability benchmarking of closed-loop end-to-end autonomous driving. arXiv preprint arXiv:2406.03877, 2024.</p>
<p>[30] Xiaosong Jia, Junqi You, Zhiyuan Zhang, and Junchi Yan. Drivetransformer: Unified transformer for scalable end-to-end autonomous driving. In International Conference on Learning Representations (ICLR), 2025.</p>
<p>[31] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340â€“8350, 2023.</p>
<p>[32] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning to drive in a day. In 2019 international conference on robotics and automation (ICRA), pages 8248â€“8254. IEEE, 2019.</p>
<p>[33] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>[34] Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Pathdreamer: A world model for indoor navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14738â€“14748, 2021.</p>
<p>[35] Fanjie Kong, Yitong Li, Weihuang Chen, Chen Min, Yizhe Li, Zhiqiang Gao, Haoyang Li, Zhongyu Guo, and Hongbin Sun. VLR-Driverï¼šå¤§å‹è¦–è¦ºèªè¨€æ¨ç†æ¨¡å‹ç”¨æ–¼å…·èº«è‡ªä¸»é§•é§›ã€‚åœ¨ IEEE/CVF åœ‹éš›é›»è…¦è¦–è¦ºæœƒè­°è«–æ–‡é›†ä¸­ï¼Œç¬¬ 26966â€“26976 é ï¼Œ2025ã€‚</p>
<p>[36] Hanyang Kong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. DreamDroneï¼šæ–‡å­—è½‰åœ–åƒæ“´æ•£æ¨¡å‹ä½œç‚ºé›¶æ¨£æœ¬æ°¸çºŒè¦–è§’ç”Ÿæˆå™¨ã€‚åœ¨æ­æ´²é›»è…¦è¦–è¦ºæœƒè­°ä¸­ï¼Œç¬¬ 324â€“341 é ã€‚Springerï¼Œ2024ã€‚</p>
<p>[37] Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan. Think2Driveï¼šé€éæ½›åœ¨ä¸–ç•Œæ¨¡å‹æ€è€ƒé€²è¡Œé«˜æ•ˆå¼·åŒ–å­¸ç¿’ç”¨æ–¼è‡ªä¸»é§•é§›ï¼ˆCARLA-V2 ä¸­ï¼‰ã€‚åœ¨æ­æ´²é›»è…¦è¦–è¦ºæœƒè­°ä¸­ï¼Œç¬¬ 142â€“158 é ã€‚Springerï¼Œ2024ã€‚</p>
<p>[38] Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yun-tao Chen, Zhaoxiang Zhang, and Tieniu Tan. é€éæ½›åœ¨ä¸–ç•Œæ¨¡å‹å¢å¼·ç«¯åˆ°ç«¯è‡ªä¸»é§•é§›ã€‚åœ¨ç¬¬åä¸‰å±†åœ‹éš›å­¸ç¿’è¡¨ç¤ºæœƒè­°ä¸­ï¼Œ2025ã€‚</p>
<p>[39] Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, Lue Fan, and Zhaoxiang Zhang. é€é BEV ä¸–ç•Œæ¨¡å‹é€²è¡Œç«¯åˆ°ç«¯é§•é§›èˆ‡ç·šä¸Šè»Œè·¡è©•ä¼°ã€‚åœ¨ IEEE/CVF åœ‹éš›é›»è…¦è¦–è¦ºæœƒè­°ï¼ˆICCVï¼‰è«–æ–‡é›†ä¸­ï¼Œç¬¬ 27137â€“27146 é ï¼Œ2025 å¹´ 10 æœˆã€‚</p>
<p>[40] Zhenxin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Zuxuan Wu, and Jose M. Alvarez. Hydra-Nextï¼šé€éé–‹ç’°è¨“ç·´é€²è¡Œé­¯æ£’é–‰ç’°é§•é§›ã€‚åœ¨ IEEE/CVF åœ‹éš›é›»è…¦è¦–è¦ºæœƒè­°ï¼ˆICCVï¼‰è«–æ–‡é›†ä¸­ï¼Œç¬¬ 27305-27314 é ï¼Œ2025 å¹´ 10 æœˆã€‚</p>
<p>[41] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. BEVFormerï¼šé€éæ™‚ç©º Transformer å¾ LiDAR-Camera å­¸ç¿’é³¥çœ¼è¦–åœ–è¡¨ç¤ºã€‚IEEE æ¨¡å¼åˆ†æèˆ‡æ©Ÿå™¨æ™ºæ…§äº¤æ˜“ï¼Œ2024ã€‚</p>
<p>[42] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, and Xinggang Wang. DiffusionDriveï¼šç”¨æ–¼ç«¯åˆ°ç«¯è‡ªä¸»é§•é§›çš„æˆªæ–·æ“´æ•£æ¨¡å‹ã€‚åœ¨ IEEE/CVF é›»è…¦è¦–è¦ºèˆ‡æ¨¡å¼è­˜åˆ¥æœƒè­°ï¼ˆCVPRï¼‰è«–æ–‡é›†ä¸­ï¼Œç¬¬ 12037-12047 é ï¼Œ2025 å¹´ 6 æœˆã€‚</p>
<p>[43] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. ç”¨æ–¼å¯†é›†ç‰©é«”åµæ¸¬çš„ Focal Lossã€‚åœ¨ IEEE åœ‹éš›é›»è…¦è¦–è¦ºæœƒè­°è«–æ–‡é›†ä¸­ï¼Œç¬¬ 2980-2988 é ï¼Œ2017ã€‚</p>
<p>[44] Yuhang Lu, Jiadong Tu, Yuexin Ma, and Xinge Zhu. Real-ADï¼šæœå‘ç«¯åˆ°ç«¯è‡ªä¸»é§•é§›ä¸­é¡äººæ¨ç†ç™¼å±•ã€‚åœ¨ IEEE/CVF åœ‹éš›é›»è…¦è¦–è¦ºæœƒè­°è«–æ–‡é›†ä¸­ï¼Œç¬¬ 27783-27793 é ï¼Œ2025ã€‚</p>
<p>[45] Ana-Maria Marcu, Long Chen, Jan HÃ¼nermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, et al. LingoQAï¼šè‡ªä¸»é§•é§›çš„è¦–è¦ºå•ç­”ã€‚åœ¨æ­æ´²é›»è…¦è¦–è¦ºæœƒè­°ä¸­ï¼Œç¬¬ 252-269 é ã€‚Springerï¼Œ2024ã€‚</p>
<p>[46] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. In RSS, 2023.</p>
<p>[47] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565-571. IEEE, 2016.</p>
<p>[48] Michael Montemerlo, Jan Becker, Suhid Bhat, Hen-drik Dahlkamp, Dmitri Dolgov, Scott Ettinger, Dirk Haehnel, Tim Hilden, Gabe Hoffmann, Burkhard Huhnke, et al. Junior: The stanford entry in the urban challenge. Journal of field Robotics, 25(9): 569-597, 2008.</p>
<p>[49] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, Yifei Zhan, Kun Zhan, Peng Jia, Xianpeng Lang, Xingang Wang, and Wenjun Mei. Recondreamer: Crafting world models for driving scene recon-struction via online restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1559-1569, June 2025.</p>
<p>[50] Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, and Zehuan Wu. Maskgwm: A generalizable driving world model with video mask reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22381-22391, June 2025.</p>
<p>[51] Brian Paden, Michal ÄŒÃ¡p, Sze Zheng Yong, Dmitry Yershov, and Emilio Frazzoli. A survey of motion planning and control techniques for self-driving urban vehicles. IEEE Transactions on Intelligent vehicles, 1(1):33-55, 2016.</p>
<p>[52] Xinlei Pan, Xiangyu Chen, Qizhi Cai, John Canny, and Fisher Yu. Semantic predictive control for explainable and efficient policy learning. In International Conference on Robotics and Automation, pages 3203-3209, 2019.</p>
<p>[53] Scott Drew Pendleton, Hans Andersen, Xinxin Du, Xiaotong Shen, Malika Meghjani, You Hong Eng, Daniela Rus, and Marcelo H Ang. Perception, planning, control, and coordination for autonomous vehicles. Machines, 5(1):6, 2017.</p>
<p>[54] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-to-end autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7077-7087, 2021.</p>
<p>[55] Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, A Sophia Koepke, Zeynep Akata, and Andreas Geiger. Plant: Explainable planning transformers via object-level representations. In Conference on Robot Learning, pages 459-470, 2023.</p>
<p>[56] Katrin Renz, Long Chen, Ana-Maria Marcu, Jan HÃ¼nermann, Benoit Hanotte, Alice Karnsund, Jamie Shotton, Elahe Arani, and Oleg Sinavski. Carllava: Vision language models for camera-only closed-loop driving. arXiv preprint arXiv:2406.10165, 2024.</p>
<p>[57] Katrin Renz, Long Chen, Elahe Arani, and Oleg Sinavski. Simlingo: Vision-only closed-loop autonomous driving with language-action alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11993-12003, June 2025.</p>
<p>[58] Nicholas Rhinehart, Rowan McAllister, and Sergey Levine. Deep imitative models for flexible inference, planning, and control. In International Conference on Learning Representations, 2020.</p>
<p>[59] Ahmad El Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. End-to-end deep re-</p>
<hr />
<p>[{"bbox": [139, 151, 606, 1466], "category": "Text", "text": "inforcement learning for lane keeping assist. <em>arXiv preprint arXiv:1612.04340</em>, 2016.\n\n[60] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data. In <em>European Conference on Computer Vision</em>, pages 683â€“700. Springer, 2020.\n\n[61] ShuYao Shang, Yuntao Chen, Yuqi Wang, Yingyan Li, and Zhaoxiang Zhang. Drivedpo: Policy learning via safety dpo for end-to-end autonomous driving. In <em>The Thirty-ninth Annual Conference on Neural Information Processing Systems</em>, 2025.\n\n[62] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and Yu Liu. Safety-enhanced autonomous driving using interpretable sensor fusion transformer. In <em>Conference on Robot Learning</em>, pages 726â€“737, 2023.\n\n[63] Hao Shao, Letian Wang, Ruobing Chen, Steven L Waslander, Hongsheng Li, and Yu Liu. Reason-net: End-to-end driving with temporal and global reasoning. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 13723â€“13733, 2023.\n\n[64] Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven L Waslander, Yu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 15120â€“15130, 2024.\n\n[65] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens BeiÃŸwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In <em>European Conference on Computer Vision</em>, pages 256â€“274. Springer, 2024.\n\n[66] Oriane SimÃ©oni, Huy V Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, MichaÃ«l Ramamonjisoa, et al. Dinov3. <em>arXiv preprint arXiv:2508.10104</em>, 2025.\n\n[67] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. <em>Advances in neural information processing systems</em>, 28, 2015.\n\n[68] Ziying Song, Caiyan Jia, Lin Liu, Hongyu Pan, Yongchang Zhang, Junming Wang, Xingyu Zhang, Shaoqing Xu, Lei Yang, and Yadan Luo. Donâ€™t shake the wheel: Momentum-aware planning in end-to-end autonomous driving. In <em>Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pages 22432â€“22441, 2025.\n\n[69] Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, Haoran Wu, and Sifa Zheng. Sparsedrive: End-to-end autonomous driving via sparse scene representation. In <em>2025 IEEE International Conference on Robotics and Automation (ICRA)</em>, pages 8795â€“8801. IEEE, 2025."}, {"bbox": [632, 154, 1111, 1425], "category": "Text", "text": "[70] Shuhan Tan, Kelvin Wong, Shenlong Wang, Siv-abalan Manivasagam, Mengye Ren, and Raquel Urtasun. Scenegen: Learning to generate realistic traffic scenes. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 892â€“901, 2021.\n[71] Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Kraehenbuehl. Language con-ditioned traffic generation. In <em>Conference on Robot Learning</em>, pages 2714â€“2752. PMLR, 2023.\n[72] Shuhan Tan, Boris Ivanovic, Yuxiao Chen, Boyi Li, Xinshuo Weng, Yulong Cao, Philipp Kraehen-buehl, and Marco Pavone. Promptable closed-loop traffic simulation. In <em>Annual Conference on Robot Learning</em>, 2024.\n[73] Yingqi Tang, Zhuoran Xu, Zhaotie Meng, and Erkang Cheng. Hip-ad: Hierarchical and multi-granularity planning with deformable attention for autonomous driving in a single decoder. <em>arXiv preprint arXiv:2503.08612</em>, 2025.\n[74] Sebastian Thrun, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James Diebel, Philip Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, et al. Stanley: The robot that won the darpa grand challenge. <em>Journal of field Robotics</em>, 23(9):661â€“692, 2006.\n[75] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free reinforcement learning for urban driving using implicit affordances. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 7153â€“7162, 2020.\n[76] Chris Urmson, Joshua Anhalt, Drew Bagnell, Christopher Baker, Robert Bittner, MN Clark, John Dolan, Dave Duggins, Tugrul Galatali, Chris Geyer, et al. Autonomous driving in urban envi-ronments: Boss and the urban challenge. <em>Journal of field Robotics</em>, 25(8):425â€“466, 2008.\n[77] Hanqing Wang, Wei Liang, Luc Van Gool, and Wenguan Wang. Dream Walker: Mental planning for continuous vision-language navigation. In <em>Proceedings of the IEEE/CVF international conference on computer vision</em>, pages 10873â€“10883, 2023.\n[78] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yun-tao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In *Proceedings of the IEEE/CVF Conference on Computer"}]</p>
<p>é¡˜æ™¯èˆ‡æ¨¡å¼è­˜åˆ¥ï¼Œé ç¢¼ 14749â€“14759ï¼Œ2024ã€‚</p>
<p>[79] Kehan Wenã€Yutong Huã€Yao Mu å’Œ Lei Keã€‚M^3PCï¼šä½¿ç”¨é è¨“ç·´é®ç½©è»Œè·¡æ¨¡å‹çš„æ¸¬è©¦æ™‚æ¨¡å‹é æ¸¬æ§åˆ¶ã€‚åœ¨ã€Šç¬¬åä¸‰å±†å­¸ç¿’è¡¨å¾µåœ‹éš›æœƒè­°ã€‹ï¼Œ2025ã€‚</p>
<p>[80] Xinshuo Wengã€Boris Ivanovicã€Yan Wangã€Yue Wang å’Œ Marco Pavoneã€‚Para-driveï¼šç”¨æ–¼å¯¦æ™‚è‡ªå‹•é§•é§›çš„ä¸¦è¡ŒåŒ–æ¶æ§‹ã€‚åœ¨ã€ŠIEEE/CVF é›»è…¦è¦–è¦ºèˆ‡æ¨¡å¼è­˜åˆ¥æœƒè­°è«–æ–‡é›† (CVPR)ã€‹ï¼Œ2024ã€‚</p>
<p>[81] Penghao Wuã€Xiaosong Jiaã€Li Chenã€Junchi Yanã€Hongyang Li å’Œ Yu Qiaoã€‚è»Œè·¡å¼•å°æ§åˆ¶é æ¸¬ç”¨æ–¼ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›ï¼šä¸€å€‹ç°¡å–®è€Œæœ‰æ•ˆçš„åŸºæº–æ–¹æ³•ã€‚ã€Šç¥ç¶“ä¿¡æ¯è™•ç†ç³»çµ±é€²å±•ã€‹ï¼Œ35:6119â€“6132ï¼Œ2022ã€‚</p>
<p>[82] Zebin Xingã€Xingyu Zhangã€Yang Huã€Bo Jiangã€Tong Heã€Qian Zhangã€Xiaoxiao Long å’Œ Wei Yinã€‚GoalFlowï¼šç”¨æ–¼ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›ä¸­å¤šæ¨¡æ…‹è»Œè·¡ç”Ÿæˆçš„ç›®æ¨™é©…å‹•æµåŒ¹é…ã€‚åœ¨ã€Šé›»è…¦è¦–è¦ºèˆ‡æ¨¡å¼è­˜åˆ¥æœƒè­°è«–æ–‡é›†ã€‹ï¼Œé ç¢¼ 1602â€“1611ï¼Œ2025ã€‚</p>
<p>[83] Zhenhua Xuã€Yujia Zhangã€Enze Xieã€Zhen Zhaoã€Yong Guoã€Kwan-Yee K Wongã€Zhenguo Li å’Œ Hengshuang Zhaoã€‚DriveGPT4ï¼šé€šéå¤§å‹èªè¨€æ¨¡å‹å¯¦ç¾å¯è§£é‡‹çš„ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›ã€‚ã€ŠIEEE æ©Ÿå™¨äººèˆ‡è‡ªå‹•åŒ–ä¿¡å‡½ã€‹ï¼Œ2024ã€‚</p>
<p>[84] Xuemeng Yangã€Licheng Wenã€Tiantian Weiã€Yukai Maã€Jianbiao Meiã€Xin Liã€Wenjie Leiã€Daoyong Fuã€Pinlong Caiã€Min Douã€Liang Heã€Yong Liuã€Botian Shi å’Œ Yu Qiaoã€‚DriveArenaï¼šç”¨æ–¼è‡ªå‹•é§•é§›çš„é–‰è¿´è·¯ç”Ÿæˆæ¨¡æ“¬å¹³å°ã€‚åœ¨ã€ŠIEEE/CVF åœ‹éš›é›»è…¦è¦–è¦ºæœƒè­° (ICCV) è«–æ–‡é›†ã€‹ï¼Œé ç¢¼ 26933â€“26943ï¼Œ2025 å¹´ 10 æœˆã€‚</p>
<p>[85] Zhenjie Yangã€Xiaosong Jiaã€Qifeng Liã€Xue Yangã€Maoqing Yao å’Œ Junchi Yanã€‚Raw2Driveï¼šä½¿ç”¨å°é½Šä¸–ç•Œæ¨¡å‹çš„å¼·åŒ–å­¸ç¿’ç”¨æ–¼ç«¯åˆ°ç«¯è‡ªå‹•é§•é§› (in CARLA v2)ï¼Œ2025ã€‚å·²è¢« NeurIPS 2025 æ¥å—ã€‚</p>
<p>[86] Jianhua Yuanã€Shuyang Sunã€Daniel Omeizaã€Bo Zhaoã€Paul Newmanã€Lars Kunze å’Œ Matthew Gaddã€‚RAG-Driverï¼šåœ¨å¤šæ¨¡æ…‹å¤§å‹èªè¨€æ¨¡å‹ä¸­ä½¿ç”¨æª¢ç´¢å¢å¼·ä¸Šæ–‡å­¸ç¿’çš„å¯æ³›åŒ–é§•é§›è§£é‡‹ã€‚arXiv é å°æœ¬ arXiv:2402.10828ï¼Œ2024ã€‚</p>
<p>[87] Ye Yuanã€Xinshuo Wengã€Yanglan Ou å’Œ Kris M Kitaniã€‚AgentFormerï¼šç”¨æ–¼ç¤¾æœƒæ™‚é–“å¤šä»£ç†é æ¸¬çš„ä»£ç†æ„ŸçŸ¥ Transformerã€‚åœ¨ã€ŠIEEE/CVF åœ‹éš›é›»è…¦è¦–è¦ºæœƒè­°è«–æ–‡é›†ã€‹ï¼Œé ç¢¼ 9813â€“9823ï¼Œ2021ã€‚</p>
<p>[88] Bowen Zhangã€Zhi Tianã€Quan Tangã€Xiangxiang Chuã€Xiaolin Weiã€Chunhua Shen ç­‰äººã€‚SegViTï¼šä½¿ç”¨ç´” Vision Transformer é€²è¡Œèªç¾©åˆ†å‰²ã€‚åœ¨ã€Šç¥ç¶“ä¿¡æ¯è™•ç†ç³»çµ±é€²å±•ã€‹ï¼Œ2022ã€‚</p>
<p>[89] Bozhou Zhangã€Nan Songã€Xin Jin å’Œ Li Zhangã€‚é€£æ¥éå»èˆ‡æœªä¾†ï¼šä½¿ç”¨æ­·å²é æ¸¬èˆ‡è¦åŠƒçš„ç«¯åˆ°ç«¯è‡ªå‹•é§•é§›ã€‚åœ¨ã€Šé›»è…¦è¦–è¦ºèˆ‡æ¨¡å¼è­˜åˆ¥æœƒè­°è«–æ–‡é›†ã€‹ï¼Œé ç¢¼ 6854â€“6863ï¼Œ2025ã€‚</p>
<p>[90] Jimuyang Zhang, Zanming Huang, and Eshed Ohn-Bar. Coaching a teachable student. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7805â€“7815, 2023.</p>
<p>[91] Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiao-Xiao Long, Xun Cao, and Wei Yin. Epona: Autoregressive diffusion world model for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 27220-27230, October 2025.</p>
<p>[92] Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, and Raquel Urtasun. Copilot4d: Learning unsupervised world models for autonomous driving via discrete diffusion. In ICLR, 2024.</p>
<p>[93] Peizhi Zhang, Lu Xiong, Zhuoping Yu, Peiyuan Fang, Senwei Yan, Jie Yao, and Yi Zhou. Reinforcement learning-based end-to-end parking for automatic parking system. Sensors, 19(18):3996, 2019.</p>
<p>[94] Wei Zhang, Pengfei Li, Junli Wang, Bingchuan Sun, Qihao Jin, Guangjun Bao, Shibo Rui, Yang Yu, Wenchao Ding, Peng Li, et al. Dual-aeb: Synergizing rule-based and multimodal large language models for effective emergency braking. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 14888â€“14895. IEEE, 2025.</p>
<p>[95] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. End-to-end urban driving by imitating a reinforcement learning coach. In Proceedings of the IEEE/CVF international conference on computer vision, pages 15222-15232, 2021.</p>
<p>[96] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. Trafficbots: Towards world models for autonomous driving simulation and motion prediction. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1522-1529. IEEE, 2023.</p>
<hr />
<p>[97] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, and Xingang Wang. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 12015â€“12026, June 2025.</p>
<p>[98] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. Occ-world: Learning a 3d occupancy world model for autonomous driving. In <em>European conference on computer vision</em>, pages 55â€“72. Springer, 2024.</p>
<p>[99] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chen-ming Zhang, and Long Chen. Genad: Generative end-to-end autonomous driving. In <em>European Conference on Computer Vision</em>, pages 87â€“104. Springer, 2024.</p>
<p>[100] Yupeng Zheng, Pengxuan Yang, Zebin Xing, Qichao Zhang, Yuhang Zheng, Yinfeng Gao, Pengfei Li, Teng Zhang, Zhongpu Xia, Peng Jia, XianPeng Lang, and Dongbin Zhao. World4drive: End-to-end autonomous driving via intention-aware physical latent world model. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 28632â€“28642, October 2025.</p>
<p>[101] Julius Ziegler, Philipp Bender, Markus Schreiber, Henning Lategahn, Tobias Strauss, Christoph Stiller, Thao Dang, Uwe Franke, Nils Appenrodt, Christoph G Keller, et al. Making bertha driveâ€”an autonomous journey on a historic route. <em>IEEE Intelligent transportation systems magazine</em>, 6(2): 8â€“20, 2014.</p>
<p>[102] Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, and Jiwen Lu. Gaussianworld: Gaussian world model for streaming 3d occupancy prediction. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 6772â€“6781, June 2025.</p><hr><h2>åœ–è¡¨</h2><div class="figures-gallery"><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig1.png" loading="lazy" alt="fig1.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig7.jpeg" loading="lazy" alt="fig7.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig8.jpeg" loading="lazy" alt="fig8.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig9.jpeg" loading="lazy" alt="fig9.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig10.jpeg" loading="lazy" alt="fig10.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig11.png" loading="lazy" alt="fig11.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig12.png" loading="lazy" alt="fig12.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig13.jpeg" loading="lazy" alt="fig13.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig14.jpeg" loading="lazy" alt="fig14.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig15.jpeg" loading="lazy" alt="fig15.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig16.jpeg" loading="lazy" alt="fig16.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig17.jpeg" loading="lazy" alt="fig17.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig18.jpeg" loading="lazy" alt="fig18.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig19.jpeg" loading="lazy" alt="fig19.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig20.jpeg" loading="lazy" alt="fig20.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig21.jpeg" loading="lazy" alt="fig21.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig22.jpeg" loading="lazy" alt="fig22.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig23.jpeg" loading="lazy" alt="fig23.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig24.jpeg" loading="lazy" alt="fig24.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig25.jpeg" loading="lazy" alt="fig25.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig26.jpeg" loading="lazy" alt="fig26.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig27.jpeg" loading="lazy" alt="fig27.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig28.jpeg" loading="lazy" alt="fig28.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig29.jpeg" loading="lazy" alt="fig29.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig30.jpeg" loading="lazy" alt="fig30.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig31.jpeg" loading="lazy" alt="fig31.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig32.jpeg" loading="lazy" alt="fig32.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig33.png" loading="lazy" alt="fig33.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig34.png" loading="lazy" alt="fig34.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig35.jpeg" loading="lazy" alt="fig35.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig36.jpeg" loading="lazy" alt="fig36.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig37.jpeg" loading="lazy" alt="fig37.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig38.jpeg" loading="lazy" alt="fig38.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig39.jpeg" loading="lazy" alt="fig39.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig40.jpeg" loading="lazy" alt="fig40.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig41.jpeg" loading="lazy" alt="fig41.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig42.jpeg" loading="lazy" alt="fig42.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig43.jpeg" loading="lazy" alt="fig43.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig44.jpeg" loading="lazy" alt="fig44.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig45.jpeg" loading="lazy" alt="fig45.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig46.jpeg" loading="lazy" alt="fig46.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig47.jpeg" loading="lazy" alt="fig47.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig48.jpeg" loading="lazy" alt="fig48.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig49.jpeg" loading="lazy" alt="fig49.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig50.jpeg" loading="lazy" alt="fig50.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig51.jpeg" loading="lazy" alt="fig51.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig52.jpeg" loading="lazy" alt="fig52.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig53.jpeg" loading="lazy" alt="fig53.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig54.jpeg" loading="lazy" alt="fig54.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig55.png" loading="lazy" alt="fig55.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig56.png" loading="lazy" alt="fig56.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig57.png" loading="lazy" alt="fig57.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig58.jpeg" loading="lazy" alt="fig58.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig59.jpeg" loading="lazy" alt="fig59.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig60.jpeg" loading="lazy" alt="fig60.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig61.jpeg" loading="lazy" alt="fig61.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig62.jpeg" loading="lazy" alt="fig62.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig63.jpeg" loading="lazy" alt="fig63.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig64.jpeg" loading="lazy" alt="fig64.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig65.png" loading="lazy" alt="fig65.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig66.jpeg" loading="lazy" alt="fig66.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig67.jpeg" loading="lazy" alt="fig67.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig68.jpeg" loading="lazy" alt="fig68.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig69.jpeg" loading="lazy" alt="fig69.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig70.jpeg" loading="lazy" alt="fig70.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig71.png" loading="lazy" alt="fig71.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig72.png" loading="lazy" alt="fig72.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig73.png" loading="lazy" alt="fig73.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig74.png" loading="lazy" alt="fig74.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig75.jpeg" loading="lazy" alt="fig75.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig76.jpeg" loading="lazy" alt="fig76.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig77.jpeg" loading="lazy" alt="fig77.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig78.jpeg" loading="lazy" alt="fig78.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig79.jpeg" loading="lazy" alt="fig79.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig80.jpeg" loading="lazy" alt="fig80.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig81.png" loading="lazy" alt="fig81.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig82.jpeg" loading="lazy" alt="fig82.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig83.png" loading="lazy" alt="fig83.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig84.jpeg" loading="lazy" alt="fig84.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig85.jpeg" loading="lazy" alt="fig85.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig86.jpeg" loading="lazy" alt="fig86.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig87.jpeg" loading="lazy" alt="fig87.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig88.png" loading="lazy" alt="fig88.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig89.png" loading="lazy" alt="fig89.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig90.png" loading="lazy" alt="fig90.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig91.png" loading="lazy" alt="fig91.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig92.jpeg" loading="lazy" alt="fig92.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig93.jpeg" loading="lazy" alt="fig93.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig94.jpeg" loading="lazy" alt="fig94.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig95.jpeg" loading="lazy" alt="fig95.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig96.jpeg" loading="lazy" alt="fig96.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig97.jpeg" loading="lazy" alt="fig97.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig98.jpeg" loading="lazy" alt="fig98.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig99.jpeg" loading="lazy" alt="fig99.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig100.png" loading="lazy" alt="fig100.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig101.jpeg" loading="lazy" alt="fig101.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig102.jpeg" loading="lazy" alt="fig102.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig103.png" loading="lazy" alt="fig103.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig104.jpeg" loading="lazy" alt="fig104.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig105.jpeg" loading="lazy" alt="fig105.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig106.jpeg" loading="lazy" alt="fig106.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig107.jpeg" loading="lazy" alt="fig107.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig108.jpeg" loading="lazy" alt="fig108.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig109.jpeg" loading="lazy" alt="fig109.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig110.jpeg" loading="lazy" alt="fig110.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig111.jpeg" loading="lazy" alt="fig111.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig112.png" loading="lazy" alt="fig112.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig113.png" loading="lazy" alt="fig113.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig114.png" loading="lazy" alt="fig114.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig115.png" loading="lazy" alt="fig115.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig116.png" loading="lazy" alt="fig116.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig117.jpeg" loading="lazy" alt="fig117.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig118.png" loading="lazy" alt="fig118.png"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig119.jpeg" loading="lazy" alt="fig119.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig120.jpeg" loading="lazy" alt="fig120.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig121.jpeg" loading="lazy" alt="fig121.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig122.jpeg" loading="lazy" alt="fig122.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig123.jpeg" loading="lazy" alt="fig123.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig124.jpeg" loading="lazy" alt="fig124.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig125.jpeg" loading="lazy" alt="fig125.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig126.jpeg" loading="lazy" alt="fig126.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig127.jpeg" loading="lazy" alt="fig127.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig128.jpeg" loading="lazy" alt="fig128.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig129.jpeg" loading="lazy" alt="fig129.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig130.jpeg" loading="lazy" alt="fig130.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig131.jpeg" loading="lazy" alt="fig131.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig132.jpeg" loading="lazy" alt="fig132.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig133.jpeg" loading="lazy" alt="fig133.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig134.jpeg" loading="lazy" alt="fig134.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig135.jpeg" loading="lazy" alt="fig135.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig136.jpeg" loading="lazy" alt="fig136.jpeg"></figure><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.23259/fig137.jpeg" loading="lazy" alt="fig137.jpeg"></figure></div>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>