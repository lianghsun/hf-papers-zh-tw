<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>è«–æ•¸æ“šå·¥ç¨‹åœ¨æ“´å±•LLMçµ‚ç«¯èƒ½åŠ›ä¸­çš„æ‡‰ç”¨ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem; color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">è«–æ•¸æ“šå·¥ç¨‹åœ¨æ“´å±•LLMçµ‚ç«¯èƒ½åŠ›ä¸­çš„æ‡‰ç”¨</h1>
  
  <div class="en-title">On Data Engineering for Scaling LLM Terminal Capabilities</div>
  

  <div class="paper-meta">
    
    <span>Renjie Pi, Grace Lam, Mohammad Shoeybi, Pooya Jannaty, Bryan Catanzaro, Wei Ping</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.21193" target="_blank">2602.21193</a></span>
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.21193" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.21193" target="_blank" rel="noopener" class="btn btn-outline">arXiv é é¢</a>
    <a href="https://huggingface.co/papers/2602.21193" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span><span class="tag domain">Code</span>
    <span class="tag method">Synthetic Data Generation</span><span class="tag method">Curriculum Learning</span><span class="tag method">Fine-tuning</span>
    <span class="tag task">Terminal Command Generation</span><span class="tag task">Agent Training</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

å„˜ç®¡å¤§å‹èªè¨€æ¨¡å‹çš„çµ‚ç«¯èƒ½åŠ›æœ€è¿‘å–å¾—å¿«é€Ÿé€²å±•ï¼Œä½†æœ€å…ˆé€²çµ‚ç«¯ä»£ç†èƒŒå¾Œçš„è¨“ç·´è³‡æ–™ç­–ç•¥åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªå…¬é–‹ã€‚æˆ‘å€‘é€šéå°çµ‚ç«¯ä»£ç†è³‡æ–™å·¥ç¨‹å¯¦è¸çš„ç³»çµ±ç ”ç©¶ä¾†å¡«è£œé€™ä¸€ç©ºç™½ï¼Œåšå‡ºäº†å…©å€‹é—œéµè²¢ç»ï¼š(1) Terminal-Task-Genï¼Œä¸€å€‹è¼•é‡ç´šåˆæˆä»»å‹™ç”Ÿæˆç®¡é“ï¼Œæ”¯æŒåŸºæ–¼ç¨®å­å’ŒåŸºæ–¼æŠ€èƒ½çš„ä»»å‹™æ§‹å»ºï¼Œä»¥åŠ (2) è³‡æ–™å’Œè¨“ç·´ç­–ç•¥çš„å…¨é¢åˆ†æï¼ŒåŒ…æ‹¬ç¯©é¸ã€èª²ç¨‹å­¸ç¿’ã€é•·ä¸Šä¸‹æ–‡è¨“ç·´å’Œç¸®æ”¾è¡Œç‚ºã€‚æˆ‘å€‘çš„ç®¡é“ç”¢ç”Ÿäº† Terminal-Corpusï¼Œä¸€å€‹å¤§è¦æ¨¡é–‹æºçµ‚ç«¯ä»»å‹™è³‡æ–™é›†ã€‚ä½¿ç”¨æ­¤è³‡æ–™é›†ï¼Œæˆ‘å€‘è¨“ç·´äº† Nemotron-Terminalï¼Œä¸€ç³»åˆ—å¾ Qwen3ï¼ˆ8Bã€14Bã€32Bï¼‰åˆå§‹åŒ–çš„æ¨¡å‹ï¼Œåœ¨ Terminal-Bench 2.0 ä¸Šå–å¾—äº†å¯¦è³ªæ€§çš„é€²å±•ï¼šNemotron-Terminal-8B å¾ 2.5% æ”¹é€²åˆ° 13.0%ï¼ŒNemotron-Terminal-14B å¾ 4.0% æ”¹é€²åˆ° 20.2%ï¼ŒNemotron-Terminal-32B å¾ 3.4% æ”¹é€²åˆ° 27.4%ï¼Œå…¶æ€§èƒ½èˆ‡è¦æ¨¡é¡¯è‘—æ›´å¤§çš„æ¨¡å‹ç›¸ç•¶ã€‚ç‚ºäº†åŠ é€Ÿé€™ä¸€é ˜åŸŸçš„ç ”ç©¶ï¼Œæˆ‘å€‘åœ¨ https://huggingface.co/collections/nvidia/nemotron-terminal é–‹æºäº†æˆ‘å€‘çš„æ¨¡å‹æª¢æŸ¥é»å’Œå¤§éƒ¨åˆ†åˆæˆè³‡æ–™é›†ã€‚</p>
  
  <div class="abstract-en">Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.</div>
  
</div>

<!-- Full paper content -->
<div class="paper-body">
  
    
      <p>[21] MAA. Aime 2024 åŸºæº–æ¸¬è©¦ï¼šä¾†è‡ªç¾åœ‹é‚€è«‹æ•¸å­¸è€ƒè©¦çš„å•é¡Œã€‚https://huggingface.co/datasets/Maxwell-Jia/AIME_2024ï¼Œ2024 å¹´ã€‚ç”¨æ–¼è©•ä¼°èªè¨€æ¨¡å‹æ•¸å­¸æ¨ç†èƒ½åŠ›çš„ AIME 2024 å•é¡ŒåŸºæº–æ¸¬è©¦ã€‚7</p>
    
  
</div>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>