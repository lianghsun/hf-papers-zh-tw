<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>OCR-Agentï¼šå…·å‚™èƒ½åŠ›èˆ‡è¨˜æ†¶åæ€çš„ä»£ç†å‹ OCR â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem; color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">OCR-Agentï¼šå…·å‚™èƒ½åŠ›èˆ‡è¨˜æ†¶åæ€çš„ä»£ç†å‹ OCR</h1>
  
  <div class="en-title">OCR-Agent: Agentic OCR with Capability and Memory Reflection</div>
  

  <div class="paper-meta">
    
    <span>Shimin Wen, Zeyu Zhang, Xingdou Bian, Hongjie Zhu, Lulu He, Layi Shama, Daji Ergu, Ying Cai</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.21053" target="_blank">2602.21053</a></span>
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.21053" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.21053" target="_blank" rel="noopener" class="btn btn-outline">arXiv é é¢</a>
    <a href="https://huggingface.co/papers/2602.21053" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">Multimodal</span>
    <span class="tag method">VLM</span><span class="tag method">Agent</span><span class="tag method">Self-correction</span><span class="tag method">Iterative Reasoning</span>
    <span class="tag task">OCR</span><span class="tag task">Visual Understanding</span><span class="tag task">Reasoning</span>
    <span class="tag open">Open Source</span>
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

å¤§å‹è¦–è¦ºèªè¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²é€éåè¦†å„ªåŒ–æ–¹æ³•åœ¨è¤‡é›œè¦–è¦ºç†è§£ä»»å‹™ä¸Šå±•ç¤ºå‡ºé¡¯è‘—çš„æ½›åŠ›ã€‚ç„¶è€Œï¼Œé€™äº›æ¨¡å‹æ™®éç¼ºä¹æœ‰æ•ˆçš„è‡ªæˆ‘ä¿®æ­£æ©Ÿåˆ¶ï¼Œä½¿å¾—å®ƒå€‘é›£ä»¥ç¨ç«‹åœ°ç³¾æ­£èªçŸ¥åå·®ã€‚å› æ­¤ï¼Œåœ¨å¤šè¼ªä¿®è¨‚éç¨‹ä¸­ï¼Œå®ƒå€‘å¾€å¾€é™·å…¥é‡è¤‡ä¸”ç„¡æ•ˆçš„å˜—è©¦ï¼Œç„¡æ³•å¯¦ç¾ç­”æ¡ˆå“è³ªçš„ç©©å®šæ”¹å–„ã€‚ç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹æ–°ç©çš„åè¦†è‡ªæˆ‘ä¿®æ­£æ¡†æ¶ï¼Œè³¦äºˆæ¨¡å‹å…©é …é—œéµèƒ½åŠ›ï¼šèƒ½åŠ›åæ€ï¼ˆCapability Reflectionï¼‰å’Œè¨˜æ†¶åæ€ï¼ˆMemory Reflectionï¼‰ã€‚è©²æ¡†æ¶å¼•å°æ¨¡å‹é¦–å…ˆé€éèƒ½åŠ›åæ€è¨ºæ–·éŒ¯èª¤ä¸¦ç”Ÿæˆä¿®æ­£è¨ˆç•«ï¼Œéš¨å¾Œåˆ©ç”¨è¨˜æ†¶åæ€ä¾†å›é¡§éå»çš„å˜—è©¦ä»¥é¿å…é‡è¤‡ä¸¦æ¢ç´¢æ–°çš„è§£æ±ºæ–¹æ¡ˆï¼Œæœ€å¾Œé€éåš´è¬¹çš„é‡æ–°æ¨ç†ä¾†å„ªåŒ–ç­”æ¡ˆã€‚åœ¨å…·æœ‰æŒ‘æˆ°æ€§çš„ OCRBench v2 åŸºæº–æ¸¬è©¦ä¸Šçš„å¯¦é©—è¡¨æ˜ï¼ŒOCR-Agent åœ¨è‹±æ–‡å­é›†ä¸Šè¶…è¶Šç¾æœ‰é–‹æºæœ€å…ˆé€²æ¨¡å‹ InternVL3-8B é” +2.0ï¼Œåœ¨ä¸­æ–‡å­é›†ä¸Šé” +1.2ï¼ŒåŒæ™‚åœ¨è¦–è¦ºç†è§£ï¼ˆ79.9ï¼‰å’Œæ¨ç†ï¼ˆ66.5ï¼‰æ–¹é¢å–å¾—æœ€å…ˆé€²çš„çµæœï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§çš„å¾®èª¿æ¨¡å‹ã€‚æˆ‘å€‘çš„æ–¹æ³•è¡¨æ˜ï¼Œçµæ§‹åŒ–çš„è‡ªæˆ‘æ„è­˜åæ€å¯ä»¥é¡¯è‘—å¢å¼· VLMs çš„æ¨ç†ç©©å¥æ€§ï¼Œè€Œç„¡éœ€é¡å¤–è¨“ç·´ã€‚ç¨‹å¼ç¢¼ï¼šhttps://github.com/AIGeeksGroup/OCR-Agentã€‚</p>
  
  <div class="abstract-en">Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs&#39; reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.</div>
  
</div>

<!-- Full paper content -->
<div class="paper-body">
  
</div>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>