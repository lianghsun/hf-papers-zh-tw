<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>QuantVLAï¼šè¦–è¦º-èªè¨€-å‹•ä½œæ¨¡å‹çš„å°ºåº¦æ ¡æº–å¾Œè¨“ç·´é‡åŒ– â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">QuantVLAï¼šè¦–è¦º-èªè¨€-å‹•ä½œæ¨¡å‹çš„å°ºåº¦æ ¡æº–å¾Œè¨“ç·´é‡åŒ–</h1>
  
  <div class="en-title">QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models</div>
  

  <div class="paper-meta">
    
    <span>Jingxuan Zhang, Yunta Hsieh, Zhongwei Wang, Haokun Lin, Xin Wang, Ziqi Wang, Yingtie Lei, Mi Zhang</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.20309" target="_blank">2602.20309</a></span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.20309" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.20309" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.20309" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">Multimodal</span><span class="tag domain">Robotics</span><span class="tag domain">RL</span>
    <span class="tag method">Quantization</span><span class="tag method">Post-Training Quantization</span><span class="tag method">Diffusion Transformer</span><span class="tag method">Attention Temperature Matching</span>
    <span class="tag task">Action Prediction</span><span class="tag task">Embodied AI</span><span class="tag task">Robot Control</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

è¦–è¦º-èªè¨€-å‹•ä½œï¼ˆVLAï¼‰æ¨¡å‹çµ±ä¸€äº†å…·èº«æ™ºèƒ½é«”çš„æ„ŸçŸ¥ã€èªè¨€å’Œæ§åˆ¶èƒ½åŠ›ï¼Œä½†åœ¨å¯¦éš›éƒ¨ç½²ä¸­é¢è‡¨é‡å¤§æŒ‘æˆ°ï¼Œç‰¹åˆ¥æ˜¯éš¨è‘—æ¨¡å‹æ“´å±•åˆ°æ›´é•·çš„æ™‚é–“ç¯„åœå’Œæ›´å¤§çš„ä¸»å¹¹ç¶²è·¯æ™‚ï¼Œè¨ˆç®—å’Œè¨˜æ†¶é«”éœ€æ±‚è¿…é€Ÿå¢åŠ ã€‚ç‚ºäº†è§£æ±ºé€™äº›ç“¶é ¸ï¼Œæˆ‘å€‘å¼•å…¥äº† QuantVLAï¼Œä¸€å€‹ç„¡è¨“ç·´çš„å¾Œè¨“ç·´é‡åŒ–ï¼ˆPTQï¼‰æ¡†æ¶ã€‚æ“šæˆ‘å€‘æ‰€çŸ¥ï¼Œé€™æ˜¯é¦–å€‹é‡å° VLA ç³»çµ±çš„ PTQ æ–¹æ³•ï¼Œä¹Ÿæ˜¯é¦–å€‹æˆåŠŸé‡åŒ–æ“´æ•£ Transformerï¼ˆDiTï¼‰å‹•ä½œé ­çš„æ–¹æ³•ã€‚QuantVLA åŒ…å«ä¸‰å€‹ç¶“éå°ºåº¦æ ¡æº–çš„çµ„ä»¶ï¼šï¼ˆ1ï¼‰é¸æ“‡æ€§é‡åŒ–ä½ˆå±€ï¼Œå°‡èªè¨€ä¸»å¹¹å’Œ DiT ä¸­çš„æ‰€æœ‰ç·šæ€§å±¤æ•´æ•¸åŒ–ï¼ŒåŒæ™‚ä¿æŒæ³¨æ„åŠ›æŠ•å½±ç‚ºæµ®é»æ ¼å¼ä»¥ä¿ç•™åŸå§‹é‹ç®—å­æ’ç¨‹ï¼›ï¼ˆ2ï¼‰æ³¨æ„åŠ›æº«åº¦åŒ¹é…ï¼Œä¸€ç¨®è¼•é‡ç´šçš„é€é ­ç¸®æ”¾æ©Ÿåˆ¶ï¼Œç”¨æ–¼ç©©å®šæ³¨æ„åŠ› logitsï¼Œä¸¦åœ¨æ¨è«–æ™‚æŠ˜ç–Šåˆ°åé‡åŒ–å°ºåº¦ä¸­ï¼›ï¼ˆ3ï¼‰è¼¸å‡ºé ­å¹³è¡¡ï¼Œä¸€ç¨®é€å±¤æ®˜å·®ä»‹é¢æ ¡æº–ï¼Œç”¨æ–¼ç·©è§£æŠ•å½±å¾Œçš„èƒ½é‡æ¼‚ç§»ã€‚è©²æ¡†æ¶ç„¡éœ€é¡å¤–è¨“ç·´ï¼Œåƒ…ä½¿ç”¨ä¸€å€‹å°å‹ç„¡æ¨™ç±¤æ ¡æº–ç·©è¡å€ï¼Œä¸¦æ”¯æ´ä½ä½å¯¬æ¬Šé‡å’Œæ¿€æ´»å€¼çš„æ•´æ•¸æ ¸å¿ƒï¼ŒåŒæ™‚ä¿æŒæ¶æ§‹ä¸è®Šã€‚åœ¨ LIBERO ä¸Šçš„ä»£è¡¨æ€§ VLA æ¨¡å‹ä¸­ï¼ŒQuantVLA è¶…è¶Šäº†å…¨ç²¾åº¦åŸºç·šçš„ä»»å‹™æˆåŠŸç‡ï¼Œåœ¨é‡åŒ–çµ„ä»¶ä¸Šé”åˆ°ç´„ 70% çš„ç›¸å°è¨˜æ†¶é«”ç¯€çœï¼Œä¸¦åœ¨ç«¯åˆ°ç«¯æ¨è«–å»¶é²ä¸Šå¯¦ç¾ 1.22 å€çš„åŠ é€Ÿï¼Œç‚ºåœ¨åš´æ ¼çš„è¨ˆç®—ã€è¨˜æ†¶é«”å’ŒåŠŸè€—é™åˆ¶ä¸‹å¯¦ç¾å¯æ“´å±•çš„ä½ä½å¯¬å…·èº«æ™ºèƒ½æä¾›äº†å¯¦ç”¨é€”å¾‘ã€‚</p>
  
  <div class="abstract-en">Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>