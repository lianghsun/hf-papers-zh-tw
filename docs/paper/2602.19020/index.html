<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>é€šéä¸»å‹•é‡å»ºå­¸ç¿’æª¢æ¸¬èªè¨€æ¨¡å‹è¨“ç·´æ•¸æ“š â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-25/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-25 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">é€šéä¸»å‹•é‡å»ºå­¸ç¿’æª¢æ¸¬èªè¨€æ¨¡å‹è¨“ç·´æ•¸æ“š</h1>
  
  <div class="en-title">Learning to Detect Language Model Training Data via Active Reconstruction</div>
  

  <div class="paper-meta">
    
    <span>Junjie Oscar Yin, John X. Morris, Vitaly Shmatikov, Sewon Min, Hannaneh Hajishirzi</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.19020" target="_blank">2602.19020</a></span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.19020" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.19020" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.19020" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">NLP</span><span class="tag domain">RL</span>
    <span class="tag method">Reinforcement Learning</span><span class="tag method">Membership Inference Attack</span><span class="tag method">Policy Finetuning</span>
    <span class="tag task">Membership Inference</span><span class="tag task">Data Detection</span><span class="tag task">Model Security</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

æª¢æ¸¬å¤§èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰è¨“ç·´è³‡æ–™é€šå¸¸è¢«æ¡†æ¶åŒ–ç‚ºæˆå“¡æ¨æ–·æ”»æ“Šï¼ˆMIAï¼‰å•é¡Œã€‚ç„¶è€Œï¼Œå‚³çµ±çš„ MIA è¢«å‹•åœ°å°å›ºå®šçš„æ¨¡å‹æ¬Šé‡é€²è¡Œæ“ä½œï¼Œä½¿ç”¨å°æ•¸æ¦‚ç‡æˆ–æ–‡æœ¬ç”Ÿæˆã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘å¼•å…¥ä¸»å‹•è³‡æ–™é‡å»ºæ”»æ“Šï¼ˆADRAï¼‰ï¼Œä¸€é¡é€éè¨“ç·´ä¸»å‹•èª˜å°æ¨¡å‹é‡å»ºçµ¦å®šæ–‡æœ¬çš„ MIA æ–¹æ³•ã€‚æˆ‘å€‘å‡è¨­è¨“ç·´è³‡æ–™çš„å¯é‡å»ºæ€§å„ªæ–¼éæˆå“¡è³‡æ–™ï¼Œä¸¦ä¸”å¯ä»¥åˆ©ç”¨å®ƒå€‘çš„å¯é‡å»ºæ€§å·®ç•°é€²è¡Œæˆå“¡æ¨æ–·ã€‚åŸºæ–¼å¼·åŒ–å­¸ç¿’ï¼ˆRLï¼‰èƒ½å¤ å¼·åŒ–æ¬Šé‡ä¸­å·²ç·¨ç¢¼è¡Œç‚ºçš„ç™¼ç¾ï¼Œæˆ‘å€‘åˆ©ç”¨ç­–ç•¥ä¸Šçš„ RL ä¾†ä¸»å‹•å¼•ç™¼è³‡æ–™é‡å»ºï¼Œæ–¹æ³•æ˜¯å¾ç›®æ¨™æ¨¡å‹åˆå§‹åŒ–ä¸€å€‹ç­–ç•¥é€²è¡Œå¾®èª¿ã€‚ç‚ºäº†æœ‰æ•ˆåœ°å°‡ RL ç”¨æ–¼ MIAï¼Œæˆ‘å€‘è¨­è¨ˆäº†é‡å»ºæŒ‡æ¨™å’Œå°æ¯”çå‹µã€‚æ‰€å¾—çš„æ¼”ç®—æ³• ADRA åŠå…¶è‡ªé©æ‡‰è®Šé«” ADRA+ï¼Œåœ¨å€™é¸è³‡æ–™æ± ä¸­æ”¹é€²äº†é‡å»ºå’Œæª¢æ¸¬æ€§èƒ½ã€‚å¯¦é©—é¡¯ç¤ºï¼Œæˆ‘å€‘çš„æ–¹æ³•åœ¨æª¢æ¸¬é è¨“ç·´ã€å¾Œè¨“ç·´å’Œè’¸é¤¾è³‡æ–™æ–¹é¢å§‹çµ‚å„ªæ–¼ç¾æœ‰ MIAï¼Œç›¸æ¯”ä¹‹å‰æœ€ä½³æ–¹æ³•å¹³å‡æ”¹é€² 10.7%ã€‚ç‰¹åˆ¥åœ°ï¼ŒADRA+ åœ¨ BookMIA é è¨“ç·´æª¢æ¸¬ä¸Šç›¸æ¯” Min-K%++ æ”¹é€² 18.8%ï¼Œåœ¨ AIME å¾Œè¨“ç·´æª¢æ¸¬ä¸Šæ”¹é€² 7.6%ã€‚</p>
  
  <div class="abstract-en">Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce Active Data Reconstruction Attack (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are more reconstructible than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, ADRA and its adaptive variant ADRA+, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\% over the previous runner-up. In particular, \MethodPlus~improves over Min-K\%++ by 18.8\% on BookMIA for pre-training detection and by 7.6\% on AIME for post-training detection.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <p style="color:var(--text-muted);font-style:italic;">å…¨æ–‡ç¿»è­¯å°šæœªç”Ÿæˆã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>