<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>å› æœé‹å‹•æ“´æ•£æ¨¡å‹ç”¨æ–¼è‡ªè¿´æ­¸é‹å‹•ç”Ÿæˆ â€” HF Papers ç¹ä¸­</title>
  <link rel="stylesheet" href="../../assets/style.css">
  
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a href="../../index.html" class="site-logo">ğŸ“„ HF Papers ç¹ä¸­</a>
      <nav>
        <a href="../../index.html">é¦–é </a>
        <a href="https://huggingface.co/papers" target="_blank" rel="noopener">HuggingFace</a>
      </nav>
    </div>
  </header>

  <main class="container">
    

<div class="paper-page-header">
  <a href="../../2026-02-27/index.html" style="font-size:0.85rem;color:var(--text-muted);">
    â† 2026-02-27 è«–æ–‡åˆ—è¡¨
  </a>
  <h1 style="margin-top:0.75rem;">å› æœé‹å‹•æ“´æ•£æ¨¡å‹ç”¨æ–¼è‡ªè¿´æ­¸é‹å‹•ç”Ÿæˆ</h1>
  
  <div class="en-title">Causal Motion Diffusion Models for Autoregressive Motion Generation</div>
  

  <div class="paper-meta">
    
    <span>Qing Yu, Akihisa Watanabe, Kent Fujiwara</span>
    
    <span>arXiv: <a href="https://arxiv.org/abs/2602.22594" target="_blank">2602.22594</a></span>
    
    <span style="color:var(--text-muted);font-size:0.8rem;">
      ä¾†æºï¼šarxiv HTML
    </span>
    
  </div>

  <div class="paper-links">
    <a href="https://arxiv.org/pdf/2602.22594" target="_blank" rel="noopener" class="btn btn-primary">PDF åŸæ–‡</a>
    <a href="https://arxiv.org/abs/2602.22594" target="_blank" rel="noopener" class="btn btn-outline">arXiv</a>
    <a href="https://huggingface.co/papers/2602.22594" target="_blank" rel="noopener" class="btn btn-outline">HF é é¢</a>
  </div>

  <div class="tags">
    <span class="tag domain">CV</span><span class="tag domain">Multimodal</span>
    <span class="tag method">Diffusion Models</span><span class="tag method">Transformer</span><span class="tag method">VAE</span><span class="tag method">Autoregressive Models</span>
    <span class="tag task">Motion Generation</span><span class="tag task">Text-to-Motion Generation</span><span class="tag task">Motion Synthesis</span>
    
  </div>
</div>

<!-- Abstract -->
<div class="abstract-box">
  <h2>æ‘˜è¦</h2>
  <p># è«–æ–‡æ‘˜è¦ç¿»è­¯

å‹•ä½œæ“´æ•£æ¨¡å‹ï¼ˆmotion diffusion modelsï¼‰è¿‘å¹´ä¾†çš„é€²å±•å·²å¤§å¹…æ”¹å–„äº†äººé¡å‹•ä½œåˆæˆçš„çœŸå¯¦æ€§ã€‚ç„¶è€Œï¼Œç¾æœ‰æ–¹æ³•è¦éº¼ä¾è³´æ–¼å…·æœ‰é›™å‘ç”Ÿæˆçš„å®Œæ•´åºåˆ—æ“´æ•£æ¨¡å‹ï¼Œé€™é™åˆ¶äº†æ™‚é–“å› æœæ€§å’Œå¯¦æ™‚é©ç”¨æ€§ï¼Œè¦éº¼ä¾è³´æ–¼å—ä¸ç©©å®šæ€§å’Œç´¯ç©èª¤å·®å›°æ“¾çš„è‡ªè¿´æ­¸æ¨¡å‹ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘æå‡ºå› æœå‹•ä½œæ“´æ•£æ¨¡å‹ï¼ˆCausal Motion Diffusion Models, CMDMï¼‰ï¼Œä¸€å€‹åŸºæ–¼åœ¨èªç¾©å°é½Šæ½›åœ¨ç©ºé–“ä¸­é‹ä½œçš„å› æœæ“´æ•£ Transformer çš„çµ±ä¸€è‡ªè¿´æ­¸å‹•ä½œç”Ÿæˆæ¡†æ¶ã€‚CMDM å»ºç«‹åœ¨å‹•ä½œ-èªè¨€-å°é½Šå› æœ VAEï¼ˆMotion-Language-Aligned Causal VAE, MAC-VAEï¼‰ä¹‹ä¸Šï¼Œè©² VAE å°‡å‹•ä½œåºåˆ—ç·¨ç¢¼ç‚ºæ™‚é–“å› æœæ½›åœ¨è¡¨ç¤ºã€‚åœ¨æ­¤æ½›åœ¨è¡¨ç¤ºä¹‹ä¸Šï¼Œè¨“ç·´ä¸€å€‹è‡ªè¿´æ­¸æ“´æ•£ Transformerï¼Œä½¿ç”¨å› æœæ“´æ•£å¼·åˆ¶ï¼ˆcausal diffusion forcingï¼‰ä¾†åŸ·è¡Œè·¨å‹•ä½œå¹€çš„æ™‚é–“æœ‰åºå»å™ªã€‚ç‚ºå¯¦ç¾å¿«é€Ÿæ¨è«–ï¼Œæˆ‘å€‘å¼•å…¥äº†å…·æœ‰å› æœä¸ç¢ºå®šæ€§çš„é€å¹€æ¡æ¨£ç­–ç•¥ï¼ˆframe-wise sampling scheduleï¼‰ï¼Œå…¶ä¸­æ¯å€‹å¾ŒçºŒå¹€å¾éƒ¨åˆ†å»å™ªçš„å‰ä¸€å¹€é€²è¡Œé æ¸¬ã€‚ç”±æ­¤ç”¢ç”Ÿçš„æ¡†æ¶æ”¯æŒé«˜å“è³ªæ–‡æœ¬-å‹•ä½œç”Ÿæˆã€æµå¼åˆæˆå’Œä»¥äº’å‹•é€Ÿç‡é€²è¡Œçš„é•·è¦–é‡å‹•ä½œç”Ÿæˆã€‚åœ¨ HumanML3D å’Œ SnapMoGen ä¸Šçš„å¯¦é©—è¡¨æ˜ï¼ŒCMDM åœ¨èªç¾©ä¿çœŸåº¦å’Œæ™‚é–“å¹³æ»‘æ€§ä¸Šéƒ½å„ªæ–¼ç¾æœ‰çš„æ“´æ•£å’Œè‡ªè¿´æ­¸æ¨¡å‹ï¼ŒåŒæ™‚å¤§å¹…é™ä½æ¨è«–å»¶é²ã€‚</p>
  
  <div class="abstract-en">Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.</div>
  
</div>

<!-- Full translated content -->
<div class="paper-body">
  
    <h1 id="_1">å› æœé‹å‹•æ“´æ•£æ¨¡å‹ç”¨æ–¼è‡ªè¿´æ­¸é‹å‹•ç”Ÿæˆ</h1>
<p>è¿‘æœŸé‹å‹•æ“´æ•£æ¨¡å‹çš„é€²å±•å¤§å¹…æå‡äº†äººé¡é‹å‹•åˆæˆçš„é€¼çœŸåº¦ã€‚ç„¶è€Œï¼Œç¾æœ‰æ–¹æ³•è¦éº¼ä¾è³´æ–¼å…·æœ‰é›™å‘ç”Ÿæˆçš„å…¨åºåˆ—æ“´æ•£æ¨¡å‹ï¼ˆé™åˆ¶äº†æ™‚é–“å› æœæ€§å’Œå³æ™‚å¯æ‡‰ç”¨æ€§ï¼‰ï¼Œè¦éº¼æ¡ç”¨è‡ªè¿´æ­¸æ¨¡å‹ï¼ˆå®¹æ˜“å‡ºç¾ä¸ç©©å®šæ€§å’Œç´¯ç©èª¤å·®ï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºå› æœé‹å‹•æ“´æ•£æ¨¡å‹ï¼ˆCMDMï¼‰ï¼Œä¸€å€‹åŸºæ–¼å› æœæ“´æ•£ Transformer çš„çµ±ä¸€æ¡†æ¶ï¼Œåœ¨èªç¾©å°é½Šçš„æ½›åœ¨ç©ºé–“ä¸­é€²è¡Œè‡ªè¿´æ­¸é‹å‹•ç”Ÿæˆã€‚CMDM å»ºç«‹åœ¨å‹•ä½œ-èªè¨€-å°é½Šå› æœ VAEï¼ˆMAC-VAEï¼‰ä¹‹ä¸Šï¼Œè©² VAE å°‡é‹å‹•åºåˆ—ç·¨ç¢¼ç‚ºæ™‚é–“ä¸Šå› æœçš„æ½›åœ¨è¡¨ç¤ºã€‚åœ¨æ­¤æ½›åœ¨è¡¨ç¤ºçš„åŸºç¤ä¸Šï¼Œè¨“ç·´ä¸€å€‹è‡ªè¿´æ­¸æ“´æ•£ Transformerï¼Œä½¿ç”¨å› æœæ“´æ•£å¼·åˆ¶å°é‹å‹•å¹€é€²è¡Œæ™‚é–“æœ‰åºçš„å»å™ªã€‚ç‚ºäº†å¯¦ç¾å¿«é€Ÿæ¨ç†ï¼Œæˆ‘å€‘å¼•å…¥äº†å…·æœ‰å› æœä¸ç¢ºå®šæ€§çš„å¹€ç´šæ¡æ¨£è¨ˆç•«ï¼Œå…¶ä¸­æ¯å€‹å¾ŒçºŒå¹€éƒ½å¾éƒ¨åˆ†å»å™ªçš„å…ˆå‰å¹€é æ¸¬å¾—å‡ºã€‚è©²æ¡†æ¶æ”¯æ´é«˜å“è³ªçš„æ–‡æœ¬åˆ°é‹å‹•ç”Ÿæˆã€æµå¼åˆæˆå’Œä»¥äº’å‹•é€Ÿç‡é€²è¡Œçš„é•·åºåˆ—é‹å‹•ç”Ÿæˆã€‚åœ¨ HumanML3D å’Œ SnapMoGen ä¸Šçš„å¯¦é©—è¡¨æ˜ï¼ŒCMDM åœ¨èªç¾©ä¿çœŸåº¦å’Œæ™‚é–“å¹³æ»‘æ€§æ–¹é¢éƒ½å„ªæ–¼ç¾æœ‰çš„æ“´æ•£å’Œè‡ªè¿´æ­¸æ¨¡å‹ï¼ŒåŒæ™‚å¤§å¹…é™ä½äº†æ¨ç†å»¶é²ã€‚</p>
<h2 id="1">1 å¼•è¨€</h2>
<p>åœ¨è‡ªç„¶èªè¨€æ¢ä»¶ä¸‹åˆæˆé€¼çœŸçš„äººé¡å‹•ä½œä»ç„¶æ˜¯é›»è…¦è¦–è¦ºå’Œåœ–å½¢å­¸ä¸­çš„åŸºæœ¬å•é¡Œã€‚æˆåŠŸçš„æ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆæ¨¡å‹ä¸åƒ…æ‡‰è©²åˆæˆç©ºé–“ç²¾ç¢ºçš„èº«é«”é‹å‹•ï¼Œé‚„æ‡‰è©²åœ¨é•·åºåˆ—ä¸­ä¿æŒæ™‚é–“ç›¸å¹²æ€§ã€‚å‹•ä½œæ“´æ•£æ¨¡å‹çš„è¿‘æœŸé€²å±• [ mdm2022human , zhang2022motiondiffuse , chen2023executing , dai2024motionlcm ] å¾—ç›Šæ–¼æ“´æ•£æ¡†æ¶çš„å¼·å¤§ç”Ÿæˆèƒ½åŠ› [ ho2020denoising , dhariwal2021diffusion ]ï¼Œåœ¨å‹•ä½œå“è³ªå’Œå¤šæ¨£æ€§æ–¹é¢å–å¾—äº†é¡¯è‘—æ”¹é€²ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•¸ç¾æœ‰çš„æ“´æ•£æ¨¡å‹éƒ½ä¾è³´æ–¼æ•´å€‹åºåˆ—ä¸Šçš„é›™å‘å»å™ªï¼Œé€™æœ¬è³ªä¸Šç ´å£äº†æ™‚é–“å› æœæ€§ï¼Œä¸¦å¦¨ç¤™äº†åœ¨ç·šç”Ÿæˆã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.22594/x1.png" loading="lazy"></figure> åœ– 1ï¼šç¾æœ‰æ–¹æ³•å’Œæ‰€ææ–¹æ³•çš„æ¦‚è¿°ã€‚ç¾æœ‰çš„åŸºæ–¼æ“´æ•£çš„æ–¹æ³•ï¼ˆå·¦ï¼‰ä½¿ç”¨ç›¸åŒçš„å™ªè²ç­‰ç´šå°å…¨åºåˆ—é€²è¡Œå»å™ªã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘å€‘æå‡ºçš„ CMDMï¼ˆå³ï¼‰å¼•å…¥äº†ä¸€å€‹åœ¨èªç¾©å› æœæ½›åœ¨ç‰¹å¾µä¸Šé‹è¡Œã€å…·æœ‰é€å¹€å™ªè²ç­‰ç´šçš„å› æœæ“´æ•£å¼·åˆ¶æ©Ÿåˆ¶ã€‚</p>
<p>è‡ªå›æ­¸æ¨¡å‹ [ zhang2023t2m , meng2024rethinking , zhao2024dartcontrol , xiao2025motionstreamer ] æä¾›äº†å¦ä¸€ç¨®æ–¹å¼ï¼Œé€šéå¾éå»çš„å¹€é æ¸¬æœªä¾†çš„å¹€ï¼Œç¢ºä¿å› æœä¸€è‡´æ€§ä¸¦æ”¯æ´åœ¨ç·šå‹•ä½œç”Ÿæˆã€‚ç„¶è€Œï¼Œå®ƒå€‘çš„åºåˆ—ä¾è³´æ€§é€šå¸¸å°è‡´èª¤å·®ç´¯ç©ï¼Œä½¿é•·æœŸåˆæˆè®Šå¾—ä¸ç©©å®šå’Œä½æ•ˆã€‚é—œéµæŒ‘æˆ°åœ¨æ–¼å¯¦ç¾æ™‚é–“æœ‰åºã€é«˜å“è³ªçš„å‹•ä½œç”Ÿæˆï¼ŒåŒæ™‚å…¼å…·æ“´æ•£æ¨¡å‹çš„ä¿çœŸåº¦å’Œè‡ªå›æ­¸ Transformer çš„å› æœçµæ§‹ã€‚</p>
<p>ç‚ºäº†æ‡‰å°é€™äº›æŒ‘æˆ°ï¼Œæˆ‘å€‘æå‡ºäº†å› æœå‹•ä½œæ“´æ•£æ¨¡å‹ï¼ˆCMDMï¼‰ï¼Œé€™æ˜¯ä¸€å€‹çµ±ä¸€æ¡†æ¶ï¼Œåœ¨èªç¾©å°é½çš„æ½›åœ¨ç©ºé–“ä¸­æ•´åˆå› æœæ“´æ•£å’Œè‡ªå›æ­¸å»ºæ¨¡ï¼Œå¦‚åœ– 1 æ‰€ç¤ºã€‚CMDM å»ºç«‹åœ¨æˆ‘å€‘çš„å‹•ä½œ-èªè¨€-æ¨¡å‹å°é½å› æœè®Šåˆ†è‡ªç·¨ç¢¼å™¨ï¼ˆMAC-VAEï¼‰ä¹‹ä¸Šï¼Œè©²ç·¨ç¢¼å™¨åœ¨å‹•ä½œ-èªè¨€é è¨“ç·´æŒ‡å°ä¸‹å°‡äººé¡å‹•ä½œç·¨ç¢¼ç‚ºæ™‚é–“å› æœæ½›åœ¨è¡¨ç¤º [ radford2021learning , petrovich2023tmr , yu2024exploring ]ã€‚é€™å€‹åŸºç¤ä½¿ CMDM èƒ½åœ¨ç·Šæ¹Šä¸”èªç¾©æœ‰æ„ç¾©çš„æ½›åœ¨ç©ºé–“ä¸­é‹ä½œï¼Œä¿æŒèªè¨€èªç¾©å’Œå‹•ä½œå‹•æ…‹ä¹‹é–“çš„å°é½ã€‚åœ¨ MAC-VAE çš„åŸºç¤ä¸Šï¼Œæˆ‘å€‘è¨­è¨ˆäº†å› æœæ“´æ•£ Transformerï¼ˆCausal-DiTï¼‰ï¼Œä»¥è‡ªå›æ­¸æ–¹å¼åŸ·è¡Œæ“´æ•£å»å™ªã€‚èˆ‡è¯åˆè™•ç†æ‰€æœ‰å¹€çš„å‚³çµ±æ“´æ•£æ¨¡å‹ä¸åŒï¼ŒCausal-DiT æ‡‰ç”¨å› æœè‡ªæ³¨æ„æ©Ÿåˆ¶ï¼Œä»¥ç¢ºä¿æ¯ä¸€å¹€åªä¾è³´æ–¼å‰é¢çš„å¹€ã€‚é€™ç¨®è¨­è¨ˆå¼·åˆ¶äº†åš´æ ¼çš„æ™‚é–“é †åºï¼Œå…è¨±æµå¼å‹•ä½œç”Ÿæˆã€‚</p>
<p>ç‚ºäº†åŠ é€Ÿæ¨ç†ï¼Œæˆ‘å€‘å¼•å…¥äº†å¸¶æœ‰å› æœä¸ç¢ºå®šæ€§çš„é€å¹€æ¡æ¨£æ™‚é–“è¡¨ï¼Œé€™å…è¨±æ¯ä¸€å¹€å¾éƒ¨åˆ†å»å™ªçš„å‰ç½®å¹€é€æ¼¸ç²¾åŒ–ï¼Œè€Œä¸éœ€è¦å®Œå…¨è‡ªå›æ­¸çš„å»å™ªæ­¥é©Ÿã€‚å— Diffusion Forcing [ chen2024diffusion ] å•Ÿç™¼ï¼Œè©²æ–¹æ³•æœ€åˆç‚ºä¸‹ä¸€å€‹ä»¤ç‰Œé æ¸¬è€Œè¨­è¨ˆï¼Œåœ¨è¨“ç·´æœŸé–“ï¼Œæˆ‘å€‘ç”¨ç¨ç«‹çš„å™ªè²ç­‰ç´šæ“¾å‹•æ¯ä¸€å¹€ï¼ŒåŒæ™‚ä¿æŒæ™‚é–“ä¸Šçš„å› æœä¾è³´ï¼Œä½¿æ¨¡å‹èƒ½å¤ å­¸ç¿’æ™‚é–“ç›¸å¹²çš„å»å™ªè½‰è®Šã€‚åœ¨æ¡æ¨£æœŸé–“ï¼Œæ¨¡å‹åŸºæ–¼å…·æœ‰ä¸åŒå™ªè²ç­‰ç´šçš„å…ˆå‰å»å™ªå¹€è¿­ä»£åœ°é æ¸¬å¾ŒçºŒå¹€ï¼Œä»¥å› æœé †åºé€æ¼¸é™ä½ä¸ç¢ºå®šæ€§ã€‚é€™å€‹åˆ†å±¤å»å™ªéç¨‹å¤§å¹…æ¸›å°‘äº†æ¨ç†æ­¥é©Ÿï¼Œå¯¦ç¾äº†é«˜æ•ˆä¸”æ™‚é–“ç›¸å¹²çš„å‹•ä½œç”Ÿæˆã€‚</p>
<p>CMDM çµ±ä¸€äº†æ“´æ•£æ¨¡å‹çš„ç©©å®šæ€§å’Œé€¼çœŸåº¦èˆ‡è‡ªå›æ­¸æ¶æ§‹çš„å› æœæ€§å’Œæ•ˆç‡ã€‚è©²æ¡†æ¶åœ¨çµ±ä¸€çš„å› æœæ¡†æ¶å…§æ”¯æ´é«˜ä¿çœŸæ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆã€å¿«é€Ÿæ¨ç†å’Œé•·æœŸåˆæˆã€‚å° HumanML3D å’Œ SnapMoGen çš„å»£æ³›è©•ä¼°è¡¨æ˜ï¼ŒCMDM åœ¨èªç¾©ä¿çœŸåº¦å’Œæ™‚é–“å¹³æ»‘æ€§æ–¹é¢éƒ½æŒçºŒè¶…è¶Šæœ€å…ˆé€²çš„æ“´æ•£å’Œè‡ªå›æ­¸æ¨¡å‹ï¼ŒåŒæ™‚å°‡æ¨ç†å»¶é²é™ä½äº†ä¸€å€‹æ•¸é‡ç´šã€‚</p>
<p>æˆ‘å€‘çš„ä¸»è¦è²¢ç»ç¸½çµå¦‚ä¸‹ï¼š</p>
<ul>
<li>â€¢ å› æœå‹•ä½œæ“´æ•£æ¡†æ¶ã€‚æˆ‘å€‘æå‡º CMDMï¼Œé€™æ˜¯ç¬¬ä¸€å€‹åœ¨å‹•ä½œ-èªè¨€-å°é½çš„æ½›åœ¨ç©ºé–“ä¸­çµ±ä¸€å› æœè‡ªå›æ­¸å’Œæ“´æ•£å»å™ªçš„å‹•ä½œæ“´æ•£æ¡†æ¶ã€‚</li>
<li>â€¢ èªç¾©å°é½çš„å› æœæ½›åœ¨å»ºæ¨¡ã€‚æˆ‘å€‘å¼•å…¥ MAC-VAEï¼Œä¸€å€‹å‹•ä½œ-èªè¨€-å°é½çš„å› æœè®Šåˆ†è‡ªç·¨ç¢¼å™¨ï¼Œç‚ºæ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆå­¸ç¿’æ™‚é–“å› æœä¸”èªç¾©æœ‰æ„ç¾©çš„æ½›åœ¨è¡¨ç¤ºã€‚</li>
<li>â€¢ å¸¶æœ‰å› æœä¸ç¢ºå®šæ€§çš„é€å¹€æ¡æ¨£ã€‚æˆ‘å€‘è¨­è¨ˆäº†ä¸€å€‹æ–°ç©çš„é€å¹€æ¡æ¨£æ™‚é–“è¡¨ï¼Œç”¨æ–¼å»ºæ¨¡å› æœä¸ç¢ºå®šæ€§ï¼Œå…è¨±æ¯ä¸€å¹€å¾éƒ¨åˆ†å»å™ªçš„å‰ç½®å¹€é€²è¡Œé æ¸¬ï¼Œä»¥å¯¦ç¾é«˜æ•ˆã€ä½å»¶é²ä¸”æ™‚é–“ç›¸å¹²çš„å‹•ä½œåˆæˆã€‚</li>
<li>â€¢ å…¨é¢çš„ç¶“é©—é©—è­‰ã€‚CMDM åœ¨ HumanML3D å’Œ SnapMoGen ä¸Šå–å¾—æœ€å…ˆé€²çš„æ€§èƒ½ï¼Œåœ¨æ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆå’Œé•·æœŸå‹•ä½œç”Ÿæˆæ–¹é¢è¶…è¶Šäº†ç¾æœ‰çš„æ“´æ•£å’Œè‡ªå›æ­¸æ–¹æ³•ã€‚</li>
</ul>
<p>å› æœå‹•ä½œæ“´æ•£æ¡†æ¶ã€‚æˆ‘å€‘æå‡º CMDMï¼Œé€™æ˜¯ç¬¬ä¸€å€‹åœ¨å‹•ä½œ-èªè¨€-å°é½çš„æ½›åœ¨ç©ºé–“ä¸­çµ±ä¸€å› æœè‡ªå›æ­¸å’Œæ“´æ•£å»å™ªçš„å‹•ä½œæ“´æ•£æ¡†æ¶ã€‚</p>
<p>èªç¾©å°é½çš„å› æœæ½›åœ¨å»ºæ¨¡ã€‚æˆ‘å€‘å¼•å…¥ MAC-VAEï¼Œä¸€å€‹å‹•ä½œ-èªè¨€-å°é½çš„å› æœè®Šåˆ†è‡ªç·¨ç¢¼å™¨ï¼Œç‚ºæ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆå­¸ç¿’æ™‚é–“å› æœä¸”èªç¾©æœ‰æ„ç¾©çš„æ½›åœ¨è¡¨ç¤ºã€‚</p>
<p>å¸¶æœ‰å› æœä¸ç¢ºå®šæ€§çš„é€å¹€æ¡æ¨£ã€‚æˆ‘å€‘è¨­è¨ˆäº†ä¸€å€‹æ–°ç©çš„é€å¹€æ¡æ¨£æ™‚é–“è¡¨ï¼Œç”¨æ–¼å»ºæ¨¡å› æœä¸ç¢ºå®šæ€§ï¼Œå…è¨±æ¯ä¸€å¹€å¾éƒ¨åˆ†å»å™ªçš„å‰ç½®å¹€é€²è¡Œé æ¸¬ï¼Œä»¥å¯¦ç¾é«˜æ•ˆã€ä½å»¶é²ä¸”æ™‚é–“ç›¸å¹²çš„å‹•ä½œåˆæˆã€‚</p>
<p>å…¨é¢çš„ç¶“é©—é©—è­‰ã€‚CMDM åœ¨ HumanML3D å’Œ SnapMoGen ä¸Šå–å¾—æœ€å…ˆé€²çš„æ€§èƒ½ï¼Œåœ¨æ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆå’Œé•·æœŸå‹•ä½œç”Ÿæˆæ–¹é¢è¶…è¶Šäº†ç¾æœ‰çš„æ“´æ•£å’Œè‡ªå›æ­¸æ–¹æ³•ã€‚</p>
<h2 id="2-related-works">2 Related Works</h2>
<h3 id="21-motion-language-alignment">2.1 Motion-Language Alignment</h3>
<p>Recent advances in visionâ€“language models [ radford2021learning , oquab2023dinov2 ] have shown that large-scale training can robustly align text and visual semantics. This has led to a surge of interest in exploring motionâ€“language alignment to enable practical control of motion using natural language. MotionCLIP [ tevet2022motionclip ] maps a single frame to CLIP space but fails to capture temporal dynamics. Subsequent methods, including TMR [ petrovich2023tmr ] and MotionPatches [ yu2024exploring ] , learn joint motionâ€“text embeddings via contrastive or generative objectives, while PartTMR [ yu2025remogpt ] introduces body-part-level features for finer alignment. However, most motionâ€“language models focus on retrieval tasks. Methods such as ReMoGPT [ yu2025remogpt ] and ReMoMask [ li2025remomask ] extend to text-to-motion generation but rely on retrieval-augmented generation rather than integrating motionâ€“language alignment directly into the generation.</p>
<h3 id="22-diffusion-based-motion-generation">2.2 Diffusion-based Motion Generation</h3>
<p>Text-conditioned motion generation has been explored through both non-diffusion and diffusion [ ho2020denoising , ho2022classifier , dhariwal2021diffusion ] paradigms. Early works used CNN- or RNN-based architectures [ yan2019convolutional , zhao2020bayesian ] and action-conditioned frameworks [ guo2020action2motion , petrovich2021action ] to synthesize motion from predefined semantics. More recently, diffusion-based methods [ dhariwal2021diffusion , rombach2022high ] have set new benchmarks for motion realism and diversity [ zhang2022motiondiffuse , chen2023executing , mdm2022human ] . MDM [ mdm2022human ] and MotionDiffuse [ zhang2022motiondiffuse ] operate directly in motion space, while MLD [ chen2023executing ] , MotionLCM [ dai2024motionlcm ] , EnergyMoGen [ zhang2025energymogen ] and SALAD [ hong2025salad ] perform diffusion in a latent space for greater stability and efficiency. However, these diffusion models rely on bidirectional attention over entire sequences, breaking temporal causality and limiting real-time or streaming generation.</p>
<h3 id="23">2.3 è‡ªè¿´æ­¸å‹•ä½œç”Ÿæˆ</h3>
<p>è‡ªè¿´æ­¸ï¼ˆARï¼‰å»ºæ¨¡é€šéå¾éå»è„ˆçµ¡é æ¸¬æœªä¾†å¹€ä¾†å¼·åˆ¶åŸ·è¡Œæ™‚é–“å› æœæ€§ã€‚é›¢æ•£æ¨™è¨˜æ–¹æ³•ï¼ˆå¦‚ T2M-GPT [ zhang2023t2m ] å’Œ MotionGPT [ jiang2023motiongpt ]ï¼‰å°‡å‹•ä½œè¦–ç‚ºã€Œèªè¨€ã€ï¼Œèƒ½å¤ é€²è¡Œå¼·å¤§çš„åºåˆ—å»ºæ¨¡ï¼Œä½†å®¹æ˜“å—åˆ°æš´éœ²åå·®å’Œç´¯ç©èª¤å·®çš„å›°æ“¾ã€‚åŸºæ–¼ VQ-VAE çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ MoMask [ guo2024momask ]ã€MMM [ pinyoanuntapong2024mmm ] å’Œ ParCo [ zou2024parco ]ï¼Œå°‡å‹•ä½œé‡åŒ–ç‚ºé›¢æ•£æ¨™è¨˜ä¸¦è‡ªè¿´æ­¸åœ°é æ¸¬å®ƒå€‘ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†å› æœç¯„å¼ç”¨æ–¼æµå¼ç”Ÿæˆï¼šDart [ zhao2024dartcontrol ] å¾æœ‰é™çš„å…©å€‹æ­·å²å¹€é æ¸¬çŸ­æœªä¾†æ®µï¼Œè€Œ MARDM [ meng2024rethinking ] å’Œ MotionStreamer [ xiao2025motionstreamer ] æ¡ç”¨å¸¶æœ‰æ“´æ•£é ­çš„é®ç½©è‡ªè¿´æ­¸ Transformers [ li2024autoregressive ]ã€‚ç„¶è€Œï¼Œå®ƒå€‘å°æ•™å¸«å¼·åˆ¶ï¼ˆTeacher Forcingï¼‰[ williams1989learning ] å’Œå¤§å‹æ“´æ•£é ­çš„ä¾è³´å°è‡´é•·æœŸé æ¸¬ä¸­çš„ä¸ç©©å®šæ€§å’Œé«˜è¨ˆç®—æˆæœ¬ï¼Œé™åˆ¶äº†å¯¦æ™‚éƒ¨ç½²ã€‚</p>
<p>æˆ‘å€‘çš„å·¥ä½œåœ¨å…©å€‹é—œéµæ–¹é¢èˆ‡ç¾æœ‰æ–¹æ³•ä¸åŒï¼š(1) æˆ‘å€‘åœ¨å‹•ä½œâ€“èªè¨€â€“å°é½Šçš„æ½›åœ¨ç©ºé–“å…§å¼•å…¥å› æœæ“´æ•£éç¨‹ï¼Œä¿æŒèªç¾©ä¸€è‡´æ€§åŒæ™‚å¼·åˆ¶åŸ·è¡Œæ™‚é–“å› æœæ€§ï¼›(2) æˆ‘å€‘è¨­è¨ˆäº†é€å¹€æ¡æ¨£è¨ˆåŠƒï¼Œèƒ½å¤ å¯¦ç¾é«˜å“è³ªçš„æµå¼å‹•ä½œç”Ÿæˆã€‚</p>
<h2 id="3">3 æ–¹æ³•</h2>
<p>æˆ‘å€‘æå‡ºçš„æ¡†æ¶ Causal Motion Diffusion Models (CMDM) é€šéæ•´åˆå› æœæ½›åœ¨ç·¨ç¢¼ã€å› æœæ“´æ•£å¼·åˆ¶å’Œé«˜æ•ˆçš„é€å¹€æ¡æ¨£ï¼Œå¯¦ç¾äº†æ™‚é–“åºåˆ—æœ‰åºã€æ–‡æœ¬æ¢ä»¶çš„å‹•ä½œç”Ÿæˆã€‚
å¦‚åœ– 2 æ‰€ç¤ºï¼ŒCMDM ç”±ä¸‰å€‹æ ¸å¿ƒå…ƒä»¶çµ„æˆï¼š
(1) Motion-Language-Aligned Causal VAE (MAC-VAE)ï¼Œå°‡å‹•ä½œåºåˆ—ç·¨ç¢¼åˆ°èªç¾©å°é½Šå’Œæ™‚é–“å› æœçš„æ½›åœ¨ç©ºé–“ä¸­ï¼Œ
(2) Causal Diffusion Transformer (Causal-DiT)ï¼Œä½¿ç”¨å› æœè‡ªæ³¨æ„åŠ›åŸ·è¡Œé€å¹€æ“´æ•£ï¼Œä»¥ç¢ºä¿è‡ªè¿´æ­¸æ™‚é–“ä¾è³´æ€§ï¼Œä»¥åŠ
(3) Frame-Wise Sampling Scheduler (FSS)ï¼Œé€šéç‚ºæœªä¾†å¹€åˆ†é…æ›´é«˜çš„é›œè¨Šï¼Œç‚ºéå»å¹€åˆ†é…è¼ƒä½çš„é›œè¨Šä¾†æ¨¡å‹åŒ–å› æœä¸ç¢ºå®šæ€§ï¼Œå…è¨±å¾éƒ¨åˆ†å»é›œè¨Šçš„å‰é¢å¹€é æ¸¬æ¯å€‹æ–°å¹€ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.22594/x2.png" loading="lazy"></figure> åœ– 2ï¼šæè­° CMDM æ¡†æ¶çš„æ¦‚è¿°ã€‚CMDM ç”±ä¸‰å€‹é—œéµå…ƒä»¶çµ„æˆï¼š(a) MAC-VAEï¼Œä½¿ç”¨å› æœç·¨ç¢¼å™¨-è§£ç¢¼å™¨çµæ§‹å°‡å‹•ä½œåºåˆ—ç·¨ç¢¼åˆ°å‹•ä½œ-èªè¨€-å°é½å’Œæ™‚é–“å› æœæ½›åœ¨ç‰¹å¾µä¸­ï¼Œç”±å‹•ä½œ-èªè¨€æ¨¡å‹å°é½Šé€²è¡Œç›£ç£ï¼›(b) Causal-DiTï¼ŒåŸ·è¡Œå…·æœ‰å› æœè‡ªæ³¨æ„åŠ›å’Œå°æ–‡æœ¬åµŒå…¥çš„äº¤å‰æ³¨æ„åŠ›çš„æ“´æ•£å»é›œè¨Šï¼Œç¢ºä¿æ™‚é–“æœ‰åºå’Œèªç¾©ä¸€è‡´çš„å¹€å„ªåŒ–ï¼›ä»¥åŠ (c) å› æœæ“´æ•£å¼·åˆ¶ï¼Œåœ¨è¨“ç·´æœŸé–“æ‡‰ç”¨ç¨ç«‹çš„å¹€ç´šé›œè¨Šï¼Œåœ¨æ¨ç†æœŸé–“æ‡‰ç”¨å› æœä¸ç¢ºå®šæ€§èª¿åº¦ï¼Œå…¶ä¸­ç´…è‰²å¼·åº¦è¡¨ç¤ºé›œè¨Šæ°´æº–ã€‚æ­¤è¨­è¨ˆä½¿ CMDM èƒ½å¤ å¯¦ç¾æ™‚é–“ä¸€è‡´ã€èªç¾©å°é½å’Œé«˜æ•ˆçš„æ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆï¼Œé©åˆæ–¼ä¸²æµå’Œé•·æœŸåœ°å¹³ç·šåˆæˆã€‚</p>
<h3 id="31-vae">3.1 å‹•ä½œ-èªè¨€å°é½å› æœ VAE</h3>
<p>ç‚ºäº†ç²å¾—æ™‚é–“ä¸Šçµæ§‹åŒ–ä¸”èªç¾©ä¸€è‡´çš„æ½›åœ¨è¡¨ç¤ºï¼ŒCMDM æ¡ç”¨èˆ‡å‹•ä½œ-èªè¨€åŸºç¤æ¨¡å‹å°é½çš„å› æœè®Šåˆ†è‡ªç·¨ç¢¼å™¨ã€‚
çµ¦å®šå‹•ä½œåºåˆ— $\mathbf{x}<em>{1:T}\in\mathbb{R}^{T\times D}$ï¼Œå…¶ä¸­ $T$ æ˜¯å¹€æ•¸ï¼Œ$D$ æ˜¯é—œç¯€è¡¨ç¤ºçš„ç¶­åº¦ï¼Œæå‡ºçš„ MAC-VAE ç·¨ç¢¼å™¨ $E</em>{\phi}$ å’Œè§£ç¢¼å™¨ $D_{\psi}$ ä»¥å› æœæ–¹å¼é‹ä½œï¼Œä½¿å¾—æ¯ä¸€å¹€åƒ…ä¾è³´æ–¼å…¶éå»ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathbf{z}<em>{t}=E</em>{\phi}(\mathbf{x}<em>{\leq t}),\quad\hat{\mathbf{x}}</em>{t}=D_{\psi}(\mathbf{z}_{\leq t}),$</th>
<th></th>
<th>(1)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ $\mathbf{z}<em>{t}\in\mathbb{R}^{d</em>{z}}$ è¡¨ç¤ºæ™‚é–“æ­¥ $t$ è™•çš„æ½›åœ¨è¡¨ç¤ºã€‚
å°æ–¼å…·æœ‰ $T$ å¹€çš„å‹•ä½œåºåˆ—ï¼Œæˆ‘å€‘å°‡å…¶ç·¨ç¢¼ç‚ºæ½›åœ¨ç©ºé–“ä¸­çš„ $T/4$ å€‹æ™‚é–“æ­¥ï¼Œæœ‰æ•ˆåœ°å¯¦ç¾ $4\times$ æ™‚é–“ä¸‹æ¡æ¨£æ¯”ã€‚
æ­¤å£“ç¸®å¹³è¡¡äº†è¡¨ç¤ºçš„ç·Šæ¹Šæ€§å’Œæ™‚é–“è§£æåº¦ï¼Œæ¸›å°‘å†—é¤˜åŒæ™‚ä¿ç•™åŸºç¤å‹•ä½œå‹•åŠ›å­¸ã€‚</p>
<p>ç·¨ç¢¼å™¨å’Œè§£ç¢¼å™¨æ”¹ç·¨è‡ª [xiao2025motionstreamer]ï¼Œç”± 1D å› æœå·ç©å’Œ 1D å› æœ ResNet å¡Šçµ„æˆï¼Œç¢ºä¿ç·¨ç¢¼å’Œé‡æ§‹éç¨‹ä¸­çš„æ™‚é–“å› æœæ€§ã€‚
åœ¨æ­¤è¨­è¨ˆä¸­ï¼Œæ¯ä¸€å¹€åƒ…ä¾è³´æ–¼å‰é¢çš„å¹€ï¼Œè€Œæœªä¾†å¹€è¢«æ’é™¤åœ¨è¨ˆç®—ä¹‹å¤–ï¼Œåœ¨æ½›åœ¨ç©ºé–“ä¸­æ˜ç¢ºå»ºæ¨¡æ™‚é–“å› æœæ€§ã€‚
åœ¨æ¨è«–æœŸé–“ï¼Œé‡æ§‹çš„å‹•ä½œå¯ä»¥å¯¦æ™‚é€åºåˆ—è§£ç¢¼ï¼Œå¯¦ç¾æµå¼ç”Ÿæˆï¼Œç„¡éœ€è¨ªå•æœªä¾†å¹€ã€‚</p>
<p>ç‚ºäº†å¢å¼·èªç¾©å°é½ï¼Œå‹•ä½œç‰¹å¾µé€šéé è¨“ç·´çš„å‹•ä½œ-èªè¨€ç·¨ç¢¼å™¨ï¼ˆPart-TMR [yu2025remogpt]ï¼‰é€²è¡ŒæŠ•å½±ï¼Œå…¶æä¾›éƒ¨åˆ†ç´šåˆ¥çš„èªç¾©ç›£ç£ã€‚
MAC-VAE ç›®æ¨™å‡½æ•¸çµåˆä¸‰é …ï¼šæ¨™æº– VAE é‡æ§‹æå¤±ã€Kullback-Leibler æ•£åº¦å’Œæ–°å¼•å…¥çš„å‹•ä½œ-èªè¨€å°é½æå¤±ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathcal{L}<em>{\text{MAC-VAE}}=\mathcal{L}</em>{\text{rec}}+\beta D_{\text{KL}}\big(q_{\phi}(\mathbf{z}|\mathbf{x})\,|\,p(\mathbf{z})\big)+\lambda\mathcal{L}_{\text{align}}.$</th>
<th></th>
<th>(2)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="_2">å‹•ä½œå°é½æå¤±</h4>
<p>ç‚ºäº†åœ¨å‹•ä½œå’Œæ–‡æœ¬ä¹‹é–“å¼·åˆ¶åŸ·è¡Œç´°ç²’åº¦èªç¾©å°é½ï¼Œæˆ‘å€‘å¼•å…¥å‹•ä½œå°é½æå¤± $\mathcal{L}_{\text{align}}$ï¼Œå…¶æ¸¬é‡å‹•ä½œåµŒå…¥ $\mathbf{z}$ å’Œå¾é è¨“ç·´ Part-TMR æ¨¡å‹çš„å‹•ä½œç·¨ç¢¼å™¨æå–çš„å‹•ä½œ-èªè¨€ç‰¹å¾µ $\mathbf{f}$ ä¹‹é–“çš„é»åˆ°é»ç‰¹å¾µç›¸ä¼¼æ€§å’Œç›¸å°çµæ§‹ä¸€è‡´æ€§ [yu2025remogpt]ã€‚
å…·é«”åœ°ï¼Œæˆ‘å€‘æ¡ç”¨å…©å€‹äº’è£œç›®æ¨™ï¼Œéµå¾ª VAVAE ä¸­çš„è¨­è¨ˆ [yao2025reconstruction]ï¼š(1) é‚Šéš›é¤˜å¼¦ç›¸ä¼¼æ€§æå¤±ï¼Œå…¶æœ€å°åŒ–å±€éƒ¨ç‰¹å¾µå·®è·ï¼Œä»¥åŠ (2) é‚Šéš›è·é›¢çŸ©é™£ç›¸ä¼¼æ€§æå¤±ï¼Œå…¶ä¿ç•™ç‰¹å¾µç©ºé–“çš„ç›¸å°å¹¾ä½•ï¼Œå®šç¾©å¦‚ä¸‹ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathcal{L}<em>{\text{align}}=\mathcal{L}</em>{\text{mcos}}+\mathcal{L}_{\text{mdms}}.$</th>
<th></th>
<th>(3)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>çµ¦å®šä¾†è‡ªæ½›åœ¨ç©ºé–“å’Œå‹•ä½œ-èªè¨€ç·¨ç¢¼å™¨çš„å°é½ç‰¹å¾µåœ– $\mathbf{Z}$ å’Œ $\mathbf{F}$ï¼Œæˆ‘å€‘é€šéç·šæ€§è®Šæ›å°‡ $\mathbf{Z}$ æŠ•å½±ä»¥åŒ¹é… $\mathbf{F}$ çš„ç‰¹å¾µç¶­åº¦ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathbf{Z}^{\prime}=W\mathbf{Z},$</th>
<th></th>
<th>(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ $W\in\mathbb{R}^{d_{f}\times d_{z}}$ æ˜¯å¯å­¸ç¿’çš„æŠ•å½±çŸ©é™£ã€‚
é‚Šéš›é¤˜å¼¦ç›¸ä¼¼æ€§æå¤± $\mathcal{L}<em>{\text{mcos}}$ æœ€å°åŒ–å°æ‡‰ç‰¹å¾µ $\mathbf{z}^{\prime}</em>{ij}$ å’Œ $\mathbf{f}_{ij}$ ä¹‹é–“çš„ç›¸ä¼¼æ€§å·®è·ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathcal{L}<em>{\text{mcos}}=\frac{1}{N}\sum</em>{i,j}\text{ReLU}!\left(1-m_{1}-\frac{\mathbf{z}^{\prime}<em>{ij}\cdot\mathbf{f}</em>{ij}}{|\mathbf{z}^{\prime}<em>{ij}|\,|\mathbf{f}</em>{ij}|}\right),$</th>
<th></th>
<th>(5)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ $N$ æ˜¯æ™‚é–“ç‰¹å¾µå…ƒç´ çš„ç¸½æ•¸ï¼Œ$m_{1}$ æ˜¯ç›¸ä¼¼æ€§é‚Šç•Œï¼Œé¼“å‹µå°ç›¸ä¼¼åº¦è¼ƒä½çš„å°é€²è¡Œæ›´å¼·çš„å°é½ã€‚
æ­¤æå¤±å°ˆæ³¨æ–¼èªç¾©éŒ¯èª¤å°é½å€åŸŸçš„å­¸ç¿’ï¼Œæ”¹å–„ç‰¹å¾µç´šåˆ¥çš„ä¸€è‡´æ€§ã€‚</p>
<p>ä½œç‚º $\mathcal{L}<em>{\text{mcos}}$ çš„è£œå……ï¼Œé‚Šéš›è·é›¢çŸ©é™£ç›¸ä¼¼æ€§æå¤± $\mathcal{L}</em>{\text{mdms}}$ é€šéåŒ¹é…å…¶æˆå°è·é›¢çŸ©é™£ä¾†å¼·åˆ¶åŸ·è¡Œå‹•ä½œå’Œæ–‡æœ¬åµŒå…¥ä¹‹é–“å…§éƒ¨çµæ§‹é—œä¿‚çš„å°é½ã€‚
æ­£å¼åœ°ï¼Œæˆ‘å€‘è¨ˆç®—ï¼š</p>
<p>|  | $\mathcal{L}<em>{\text{mdms}}=\frac{1}{N^{2}}\sum</em>{i,j}\text{ReLU}!\left(\left|\frac{\mathbf{z}<em>{i}\cdot\mathbf{z}</em>{j}}{|\mathbf{z}<em>{i}||\mathbf{z}</em>{j}|}-\frac{\mathbf{f}<em>{i}\cdot\mathbf{f}</em>{j}}{|\mathbf{f}<em>{i}||\mathbf{f}</em>{j}|}\right|-m_{2}\right),$ |  | (6) |
| --- | --- | --- | --- |</p>
<p>å…¶ä¸­ $m_{2}$ æ˜¯è·é›¢é‚Šç•Œï¼Œå°ç›¸ä¼¼å°çš„å°é½ç´„æŸé€²è¡Œæ”¾é¬†ã€‚
æ­¤ç›®æ¨™ä¿ƒé€²æ½›åœ¨ç©ºé–“å’Œæ–‡æœ¬ç©ºé–“ä¹‹é–“çš„çµæ§‹ä¸€è‡´æ€§ï¼Œç¢ºä¿å‹•ä½œåµŒå…¥çš„ç›¸å°å¹¾ä½•åŒ¹é…å°é½åŸºç¤ç‰¹å¾µçš„ç›¸å°å¹¾ä½•ã€‚</p>
<h3 id="32-causal-diffusion-forcing">3.2 å› æœæ“´æ•£å¼·åˆ¶ï¼ˆCausal Diffusion Forcingï¼‰</h3>
<p>æˆ‘å€‘å°‡ Diffusion Forcing [chen2024diffusion]ï¼ˆåŸæœ¬ç‚ºä¸‹ä¸€å€‹tokené æ¸¬è€Œæå‡ºï¼‰æ“´å±•åˆ°å‹•ä½œé ˜åŸŸï¼Œä»¥åœ¨æ½›åœ¨ç©ºé–“ä¸­å°è‡ªè¿´æ­¸æ™‚é–“å‹•æ…‹é€²è¡Œå»ºæ¨¡ã€‚
ä¸åŒæ–¼è¯åˆå»å™ªæ‰€æœ‰å¹€çš„å‚³çµ±æ“´æ•£æ¨¡å‹ï¼Œæˆ‘å€‘çš„ CMDM å¼•å…¥äº†å¹€ç´šå™ªè²ï¼Œç‚ºæ½›åœ¨åºåˆ—ä¸­çš„æ¯å€‹å‹•ä½œå¹€åˆ†é…ç¨ç«‹çš„æ“´æ•£æ™‚æ­¥ï¼Œå¼·åˆ¶åŸ·è¡Œéå»å’Œæœªä¾†å¹€ä¹‹é–“çš„å› æœä¾è³´é—œä¿‚ã€‚</p>
<p>åœ¨æ¨™æº–å…¨åºåˆ—æ“´æ•£ä¸­ï¼Œç›¸åŒçš„å™ªè²æ°´æº– $k\in[0,K]$ï¼ˆå…¶ä¸­ $K$ ç‚ºæ“´æ•£æ­¥é©Ÿçš„ç¸½æ•¸ï¼‰æ‡‰ç”¨æ–¼æ•´å€‹åºåˆ—ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\tilde{\mathbf{z}}^{k}=\sqrt{\bar{\alpha}<em>{k}}\,\mathbf{z}^{k}+\sqrt{1-\bar{\alpha}</em>{k}}\,\boldsymbol{\epsilon}^{k},\quad\boldsymbol{\epsilon}^{k}\sim\mathcal{N}(0,I),$</th>
<th></th>
<th>(7)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>è€Œå»å™ªæ¨¡å‹ $\epsilon_{\theta}$ è¢«è¨“ç·´ä»¥åŒæ™‚æ¢å¾©åŸå§‹åºåˆ—ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathcal{L}=\mathbb{E}<em>{k,\boldsymbol{\epsilon}^{k}}\Big[|\boldsymbol{\epsilon}^{k}-\epsilon</em>{\theta}(\tilde{\mathbf{z}}^{k},k,\mathbf{c})|_{2}^{2}\Big],$</th>
<th></th>
<th>(8)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ $\mathbf{c}$ è¡¨ç¤ºæ–‡æœ¬åµŒå…¥ã€‚</p>
<p>åœ¨å› æœæ“´æ•£å¼·åˆ¶ä¸­ï¼Œæ¯ä¸€å¹€ $t$ è¢«åˆ†é…ä¸€å€‹ç¨ç«‹çš„å™ªè²æ°´æº– $k_{t}\in[0,K]$ï¼Œè€Œå¸¶å™ªè²çš„æ½›åœ¨è¡¨ç¤ºå®šç¾©ç‚ºï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\tilde{\mathbf{z}}<em>{t}^{k</em>{t}}=\sqrt{\bar{\alpha}<em>{k</em>{t}}}\,\mathbf{z}<em>{t}^{k</em>{t}}+\sqrt{1-\bar{\alpha}<em>{k</em>{t}}}\,\boldsymbol{\epsilon}<em>{t}^{k</em>{t}},\quad\boldsymbol{\epsilon}<em>{t}^{k</em>{t}}\sim\mathcal{N}(0,I).$</th>
<th></th>
<th>(9)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>æ“´æ•£ Transformer $\epsilon_{\theta}$ ä½¿ç”¨å› æœè‡ªæ³¨æ„åŠ›é æ¸¬å™ªè²æ®˜å·®ï¼Œé€™é™åˆ¶äº†æ¯ä¸€å¹€åªèƒ½åƒèˆ‡å…¶éå»çš„è¡¨ç¤ºï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathcal{L}<em>{\text{DF}}=\mathbb{E}</em>{k_{t},\boldsymbol{\epsilon}<em>{t}^{k</em>{t}}}\Big[|\boldsymbol{\epsilon}<em>{t}^{k</em>{t}}-\epsilon_{\theta}(\tilde{\mathbf{z}}<em>{\leq t},k</em>{t},\mathbf{c})|_{2}^{2}\Big].$</th>
<th></th>
<th>(10)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>é€™å€‹å› æœè¨“ç·´è¨­ç½®ç¢ºä¿å»å™ªéç¨‹åœ¨æ™‚é–“ä¸Šå‘å‰æ¼”é€²ï¼Œæ¯å€‹é æ¸¬åƒ…ä¾è³´æ–¼å¯ç”¨çš„æ­·å²è¨˜éŒ„ï¼Œæœ‰æ•ˆåœ°æ©‹æ¥äº†æ“´æ•£èˆ‡è‡ªè¿´æ­¸ã€‚</p>
<p>é€šéå°‡å…¨å±€åŒæ­¥çš„å™ªè²æ’ç¨‹æ›¿æ›ç‚ºå¹€ç‰¹å®šçš„æ“¾å‹•ï¼ŒCMDM å¯¦ç¾äº†å¤šé …å„ªå‹¢ã€‚
é¦–å…ˆï¼Œè©²æ¨¡å‹å­¸æœƒåœ¨æ¯ä¸€å¹€çš„å¤šæ¨£åŒ–å™ªè²æ¢ä»¶ä¸‹é‹ä½œï¼Œé€™æé«˜äº†æ™‚é–“é­¯æ£’æ€§å’Œå°å¯è®Šé•·åº¦åºåˆ—çš„æ³›åŒ–èƒ½åŠ›ã€‚
å…¶æ¬¡ï¼Œå› æœæ³¨æ„åŠ›é®ç½©æ˜ç¢ºåœ°å¼·åˆ¶åŸ·è¡Œ Transformer éª¨å¹¹å…§çš„æ™‚é–“é †åºï¼Œé˜²æ­¢æœªä¾†å¹€çš„è³‡è¨Šæ´©æ¼ï¼Œä¸¦èƒ½å¤ é€²è¡Œå¯¦æ™‚æˆ–æµå¼ç”Ÿæˆã€‚
æœ€å¾Œï¼Œæ“´æ•£å¼·åˆ¶çš„é€å¹€éš¨æ©Ÿæ€§å……ç•¶è‡ªç„¶æ­£å‰‡åŒ–é …ï¼Œä¿ƒé€²å¹³é †çš„æ™‚é–“è½‰æ›ï¼ŒåŒæ™‚ä¿ç•™å‹•ä½œå¤šæ¨£æ€§ã€‚</p>
<h3 id="33-transformer">3.3 å› æœæ“´æ•£ Transformer</h3>
<p>ç‚ºäº†å°æ“´æ•£å¼·åˆ¶çš„æ™‚é–“ä¾è³´æ€§é€²è¡Œå»ºæ¨¡ï¼ŒCMDM æ¡ç”¨å› æœæ“´æ•£ Transformerï¼ˆCausal-DiTï¼‰ï¼Œåœ¨åš´æ ¼çš„å› æœç´„æŸä¸‹åŸ·è¡ŒåŸºæ–¼æ“´æ•£çš„å»å™ªã€‚èˆ‡å…·æœ‰é›™å‘æ³¨æ„åŠ›çš„å‚³çµ± Transformer ä¸åŒï¼ŒCausal-DiT ä½¿ç”¨å› æœé®ç½©ï¼Œä½¿å¾—æ¯ä¸€å¹€åªèƒ½è¨ªå•å…¶éå»å’Œç•¶å‰çš„ä¸Šä¸‹æ–‡ï¼Œç¢ºä¿èˆ‡è‡ªè¿´æ­¸æ¨ç†ä¸€è‡´çš„é †åºç”Ÿæˆï¼ŒåŒæ™‚ä¿æŒæ“´æ•£çš„çœŸå¯¦æ€§ã€‚</p>
<p>æ¯å€‹ Transformer å€å¡Šæ•´åˆä¸‰å€‹é—œéµæ©Ÿåˆ¶ï¼š(1) å› æœè‡ªæ³¨æ„åŠ›ï¼Œæ¡ç”¨ä¸‹ä¸‰è§’æ³¨æ„åŠ›é®ç½©ä¾†é˜²æ­¢æ¯ä¸€å¹€æ³¨æ„åˆ°æœªä¾†çš„å¹€ã€‚æ­¤ç´„æŸä¿ç•™äº†è‡ªè¿´æ­¸å»ºæ¨¡æ‰€éœ€çš„å› æœé †åºï¼Œä¸¦ç¢ºä¿æ¨¡å‹åƒ…åŸºæ–¼å…ˆå‰è§€å¯Ÿåˆ°çš„è³‡è¨Šä¾†é æ¸¬æœªä¾†çš„é‹å‹•å‹•æ…‹ã€‚(2) äº¤å‰æ³¨æ„åŠ›ï¼Œé€éå°‡å¹€ç´šé‹å‹•æ½›åœ¨å‘é‡æ¢ä»¶åŒ–æ–¼å¾ DistilBERT [ sanh2019distilbert ] æå–çš„è©ç´šæ–‡æœ¬åµŒå…¥ï¼Œå»ºç«‹é‹å‹•èˆ‡èªè¨€ä¹‹é–“çš„é€£çµã€‚é€éæ­¤æ©Ÿåˆ¶ï¼Œä¾†è‡ªè‡ªç„¶èªè¨€çš„èªç¾©ç·šç´¢å¼•å°é‹å‹•ç‰¹å¾µçš„æ™‚é–“æ¼”åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆæˆè·¨åºåˆ—èˆ‡æ–‡æœ¬æè¿°ä¿æŒä¸€è‡´çš„å‹•ä½œã€‚(3) è‡ªé©æ‡‰å±¤æ­£è¦åŒ–ï¼ˆAdaLNï¼‰[ peebles2023scalable ] çµåˆæ—‹è½‰ä½ç½®ç·¨ç¢¼ï¼ˆROPEï¼‰[ su2024roformer ]ï¼Œå…¶ä¸­ AdaLN åµŒå…¥é€å¹€æ“´æ•£æ™‚é–“æ­¥è³‡è¨Šï¼Œç¢ºä¿æ™‚é–“å™ªè²ç´šåˆ¥ $k_t$ ç„¡ç¸«æ•´åˆåˆ°å»å™ªéç¨‹ä¸­ï¼Œè€Œ ROPE å‰‡é€šéç›¸å°ä½ç½®ç·¨ç¢¼ç©©å®šé•·æœŸåœ°å¹³ç·šå»å™ªã€‚</p>
<p>åœ¨è¨“ç·´æœŸé–“ï¼Œæ¯ä¸€å¹€ $t$ ä½¿ç”¨ç¨ç«‹çš„å™ªè²ç´šåˆ¥ $k_t$ é€²è¡Œæ“´æ•£ï¼Œæ¨¡å‹å­¸ç¿’æŒ‰ä»¥ä¸‹æ–¹å¼å°å…¶é€²è¡Œå»å™ªï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\epsilon_{\theta}(\tilde{\mathbf{z}}<em>{\leq t},k</em>{t},\mathbf{c})=\text{CausalDiT}(\tilde{\mathbf{z}}<em>{\leq t},k</em>{t},\mathbf{c})$</th>
<th></th>
<th>(11)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ $\tilde{\mathbf{z}}_{\leq t}$ è¡¨ç¤ºéƒ¨åˆ†åŠ å™ªçš„å› æœæ½›åœ¨åºåˆ—ï¼Œ$\mathbf{c}$ æ˜¯æ–‡æœ¬åµŒå…¥ã€‚</p>
<h3 id="34">3.4 æ¨ç†å’Œæµå¼ç”Ÿæˆ</h3>
<p>åœ¨æ¨ç†éç¨‹ä¸­ï¼ŒCMDM é€šéä»¥å› æœæ–¹å¼é€æ­¥å»å™ªæ¯ä¸€å¹€ä¾†è‡ªå›æ­¸åœ°ç”Ÿæˆå‹•ä½œã€‚
çµ¦å®šæ–‡æœ¬æ¢ä»¶ $\mathbf{c}$ å’Œåˆå§‹å™ªè²åºåˆ— ${\tilde{\mathbf{z}}<em>{t}^{K}}</em>{t=1}^{T}\sim\mathcal{N}(0,I)$ï¼Œè©²æ¨¡å‹åŸºæ–¼å…ˆå‰å»å™ªçš„æ½›åœ¨å‘é‡é æ¸¬æ¯ä¸€å¹€ï¼š</p>
<table>
<thead>
<tr>
<th></th>
<th>$\tilde{\mathbf{z}}<em>{t}^{k</em>{t}-1}=G_{\theta}({\tilde{\mathbf{z}}^{0}<em>{&lt;t},\tilde{\mathbf{z}}</em>{t}^{k_{t}}},k_{t},\mathbf{c}),\quad\hat{\mathbf{x}}<em>{t}=D</em>{\psi}(\tilde{\mathbf{z}}^{0}_{\leq t})$</th>
<th></th>
<th>(12)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>å…¶ä¸­ $G_{\theta}$ è¡¨ç¤ºå› æœæ“´æ•£ç”Ÿæˆå™¨ã€‚
æ­¤å…¬å¼ç¢ºä¿åš´æ ¼çš„å› æœåˆæˆï¼Œä¸¦é€šéå°è‡ªå›æ­¸å±•é–‹é€²è¡Œéµå€¼å¿«å–ï¼Œå¯¦ç¾å¯¦æ™‚ç”Ÿæˆã€‚ç„¶è€Œï¼Œæ­¤æ–¹æ¡ˆå®¹æ˜“ç´¯ç©å–®æ­¥éŒ¯èª¤ï¼Œå› ç‚ºå®ƒå°‡é æ¸¬çš„ $\tilde{\mathbf{z}}_{t}^{0}$ è¦–ç‚ºçœŸå¯¦è§€å¯Ÿï¼Œé€™ç¨®åšæ³•æ›´å»£æ³›åœ°ç¨±ç‚ºæ›éœ²åå·® [schmidt2019generalization, ning2023elucidating]ã€‚</p>
<h4 id="fss">é€å¹€æ¡æ¨£æ’ç¨‹ (FSS)</h4>
<p>ç‚ºäº†åŠ é€Ÿæ¨ç†ä¸¦ç·©è§£æ›éœ²åå·®ï¼ŒCMDM æ¡ç”¨å…·æœ‰å› æœä¸ç¢ºå®šæ€§çš„é€å¹€æ¡æ¨£æ’ç¨‹ï¼Œç‚ºéå»çš„å¹€åˆ†é…è¼ƒä½çš„å™ªè²ï¼Œç‚ºæœªä¾†çš„å¹€åˆ†é…è¼ƒé«˜çš„å™ªè²ã€‚
åœ¨æ¯ä¸€æ­¥ï¼Œè©²æ¨¡å‹ä½¿ç”¨éƒ¨åˆ†å»å™ªçš„æ­·å²è¨˜éŒ„ä¾†ç´°åŒ–ä¸‹ä¸€å¹€ã€‚ä¾‹å¦‚ï¼Œå…·æœ‰ä¸ç¢ºå®šæ€§å°ºåº¦ $L$ çš„å› æœä¸ç¢ºå®šæ€§æ’ç¨‹å¯å®šç¾©ç‚ºï¼š</p>
<p>|  | $K_{m,t}=\begin{bmatrix}K&amp;K&amp;K\
K{-}L&amp;K&amp;K\
K{-}2L&amp;K{-}L&amp;K\
\vdots&amp;\ddots&amp;K{-}L\
0&amp;\cdots&amp;\vdots\
0&amp;0&amp;0\end{bmatrix}$ |  | (13) |
| --- | --- | --- | --- |</p>
<p>å…¶ä¸­ $K_{m,t}$ è¡¨ç¤ºåœ¨è¿­ä»£ $m$ ä¸­æ‡‰ç”¨æ–¼å¹€ $t$ çš„å™ªè²ï¼Œä¸ç¢ºå®šæ€§å°ºåº¦ $L$ è¡¨ç¤ºä¸‹ä¸€å¹€çš„å»å™ªå¾ç•¶å‰å¹€çš„æ­¥é©Ÿ $K{-}L$ é–‹å§‹ã€‚ç‚ºæ¸…æ¥šèµ·è¦‹ï¼Œçœç•¥äº† $K$ã€$K{-}L$ã€$K{-}2L$ ç­‰ä¹‹é–“çš„ä¸­é–“æ­¥é©Ÿã€‚æ¯å€‹éƒ¨åˆ†å»å™ªçš„å¹€ $\tilde{\mathbf{z}}_{t}$ è¢«é‡æ–°ç”¨ä½œå¾ŒçºŒé æ¸¬çš„ä¸Šä¸‹æ–‡ï¼Œå¯¦ç¾é€£çºŒã€ä½å»¶é²çš„ç”Ÿæˆï¼Œå…·æœ‰é«˜åº¦çš„æ™‚é–“ç›¸é—œæ€§å’Œå¹³æ»‘çš„éæ¸¡ï¼ŒåŒæ™‚èˆ‡å®Œæ•´çš„è‡ªå›æ­¸æ“´æ•£ç›¸æ¯”å¤§å¤§é™ä½äº†æ¨ç†æˆæœ¬ã€‚</p>
<h2 id="4">4 å¯¦é©—</h2>
<p>è¡¨ 1ï¼šHumanML3D ä¸Šæ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆçš„çµæœã€‚å¹³å‡å€¼æ ¹æ“š 10 æ¬¡é‹è¡Œå ±å‘Šï¼Œå…·æœ‰ 95% ä¿¡è³´å€é–“ã€‚æ¨™æœ‰ â€  çš„æ–¹æ³•æœ€åˆæ¡ç”¨ä¸åŒçš„å‹•ä½œè¡¨ç¤ºæ³•å¯¦ç¾ï¼Œå·²ä½¿ç”¨æˆ‘å€‘çš„ç¨‹å¼ç¢¼åº«é‡æ–°è¨“ç·´ä»¥ç¢ºä¿å…¬å¹³æ¯”è¼ƒã€‚ç²—é«”è¡¨ç¤ºæœ€ä½³çµæœï¼Œä¸‹åŠƒç·šè¡¨ç¤ºç¬¬äºŒå¥½çš„çµæœã€‚</p>
<h3 id="41-experimental-setup">4.1 Experimental Setup</h3>
<h4 id="datasets">Datasets.</h4>
<p>To evaluate CMDM, we conduct experiments on two benchmarks: HumanML3D [ Guo_2022_CVPR_humanml3d ] and SnapMoGen [ guo2025snapmogen ] .
HumanML3D contains 14,616 motion clips from AMASS [ AMASS_ICCV2019 ] paired with 44,970 short textual descriptions of common actions ( e.g . , walking, jumping, sitting).
SnapMoGen includes 20,450 motion capture clips with 122K expressive captions (average 48 words) covering about 43.7 hours of data.
Unlike HumanML3D, SnapMoGen features temporally continuous, long-horizon activities ( e.g . , sports and performances), allowing evaluation of smooth, consistent motion generation.
For fair comparison, we follow the standard 3D motion representation of each dataset: 263 dimensions for HumanML3D and 296 for SnapMoGen, including joint velocities, positions, and rotations.</p>
<h4 id="evaluation-metrics">Evaluation Metrics.</h4>
<p>Following prior work [ Guo_2022_CVPR_humanml3d , jiang2023motiongpt , guo2025snapmogen ] , we evaluate CMDM using several standard metrics.
(1) Motion quality is measured by FrÃ©chet Inception Distance (FID), which assesses the realism of generated motions relative to ground truth.
(2) Multi-modality quantifies the diversity of motions generated from identical text prompts.
(3) Textâ€“motion alignment is evaluated by R-Precision (R@1, R@2, R@3) and Multi-Modal Distance (MM Dist) using a pretrained textâ€“motion retrieval model.
We also report the CLIP-Score [ meng2024rethinking ] , which measures the cosine similarity between generated motion features and their corresponding captions in the CLIP embedding space.</p>
<h4 id="implementation-details">Implementation Details.</h4>
<p>MAC-VAE comprises seven causal convolution layers and two causal ResNet blocks with left padding in both the encoder and decoder. The latent feature dimension is set to 64. We modify and retrain Part-TMR [ yu2025remogpt ] to extract frame-level semantic motionâ€“language features for supervising MAC-VAE. The loss weight Î» \lambda for semantic alignments is automatically adjusted according to the gradient norm at the final layer of the encoder to maintain balance with other losses. The Causal-DiT is implemented as a lightweight Transformer [ vaswani2017attention ] with eight layers, four attention heads, and a hidden dimension of 512. Flow Matching [ lipman2022flow , ma2024sit , albergo2023stochastic , albergo2022building ] is adopted as the ODE sampler for causal diffusion forcing. For FSS, we set 50 denoising step with K = 50 K=50 for each frame and start to denoise the next frame at K âˆ’ 2 K-2 ( i.e . , L = 2 L=2 ) during inference.</p>
<p>Table 2 : Results of text-to-motion generation on SnapMoGen. The average is reported over 10 runs with 95% confidence intervals.</p>
<p>Table 3 : Results of long-horizon motion generation on HumanML3D and SnapMoGen. The motion quality of each subsequence and the smoothness of each transition are evaluated.</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.22594/x3.png" loading="lazy"></figure> Figure 3 : Qualitative results of long-horizon motion generation. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences.</p>
<h3 id="42-quantitative-results">4.2 Quantitative Results</h3>
<h4 id="results-on-humanml3d">Results on HumanML3D.</h4>
<p>We compare CMDM with state-of-the-art models across three paradigms: (1) VQ-based [ zhang2023t2m , pinyoanuntapong2024mmm , guo2024momask ] ; (2) Diffusion-based [ mdm2022human , chen2023executing , dai2024motionlcm , huang2024stablemofusion , zhang2025energymogen , hong2025salad ] ; and (3) Autoregressive-based [ meng2024rethinking , xiao2025motionstreamer ] .
As shown inÂ Table 1 , CMDM consistently achieves superior or comparable results across all metrics. CMDM with frame-wise sampling (CMDM w/ FSS) attains the best overall performance, achieving an R-Precision of 0.588/0.778/0.860 (Top-1/2/3), the second lowest FID (0.068), and the highest CLIP-Score (0.685). These results indicate high motion fidelity and strong textâ€“motion alignment.
Compared to standard autoregressive sampling (CMDM w/ AR), FSS further improves temporal stability and smoothness while reducing inference latency. This improvement arises from the causal uncertainty mechanism, where each subsequent frame is generated from partially denoised preceding frames. This design allows the model to reduce the accumulated error of autoregressive and adaptively refine local temporal transitions while maintaining global coherence, leading to smoother motion dynamics and improved semantic alignment for stable generation.</p>
<h4 id="results-on-snapmogen">Results on SnapMoGen.</h4>
<p>We further evaluate the proposed CMDM on the motion clips of SnapMoGen, which contains expressive motion sequences paired with rich textual descriptions. As shown inÂ Table 2 , CMDM achieves state-of-the-art performance across all evaluation metrics, demonstrating its strong generalization to complex motions. CMDM with the frame-wise sampling schedule (CMDM w/ FSS) achieves the best overall results, surpassing all previous VQ-, diffusion-, and autoregressive-based methods. It also achieves the lowest FID score and a high CLIP-Score, indicating superior motion realism and semantic alignment.</p>
<h4 id="long-horizon-motion-generation">Long-Horizon Motion Generation</h4>
<p>To evaluate CMDM on long-horizon motion generation, we compare it with the motion composition method FlowMDM [ barquero2024seamless ] and the autoregressive model MARDM [ meng2024rethinking ] .
Following the protocol of FlowMDM, we synthesize 64 long-horizon sequences on HumanML3D by composing 32 captionâ€“duration pairs per sequence, evaluating 32 subsequences and 31 transitions for local quality and temporal continuity. The ground-truth metrics are computed using randomly sampled motion clips from HumanML3D.
For SnapMoGen, which provides ground-truth long sequences, we select 128 samples with over five continuous motions and use the same captions for generation. We further employ Peak Jerk (PJ) and Area Under the Jerk (AUJ) [ barquero2024seamless ] to measure transition smoothness.
Owing to differences in skeleton scale, the magnitude of the metrics differs from those reported on HumanML3D.
The results are shown inÂ Table 3 . Although FlowMDM reports lower PJ and AUJ values on SnapMoGen, this is primarily because its generated motions often remain static or frozen as can be seen from the ensuing qualitative analysis inÂ Fig. 3 . In contrast, CMDM produces temporally consistent, smoothly transitioning, and realistic long-horizon motions at real-time speed, demonstrating its effectiveness for streaming and continuous text-to-motion generation.</p>
<h3 id="43">4.3 å®šæ€§çµæœ</h3>
<p>åœ– 3 å±•ç¤ºäº† CMDM èˆ‡ FlowMDM [ barquero2024seamless ] å’Œ MARDM [ meng2024rethinking ] åœ¨é•·è¦–è§’å‹•ä½œç”Ÿæˆä¸Šçš„å®šæ€§æ¯”è¼ƒã€‚åœ¨çµ¦å®šä¸€ç³»åˆ—æ¨™é¡Œçš„æƒ…æ³ä¸‹ï¼ŒCMDM ç”Ÿæˆé€£çºŒä¸”ç„¡ç¸«çš„å‹•ä½œï¼Œå…·æœ‰æº–ç¢ºçš„èªç¾©å’Œç‰‡æ®µé–“çš„å¹³æ»‘éæ¸¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒFlowMDM å’Œ MARDM ç¶“å¸¸ç”¢ç”Ÿä¸æ­£ç¢ºçš„å‹•ä½œï¼Œä¾‹å¦‚ã€Œæ²’æœ‰è·³èºã€æˆ–ä¸è‡ªç„¶çš„éæ¸¡ï¼Œä¾‹å¦‚éª¨éª¼ç¿»è½‰ã€‚é€™äº›æ”¹é€²æºè‡ª CMDM çš„å› æœéš±ç·¨ç¢¼ï¼ˆcausal latent encodingï¼‰å’Œå¹€ç´šæ¡æ¨£æ’ç¨‹ï¼Œå…¶æ¢ä»¶åŒ–æ¯ä¸€å¹€ä»¥åŸºæ–¼éƒ¨åˆ†å»å™ªçš„å‰é©…å¹€ï¼Œä»¥ç¢ºä¿ç©©å®šæ€§å’Œæ™‚é–“ä¸€è‡´æ€§ã€‚è«‹åƒè€ƒè£œå……è¦–é »ä»¥ç²å¾—å®Œæ•´çš„å‹•ä½œåºåˆ—å’Œæ›´å¤šå¯è¦–åŒ–çµæœã€‚</p>
<h3 id="44">4.4 åˆ†æ</h3>
<h4 id="_3">è¨ˆç®—æ•ˆç‡</h4>
<p>ç›¸æ¯”æ–¼å…¶ä»–ç¾æœ‰çš„è‡ªè¿´æ­¸æ–¹æ³•ï¼ŒCMDM é€šéå…¶å…ˆé€²çš„æ¶æ§‹å’Œé€å¹€æ¡æ¨£æ’ç¨‹å¯¦ç¾äº†æ¨ç†æ•ˆç‡çš„é‡å¤§æ”¹é€²ï¼Œåœ¨ä¿æŒå“è¶Šå‹•ä½œçœŸå¯¦æ„Ÿçš„åŒæ™‚é¡¯è‘—æ¸›å°‘äº†ç”Ÿæˆæ™‚é–“ã€‚æˆ‘å€‘é€šéåœ¨ NVIDIA A100 GPU ä¸Šç”Ÿæˆ 6 ç§’å‹•ä½œåºåˆ— 100 æ¬¡ä¾†è©•ä¼°æ¡†æ¶çš„è¨ˆç®—æ•ˆç‡ã€‚å› æ­¤ï¼ŒMARDM ä»¥ 310M å€‹åƒæ•¸åœ¨ 20 fps çš„é€Ÿåº¦é‹è¡Œï¼ŒMotionStreamer å‰‡ä»¥ 318M å€‹åƒæ•¸åœ¨ 11 fps çš„é€Ÿåº¦é‹è¡Œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘å€‘æå‡ºçš„ CMDM åƒ…åŒ…å« 114M å€‹åƒæ•¸ï¼ˆåŒ…æ‹¬ MAC-VAE å’Œ Causal-DiTï¼‰ï¼Œä½¿ç”¨æ¨™æº–è‡ªè¿´æ­¸éç¨‹å¯é” 28 fpsï¼Œæ¡ç”¨æå‡ºçš„é€å¹€æ¡æ¨£æ’ç¨‹å‰‡å¯é” 125 fpsã€‚é€™äº›çµæœçªé¡¯äº†æˆ‘å€‘å› æœæ“´æ•£æ¡†æ¶çš„å“è¶Šæ•ˆç‡ä»¥åŠæ‰€ææ¡æ¨£ç­–ç•¥åœ¨å¯¦æ™‚å‹•ä½œç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<h4 id="_4">æ¶ˆèç ”ç©¶</h4>
<p>ç‚ºäº†æ¢ç©¶ CMDM ä¸­å„å€‹çµ„ä»¶çš„å½±éŸ¿ï¼Œæˆ‘å€‘åœ¨ HumanML3D ä¸Šé‡å°å› æœæ½›åœ¨å»ºæ¨¡ã€å› æœæ“´æ•£å¼·åˆ¶å’Œé€å¹€æ¡æ¨£æ’ç¨‹ï¼ˆFSSï¼‰é€²è¡Œæ¶ˆèç ”ç©¶ã€‚</p>
<p>å¦‚è¡¨ 4 æ‰€ç¤ºï¼Œç”¨æ¨™æº– VAE æ›¿æ› MAC-VAE æœƒé¡¯è‘—é™ä½å‹•ä½œå“è³ªã€æ–‡æœ¬ä¿çœŸåº¦å’Œéæ¸¡å¹³æ»‘æ€§ï¼Œç¢ºèªäº†å› æœæ½›åœ¨å»ºæ¨¡å’Œèªæ„ç›£ç£çš„é‡è¦æ€§ã€‚ç§»é™¤å‹•ä½œ-èªè¨€å°é½Šï¼ˆC-VAE w/o MAï¼‰ç”¢ç”Ÿçš„å‹•ä½œå“è³ªèˆ‡ MAC-VAE ç›¸ç•¶ï¼Œä½†å¼•å…¥äº†èªæ„ä¸ä¸€è‡´ï¼Œå¼·èª¿äº†èªæ„ç‰¹å¾µå°æ–¼ä¿æŒä¿çœŸåº¦å’Œé€£è²«æ€§çš„é‡è¦æ€§ã€‚å°‡å› æœæ“´æ•£æ›¿æ›ç‚ºå…¨åºåˆ—æ“´æ•£ï¼ˆw/ Full-Seq. Diff.ï¼‰æœƒå¢åŠ éæ¸¡ FID å’Œ AUJï¼Œé©—è­‰äº†é€å¹€å› æœæ“´æ•£å¼·åˆ¶äº†æ›´å¼·çš„æ™‚é–“ç©©å®šæ€§ã€‚ç§»é™¤ AdaLNï¼ˆw/o AdaLNï¼‰æˆ– ROPEï¼ˆw/o ROPEï¼‰æœƒå°è‡´æ›´é«˜çš„ FID å’Œè¼ƒå¼±çš„é•·æœŸé€£è²«æ€§ï¼ŒåŒæ™‚ç§»é™¤å…©è€…æœƒé€²ä¸€æ­¥æ”¾å¤§é€™äº›æ€§èƒ½ä¸‹é™ã€‚æœ€å¾Œï¼ŒFSS è®Šé«”è¡¨æ˜è¼ƒå°çš„ä¸ç¢ºå®šæ€§å°ºåº¦ï¼ˆ$L{=}5$ï¼‰èƒ½å¯¦ç¾æ›´å¹³æ»‘çš„éæ¸¡å’Œæ›´ä½çš„ AUJï¼Œè€Œéåº¦å¤§çš„ $L$ æˆ–è¼ƒå°çš„ $K$ æœƒé™ä½ç©©å®šæ€§ã€‚é€™äº›çµæœè¡¨æ˜æ‰€æœ‰çµ„ä»¶å…±åŒä¿ƒé€²äº† CMDM çš„é«˜æ€§èƒ½ã€‚</p>
<p>è¡¨ 4ï¼šCMDM çš„æ¶ˆèç ”ç©¶ã€‚</p>
<h2 id="5">5 é™åˆ¶</h2>
<p>é›–ç„¶ CMDM åœ¨æ–‡æœ¬æ¢ä»¶åŒ–å’Œé•·æœŸè¦–è§’å‹•ä½œç”Ÿæˆä¸­é”åˆ°äº†æœ€å…ˆé€²çš„æ•ˆèƒ½ï¼Œä½†ä»å­˜åœ¨å¹¾é …é™åˆ¶ã€‚é¦–å…ˆï¼Œå› æœæ½›åœ¨ç·¨ç¢¼ä¾è³´æ–¼é è¨“ç·´å‹•ä½œ-èªè¨€æ¨¡å‹ï¼ˆå¦‚ Part-TMRï¼‰çš„å‹•ä½œ-èªè¨€å°é½å“è³ªï¼Œåœ¨è™•ç†é«˜åº¦æŠ½è±¡æˆ–æ¨¡ç³Šçš„æ–‡æœ¬æè¿°æ™‚å¯èƒ½æœƒé™åˆ¶æ•ˆèƒ½ã€‚å…¶æ¬¡ï¼Œå„˜ç®¡é€å¹€å–æ¨£æ’ç¨‹å¤§å¹…æ”¹å–„äº†æ¨è«–æ•ˆç‡ï¼Œä½†åœ¨ç”Ÿæˆæ¥µé•·åºåˆ—æ™‚ï¼ˆä¾‹å¦‚è¶…éæ•¸åˆ†é˜ï¼‰ä»å¯èƒ½ç´¯ç©è¼•å¾®çš„æ™‚é–“ç¥å™¨ã€‚ç´å…¥å‹•ä½œæ„ŸçŸ¥åé¥‹æˆ–è‡ªé©æ‡‰é‡éŒ¨å®šæ©Ÿåˆ¶å¯é€²ä¸€æ­¥æ”¹å–„é•·æœŸè¦–è§’ç©©å®šæ€§ã€‚æœ€å¾Œï¼ŒCMDM ä¸»è¦å°ˆæ³¨æ–¼å–®äººå‹•ä½œï¼Œå°šæœªæ“´å±•åˆ°äº’å‹•æˆ–å¤šè§’è‰²å ´æ™¯ [tanaka2023interaction, liang2024intergen, fan2024freemotion, ota2025pino]ï¼Œé€™å°‡æ˜¯æœªä¾†å·¥ä½œçš„ä¸€å€‹æœ‰è¶£æ–¹å‘ã€‚</p>
<h2 id="6">6 çµè«–</h2>
<p>åœ¨æœ¬è«–æ–‡ä¸­ï¼Œæˆ‘å€‘æå‡ºäº† CMDMï¼Œä¸€å€‹çµ±ä¸€çš„æ¡†æ¶ï¼Œçµåˆäº†æ“´æ•£æ¨¡å‹çš„çœŸå¯¦æ€§å’Œç©©å®šæ€§ï¼Œä»¥åŠè‡ªè¿´æ­¸ç”Ÿæˆçš„æ™‚é–“å› æœæ€§å’Œæ•ˆç‡ã€‚CMDM å¼•å…¥äº† MAC-VAE ç”¨æ–¼èªç¾©åŸºç¤çš„å› æœæ½›åœ¨ç·¨ç¢¼ã€ç”¨æ–¼æ™‚é–“æœ‰åºæ“´æ•£å»å™ªçš„ Causal-DiTï¼Œä»¥åŠä½¿èƒ½å¯¦æ™‚æµå¼ç”Ÿæˆçš„ FSSã€‚åœ¨ HumanML3D å’Œ SnapMoGen ä¸Šçš„å»£æ³›å¯¦é©—è¡¨æ˜ï¼Œèˆ‡ç¾æœ‰çš„æ“´æ•£å’Œè‡ªè¿´æ­¸æ¨¡å‹ç›¸æ¯”ï¼ŒCMDM åœ¨å‹•ä½œçœŸå¯¦æ€§ã€èªç¾©å°é½å’Œæ•ˆç‡æ–¹é¢éƒ½é”åˆ°äº†å„ªè¶Šçš„æ•ˆèƒ½ã€‚æˆ‘å€‘ç›¸ä¿¡ CMDM ç‚ºå¯æ“´å±•ã€å¯¦æ™‚å’Œèªç¾©ç›¸å¹²çš„å‹•ä½œç”Ÿæˆæä¾›äº†ä¸€å€‹æœ‰å‰æ™¯çš„æ­¥é©Ÿã€‚</p>
<h2 id="references">References</h2>
<p>è£œå……ææ–™</p>
<h2 id="appendix-a">Appendix A å¯¦ç¾ç´°ç¯€</h2>
<h3 id="a1-mac-vae">A.1 MAC-VAE</h3>
<p>The proposed MAC-VAE consists of seven causal convolutional layers and two causal ResNet blocks with left padding in both the encoder and decoder to ensure strict temporal causality. Each convolutional layer uses a kernel size of 3 and a stride of 1, followed by ReLU activation. The latent feature dimension is set to 64, and motion sequences are downsampled/upsampled by a factor of 4 along the temporal axis using stride-2 convolutional layers within the ResNet blocks.</p>
<p>To achieve semantic alignment between motion and text, we modify Part-TMR [ yu2025remogpt ] to extract frame-level motionâ€“language embeddings. Part-TMR uses a [class] token to aggregate frames into a global feature, whereas we directly extract features from each frame and align them with the corresponding text features via contrastive learning, which serves as the supervision signal for MAC-VAE. The loss weighting coefficient is set to Î² = 1.0 \beta{=}1.0 , and the margin parameters are set to m 1 = 0.5 m_{1}{=}0.5 and m 2 = 0.25 m_{2}{=}0.25 .</p>
<p>We train MAC-VAE using the AdamW optimizer with a learning rate of 1 Ã— 10 âˆ’ 4 1{\times}10^{-4} and a batch size of 128 for 50 epochs on a single NVIDIA A100 GPU. The learning rate follows a cosine decay schedule, and gradient clipping with a maximum norm of 1.0 is applied for training stability.</p>
<h3 id="a2-causal-dit">A.2 Causal-DiT</h3>
<p>The Causal-DiT is implemented as a lightweight transformer-based denoiser with 8 layers, 4 attention heads, and a hidden dimension of 512. Causal self-attention is applied using a lower-triangular mask to enforce temporal order, while cross-attention conditions motion latents on text embeddings extracted from DistilBERT [ sanh2019distilbert ] . We incorporate Adaptive Layer Normalization (AdaLN) [ peebles2023scalable ] and Rotary Positional Encoding (ROPE) [ su2024roformer ] to embed timestep information and stabilize long-horizon attention. During training, the text condition is randomly dropped with a probability of 0.1 to enable classifier-free guidance. The model is optimized using AdamW with the same hyperparameter settings as MAC-VAE. The scale of classifier-free guidance is set to 3.0 during inference.</p>
<h3 id="a3">A.3 å› æœæ“´æ•£å¼·åˆ¶</h3>
<p>åœ¨ CMDM ä¸­ï¼Œæ¡ç”¨å› æœæ“´æ•£å¼·åˆ¶ä¾†å¯¦ç¾æ™‚é–“æœ‰åºçš„å»å™ªï¼ŒåŒæ™‚ä¿æŒå¹€ç´šéš¨æ©Ÿæ€§ã€‚åœ¨è¨“ç·´æœŸé–“ï¼Œæ¯ä¸€å¹€ $t$ å—åˆ°ç¨ç«‹å™ªéŸ³ç­‰ç´š $k_{t}\in[0,K]$ çš„æ“¾å‹•ï¼Œå…¶ä¸­ $K{=}1000$ è¡¨ç¤ºæ“´æ•£æ­¥æ•¸çš„ç¸½æ•¸ã€‚Causal-DiT ä½œç‚ºå»å™ªå™¨ï¼Œå­¸ç¿’é æ¸¬å™ªéŸ³æ®˜å·® $\boldsymbol{\epsilon}<em>{\theta}(\tilde{\mathbf{z}}</em>{\leq t},k_{t},\mathbf{c})ï¼Œè©²æ®˜å·®ä»¥æ‰€æœ‰å‰åºæ½›åœ¨å¹€å’Œæ–‡æœ¬åµŒå…¥ $\mathbf{c}$ ç‚ºæ¢ä»¶ã€‚é€™å€‹å…¬å¼åŒ–ç¢ºä¿æ¯ä¸€å¹€åƒ…åŸºæ–¼å…¶å› æœæ­·å²é€²è¡Œå»å™ªï¼Œå¾è€Œå¼·åˆ¶åŸ·è¡Œåš´æ ¼çš„æ™‚é–“ä¾è³´é—œä¿‚ã€‚æ•´é«”è¨“ç·´éç¨‹åœ¨æ¼”ç®—æ³• 1 ä¸­ç¸½çµã€‚</p>
<p>åœ¨æ¨è«–éç¨‹ä¸­ï¼Œæˆ‘å€‘æ¡ç”¨å¹€ç´šæ¡æ¨£æ’ç¨‹ï¼ˆFSSï¼‰ï¼Œå…¶ä¸­æ“´æ•£æ­¥æ•¸ $K{=}50$ å’Œä¸ç¢ºå®šæ€§å°ºåº¦ $L{=}2$ã€‚åœ¨æ­¤è¨­å®šä¸­ï¼Œç¬¬ $t{+}1$ å¹€çš„å»å™ªåœ¨ç¬¬ $t$ å¹€çš„ç¬¬ $K{-}L$ æ­¥é–‹å§‹ï¼Œå…è¨±éƒ¨åˆ†å»å™ªçš„å¹€å¼•å°å¾ŒçºŒç”Ÿæˆã€‚é€™å€‹å› æœæ’ç¨‹æ©Ÿåˆ¶é€šéæ¸›å°‘å†—é¤˜çš„æ“´æ•£æ­¥é©Ÿï¼ŒåŒæ™‚ä¿æŒè·¨å¹€çš„æ™‚é–“ä¸€è‡´æ€§ï¼Œé¡¯è‘—åŠ å¿«æ¨è«–é€Ÿåº¦ã€‚ä½¿ç”¨ FSS çš„æ•´é«”æ¨è«–éç¨‹åœ¨æ¼”ç®—æ³• 2 ä¸­ç¸½çµã€‚</p>
<p>æ¼”ç®—æ³• 1ï¼šå…·æœ‰å› æœæ“´æ•£å¼·åˆ¶çš„ CMDM è¨“ç·´</p>
<p>æ¼”ç®—æ³• 2ï¼šå…·æœ‰å¹€ç´šæ¡æ¨£æ’ç¨‹ï¼ˆFSSï¼‰çš„ CMDM æµå¼ç”Ÿæˆ</p>
<h2 id="b">é™„éŒ„ B é¡å¤–çš„å®šé‡çµæœ</h2>
<h3 id="b1-babel">B.1 BABEL ä¸Šçš„å¯¦é©—</h3>
<p>æˆ‘å€‘é€²ä¸€æ­¥åœ¨ BABEL è³‡æ–™é›† [punnakkal2021babel] ä¸Šè©•ä¼° CMDMï¼Œä»¥è©•ä¼°å…¶å°å¤šæ¨£åŒ–å‹•ä½œçµ„æˆçš„æ³›åŒ–èƒ½åŠ›ã€‚
BABEL åŒ…å«å…·æœ‰å¤šå€‹å‹•ä½œå’Œè½‰æ›çš„å¯†é›†æ¨™è¨»åºåˆ—ï¼Œé©åˆç”¨æ–¼é•·æœŸå‹•ä½œåˆæˆå’Œè©•ä¼°ã€‚
æˆ‘å€‘é€šéå¾ BABEL ä¸­ç›¸é„°çš„å­åºåˆ—æ§‹é€ è¨“ç·´æ¨£æœ¬ä¾†è¨“ç·´ CMDMï¼Œå…¶ä¸­æ¯å°é€£çºŒçš„ç‰‡æ®µç”¨æ–¼å­¸ç¿’è·¨é•·åºåˆ—çš„å‹•ä½œå»¶çºŒã€‚
å¦‚è¡¨ 5 æ‰€ç¤ºï¼Œæˆ‘å€‘çš„æ–¹æ³•åœ¨å­åºåˆ—å’Œè½‰æ›æŒ‡æ¨™ä¸­éƒ½å¯¦ç¾äº†æœ€ä½³çš„æ•´é«”æ•ˆèƒ½ï¼Œè­‰æ˜äº† CMDM åœ¨ä¿æŒå‹•ä½œé‚Šç•Œä¸€è‡´æ€§å’Œç”Ÿæˆå¹³æ»‘ã€é€£çºŒå‹•ä½œæ–¹é¢çš„å„ªå‹¢ã€‚</p>
<p>è¡¨ 5ï¼šBABEL ä¸Šçš„é•·æœŸå‹•ä½œç”Ÿæˆæ¯”è¼ƒã€‚
å­åºåˆ—æŒ‡æ¨™è©•ä¼°ç‰‡æ®µå…§çš„å‹•ä½œè³ªé‡å’Œå¤šæ¨£æ€§ï¼Œè€Œè½‰æ›æŒ‡æ¨™è©•ä¼°ç‰‡æ®µé–“çš„æ™‚é–“é€£çºŒæ€§å’Œå¹³æ»‘æ€§ã€‚</p>
<h3 id="b2-evaluation-on-other-motion-features">B.2 Evaluation on Other Motion Features</h3>
<p>To further examine the generalization ability of CMDM, we conduct experiments using motion features with redundant dimensions removed, following the analysis in [ meng2024rethinking ] .
As discussed in prior work, the standard HumanML3D motion representation contains redundant components such as local joint rotations and contact features that do not directly influence the final human pose.
Removing these redundant features yields a more compact and physically meaningful representation better suited for continuous diffusion modeling.</p>
<p>Table 6 reports the results on HumanML3D using only essential motion features.
Compared to the baseline methods, CMDM consistently improves generation quality and semantic alignment under both autoregressive (AR) and diffusion (FSS) configurations.
Specifically, CMDM w/ FSS achieves the best overall performance, reaching an R-Precision of 0.563/0.759/0.849 for Top-1/Top-2/Top-3 accuracy and the lowest FID of 0.078, confirming that our causal diffusion formulation effectively models temporally coherent motion even in compact feature spaces.
These results demonstrate that CMDM remains robust across different motion representations, further validating its adaptability to feature compression and reparameterized motion distributions.</p>
<p>Table 6 : Results of text-to-motion generation on HumanML3D without redundant features.
The average is reported over 10 runs with 95% confidence intervals. Bold indicates the best result, and underline denotes the second-best result.</p>
<h3 id="b3-compositional-motion-generation">B.3 Compositional Motion Generation</h3>
<p>We evaluate CMDM on the compositional motion generation task following the protocol of Multi-Track Timeline (MTT) [ petrovich2024multi ] , which requires generating coherent motions conditioned on multiple temporally structured text descriptions. This task evaluates both semantic composition, i.e . , correctly realizing multiple concepts within a single sequence, and temporal composition, i.e . , ensuring smooth and consistent transitions across segments.</p>
<p>Specifically, following prior work [ petrovich2024multi , zhang2025energymogen ] , we report per-crop semantic correctness metrics (R@1, R@3, and TMR-Score for M2T and M2M), as well as realism metrics including FID and transition distance. As shown in Table 7 , CMDM, under the single-track multi-crop setting, consistently outperforms EnergyMoGen and other compositional baselines across all metrics. Notably, CMDM achieves substantial improvements in semantic alignment while simultaneously reducing FID and transition distance, demonstrating stronger long-horizon consistency and smoother transitions between composed motion segments.</p>
<p>Table 7 : Comparison with prior compositional motion generation methods on the Multi-track timeline (MTT) dataset [ petrovich2024multi ] .</p>
<h3 id="b4">B.4 æ½›å»¶é²åˆ†æ</h3>
<p>ç‚ºäº†è©•ä¼°ä¸åŒå› æœå‹•ä½œç”Ÿæˆæ–¹æ³•çš„å¯¦éš›æ•ˆç‡ï¼Œæˆ‘å€‘åœ¨å–®å€‹ NVIDIA A100 GPU ä¸Šæ¸¬é‡ç”Ÿæˆæ¯å€‹ tokenï¼ˆ4 å¹€ï¼‰çš„æ½›å»¶é²ã€‚
MARDM [ meng2024rethinking ]ã€MotionStreamer [ xiao2025motionstreamer ] å’Œ CMDM w/ AR åˆ†åˆ¥éœ€è¦ç´„ 210 msã€360 ms å’Œ 150 ms ä¾†ç”Ÿæˆç¬¬ä¸€å€‹ tokenï¼Œéš¨å¾Œæ¯å€‹ token çš„æ½›å»¶é²é¡ä¼¼ã€‚
é€™æ˜¯å› ç‚ºé€™äº›è‡ªè¿´æ­¸æ“´æ•£æ–¹æ³•å°æ¯å€‹ token ç¨ç«‹åŸ·è¡Œå®Œæ•´çš„æ“´æ•£å»å™ªï¼Œæ¯å¹€éœ€è¦å¤šå€‹å»å™ªæ­¥é©Ÿï¼Œç„¡è«–å…¶æ™‚é–“ä½ç½®å¦‚ä½•ã€‚
ç›¸æ¯”ä¹‹ä¸‹ï¼ŒCMDM w/ FSS çš„ç¬¬ä¸€å€‹ token éœ€è¦ç´„ 220 msï¼Œä½†å¾ŒçºŒæ¯å€‹ token åƒ…éœ€ 30 msï¼Œåœ¨ä¸²æµç”Ÿæˆä¸­å¯¦ç¾äº† 5 Ã— 5\times â€“ 12 Ã— 12\times çš„åŠ é€Ÿã€‚
æ¯å€‹ token æ½›å»¶é²çš„é¡¯è‘—ä¸‹é™æºæ–¼æˆ‘å€‘çš„å¹€ç´šæ¡æ¨£æ’ç¨‹ï¼Œå®ƒå…è¨±å¾éƒ¨åˆ†å»å™ªçš„å‰åºå¹€é æ¸¬æ¯ä¸€å¹€ï¼Œè€Œç„¡éœ€é€²è¡Œå®Œæ•´çš„è¿­ä»£ç´°åŒ–ã€‚</p>
<h3 id="b5">B.5 æ¶ˆèç ”ç©¶</h3>
<h4 id="mac-vae">MAC-VAE çš„æ¶æ§‹</h4>
<p>æˆ‘å€‘è©•ä¼°äº† MAC-VAE çš„å¤šç¨®é…ç½®ï¼Œä»¥åˆ†ææ½›åœ¨ç¶­åº¦å’Œæ™‚é–“ä¸‹æ¡æ¨£ç‡å°é‡å»ºå’Œç”Ÿæˆæ€§èƒ½çš„å½±éŸ¿ã€‚
ç¬¦è™Ÿ $(d, r)$ è¡¨ç¤ºæ½›åœ¨ç¶­åº¦ $d$ å’Œæ™‚é–“ä¸‹æ¡æ¨£ç‡ $r$ã€‚
å¦‚è¡¨ 8 æ‰€ç¤ºï¼Œå¢åŠ æ½›åœ¨ç¶­åº¦æ”¹å–„äº†é‡å»ºæº–ç¢ºåº¦ï¼Œä½†ä¹Ÿå¼•å…¥äº†å†—é¤˜ï¼Œé€™æœƒç•¥å¾®å½±éŸ¿ç”Ÿæˆå“è³ªï¼ˆä»¥ FID è¡¡é‡ï¼‰ã€‚
ç›¸ååœ°ï¼Œè¼ƒå¤§çš„æ™‚é–“ä¸‹æ¡æ¨£ç‡ï¼ˆä¾‹å¦‚ $r=1/8$ï¼‰æœƒé™ä½æ™‚é–“è§£æåº¦ï¼Œä¸¦å°è‡´ R-Precision å’Œ MM-Dist å› è³‡è¨Šéºå¤±è€Œç•¥å¾®ä¸‹é™ã€‚
åœ¨æ‰€æœ‰é…ç½®ä¸­ï¼ŒMAC-VAE é…ç½® $(64, 1/4)$ åœ¨é‡å»ºä¿çœŸåº¦ï¼ˆFID = 0.000ã€MPJPE = 0.012ï¼‰å’Œç”Ÿæˆå“è³ªï¼ˆR-Top1 = 0.588ã€FID = 0.068ã€MM-Dist = 2.620ï¼‰ä¹‹é–“é”åˆ°æœ€ä½³å¹³è¡¡ï¼Œæˆ‘å€‘åœ¨æ‰€æœ‰å¾ŒçºŒå¯¦é©—ä¸­æ¡ç”¨æ­¤ç‚ºé è¨­è¨­å®šã€‚
é€™äº›çµæœè­‰å¯¦äº†å…·æœ‰é©åº¦æ™‚é–“å£“ç¸®çš„ç·Šæ¹Šæ½›åœ¨ç©ºé–“èƒ½å¤ æœ‰æ•ˆåœ°æ•æ‰èªç¾©å’Œæ™‚é–“ä¾è³´é—œä¿‚ä»¥ä¾›ä¸‹æ¸¸å‹•ä½œç”Ÿæˆä½¿ç”¨ã€‚</p>
<p>è¡¨ 8ï¼šHumanML3D ä¸Šé‡å»ºå’Œç”Ÿæˆæ€§èƒ½çš„æ¯”è¼ƒã€‚MPJPE ä»¥æ¯«ç±³ç‚ºå–®ä½æ¸¬é‡ã€‚ç¬¦è™Ÿ $(d, r)$ è¡¨ç¤ºæ½›åœ¨ç¶­åº¦ $d$ å’Œæ™‚é–“ä¸‹æ¡æ¨£ç‡ $r$ã€‚</p>
<h4 id="-">å‹•ä½œ-èªè¨€æ¨¡å‹</h4>
<p>ç‚ºäº†è©•ä¼°ä¸åŒå‹•ä½œ-èªè¨€å°é½Šç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘å€‘æ¯”è¼ƒäº†æ•´åˆåˆ° MAC-VAE æ¡†æ¶ä¸­çš„å¤šå€‹é è¨“ç·´å‹•ä½œ-èªè¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ TMR [ petrovich2023tmr ]ã€MotionPatches [ yu2024exploring ] å’Œ Part-TMR [ yu2025remogpt ]ã€‚
å¦‚è¡¨ 9 æ‰€ç¤ºï¼Œèˆ‡åŸºç·š VAE å’Œ C-VAE ç›¸æ¯”ï¼Œæ‰€æœ‰å‹•ä½œ-èªè¨€æ¨¡å‹éƒ½æ”¹å–„äº†ç”Ÿæˆå“è³ªåŒæ™‚ä¿æŒé‡å»ºæ€§èƒ½ï¼Œé€™è­‰æ˜äº†å‹•ä½œå’Œæ–‡å­—ä¹‹é–“èªç¾©å°é½Šçš„æœ‰æ•ˆæ€§ã€‚
å…¶ä¸­ï¼ŒPart-TMR ä»¥æœ€ä½çš„é‡å»ºèª¤å·®ï¼ˆFID = 0.000ã€MPJPE = 0.012ï¼‰å’Œæœ€é«˜çš„ R-Precisionï¼ˆ0.588ï¼‰é”åˆ°äº†æœ€ä½³çš„æ•´é«”æ€§èƒ½ï¼Œç¢ºèªäº†å…¶æ•æ‰æ–‡å­—å’Œå‹•ä½œä¹‹é–“ç´°ç²’åº¦éƒ¨åˆ†ç´šå°æ‡‰çš„å¼·å¤§èƒ½åŠ›ã€‚
é€™äº›çµæœé©—è­‰äº†é¸æ“‡ Part-TMR ä½œç‚º MAC-VAE ä¸­å°é½Šéª¨å¹¹çš„æ­£ç¢ºæ€§ï¼Œå¾è€Œèƒ½å¤ å¯¦ç¾æ›´å…·èªç¾©é€£è²«æ€§å’Œæ™‚é–“ä¸€è‡´æ€§çš„å‹•ä½œç”Ÿæˆã€‚</p>
<p>è¡¨ 9ï¼šHumanML3D ä¸Š MAC-VAE ä¸­å‹•ä½œ-èªè¨€æ¨¡å‹çš„æ¯”è¼ƒã€‚MPJPE ä»¥æ¯«ç±³ç‚ºå–®ä½æ¸¬é‡ã€‚</p>
<h4 id="causal-dit">Causal-DiT çš„æ¨¡å‹å¤§å°</h4>
<p>æˆ‘å€‘é€šéæ”¹è®Š Causal-DiT ä¸­çš„æ³¨æ„åŠ›é ­æ•¸ï¼ˆ$H$ï¼‰ã€å±¤æ•¸ï¼ˆ$L$ï¼‰å’Œéš±è—ç¶­åº¦ï¼ˆ$D$ï¼‰ä¾†èª¿æŸ¥æ¨¡å‹å¤§å°å°ç”Ÿæˆå“è³ªçš„å½±éŸ¿ã€‚
å¦‚è¡¨ 10 æ‰€ç¤ºï¼Œç”±æ–¼è¡¨ç¤ºå®¹é‡å¢åŠ ï¼Œè¼ƒå¤§çš„æ¨¡å‹é€šå¸¸èƒ½é”åˆ°æ›´å¥½çš„æ€§èƒ½ã€‚
ä¸­ç­‰è¦æ¨¡çš„æ¨¡å‹ï¼ˆ38M åƒæ•¸ï¼‰å·²ç¶“æä¾›äº†å¼·å¤§çš„çµæœï¼ŒR-Precision ç‚º 0.588ã€FID ç‚º 0.068ï¼Œåœ¨å“è³ªå’Œæ•ˆç‡ä¹‹é–“é”åˆ°å¹³è¡¡ã€‚
é€²ä¸€æ­¥æ“´å±•åˆ° 304M åƒæ•¸ç²å¾—äº†é‚Šéš›æ”¹é€²ï¼ˆR-Precision = 0.590ã€FID = 0.042ï¼‰ï¼Œè¨¼æ˜äº† Causal-DiT èƒ½æœ‰æ•ˆæ“´å±•åŒæ™‚ä¿æŒè¨ˆç®—å¯¦ç”¨æ€§ã€‚
é™¤éå¦æœ‰èªªæ˜ï¼Œå¦å‰‡æˆ‘å€‘åœ¨æ‰€æœ‰ä¸»è¦å¯¦é©—ä¸­ä½¿ç”¨ä¸­ç­‰è¦æ¨¡ï¼ˆ38Mï¼‰é…ç½®ã€‚</p>
<p>è¡¨ 10ï¼šHumanML3D ä¸Šæ¨¡å‹å¤§å°çš„æ¯”è¼ƒã€‚ç¬¦è™Ÿ $(H, L, D)$ è¡¨ç¤ºæ³¨æ„åŠ›é ­æ•¸ $H$ã€å±¤æ•¸ $L$ å’Œéš±è—ç¶­åº¦ $D$ã€‚</p>
<h4 id="_5">æ–‡æœ¬ç·¨ç¢¼å™¨</h4>
<p>è¡¨ 11ï¼šHumanML3D ä¸Šæ–‡æœ¬ç·¨ç¢¼å™¨çš„æ¯”è¼ƒã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.22594/x4.png" loading="lazy"></figure> åœ– 4ï¼šHumanML3D ä¸Šé•·åºåˆ—å‹•ä½œç”Ÿæˆçš„å®šæ€§çµæœã€‚æˆ‘å€‘çš„ CMDM èˆ‡å…ˆå‰æ–¹æ³•çš„æ¯”è¼ƒã€‚ç”Ÿæˆçš„å‹•ä½œæ˜¯é€£çºŒç„¡ç¸«çš„ï¼›ç‚ºäº†å¯è¦–åŒ–ç›®çš„ï¼Œæˆ‘å€‘å°‡æ¯å€‹é•·åºåˆ—åˆ†å‰²æˆèˆ‡å…¶å°æ‡‰æ¨™é¡Œç›¸å°æ‡‰çš„è¼ƒçŸ­æ®µã€‚è«‹åƒè€ƒè£œå……ææ–™ä¸­çš„è¦–é »ä»¥ç²å–å®Œæ•´çš„å‹•ä½œåºåˆ—ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.22594/x5.png" loading="lazy"></figure> åœ– 5ï¼šSnapMoGen ä¸Šé•·åºåˆ—å‹•ä½œç”Ÿæˆçš„å®šæ€§çµæœã€‚æˆ‘å€‘çš„ CMDM èˆ‡å…ˆå‰æ–¹æ³•çš„æ¯”è¼ƒã€‚ç”Ÿæˆçš„å‹•ä½œæ˜¯é€£çºŒç„¡ç¸«çš„ï¼›ç‚ºäº†å¯è¦–åŒ–ç›®çš„ï¼Œæˆ‘å€‘å°‡æ¯å€‹é•·åºåˆ—åˆ†å‰²æˆèˆ‡å…¶å°æ‡‰æ¨™é¡Œç›¸å°æ‡‰çš„è¼ƒçŸ­æ®µã€‚è«‹åƒè€ƒè£œå……ææ–™ä¸­çš„è¦–é »ä»¥ç²å–å®Œæ•´çš„å‹•ä½œåºåˆ—ã€‚</p>
<p>æˆ‘å€‘æ¯”è¼ƒäº†å¤šå€‹é è¨“ç·´èªè¨€æ¨¡å‹ä½œç‚ºæ–‡æœ¬ç·¨ç¢¼å™¨ï¼Œä»¥è©•ä¼°å®ƒå€‘å°èªç¾©å°é½å’Œå‹•ä½œå“è³ªçš„å½±éŸ¿ã€‚å¦‚è¡¨ 11 æ‰€ç¤ºï¼Œæ–‡æœ¬ç·¨ç¢¼å™¨çš„é¸æ“‡å½±éŸ¿äº†æ–‡å­—-å‹•ä½œå°æ‡‰ï¼ˆR-Precisionï¼‰å’Œè¦–è¦ºçœŸå¯¦æ€§ï¼ˆFIDï¼‰ã€‚DistilBERT [ sanh2019distilbert ]ï¼Œæä¾›è©ç´šåµŒå…¥ï¼Œä»¥æœ€é«˜çš„ R-Precisionï¼ˆ0.588ï¼‰å’Œæœ€ä½çš„ FIDï¼ˆ0.068ï¼‰é”åˆ°äº†æœ€ä½³çš„æ•´é«”æ€§èƒ½ï¼Œè­‰æ˜äº†å…¶æ•æ‰èˆ‡å‹•ä½œç‰¹å¾µå°é½è‰¯å¥½çš„ç´°ç²’åº¦èªç¾©ç·šç´¢çš„èƒ½åŠ›ã€‚ä½¿ç”¨åŸºæ–¼ CLIP çš„ç·¨ç¢¼å™¨ï¼Œè©ç´šè®Šé«”ï¼ˆèˆ‡ StableMoFusion [ huang2024stablemofusion ] ä¸­æ¡ç”¨çš„ç·¨ç¢¼å™¨ç›¸åŒï¼‰ä¹Ÿå„ªæ–¼ StableMoFusionï¼Œé€²ä¸€æ­¥ç¢ºèªäº†è©ç´šè¡¨ç¤ºçš„å„ªå‹¢ã€‚é€™äº›æ¨™è¨˜ç´šåµŒå…¥å°æ–¼ç¶­æŒèªè¨€æ¨™è¨˜å’Œå‹•ä½œå¹€ä¹‹é–“çš„å› æœä¾è³´è‡³é—œé‡è¦ï¼Œé€™å°æ–¼ CMDM ä¸­çš„ç©©å®šè‡ªè¿´æ­¸ç”Ÿæˆæ˜¯å¿…è¦çš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¾†è‡ª CLIP [ radford2021learning ] çš„å¥ç´šåµŒå…¥ç”±æ–¼æ™‚é–“ç²’åº¦æå¤±è€Œè¡¨ç¾å‡ºé™ä½çš„ç²¾åº¦å’Œæ›´é«˜çš„ FIDã€‚åŒæ™‚ï¼ŒSentence-T5 [ ni2022sentence ] å„ªæ–¼åŸºæ–¼ CLIP çš„æ¨¡å‹ï¼Œä¹Ÿå„ªæ–¼ MotionLCM V2 [ huang2024stablemofusion ]ï¼Œå„˜ç®¡ MotionLCM V2 ä¹Ÿä½¿ç”¨ Sentence-T5ã€‚é€™äº›ç™¼ç¾é©—è­‰äº†æˆ‘å€‘é¸æ“‡ DistilBERT ä½œç‚º CMDM æ–‡æœ¬ç·¨ç¢¼å™¨çš„æ­£ç¢ºæ€§ï¼Œå› ç‚ºå®ƒæœ‰æ•ˆåœ°ä¿ç•™äº†å±€éƒ¨èªç¾©ä¸¦å¯¦ç¾äº†å› æœä¸€è‡´çš„å‹•ä½œ-èªè¨€å»ºæ¨¡ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.22594/x6.png" loading="lazy"></figure> åœ– 6ï¼šHumanML3D ä¸Šæ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆçš„å®šæ€§çµæœã€‚CMDM ç”Ÿæˆçš„å‹•ä½œç›¸æ¯”å…ˆå‰æ–¹æ³•èƒ½æ›´å¥½åœ°æ•æ‰ç´°ç²’åº¦çš„æ–‡æœ¬èªç¾©ä¸¦ä¿æŒè‡ªç„¶çš„èº«é«”æ§‹æˆã€‚è«‹åƒè€ƒè£œå……è¦–é »ä»¥ç²å–æ›´æ¸…æ™°çš„å¯è¦–åŒ–ã€‚</p>
<p><figure class="paper-figure"><img src="../../figures/2026-02-27/2602.22594/x7.png" loading="lazy"></figure> åœ– 7ï¼šSnapMoGen ä¸Šæ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆçš„å®šæ€§çµæœã€‚æˆ‘å€‘çš„ CMDM èˆ‡å…ˆå‰æ–¹æ³•çš„æ¯”è¼ƒã€‚æˆ‘å€‘ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬æç¤ºè€Œç„¡éœ€ä»»ä½•åŸºæ–¼ LLM çš„æ“´å……ï¼ŒCMDM ä»ç„¶é”åˆ°å¼·å¤§çš„ç”Ÿæˆå“è³ªã€‚è«‹åƒè€ƒè£œå……è¦–é »ä»¥ç²å–æ›´æ¸…æ™°çš„å¯è¦–åŒ–ã€‚</p>
<h2 id="c">é™„éŒ„ C é¡å¤–çš„å®šæ€§çµæœ</h2>
<p>ç‚ºé€²ä¸€æ­¥å±•ç¤º CMDM çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘å€‘æä¾›äº†é•·åºåˆ—å’Œæ–‡æœ¬åˆ°å‹•ä½œç”Ÿæˆæ–¹é¢çš„é¡å¤–å®šæ€§æ¯”è¼ƒã€‚
åœ– 4 å’Œåœ– 5 åˆ†åˆ¥åœ¨ HumanML3D å’Œ SnapMoGen ä¸Šæ¯”è¼ƒäº† CMDM èˆ‡ FlowMDM [ barquero2024seamless ] å’Œ MARDM [ meng2024rethinking ] åœ¨é•·åºåˆ—å‹•ä½œç”Ÿæˆä¸Šçš„è¡¨ç¾ã€‚
CMDM ç”¢ç”Ÿåœ¨æ™‚é–“ä¸Šé€£è²«ä¸”èªç¾©ä¸Šæº–ç¢ºçš„å‹•ä½œï¼Œæ²’æœ‰å…§å®¹æ¼‚ç§»æˆ–éª¨éª¼ç¿»è½‰ç¾è±¡ï¼Œè€Œå…ˆå‰çš„æ–¹æ³•ç¶“å¸¸é­å—éœæ­¢å§¿æ…‹ã€ä¸æ­£ç¢ºçš„è½‰æ›æˆ–è·¨ç‰‡æ®µçš„ä¸ä¸€è‡´å‹•ä½œå•é¡Œã€‚
é€™äº›ä¾‹å­çªå‡ºäº† CMDM åœ¨ä¿æŒå¹³é †çš„æ™‚é–“å‹•æ…‹å’Œå› æœä¸€è‡´æ€§æ–¹é¢è²«ç©¿æ•´å€‹é•·åºåˆ—çš„èƒ½åŠ›ã€‚</p>
<p>åœ– 6 å±•ç¤ºäº† HumanML3D ä¸Šçš„å®šæ€§çµæœã€‚
èˆ‡ MoMask [ guo2024momask ]ã€MotionLCM [ dai2024motionlcm ] å’Œ StableMoFusion [ huang2024stablemofusion ] ç›¸æ¯”ï¼ŒCMDM ç”Ÿæˆçš„å‹•ä½œæ›´å¿ å¯¦åœ°åæ˜ ç´°ç²’åº¦çš„æ–‡æœ¬èªç¾©ï¼ˆä¾‹å¦‚ï¼Œæ‰‹è‡‚æ—‹è½‰ã€è…¿éƒ¨é‹å‹•æˆ–è¡Œèµ°æ–¹å‘ï¼‰ï¼ŒåŒæ™‚ä¿æŒè‡ªç„¶çš„èº«é«”é—œç¯€é‹å‹•ã€‚
åœ– 7 å±•ç¤ºäº† SnapMoGen ä¸Šçš„é¡å¤–çµæœï¼Œå…¶ä¸­ CMDM ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬æç¤ºè€Œç„¡éœ€åŸºæ–¼ LLM çš„å¢å¼·ï¼Œä»ç„¶ç”¢ç”Ÿæ¯”å…ˆå‰æ–¹æ³•æ›´é€¼çœŸçš„å‹•ä½œã€‚</p>
<p>è«‹åƒè€ƒæ¼”ç¤ºé é¢ä¸Šçš„è£œå……å½±ç‰‡ä»¥ç²å¾—å®Œæ•´é•·åº¦çš„è¦–è¦ºåŒ–ã€‚</p>
<h2 id="d">é™„éŒ„ D ç¯„ä¾‹ç¨‹å¼ç¢¼</h2>
<p>ç¨‹å¼ç¢¼å°‡åœ¨ https://github.com/YU1ut/CMDM ç™¼ä½ˆã€‚æˆ‘å€‘æä¾›äº†ä½¿ç”¨ HumanML3D è³‡æ–™é›†æ§‹å»ºå’Œè©•ä¼°æ‰€æè­° CMDM çš„è¨“ç·´ç¨‹å¼ç¢¼ã€‚è«‹åƒè€ƒç¨‹å¼ç¢¼ç›®éŒ„ä¸­çš„ README æª”æ¡ˆä»¥ç²å–è©³ç´°è³‡è¨Šã€‚</p>
  
</div>


  </main>

  <footer class="site-footer">
    <div class="container">
      <p>æ¯æ—¥è‡ªå‹•æŠ“å– <a href="https://huggingface.co/papers" target="_blank">HuggingFace Daily Papers</a>ï¼Œä»¥ Claude AI ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚</p>
    </div>
  </footer>
</body>
</html>