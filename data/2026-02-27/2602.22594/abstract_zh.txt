# 論文摘要翻譯

動作擴散模型（motion diffusion models）近年來的進展已大幅改善了人類動作合成的真實性。然而，現有方法要麼依賴於具有雙向生成的完整序列擴散模型，這限制了時間因果性和實時適用性，要麼依賴於受不穩定性和累積誤差困擾的自迴歸模型。在本研究中，我們提出因果動作擴散模型（Causal Motion Diffusion Models, CMDM），一個基於在語義對齊潛在空間中運作的因果擴散 Transformer 的統一自迴歸動作生成框架。CMDM 建立在動作-語言-對齊因果 VAE（Motion-Language-Aligned Causal VAE, MAC-VAE）之上，該 VAE 將動作序列編碼為時間因果潛在表示。在此潛在表示之上，訓練一個自迴歸擴散 Transformer，使用因果擴散強制（causal diffusion forcing）來執行跨動作幀的時間有序去噪。為實現快速推論，我們引入了具有因果不確定性的逐幀採樣策略（frame-wise sampling schedule），其中每個後續幀從部分去噪的前一幀進行預測。由此產生的框架支持高品質文本-動作生成、流式合成和以互動速率進行的長視野動作生成。在 HumanML3D 和 SnapMoGen 上的實驗表明，CMDM 在語義保真度和時間平滑性上都優於現有的擴散和自迴歸模型，同時大幅降低推論延遲。