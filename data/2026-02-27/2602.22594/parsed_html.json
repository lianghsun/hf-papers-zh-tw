{
  "source": "html",
  "markdown": "# Causal Motion Diffusion Models for Autoregressive Motion Generation\n\nRecent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.\n\n## 1 Introduction\n\nSynthesizing realistic human motion conditioned on natural language remains a fundamental problem in computer vision and graphics. A successful text-to-motion generation model should not only synthesize spatially accurate body movements but also maintain temporal coherence across long sequences. Recent progress in motion diffusion models [ mdm2022human , zhang2022motiondiffuse , chen2023executing , dai2024motionlcm ] has led to significant improvements in motion quality and diversity, benefiting from the strong generative capacity of diffusion-based frameworks [ ho2020denoising , dhariwal2021diffusion ] . However, most existing diffusion models rely on bidirectional denoising over the entire sequence, which inherently breaks temporal causality and prevents online generation.\n\n[FIGURE:x1.png] Figure 1 : Overview of the existing methods and the proposed method. Existing diffusion-based methods (left) perform full-sequence denoising using the same noise level across all frames. In contrast, our proposed CMDM (right) introduces a causal diffusion forcing mechanism that operates on semantic causal latent features with frame-wise noise levels.\n\nAutoregressive models [ zhang2023t2m , meng2024rethinking , zhao2024dartcontrol , xiao2025motionstreamer ] offer an alternative by predicting future frames from past ones, ensuring causal consistency and supporting online motion generation. Yet, their sequential dependency often leads to error accumulation, making long-horizon synthesis unstable and inefficient. The key challenge lies in achieving temporally ordered, high-quality motion generation with both the fidelity of diffusion models and the causal structure of autoregressive transformers.\n\nTo address these challenges, we propose C ausal M otion D iffusion M odels ( CMDM ), a unified framework that integrates causal diffusion and autoregressive modeling within a semantically aligned latent space as shown in¬†Fig. 1 . CMDM is built upon our Motion-Language-Models-Aligned Causal Variational Autoencoder (MAC-VAE), which encodes human motion into temporally causal latent representations guided by motion-language pretraining [ radford2021learning , petrovich2023tmr , yu2024exploring ] . This foundation enables CMDM to operate in a compact and semantically meaningful latent space, preserving alignment between linguistic semantics and motion dynamics. On top of MAC-VAE, we design a Causal Diffusion Transformer (Causal-DiT) that performs diffusion denoising in an autoregressive manner. Unlike conventional diffusion models that jointly process all frames, Causal-DiT applies causal self-attention to ensure each frame depends only on preceding frames. This design enforces strict temporal ordering, allowing streaming motion generation.\n\nTo accelerate inference, we introduce a frame-wise sampling schedule with causal uncertainty, which allows each frame to be progressively refined from partially denoised preceding frames rather than requiring a fully autoregressive denoising step. Inspired by Diffusion Forcing [ chen2024diffusion ] , originally designed for next-token prediction, during training we perturb each frame with independent noise levels while maintaining causal dependencies across time, enabling the model to learn temporally consistent denoising transitions. During sampling, the model iteratively predicts the subsequent frames based on previously denoised frames with varying noise levels, gradually reducing uncertainty in a causal order. This hierarchical denoising process significantly reduces inference steps, achieving efficient and temporally coherent motion generation.\n\nCMDM unifies the stability and realism of diffusion models with the causality and efficiency of autoregressive architectures. The framework enables high-fidelity text-to-motion generation, fast inference, and long-horizon synthesis within a unified causal formulation. Extensive evaluations on HumanML3D and SnapMoGen demonstrate that CMDM consistently outperforms state-of-the-art diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while reducing inference latency by an order of magnitude.\n\nOur main contributions are summarized as follows:\n\n- ‚Ä¢ Causal motion diffusion framework. We propose CMDM, the first motion diffusion framework that unifies causal autoregression and diffusion denoising within a motion‚Äìlanguage‚Äìaligned latent space.\n- ‚Ä¢ Semantically aligned causal latent modeling. We introduce MAC-VAE, a motion‚Äìlanguage‚Äìaligned causal VAE that learns temporally causal and semantically meaningful latent representations for text-to-motion generation.\n- ‚Ä¢ Frame-wise sampling with causal uncertainty. We design a novel frame-wise sampling schedule that models causal uncertainty, allowing each frame to be predicted from partially denoised preceding frames for efficient, low-latency, and temporally consistent motion synthesis.\n- ‚Ä¢ Comprehensive empirical validation. CMDM achieves state-of-the-art performance on HumanML3D and SnapMoGen, surpassing existing diffusion and autoregressive methods on text-to-motion generation and long-horizon motion generation.\n\nCausal motion diffusion framework. We propose CMDM, the first motion diffusion framework that unifies causal autoregression and diffusion denoising within a motion‚Äìlanguage‚Äìaligned latent space.\n\nSemantically aligned causal latent modeling. We introduce MAC-VAE, a motion‚Äìlanguage‚Äìaligned causal VAE that learns temporally causal and semantically meaningful latent representations for text-to-motion generation.\n\nFrame-wise sampling with causal uncertainty. We design a novel frame-wise sampling schedule that models causal uncertainty, allowing each frame to be predicted from partially denoised preceding frames for efficient, low-latency, and temporally consistent motion synthesis.\n\nComprehensive empirical validation. CMDM achieves state-of-the-art performance on HumanML3D and SnapMoGen, surpassing existing diffusion and autoregressive methods on text-to-motion generation and long-horizon motion generation.\n\n## 2 Related Works\n\n### 2.1 Motion-Language Alignment\n\nRecent advances in vision‚Äìlanguage models [ radford2021learning , oquab2023dinov2 ] have shown that large-scale training can robustly align text and visual semantics. This has led to a surge of interest in exploring motion‚Äìlanguage alignment to enable practical control of motion using natural language. MotionCLIP [ tevet2022motionclip ] maps a single frame to CLIP space but fails to capture temporal dynamics. Subsequent methods, including TMR [ petrovich2023tmr ] and MotionPatches [ yu2024exploring ] , learn joint motion‚Äìtext embeddings via contrastive or generative objectives, while PartTMR [ yu2025remogpt ] introduces body-part-level features for finer alignment. However, most motion‚Äìlanguage models focus on retrieval tasks. Methods such as ReMoGPT [ yu2025remogpt ] and ReMoMask [ li2025remomask ] extend to text-to-motion generation but rely on retrieval-augmented generation rather than integrating motion‚Äìlanguage alignment directly into the generation.\n\n### 2.2 Diffusion-based Motion Generation\n\nText-conditioned motion generation has been explored through both non-diffusion and diffusion [ ho2020denoising , ho2022classifier , dhariwal2021diffusion ] paradigms. Early works used CNN- or RNN-based architectures [ yan2019convolutional , zhao2020bayesian ] and action-conditioned frameworks [ guo2020action2motion , petrovich2021action ] to synthesize motion from predefined semantics. More recently, diffusion-based methods [ dhariwal2021diffusion , rombach2022high ] have set new benchmarks for motion realism and diversity [ zhang2022motiondiffuse , chen2023executing , mdm2022human ] . MDM [ mdm2022human ] and MotionDiffuse [ zhang2022motiondiffuse ] operate directly in motion space, while MLD [ chen2023executing ] , MotionLCM [ dai2024motionlcm ] , EnergyMoGen [ zhang2025energymogen ] and SALAD [ hong2025salad ] perform diffusion in a latent space for greater stability and efficiency. However, these diffusion models rely on bidirectional attention over entire sequences, breaking temporal causality and limiting real-time or streaming generation.\n\n### 2.3 Autoregressive Motion Generation\n\nAutoregressive (AR) modeling enforces temporal causality by predicting future frames from past context. Discrete-token methods such as T2M-GPT [ zhang2023t2m ] and MotionGPT [ jiang2023motiongpt ] treat motion as ‚Äúlanguage,‚Äù enabling powerful sequence modeling but suffering from exposure bias and cumulative errors. VQ-VAE-based approaches, including MoMask [ guo2024momask ] , MMM [ pinyoanuntapong2024mmm ] , and ParCo [ zou2024parco ] , quantize motion into discrete tokens and predict them autoregressively. Recent works explore causal paradigms for streaming generation: Dart [ zhao2024dartcontrol ] predicts short future segments from limited two historical frames, while MARDM [ meng2024rethinking ] and MotionStreamer [ xiao2025motionstreamer ] employ masked autoregressive transformers [ li2024autoregressive ] with diffusion heads. However, their reliance on teacher forcing [ williams1989learning ] and large diffusion heads causes instability in long-horizon inference and high computational cost, limiting real-time deployment.\n\nOur work differs from existing methods in two key aspects: (1) we introduce a causal diffusion process within a motion‚Äìlanguage‚Äìaligned latent space, preserving semantic consistency while enforcing temporal causality; and (2) we design a frame-wise sampling schedule that enables high-quality, streaming motion generation.\n\n## 3 Method\n\nOur proposed framework, Causal Motion Diffusion Models (CMDM), enables temporally ordered, text-conditioned motion generation by integrating causal latent encoding, causal diffusion forcing, and efficient frame-wise sampling.\nAs illustrated in¬†Fig. 2 , CMDM consists of three core components:\n(1) a Motion-Language-Aligned Causal VAE (MAC-VAE) that encodes motion sequences into semantically aligned and temporally causal latent spaces,\n(2) a Causal Diffusion Transformer (Causal-DiT) that performs frame-wise diffusion with causal self-attention to ensure autoregressive temporal dependencies, and\n(3) a Frame-Wise Sampling Scheduler (FSS) that models causal uncertainty by assigning higher noise to future frames and lower noise to past frames, allowing each new frame to be predicted from partially denoised preceding frames.\n\n[FIGURE:x2.png] Figure 2 : Overview of the proposed CMDM framework. CMDM consists of three key components: (a) MAC-VAE, which encodes motion sequences into motion‚Äìlanguage‚Äìaligned and temporally causal latent features using a causal encoder‚Äìdecoder structure supervised by motion-language model alignment; (b) Causal-DiT, which performs diffusion denoising with causal self-attention and cross-attention to text embeddings, ensuring temporally ordered and semantically consistent frame refinement; and (c) Causal Diffusion Forcing, which applies independent frame-level noise during training and a causal uncertainty schedule during inference, where the redness intensity represents the noise level. This design enables CMDM to achieve temporally consistent, semantically aligned, and efficient text-to-motion generation suitable for streaming and long-horizon synthesis.\n\n### 3.1 Motion-Language-Aligned Causal VAE\n\nTo obtain a temporally structured and semantically consistent latent representation, CMDM employs a causal variational autoencoder aligned with a motion-language foundation model.\nGiven a motion sequence ùê± 1 : T ‚àà ‚Ñù T √ó D \\mathbf{x}_{1:T}\\in\\mathbb{R}^{T\\times D} , where T T is the number of frames and D D is the dimension of the joint representation, the proposed MAC-VAE encoder E œï E_{\\phi} and decoder D œà D_{\\psi} operate causally such that each frame depends only on its past:\n\n|  | ùê≥ t = E œï ‚Äã ( ùê± ‚â§ t ) , ùê± ^ t = D œà ‚Äã ( ùê≥ ‚â§ t ) , \\mathbf{z}_{t}=E_{\\phi}(\\mathbf{x}_{\\leq t}),\\quad\\hat{\\mathbf{x}}_{t}=D_{\\psi}(\\mathbf{z}_{\\leq t}), |  | (1) |\n| --- | --- | --- | --- |\n\nwhere ùê≥ t ‚àà ‚Ñù d z \\mathbf{z}_{t}\\in\\mathbb{R}^{d_{z}} denotes the latent representation at timestep t t .\nFor a motion sequence with T T frames, we encode it into T / 4 T/4 temporal steps in the latent space, effectively achieving a 4 √ó 4\\times temporal downsampling ratio.\nThis compression balances representation compactness and temporal resolution, reducing redundancy while preserving the underlying motion dynamics.\n\nThe encoder and decoder are adapted from [ xiao2025motionstreamer ] and are composed of 1D causal convolution and 1D causal ResNet blocks, ensuring temporal causality during both encoding and reconstruction.\nIn this design, each frame only depends on preceding frames, while future frames are excluded from computation, explicitly modeling temporal causality in the latent space.\nDuring inference, reconstructed motions can be decoded sequentially in real time, enabling streaming generation without requiring access to future frames.\n\nTo enhance semantic alignment, motion features are projected through a pretrained motion‚Äìlanguage encoder (Part-TMR [ yu2025remogpt ] ), which provides part-level semantic supervision.\nThe MAC-VAE objective combines three terms: the standard VAE reconstruction loss, the Kullback‚ÄìLeibler divergence, and a newly introduced motion‚Äìlanguage alignment loss:\n\n|  | ‚Ñí MAC-VAE = ‚Ñí rec + Œ≤ ‚Äã D KL ‚Äã ( q œï ‚Äã ( ùê≥ | ùê± ) ‚à• p ‚Äã ( ùê≥ ) ) + Œª ‚Äã ‚Ñí align . \\mathcal{L}_{\\text{MAC-VAE}}=\\mathcal{L}_{\\text{rec}}+\\beta D_{\\text{KL}}\\big(q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\,\\|\\,p(\\mathbf{z})\\big)+\\lambda\\mathcal{L}_{\\text{align}}. |  | (2) |\n| --- | --- | --- | --- |\n\n#### Motion Alignment Loss.\n\nTo enforce fine-grained semantic alignment between motion and text, we introduce a motion alignment loss ‚Ñí align \\mathcal{L}_{\\text{align}} that measures both point-to-point feature similarity and relative structural consistency between motion embeddings ùê≥ \\mathbf{z} and motion-language features ùêü \\mathbf{f} extracted from the motion encoder of the pretrained Part-TMR model [ yu2025remogpt ] .\nSpecifically, we employ two complementary objectives, following the design in VAVAE [ yao2025reconstruction ] : (1) a marginal cosine similarity loss that minimizes local feature gaps, and (2) a marginal distance matrix similarity loss that preserves the relational geometry of feature spaces, as defined below:\n\n|  | ‚Ñí align = ‚Ñí mcos + ‚Ñí mdms . \\mathcal{L}_{\\text{align}}=\\mathcal{L}_{\\text{mcos}}+\\mathcal{L}_{\\text{mdms}}. |  | (3) |\n| --- | --- | --- | --- |\n\nGiven aligned feature maps ùêô \\mathbf{Z} and ùêÖ \\mathbf{F} from the latent space and the motion‚Äìlanguage encoder, respectively, we project ùêô \\mathbf{Z} to match the feature dimensionality of ùêÖ \\mathbf{F} via a linear transformation:\n\n|  | ùêô ‚Ä≤ = W ‚Äã ùêô , \\mathbf{Z}^{\\prime}=W\\mathbf{Z}, |  | (4) |\n| --- | --- | --- | --- |\n\nwhere W ‚àà ‚Ñù d f √ó d z W\\in\\mathbb{R}^{d_{f}\\times d_{z}} is a learnable projection matrix.\nThe marginal cosine similarity loss ‚Ñí mcos \\mathcal{L}_{\\text{mcos}} minimizes the similarity gap between corresponding features ùê≥ i ‚Äã j ‚Ä≤ \\mathbf{z}^{\\prime}_{ij} and ùêü i ‚Äã j \\mathbf{f}_{ij} :\n\n|  | ‚Ñí mcos = 1 N ‚Äã ‚àë i , j ReLU ‚Äã ( 1 ‚àí m 1 ‚àí ùê≥ i ‚Äã j ‚Ä≤ ‚ãÖ ùêü i ‚Äã j ‚Äñ ùê≥ i ‚Äã j ‚Ä≤ ‚Äñ ‚Äã ‚Äñ ùêü i ‚Äã j ‚Äñ ) , \\mathcal{L}_{\\text{mcos}}=\\frac{1}{N}\\sum_{i,j}\\text{ReLU}\\!\\left(1-m_{1}-\\frac{\\mathbf{z}^{\\prime}_{ij}\\cdot\\mathbf{f}_{ij}}{\\|\\mathbf{z}^{\\prime}_{ij}\\|\\,\\|\\mathbf{f}_{ij}\\|}\\right), |  | (5) |\n| --- | --- | --- | --- |\n\nwhere N N is the total number of temporal feature elements, and m 1 m_{1} is a similarity margin that encourages stronger alignment for less similar pairs.\nThis loss focuses learning on semantically misaligned regions, improving feature-level consistency.\n\nComplementary to ‚Ñí mcos \\mathcal{L}_{\\text{mcos}} , the marginal distance matrix similarity loss ‚Ñí mdms \\mathcal{L}_{\\text{mdms}} enforces the alignment of internal structural relationships between motion and text embeddings by matching their pairwise distance matrices.\nFormally, we compute:\n\n|  | ‚Ñí mdms = 1 N 2 ‚Äã ‚àë i , j ReLU ‚Äã ( | ùê≥ i ‚ãÖ ùê≥ j ‚Äñ ùê≥ i ‚Äñ ‚Äã ‚Äñ ùê≥ j ‚Äñ ‚àí ùêü i ‚ãÖ ùêü j ‚Äñ ùêü i ‚Äñ ‚Äã ‚Äñ ùêü j ‚Äñ | ‚àí m 2 ) , \\mathcal{L}_{\\text{mdms}}=\\frac{1}{N^{2}}\\sum_{i,j}\\text{ReLU}\\!\\left(\\left|\\frac{\\mathbf{z}_{i}\\cdot\\mathbf{z}_{j}}{\\|\\mathbf{z}_{i}\\|\\|\\mathbf{z}_{j}\\|}-\\frac{\\mathbf{f}_{i}\\cdot\\mathbf{f}_{j}}{\\|\\mathbf{f}_{i}\\|\\|\\mathbf{f}_{j}\\|}\\right|-m_{2}\\right), |  | (6) |\n| --- | --- | --- | --- |\n\nwhere m 2 m_{2} is a distance margin that relaxes the alignment constraint for similar pairs.\nThis objective promotes structural consistency between latent and text spaces, ensuring that the relative geometry of motion embeddings matches that of the aligned foundation features.\n\n### 3.2 Causal Diffusion Forcing\n\nWe extend Diffusion Forcing [ chen2024diffusion ] , originally proposed for next-token prediction, to the motion domain to model autoregressive temporal dynamics in the latent space.\nUnlike conventional diffusion models that jointly denoise all frames, our CMDM introduces frame-level noise with independent diffusion timesteps for each motion frame in the latent sequence, enforcing causal dependencies between past and future frames.\n\nIn standard full-sequence diffusion, the same level of noise k ‚àà [ 0 , K ] k\\in[0,K] , where K K is the total number of diffusion steps, is applied to the entire sequence as:\n\n|  | ùê≥ ~ k = Œ± ¬Ø k ‚Äã ùê≥ k + 1 ‚àí Œ± ¬Ø k ‚Äã œµ k , œµ k ‚àº ùí© ‚Äã ( 0 , I ) , \\tilde{\\mathbf{z}}^{k}=\\sqrt{\\bar{\\alpha}_{k}}\\,\\mathbf{z}^{k}+\\sqrt{1-\\bar{\\alpha}_{k}}\\,\\boldsymbol{\\epsilon}^{k},\\quad\\boldsymbol{\\epsilon}^{k}\\sim\\mathcal{N}(0,I), |  | (7) |\n| --- | --- | --- | --- |\n\nand the denoising model œµ Œ∏ \\epsilon_{\\theta} is trained to recover the original sequence simultaneously:\n\n|  | ‚Ñí = ùîº k , œµ k ‚Äã [ ‚Äñ œµ k ‚àí œµ Œ∏ ‚Äã ( ùê≥ ~ k , k , ùêú ) ‚Äñ 2 2 ] , \\mathcal{L}=\\mathbb{E}_{k,\\boldsymbol{\\epsilon}^{k}}\\Big[\\|\\boldsymbol{\\epsilon}^{k}-\\epsilon_{\\theta}(\\tilde{\\mathbf{z}}^{k},k,\\mathbf{c})\\|_{2}^{2}\\Big], |  | (8) |\n| --- | --- | --- | --- |\n\nwhere ùêú \\mathbf{c} denotes the text embedding.\n\nIn causal diffusion forcing , each frame t t is assigned an independent noise level k t ‚àà [ 0 , K ] k_{t}\\in[0,K] , and the noisy latent representation is defined as:\n\n|  | ùê≥ ~ t k t = Œ± ¬Ø k t ‚Äã ùê≥ t k t + 1 ‚àí Œ± ¬Ø k t ‚Äã œµ t k t , œµ t k t ‚àº ùí© ‚Äã ( 0 , I ) . \\tilde{\\mathbf{z}}_{t}^{k_{t}}=\\sqrt{\\bar{\\alpha}_{k_{t}}}\\,\\mathbf{z}_{t}^{k_{t}}+\\sqrt{1-\\bar{\\alpha}_{k_{t}}}\\,\\boldsymbol{\\epsilon}_{t}^{k_{t}},\\quad\\boldsymbol{\\epsilon}_{t}^{k_{t}}\\sim\\mathcal{N}(0,I). |  | (9) |\n| --- | --- | --- | --- |\n\nThe diffusion transformer œµ Œ∏ \\epsilon_{\\theta} predicts the noise residual using causal self-attention , which restricts each frame to attend only to its past representations:\n\n|  | ‚Ñí DF = ùîº k t , œµ t k t ‚Äã [ ‚Äñ œµ t k t ‚àí œµ Œ∏ ‚Äã ( ùê≥ ~ ‚â§ t , k t , ùêú ) ‚Äñ 2 2 ] . \\mathcal{L}_{\\text{DF}}=\\mathbb{E}_{k_{t},\\boldsymbol{\\epsilon}_{t}^{k_{t}}}\\Big[\\|\\boldsymbol{\\epsilon}_{t}^{k_{t}}-\\epsilon_{\\theta}(\\tilde{\\mathbf{z}}_{\\leq t},k_{t},\\mathbf{c})\\|_{2}^{2}\\Big]. |  | (10) |\n| --- | --- | --- | --- |\n\nThis causal training setup ensures that the denoising process evolves forward in time, with each prediction depending solely on the available history, effectively bridging diffusion and autoregression.\n\nBy replacing the globally synchronized noise schedule with frame-specific perturbations, CMDM achieves several advantages.\nFirst, the model learns to operate under diverse noise conditions at each frame, which improves temporal robustness and generalization to variable-length sequences.\nSecond, the causal attention mask explicitly enforces temporal order within the transformer backbone, preventing information leakage from future frames and enabling real-time or streaming generation.\nFinally, the per-frame stochasticity of diffusion forcing acts as a natural regularizer, encouraging smooth temporal transitions while preserving motion diversity.\n\n### 3.3 Causal Diffusion Transformer\n\nTo model the temporal dependencies of diffusion forcing, CMDM employs a causal diffusion transformer (Causal-DiT) that performs diffusion-based denoising under strict causal constraints. Unlike conventional transformers with bidirectional attention, Causal-DiT uses causal masking so each frame accesses only its past and current context, ensuring sequential generation consistent with autoregressive reasoning while maintaining diffusion fidelity.\n\nEach transformer block integrates three key mechanisms: (1) causal self-attention, which employs a lower-triangular attention mask to prevent each frame from attending to future frames. This constraint preserves the causal order required for autoregressive modeling and ensures that the model predicts future motion dynamics based solely on previously observed information. (2) cross-attention, which establishes the link between motion and language by conditioning frame-level motion latents on word-level text embeddings extracted from DistilBERT [ sanh2019distilbert ] . Through this mechanism, semantic cues from natural language guide the temporal evolution of motion features, allowing the model to synthesize actions that remain coherent with the textual descriptions across the sequence. (3) adaptive layer normalization (AdaLN) [ peebles2023scalable ] combined with rotary positional encoding (ROPE) [ su2024roformer ] , where AdaLN embeds frame-wise diffusion timestep information, ensuring that temporal noise levels k t k_{t} are seamlessly integrated into the denoising process, while ROPE stabilizes long-horizon denoising with relative positional encoding.\n\nDuring training, each frame t t is diffused with an independent noise level k t k_{t} , and the model learns to denoise them as:\n\n|  | œµ Œ∏ ‚Äã ( ùê≥ ~ ‚â§ t , k t , ùêú ) = CausalDiT ‚Äã ( ùê≥ ~ ‚â§ t , k t , ùêú ) , \\epsilon_{\\theta}(\\tilde{\\mathbf{z}}_{\\leq t},k_{t},\\mathbf{c})=\\text{CausalDiT}(\\tilde{\\mathbf{z}}_{\\leq t},k_{t},\\mathbf{c}), |  | (11) |\n| --- | --- | --- | --- |\n\nwhere ùê≥ ~ ‚â§ t \\tilde{\\mathbf{z}}_{\\leq t} denotes the partially noised causal latent sequence and ùêú \\mathbf{c} is the text embedding.\n\n### 3.4 Inference and Streaming Generation\n\nDuring inference, CMDM generates motion autoregressively by progressively denoising each frame in a causal manner.\nGiven text condition ùêú \\mathbf{c} and an initial noise sequence { ùê≥ ~ t K } t = 1 T ‚àº ùí© ‚Äã ( 0 , I ) \\{\\tilde{\\mathbf{z}}_{t}^{K}\\}_{t=1}^{T}\\sim\\mathcal{N}(0,I) , the model predicts each frame conditioned on previously denoised latents:\n\n|  | ùê≥ ~ t k t ‚àí 1 = G Œ∏ ‚Äã ( { ùê≥ ~ < t 0 , ùê≥ ~ t k t } , k t , ùêú ) , ùê± ^ t = D œà ‚Äã ( ùê≥ ~ ‚â§ t 0 ) , \\tilde{\\mathbf{z}}_{t}^{k_{t}-1}=G_{\\theta}(\\{\\tilde{\\mathbf{z}}^{0}_{<t},\\tilde{\\mathbf{z}}_{t}^{k_{t}}\\},k_{t},\\mathbf{c}),\\quad\\hat{\\mathbf{x}}_{t}=D_{\\psi}(\\tilde{\\mathbf{z}}^{0}_{\\leq t}), |  | (12) |\n| --- | --- | --- | --- |\n\nwhere G Œ∏ G_{\\theta} denotes the causal diffusion generator.\nThis formulation ensures strictly causal synthesis and, with key-value caching for autoregressive rollout, enables real-time generation. However, this scheme is prone to accumulating single-step errors, as it treats the predicted ùê≥ ~ t 0 \\tilde{\\mathbf{z}}_{t}^{0} as a ground truth observation, a practice more broadly referred to as exposure bias [ schmidt2019generalization , ning2023elucidating ] .\n\n#### Frame-wise Sampling Schedule (FSS).\n\nTo accelerate inference and mitigate exposure bias, CMDM adopts a frame-wise sampling schedule with causal uncertainty , assigning lower noise to past frames and higher noise to future ones.\nAt each step, the model refines the next frame using partially denoised histories. For example, a causal uncertainty schedule with uncertainty scale L L can be defined as:\n\n|  | K m , t = [ K K K K ‚àí L K K K ‚àí 2 ‚Äã L K ‚àí L K ‚ãÆ ‚ã± K ‚àí L 0 ‚ãØ ‚ãÆ 0 0 0 ] , K_{m,t}=\\begin{bmatrix}K&K&K\\\\\nK{-}L&K&K\\\\\nK{-}2L&K{-}L&K\\\\\n\\vdots&\\ddots&K{-}L\\\\\n0&\\cdots&\\vdots\\\\\n0&0&0\\end{bmatrix}, |  | (13) |\n| --- | --- | --- | --- |\n\nwhere K m , t K_{m,t} indicates the noise applied to frame t t at iteration m m and the uncertainty scale L L indicates that the denoising of the next frame begins at step K ‚àí L K{-}L of the current frame. Intermediate steps between K K , K ‚àí L K{-}L , K ‚àí 2 ‚Äã L K{-}2L , and so on are omitted for clarity. Each partially denoised frame ùê≥ ~ t \\tilde{\\mathbf{z}}_{t} is reused as context for subsequent predictions, enabling continuous, low-latency generation with high temporal coherence and smooth transitions, while greatly reducing inference cost compared to full autoregressive diffusion.\n\n## 4 Experiments\n\n[FIGURE_CAPTION] Table 1 : Results of text-to-motion generation on HumanML3D. The average is reported over 10 runs with 95% confidence intervals. Methods marked with ‚Ä† \\dagger were originally implemented with different motion representations and have been re-trained using our codebase to ensure a fair comparison. Bold indicates the best result, while underline denotes the second-best result.\n\n### 4.1 Experimental Setup\n\n#### Datasets.\n\nTo evaluate CMDM, we conduct experiments on two benchmarks: HumanML3D [ Guo_2022_CVPR_humanml3d ] and SnapMoGen [ guo2025snapmogen ] .\nHumanML3D contains 14,616 motion clips from AMASS [ AMASS_ICCV2019 ] paired with 44,970 short textual descriptions of common actions ( e.g . , walking, jumping, sitting).\nSnapMoGen includes 20,450 motion capture clips with 122K expressive captions (average 48 words) covering about 43.7 hours of data.\nUnlike HumanML3D, SnapMoGen features temporally continuous, long-horizon activities ( e.g . , sports and performances), allowing evaluation of smooth, consistent motion generation.\nFor fair comparison, we follow the standard 3D motion representation of each dataset: 263 dimensions for HumanML3D and 296 for SnapMoGen, including joint velocities, positions, and rotations.\n\n#### Evaluation Metrics.\n\nFollowing prior work [ Guo_2022_CVPR_humanml3d , jiang2023motiongpt , guo2025snapmogen ] , we evaluate CMDM using several standard metrics.\n(1) Motion quality is measured by Fr√©chet Inception Distance (FID), which assesses the realism of generated motions relative to ground truth.\n(2) Multi-modality quantifies the diversity of motions generated from identical text prompts.\n(3) Text‚Äìmotion alignment is evaluated by R-Precision (R@1, R@2, R@3) and Multi-Modal Distance (MM Dist) using a pretrained text‚Äìmotion retrieval model.\nWe also report the CLIP-Score [ meng2024rethinking ] , which measures the cosine similarity between generated motion features and their corresponding captions in the CLIP embedding space.\n\n#### Implementation Details.\n\nMAC-VAE comprises seven causal convolution layers and two causal ResNet blocks with left padding in both the encoder and decoder. The latent feature dimension is set to 64. We modify and retrain Part-TMR [ yu2025remogpt ] to extract frame-level semantic motion‚Äìlanguage features for supervising MAC-VAE. The loss weight Œª \\lambda for semantic alignments is automatically adjusted according to the gradient norm at the final layer of the encoder to maintain balance with other losses. The Causal-DiT is implemented as a lightweight Transformer [ vaswani2017attention ] with eight layers, four attention heads, and a hidden dimension of 512. Flow Matching [ lipman2022flow , ma2024sit , albergo2023stochastic , albergo2022building ] is adopted as the ODE sampler for causal diffusion forcing. For FSS, we set 50 denoising step with K = 50 K=50 for each frame and start to denoise the next frame at K ‚àí 2 K-2 ( i.e . , L = 2 L=2 ) during inference.\n\n[FIGURE_CAPTION] Table 2 : Results of text-to-motion generation on SnapMoGen. The average is reported over 10 runs with 95% confidence intervals.\n\n[FIGURE_CAPTION] Table 3 : Results of long-horizon motion generation on HumanML3D and SnapMoGen. The motion quality of each subsequence and the smoothness of each transition are evaluated.\n\n[FIGURE:x3.png] Figure 3 : Qualitative results of long-horizon motion generation. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences.\n\n### 4.2 Quantitative Results\n\n#### Results on HumanML3D.\n\nWe compare CMDM with state-of-the-art models across three paradigms: (1) VQ-based [ zhang2023t2m , pinyoanuntapong2024mmm , guo2024momask ] ; (2) Diffusion-based [ mdm2022human , chen2023executing , dai2024motionlcm , huang2024stablemofusion , zhang2025energymogen , hong2025salad ] ; and (3) Autoregressive-based [ meng2024rethinking , xiao2025motionstreamer ] .\nAs shown in¬†Table 1 , CMDM consistently achieves superior or comparable results across all metrics. CMDM with frame-wise sampling (CMDM w/ FSS) attains the best overall performance, achieving an R-Precision of 0.588/0.778/0.860 (Top-1/2/3), the second lowest FID (0.068), and the highest CLIP-Score (0.685). These results indicate high motion fidelity and strong text‚Äìmotion alignment.\nCompared to standard autoregressive sampling (CMDM w/ AR), FSS further improves temporal stability and smoothness while reducing inference latency. This improvement arises from the causal uncertainty mechanism, where each subsequent frame is generated from partially denoised preceding frames. This design allows the model to reduce the accumulated error of autoregressive and adaptively refine local temporal transitions while maintaining global coherence, leading to smoother motion dynamics and improved semantic alignment for stable generation.\n\n#### Results on SnapMoGen.\n\nWe further evaluate the proposed CMDM on the motion clips of SnapMoGen, which contains expressive motion sequences paired with rich textual descriptions. As shown in¬†Table 2 , CMDM achieves state-of-the-art performance across all evaluation metrics, demonstrating its strong generalization to complex motions. CMDM with the frame-wise sampling schedule (CMDM w/ FSS) achieves the best overall results, surpassing all previous VQ-, diffusion-, and autoregressive-based methods. It also achieves the lowest FID score and a high CLIP-Score, indicating superior motion realism and semantic alignment.\n\n#### Long-Horizon Motion Generation\n\nTo evaluate CMDM on long-horizon motion generation, we compare it with the motion composition method FlowMDM [ barquero2024seamless ] and the autoregressive model MARDM [ meng2024rethinking ] .\nFollowing the protocol of FlowMDM, we synthesize 64 long-horizon sequences on HumanML3D by composing 32 caption‚Äìduration pairs per sequence, evaluating 32 subsequences and 31 transitions for local quality and temporal continuity. The ground-truth metrics are computed using randomly sampled motion clips from HumanML3D.\nFor SnapMoGen, which provides ground-truth long sequences, we select 128 samples with over five continuous motions and use the same captions for generation. We further employ Peak Jerk (PJ) and Area Under the Jerk (AUJ) [ barquero2024seamless ] to measure transition smoothness.\nOwing to differences in skeleton scale, the magnitude of the metrics differs from those reported on HumanML3D.\nThe results are shown in¬†Table 3 . Although FlowMDM reports lower PJ and AUJ values on SnapMoGen, this is primarily because its generated motions often remain static or frozen as can be seen from the ensuing qualitative analysis in¬†Fig. 3 . In contrast, CMDM produces temporally consistent, smoothly transitioning, and realistic long-horizon motions at real-time speed, demonstrating its effectiveness for streaming and continuous text-to-motion generation.\n\n### 4.3 Qualitative Results\n\nFig. 3 shows qualitative comparisons with FlowMDM [ barquero2024seamless ] and MARDM [ meng2024rethinking ] on long-horizon motion generation. Given a sequence of captions, CMDM generates continuous and seamless motions with accurate semantics and smooth transitions across segments. In contrast, FlowMDM and MARDM often produce incorrect actions, e.g . , ‚Äúno jumping‚Äù or unnatural transitions, e.g . , skeleton flips. These improvements stem from causal latent encoding and frame-wise sampling schedule of CMDM, which condition each frame on partially denoised preceding frames to ensure stability and temporal consistency. Please refer to the supplementary videos for the complete motion sequences and more visualization results.\n\n### 4.4 Analysis\n\n#### Computational Efficiency\n\nCompared to other existing autoregressive methods, CMDM achieves a substantial improvement in inference efficiency through its advanced architecture and frame-wise sampling schedule, which significantly reduces generation time while maintaining superior motion realism. We evaluate the computational efficiency of our framework by generating 6-second motion sequences on an NVIDIA A100 GPU over 100 repetitions. As a result, MARDM operates with 310M parameters at 20¬†fps, and MotionStreamer with 318M parameters at 11¬†fps. In contrast, our proposed CMDM contains only 114M parameters (including both the MAC-VAE and Causal-DiT) and achieves 28¬†fps using the standard autoregressive process, and up to 125¬†fps with the proposed frame-wise sampling schedule. These highlight the remarkable efficiency of our causal diffusion framework and the effectiveness of the proposed sampling strategy for real-time motion generation.\n\n#### Ablation Studies\n\nTo investigate the impact of each component in CMDM, we conduct ablation studies on causal latent modeling, causal diffusion forcing, and the frame-wise sampling schedule (FSS) on HumanML3D.\nAs shown in¬†Table 4 , replacing the MAC-VAE with a standard VAE significantly degrades motion quality, text fidelity, and transition smoothness, confirming the importance of causal latent modeling and semantic supervision.\nRemoving motion‚Äìlanguage alignment (C-VAE w/o MA) produces motion quality comparable to MAC-VAE but introduces semantic inconsistencies, underscoring the importance of semantic features for maintaining fidelity and coherence.\nSubstituting causal diffusion with full-sequence diffusion (w/ Full-Seq. Diff.) increases transition FID and AUJ, verifying that per-frame causal diffusion enforces stronger temporal stability. Eliminating AdaLN (w/o AdaLN) or ROPE (w/o ROPE) results in higher FID and weaker long-horizon coherence, while removing both further amplifies these degradations.\nFinally, the FSS variants show that smaller uncertainty scales ( L = 5 L{=}5 ) achieve smoother transitions and lower AUJ, whereas excessively large L L or smaller K K degrade stability. These results show that all the components jointly contribute to the high performance of CMDM.\n\n[FIGURE_CAPTION] Table 4 : Ablation studies of CMDM.\n\n## 5 Limitations\n\nAlthough CMDM achieves state-of-the-art performance in text-conditioned and long-horizon motion generation, several limitations remain. First, the causal latent encoding relies on motion‚Äìlanguage alignment quality from pretrained motion-language models such as Part-TMR, which may limit performance when processing highly abstract or ambiguous text descriptions. Second, while the frame-wise sampling schedule substantially improves inference efficiency, it may still accumulate minor temporal artifacts when generating extremely long sequences, e.g . , over several minutes. Incorporating motion-aware feedback or adaptive re-anchoring mechanisms could further improve long-horizon stability. Finally, CMDM focuses primarily on single-person motion and has not yet been extended to interactive or multi-character scenarios [ tanaka2023interaction , liang2024intergen , fan2024freemotion , ota2025pino ] , which will be an interesting direction for future work.\n\n## 6 Conclusion\n\nIn this paper, we presented CMDM, a unified framework that combines the realism and stability of diffusion models with the temporal causality and efficiency of autoregressive generation. CMDM introduces a MAC-VAE for semantically grounded causal latent encoding, a Causal-DiT for temporally ordered diffusion denoising, and a FSS that enables real-time streaming generation. Extensive experiments on HumanML3D and SnapMoGen demonstrate that CMDM achieves superior motion fidelity, semantic alignment, and efficiency compared to existing diffusion and autoregressive models. We believe CMDM provides a promising step toward scalable, real-time, and semantically coherent motion generation.\n\n## References\n\nSupplementary Material\n\n## Appendix A Implementation Details\n\n### A.1 MAC-VAE\n\nThe proposed MAC-VAE consists of seven causal convolutional layers and two causal ResNet blocks with left padding in both the encoder and decoder to ensure strict temporal causality. Each convolutional layer uses a kernel size of 3 and a stride of 1, followed by ReLU activation. The latent feature dimension is set to 64, and motion sequences are downsampled/upsampled by a factor of 4 along the temporal axis using stride-2 convolutional layers within the ResNet blocks.\n\nTo achieve semantic alignment between motion and text, we modify Part-TMR [ yu2025remogpt ] to extract frame-level motion‚Äìlanguage embeddings. Part-TMR uses a [class] token to aggregate frames into a global feature, whereas we directly extract features from each frame and align them with the corresponding text features via contrastive learning, which serves as the supervision signal for MAC-VAE. The loss weighting coefficient is set to Œ≤ = 1.0 \\beta{=}1.0 , and the margin parameters are set to m 1 = 0.5 m_{1}{=}0.5 and m 2 = 0.25 m_{2}{=}0.25 .\n\nWe train MAC-VAE using the AdamW optimizer with a learning rate of 1 √ó 10 ‚àí 4 1{\\times}10^{-4} and a batch size of 128 for 50 epochs on a single NVIDIA A100 GPU. The learning rate follows a cosine decay schedule, and gradient clipping with a maximum norm of 1.0 is applied for training stability.\n\n### A.2 Causal-DiT\n\nThe Causal-DiT is implemented as a lightweight transformer-based denoiser with 8 layers, 4 attention heads, and a hidden dimension of 512. Causal self-attention is applied using a lower-triangular mask to enforce temporal order, while cross-attention conditions motion latents on text embeddings extracted from DistilBERT [ sanh2019distilbert ] . We incorporate Adaptive Layer Normalization (AdaLN) [ peebles2023scalable ] and Rotary Positional Encoding (ROPE) [ su2024roformer ] to embed timestep information and stabilize long-horizon attention. During training, the text condition is randomly dropped with a probability of 0.1 to enable classifier-free guidance. The model is optimized using AdamW with the same hyperparameter settings as MAC-VAE. The scale of classifier-free guidance is set to 3.0 during inference.\n\n### A.3 Causal Diffusion Forcing\n\nIn CMDM, causal diffusion forcing is employed to enable temporally ordered denoising while maintaining frame-level stochasticity. During training, each frame t t is perturbed with an independent noise level k t ‚àà [ 0 , K ] k_{t}\\in[0,K] , where K = 1000 K{=}1000 denotes the total number of diffusion steps. The Causal-DiT serves as the denoiser, learning to predict noise residuals œµ Œ∏ ‚Äã ( ùê≥ ~ ‚â§ t , k t , ùêú ) \\boldsymbol{\\epsilon}_{\\theta}(\\tilde{\\mathbf{z}}_{\\leq t},k_{t},\\mathbf{c}) conditioned on all preceding latent frames and the text embedding ùêú \\mathbf{c} . This formulation ensures that each frame is denoised based solely on its causal history, thereby enforcing strict temporal dependencies. The overall training process is summarized in Algorithm 1 .\n\nDuring inference, we adopt the Frame-Wise Sampling Schedule (FSS) with diffusion steps K = 50 K{=}50 and uncertainty scale L = 2 L{=}2 . In this setting, the denoising of frame t + 1 t{+}1 begins at step K ‚àí L K{-}L of frame t t , allowing partially denoised frames to guide subsequent generations. This causal scheduling mechanism significantly accelerates inference by reducing redundant diffusion steps while maintaining temporal consistency across frames. The overall inference process with FSS is summarized in Algorithm 2 .\n\n[FIGURE_CAPTION] Algorithm 1 CMDM Training with Causal Diffusion Forcing\n\n[FIGURE_CAPTION] Algorithm 2 CMDM Streaming Generation with Frame-wise Sampling Schedule (FSS)\n\n## Appendix B Additional Quantitative Results\n\n### B.1 Experiments on BABEL\n\nWe further evaluate CMDM on the BABEL dataset [ punnakkal2021babel ] to assess its generalization ability to diverse motion compositions.\nBABEL contains densely annotated sequences with multiple actions and transitions, making it suitable for long-horizon motion synthesis and evaluation.\nWe train CMDM by constructing training samples from adjacent subsequences in BABEL, where each pair of consecutive segments is used to learn motion continuation across long sequences.\nAs shown in¬†Table 5 , our method achieves the best overall performance across both subsequence and transition metrics, demonstrating the advantage of CMDM in maintaining consistency across action boundaries and generating smooth, continuous motions.\n\n[FIGURE_CAPTION] Table 5 : Comparison of long-horizon motion generation on BABEL.\nSubsequence metrics evaluate motion quality and diversity within segments, while transition metrics assess temporal continuity and smoothness between segments.\n\n### B.2 Evaluation on Other Motion Features\n\nTo further examine the generalization ability of CMDM, we conduct experiments using motion features with redundant dimensions removed, following the analysis in [ meng2024rethinking ] .\nAs discussed in prior work, the standard HumanML3D motion representation contains redundant components such as local joint rotations and contact features that do not directly influence the final human pose.\nRemoving these redundant features yields a more compact and physically meaningful representation better suited for continuous diffusion modeling.\n\nTable 6 reports the results on HumanML3D using only essential motion features.\nCompared to the baseline methods, CMDM consistently improves generation quality and semantic alignment under both autoregressive (AR) and diffusion (FSS) configurations.\nSpecifically, CMDM w/ FSS achieves the best overall performance, reaching an R-Precision of 0.563/0.759/0.849 for Top-1/Top-2/Top-3 accuracy and the lowest FID of 0.078, confirming that our causal diffusion formulation effectively models temporally coherent motion even in compact feature spaces.\nThese results demonstrate that CMDM remains robust across different motion representations, further validating its adaptability to feature compression and reparameterized motion distributions.\n\n[FIGURE_CAPTION] Table 6 : Results of text-to-motion generation on HumanML3D without redundant features.\nThe average is reported over 10 runs with 95% confidence intervals. Bold indicates the best result, and underline denotes the second-best result.\n\n### B.3 Compositional Motion Generation\n\nWe evaluate CMDM on the compositional motion generation task following the protocol of Multi-Track Timeline (MTT) [ petrovich2024multi ] , which requires generating coherent motions conditioned on multiple temporally structured text descriptions. This task evaluates both semantic composition, i.e . , correctly realizing multiple concepts within a single sequence, and temporal composition, i.e . , ensuring smooth and consistent transitions across segments.\n\nSpecifically, following prior work [ petrovich2024multi , zhang2025energymogen ] , we report per-crop semantic correctness metrics (R@1, R@3, and TMR-Score for M2T and M2M), as well as realism metrics including FID and transition distance. As shown in Table 7 , CMDM, under the single-track multi-crop setting, consistently outperforms EnergyMoGen and other compositional baselines across all metrics. Notably, CMDM achieves substantial improvements in semantic alignment while simultaneously reducing FID and transition distance, demonstrating stronger long-horizon consistency and smoother transitions between composed motion segments.\n\n[FIGURE_CAPTION] Table 7 : Comparison with prior compositional motion generation methods on the Multi-track timeline (MTT) dataset [ petrovich2024multi ] .\n\n### B.4 Latency analysis\n\nTo evaluate the practical efficiency of different causal motion generation methods, we measure the latency for generating each token (4 frames) on a single NVIDIA A100 GPU.\nMARDM [ meng2024rethinking ] , MotionStreamer [ xiao2025motionstreamer ] , and CMDM w/ AR require approximately 210‚Äâms, 360‚Äâms, and 150‚Äâms, respectively, to generate the first token, with similar latency for each subsequent token.\nThis is because these autoregressive diffusion methods perform full diffusion denoising for each token independently, requiring multiple denoising steps per frame regardless of its temporal position.\nIn contrast, CMDM w/ FSS takes about 220‚Äâms for the first token but only 30‚Äâms per subsequent token, achieving a 5 √ó 5\\times ‚Äì 12 √ó 12\\times speedup for streaming generation.\nThis dramatic reduction in per-token latency stems from our frame-wise sampling schedule, which allows each frame to be predicted from partially denoised preceding frames rather than requiring full iterative refinement.\n\n### B.5 Ablation Studies\n\n#### Architecture of MAC-VAE.\n\nWe evaluate several configurations of MAC-VAE to analyze the effects of latent dimension and temporal downsampling rate on both reconstruction and generation performance.\nThe notation ( d , r ) (d,r) denotes the latent dimension d d and the temporal downsampling rate r r .\nAs shown in ¬†Table 8 , increasing the latent dimension improves reconstruction accuracy but also introduces redundancy that slightly affects generation quality in terms of FID.\nConversely, larger temporal downsampling rates ( e.g . , r = 1 / 8 r=1/8 ) reduce temporal resolution and lead to minor degradation in R-Precision and MM-Dist due to information loss.\nAmong all configurations, MAC-VAE with ( 64 , 1 / 4 ) (64,1/4) achieves the best balance between reconstruction fidelity (FID = 0.000 =0.000 , MPJPE = 0.012 =0.012 ) and generation quality (R-Top1 = 0.588 =0.588 , FID = 0.068 =0.068 , MM-Dist = 2.620 =2.620 ), which we adopt as the default setting in all subsequent experiments.\nThese results confirm that a compact latent space with moderate temporal compression effectively captures semantic and temporal dependencies for downstream motion generation.\n\n[FIGURE_CAPTION] Table 8 : Comparison of reconstruction and generation performance on HumanML3D. MPJPE is measured in millimeters. The notation ( d , r ) (d,r) denotes the latent dimension d d and the temporal downsampling rate r r .\n\n#### Motion-Language Models\n\nTo evaluate the effectiveness of different motion‚Äìlanguage alignment strategies, we compare several pretrained motion‚Äìlanguage models integrated into the MAC-VAE framework, including TMR [ petrovich2023tmr ] , MotionPatches [ yu2024exploring ] , and Part-TMR [ yu2025remogpt ] .\nAs shown in¬†Table 9 , all motion‚Äìlanguage models improve generation quality while maintaining reconstruction performance compared to the baseline VAE and C-VAE, demonstrating the effectiveness of semantic alignment between motion and text.\nAmong them, Part-TMR achieves the best overall performance with the lowest reconstruction error (FID = 0.000 =0.000 , MPJPE = 0.012 =0.012 ) and the highest R-Precision ( 0.588 0.588 ), confirming its strong ability to capture fine-grained part-level correspondences between text and motion.\nThese results validate the choice of Part-TMR as the alignment backbone in MAC-VAE, enabling more semantically coherent and temporally consistent motion generation.\n\n[FIGURE_CAPTION] Table 9 : Comparison of motion-language models in MAC-VAE on HumanML3D. MPJPE is measured in millimeters.\n\n#### Model Size of Causal-DiT.\n\nWe investigate the impact of model size on generation quality by varying the number of attention heads ( H ) (H) , layers ( L ) (L) , and hidden dimensions ( D ) (D) in Causal-DiT.\nAs shown in¬†Table 10 , larger models generally achieve better performance due to increased representational capacity.\nThe medium-sized model (38M parameters) already provides strong results with an R-Precision of 0.588 0.588 and FID of 0.068 0.068 , balancing quality and efficiency.\nFurther scaling to 304M parameters yields marginal improvements (R-Precision = 0.590 =0.590 , FID = 0.042 =0.042 ), demonstrating that Causal-DiT scales effectively while maintaining computational practicality.\nUnless otherwise specified, we use the medium (38M) configuration in all main experiments.\n\n[FIGURE_CAPTION] Table 10 : Comparison of model sizes on HumanML3D. The notation ( H , L , D ) (H,L,D) denotes the number of attention heads H H , layers L L , and hidden dimension D D .\n\n#### Text Encoder\n\n[FIGURE_CAPTION] Table 11 : Comparison of text encoders on HumanML3D.\n\n[FIGURE:x4.png] Figure 4 : Qualitative results of long-horizon motion generation on HumanML3D. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences.\n\n[FIGURE:x5.png] Figure 5 : Qualitative results of long-horizon motion generation on SnapMoGen. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences.\n\nWe compare several pretrained language models as text encoders to evaluate their impact on semantic alignment and motion quality. As shown in¬†Table 11 , the choice of text encoder influences both text‚Äìmotion correspondence (R-Precision) and visual realism (FID). DistilBERT [ sanh2019distilbert ] , which provides word-level embeddings, achieves the best overall performance with the highest R-Precision ( 0.588 0.588 ) and lowest FID ( 0.068 0.068 ), demonstrating its ability to capture fine-grained semantic cues that align well with motion features. Using the CLIP-based encoder, the word-level variant, which is identical to that employed in StableMoFusion [ huang2024stablemofusion ] , also outperforms StableMoFusion, further confirming the benefits of word-level representations. Such token-level embeddings are crucial for maintaining causal dependencies between linguistic tokens and motion frames, which is necessary for stable autoregressive generation in CMDM. In contrast, sentence-level embeddings from CLIP [ radford2021learning ] exhibit reduced precision and higher FID due to the loss of temporal granularity. Meanwhile, Sentence-T5 [ ni2022sentence ] performs better than the CLIP-based models and also outperforms MotionLCM V2 [ huang2024stablemofusion ] , despite MotionLCM V2 also using Sentence-T5. These findings validate our choice of DistilBERT as the text encoder for CMDM, as it effectively preserves local semantics and enables causally consistent motion‚Äìlanguage modeling.\n\n[FIGURE:x6.png] Figure 6 : Qualitative results of text-to-motion generation on HumanML3D. CMDM produces motions that better capture fine-grained textual semantics and maintain natural body articulation compared to previous methods. Please refer to the supplementary videos for clearer visualization.\n\n[FIGURE:x7.png] Figure 7 : Qualitative results of text-to-motion generation on SnapMoGen. Comparison between our CMDM and previous methods. We directly use the raw text prompts without any LLM-based augmentation and CMDM still achieves strong generation quality. Please refer to the supplementary videos for clearer visualization.\n\n## Appendix C Additional Qualitative results\n\nTo further demonstrate the effectiveness of CMDM, we provide additional qualitative comparisons on long-horizon and text-to-motion generation.\nFig. 4 and¬†Fig. 5 compares CMDM with FlowMDM [ barquero2024seamless ] and MARDM [ meng2024rethinking ] on long-horizon motion generation for HumanML3D and SnapMoGen, respectively.\nCMDM produces temporally coherent and semantically accurate motions without content drift or skeleton flipping, whereas previous methods often suffer from static poses, incorrect transitions, or inconsistent actions across segments.\nThese examples highlight the ability of CMDM to maintain smooth temporal dynamics and causal consistency throughout extended sequences.\n\nFig. 6 presents qualitative results on HumanML3D.\nCompared with MoMask [ guo2024momask ] , MotionLCM [ dai2024motionlcm ] , and StableMoFusion [ huang2024stablemofusion ] , CMDM generates motions that more faithfully reflect fine-grained textual semantics ( e.g . , arm rotations, leg movements, or walking direction) while preserving natural body articulation.\nFig. 7 shows additional results on SnapMoGen, where CMDM directly uses the raw text prompts without LLM-based augmentation and still produces more realistic motions than prior methods.\n\nPlease refer to the supplementary videos on the demo page for full-length visualizations.\n\n## Appendix D Sample Code\n\nThe code will be released at https://github.com/YU1ut/CMDM . We provide the training codes for building and evaluating the proposed CMDM with the HumanML3D dataset. Please refer to the README file in the code directory for details.",
  "figures": []
}