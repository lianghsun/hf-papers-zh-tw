# 論文摘要翻譯

人類智能自然地將全模態感知——跨越視覺、音訊和語言——與複雜推理和工具使用相互交織，以與世界互動。然而，當前的多模態大語言模型主要侷限於雙模態互動（例如，視覺-語言），缺乏通用AI助手所需的統一認知能力。為彌補此差距，我們提出OmniGAIA，一個綜合基準旨在評估全模態智能體在需要深度推理和跨越視訊、音訊和影像模態的多輪工具執行的任務上的表現。通過新穎的全模態事件圖方法構建，OmniGAIA 綜合了源自真實世界資料的複雜多跳查詢，這些查詢需要跨模態推理和外部工具整合。此外，我們提出OmniAtlas，一個在工具整合推理範式下的原生全模態基礎智能體，具有主動全模態感知能力。通過後見之明引導樹探索策略和OmniDPO 合成的軌跡進行訓練以進行細粒度錯誤修正，OmniAtlas 有效增強了現有開源模型的工具使用能力。此項工作標誌著邁向針對真實場景的下一代原生全模態AI助手的一步。