{
  "source": "pdf",
  "markdown": "arXiv:2602.23229v1 [cs.CV] 26 Sep 2022\n\n# Risk-Aware World Model Predictive Control for\nGeneralizable End-to-End Autonomous Driving\n\nJiangxin Sun¹, Feng Xue¹, Teng Long¹, Chang Liu¹, Jian-Fang Hu², Wei-Shi Zheng², Nicu Sebe¹\n\n¹University of Trento, Trento, Italy, ²Sun Yat-sen University, Guangzhou, China\n\njiangxin.sun@unitn.it, feng.xue@unitn.it, teng.long@unitn.it, chang.liu@unitn.it, hujianf@mail.sysu.edu.cn, wszheng@ieee.org, nicu.sebe@unitn.it\n\n## Abstract\n\nWith advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of “only driving like the expert” suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill risk-avoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.\n\nDate: February 27, 2026\nCorrespondence: Feng Xue at feng.xue@unitn.it\n\n# 1 Introduction\n\nEnd-to-end autonomous driving (E2E-AD) [20, 100, 44, 102] aims to map sensor observations to control actions, e.g., steering, throttle, and brake, without relying on hand-crafted perception, prediction, or planning modules. Compared to traditional modular\n\npipelines [48, 51, 53, 74, 76, 101], E2E-AD offers a more unified representation of the driving task, enabling the policy to reason about complex interactions between the ego vehicle and dynamic environments. As such, E2E-AD has attracted growing attention due to its potential for simplified system design, joint optimization, and real-time decision-making.\n\n---\n\nFigure 1 Comparison between existing E2E-AD methods and RaWMPC. The first row shows the predicted trajectories, and the second row compares the core workflows. Black arrows denote test-time execution, while pink arrows indicate training-only steps. The comparison shows that prior methods often omit explicit hazard modeling and may trigger traffic violations, whereas RaWMPC uses a risk-aware world model to evaluate action consequences and select safe, compliant actions in critical scenes.\n\nEarly studies on E2E-AD [4, 6, 75] primarily focused on learning driving policies via reinforcement learning (RL) through online exploration. More recent research [5, 95] demonstrated that **rule-based** or **RL-based agents** leveraging privileged information (*e.g.*, bird's-eye-view segmentation and high-definition maps) can produce superior driving decisions. Building upon these insights, state-of-the-art methods [28, 62, 63, 81, 90] generally follow an **imitation learning (IL) framework** as shown in Fig. 1, where agents using only sensor inputs (*e.g.*, RGB images and LiDAR) are trained to replicate the privileged experts' behavior through knowledge distillation on both the driving policy and latent features. Although some works have attempted to enhance the driving performance via future motion modeling [55, 62, 63, 73], action-aware future prediction [23, 28, 38, 39], and the integration of large language models [14, 21, 35, 57], these approaches still adhere to the learning objective of “*driving like an expert*”, as demonstrated in Fig. 1 (a). They cannot fundamentally resolve the inherent generalization dilemma of imitation learning: *Since expert demonstrations cannot cover all scenarios and situations, imitation-based policies tend to produce unpredictable and often unsafe driving behavior when encountering unseen scenarios outside of expert demonstrations*. More recently, **model-based RL methods** [37, 85] have emerged and attempted to improve generalization by learning environmental dynamics and planning\n\nover them. However, as illustrated in Fig. 1 (b), most of them still aim to maximize the expected reward and lack explicit modeling and sampling of rare but high-risk situations, and thus continue to struggle to guarantee safety in these scenarios.\n\nIn this paper, we argue that “*enabling an E2E-AD system to learn and proactively avoid risky actions is more important than replicating expert driving behavior verbatim*”. Motivated by this perspective, we propose an E2E-AD framework that does not require any expert action supervision, called **Risk-aware World Model Predictive Control (RaWMPC)**, as shown in Fig. 1(c). This framework discards expert demonstrations and instead leverages a risk-aware world model to drive predictive control to overcome the generalization challenge. Different from model-based RL that trains an actor to maximize reward from imagined rollouts of a world model, the world model in RaWMPC predicts near-future states for a set of “candidate” driving behaviors and explicitly evaluates their risk, so as to select the lowest-risk candidate. To endow our world model with risk-awareness, we introduce a **risk-aware interaction** strategy: Starting from scratch, the model selects self-identified high-risk actions to interact with the environment, from which our world model learns to predict the consequences of diverse risky behaviors. Without any expert demonstration, our world model can reach strong performance from scratch, and can be further accelerated if a few video clips are provided by benchmarks\n\n---\n\nfor a light warm-up. Finally, to efficiently provide low-risk candidates at test time, we further propose a **self-evaluation distillation** for driving policy learning. The well-trained world model is leveraged to identify safe and risky behaviors from the sampled action space, and to distill this knowledge, via safety-risk contrastive learning, into a generative action proposal network. Experiments on Bench2Drive and NAVSIM show that RaWMPC, even without expert demonstrations, surpasses previous state-of-the-art methods, while the optional light warm-up further helps to accelerate the convergence and improve performance. More importantly, since RaWMPC learns risk-awareness from interaction rather than expert labels, it achieves substantially higher driving performance in previously unseen scenarios. Our main contributions can be summarized as follows:\n\n* We propose RaWMPC, an E2E-AD framework with zero expert requirement. Unlike IL and MBRL methods, RaWMPC uses a world model to select low-risk behaviors from multiple candidate actions, which naturally improves the interpretability and reliability of its decisions.\n* Within RaWMPC, we design a risk-aware interaction learning strategy that enables the world model to acquire risk-awareness purely from environment interaction, without any expert demonstration.\n* We introduce a self-evaluation distillation scheme for driving policy learning, which provides high-quality candidate actions at test time, even outperforming policies learned directly from expert demonstrations.\n\n# 2 Related Work\n\n## 2.1 End-to-End Learning in Autonomous Driving\n\nLearning-based autonomous driving approaches typically follow two main paradigms: imitation learning (IL) and reinforcement learning (RL) [7, 9]. Early studies explored RL extensively [1, 4, 6, 20, 32, 51, 58, 59, 75, 93] for its natural capability to refine driving strategies through interactive feedback. Subsequent works [37, 95] demonstrated that training sensor-based agents to imitate RL experts' behavior could lead to superior performance, prompting a shift toward leveraging privileged information (e.g., BEV segmentation and HD maps) to train stronger RL experts. More recently, model-based RL has revisited this line by learning explicit dynamics/world\n\nmodels and performing look-ahead rollouts [85], improving consequence-aware evaluation and sample efficiency. Nevertheless, these RL approaches are commonly driven by maximizing expected return, and they rarely provide an explicit mechanism to systematically discover and model rare-but-catastrophic outcomes, making reliable decisions in long-tail, high-risk scenarios still challenging.\n\nWith large-scale driving datasets, imitation learning has attracted increased attention and achieved state-of-the-art performance in closed-loop autonomous driving [40, 56, 72, 83]. Most IL approaches rely on collecting trajectory data and features from RL-based [37, 95] or rule-based [10, 65] experts using privileged information, and train sensor-based IL agents to replicate expert behaviors through knowledge distillation. A variety of research studies have explored ways to improve IL performance, such as multi-modal information fusion [10, 26, 54], object motion modeling [30, 55, 62, 63], action-aware future prediction [23, 28, 38, 39], feature alignment [27, 90], and the integration of large language models [14, 45, 56, 57, 64, 65, 83, 86]. Despite impressive results, the core objective of “driving like the expert” inherently limits generalization: expert demonstrations cannot cover all long-tail situations, and experts typically avoid dangerous behaviors, leaving IL agents with limited supervision on how to recognize and proactively avoid high-risk actions. Moreover, pure imitation often provides limited interpretability because it outputs a single action without explicitly comparing alternative actions via consequence evaluation. Motivated by these insights, we propose RaWMPC, a unified framework that replaces expert action supervision with risk-aware predictive control: it learns a risk-aware world model via a risk-aware interaction strategy that deliberately exposes the model to risky behaviors, and uses the learned model to predict and evaluate the consequences of multiple candidate actions, selecting low-risk behaviors with explicit risk evaluation.\n\n## 2.2 World Models\n\nWorld models approximate environment transitions under the Markov Decision Process and have demonstrated success in RL [17-20, 34, 46, 75, 77, 79] by predicting future states and rewards from current observations and actions. However, applying these models to complex tasks such as autonomous driving remains challenging. Previous research [1, 12, 15, 24, 36, 49, 50, 70-72, 78, 92, 96-98, 102] has mainly leveraged world models to generate controllable future driving trajectories (e.g., RGB\n\n---\n\nimages and 3D/4D representations) conditioned on specific actions and scene descriptions. Such predictive models can enlarge training data and increase diversity, potentially benefiting downstream imitation learning, especially for rare scenes such as traffic accidents.\n\nBeyond prediction, a few recent attempts have utilized world models to improve closed-loop autonomous driving performance. In particular, some works have started to connect world modeling with planning and online evaluation in driving settings [8, 39, 91], while others provide high-fidelity generative platforms that enable closed-loop evaluation [3, 11, 84]. In particular, model-based IL methods [23, 38, 39, 61] employ world models to support better imitation: LAW [38] enhances end-to-end driving by predicting future information in a latent world model to assist policy learning, and WoTE [39] performs online trajectory evaluation via a BEV world model to score candidate trajectories. More recently, model-based RL methods have drawn increasing attention. Think2Drive [37] designed a model-based RL expert to forecast action-conditioned future rewards for training a more effective critic network, and Raw2Drive [85] leveraged a world model pretrained from privileged experts to guide the learning of sensor agents. Despite these advances, most existing works still inherit supervision from experts or rewards, and they largely focus on imitation fidelity or expected-return maximization, lacking explicit mechanisms to systematically discover, model, and avoid rare-but-high-risk outcomes. In contrast, RaWMPC uses the world model as a risk evaluator within predictive control: we introduce a risk-aware interaction strategy to intentionally explore risky behaviors so that catastrophic consequences become predictable and avoidable, and we select actions by explicitly minimizing risk over multiple candidates, enhancing the interpretability, reliability, and generalization of the decision-making process.\n\n3 Method\n\nTo demonstrate our solution, Sec. 3.1 introduces the overall network structure and pipeline of our RaWMPC. Sec. 3.2 presents our training scheme, i.e., risk-aware interactive training, for efficient optimization. To ensure the whole E2E-AD system runs efficiently during testing, Sec. 3.3 illustrates a self-evaluation distillation method to train an action proposal network.\n\n3.1 Network Structure of RaWMPC\n\n3.1.1 Problem Setup\n\nWe consider a closed-loop end-to-end autonomous driving (E2E-AD) setting. At each time step $t$, the agent receives three inputs: a visual input $I_t$ (multi-view RGB images), an ego-centric measurement $M_t$ (velocity and position), and a set of candidate driving behaviors {$A_{t:t+H-1}^n$}$_{n=1}^N$, where $N$ is the number of candidates and $H$ is the planning horizon. Each step of action consists of three values: $\\mathbf{A} = (\\text{steer} \\in [-1, 1], \\text{throttle} \\in [0, 1], \\text{brake} \\in [0, 1])$. Based on the driving history ($I_{1:t}, M_{1:t}, A_{1:t-1}$), RaWMPC aims to select the best one $A_{t:t+H-1}^n$ from the candidates, so that the vehicle can move toward a destination while ensuring safety and compliance with traffic rules.\n\n3.1.2 Overview\n\nAs illustrated in Fig. 2, RaWMPC begins from input encoding. A visual encoder, an action encoder and an ego-state encoder are used to map $\\mathbf{I}_t, \\{\\mathbf{A}_{t:t+H-1}^n\\}_{n=1}^N$ and $\\mathbf{M}_t$ into embeddings $\\mathbf{r}_t, \\{\\mathbf{a}_{t:t+H-1}^n\\}_{n=1}^N$, and $\\mathbf{m}_t$, respectively. Then, based on the observed states $s_{1:t} = (\\mathbf{i}_{1:t}, \\mathbf{m}_{1:t})$, we use a world model to estimate its future state $\\hat{\\mathbf{s}}_{t+1:t+H}^n$ conditioned on each action embeddings $\\mathbf{a}_{t:t+H-1}^n$. Finally, we select the action that enables the ego vehicle to advance safely while avoiding traffic infractions, by decoding $\\hat{\\mathbf{s}}_{t+1:t+H}^n$ and computing a cost value:\n\n$$ A_{t:t+H-1}^* = A_{t:t+H-1}^{n*}, \\\\ \\text{where } n^* = \\underset{n \\in \\{1, \\dots, N\\}}{\\arg \\min} C(\\hat{\\mathbf{s}}_{t+1:t+H}^n). \\quad (1) $$\n\nwhere $C(\\cdot)$ denotes the cost function in the decoding process (will be detailed in Eq. (6)), and $n^*$ is the index of optimal action. Compared to imitation learning schemes, RaWMPC offers improved interpretability and introduces an explicit mechanism for decision validation and risk mitigation.\n\n3.1.3 World Model\n\nIn our pipeline, the world model, denoted as $\\mathcal{M}$, is employed to predict future states given an action. Specifically, conditioned on the observed states $s_{1:t} = (\\mathbf{i}_{1:t}, \\mathbf{m}_{1:t})$ and the potential next action $\\mathbf{a}_t^n$, the world model $\\mathcal{M}$ predicts a near-future state $\\hat{\\mathbf{s}}_{t+1}^n$. Then, for the further-future time step $\\{2, \\dots, H\\}$, the world model recursively rolls out, which can be formalized as an autoregressive factorization:\n\n$$ p_M(\\hat{\\mathbf{s}}_{t+1:t+H}^n | s_{1:t}, \\mathbf{a}_{1:t+H-1}^n) = \\prod_{k=1}^{H} p_M(\\hat{\\mathbf{s}}_{t+k}^n | (s_{1:t}, \\hat{\\mathbf{s}}_{t+1:t+k-1}^n), \\mathbf{a}_{1:t+k-1}^n) \\quad (2) $$\n\n---\n\n**Figure 2** Overview of RdWMPC. Multi-view images **I**<sub>t</sub>, ego state **M**<sub>t</sub>, and candidate action sequences {A<sup>n</sup><sub>t:t+H−1</sub>}<sup>N</sup><sub>n=1</sub> are encoded and rolled out by a world model over horizon H. Three decoders predict semantic segmentation, semantic-guided traffic events, and future ego states, enabling action evaluation for predictive control. Training combines offline warm-up on logged trajectories with online simulator interaction using world-model-guided exploration.\n\nwhere $p_M$ denotes the conditional distribution defined by the world model $M$. $\\mathbf{a}_{1:t+k-1}^n = [\\mathbf{a}_{1:t-1}, \\mathbf{a}_{t:t+k-1}^n]$ denotes a candidate action sequence containing action history.\n\n### **3.1.4 Semantic-Guided Decoding**\n\nOnce the world model predicts a sequence of future states, $\\hat{\\mathbf{s}}_{t+1:t+H}^n$, three transformer decoders separately map these states to task-specific outputs: semantic segmentation, potential traffic events (e.g., collision), and future ego-states (e.g., position). In what follows, we describe these decoders using the predicted state at time step $t+k$, i.e., $\\hat{\\mathbf{s}}_{t+k}^n$, where $k \\in \\{1, \\dots, H\\}$.\n\nTo enable a higher-level understanding of driving scenes and provide visual explanations for predicted traffic events, we inject semantic attention from the segmentation decoder into the event decoder. The **segmentation** decoder adopts a standard transformer attention:\n\n$$ \\text{Att}_{\\text{seg}}(\\mathbf{Q}_c, \\mathbf{K}_c, V_c) = \\text{softmax}(\\text{sim}(\\mathbf{Q}_c, \\mathbf{K}_c)) \\cdot V_c, \\quad (3) $$\n\nwhere $\\mathbf{Q}_c$ are learnable class queries, and $\\mathbf{K}_c, V_c$ are derived from visual tokens $\\mathbf{\\dot{i}}_{t+k}^n \\subset \\hat{\\mathbf{s}}_{t+k}^n$. In the final layer, we follow SegViT [88] to predict the one-hot semantic segmentation map $\\mathbf{Y}_{t+k}^n$ for each input class query. We then augment the **event** decoder by fusing its attention logs with the corresponding semantic attention logs from the last segmentation layer:\n\n$$ \\mathbf{Z}_e = \\mathbf{Q}_e \\mathbf{K}_e^{\\top}, \\quad \\mathbf{Z}_c = \\text{pad}(\\mathbf{Q}_c \\mathbf{K}_c^{\\top}) $$\n\n$$ \\hat{E}_{t+k}^n = \\text{sigmoid}(\\text{softmax}(\\mathbf{W}_e * [\\mathbf{Z}_e, \\mathbf{Z}_c])\\mathbf{V}_e). \\quad (4) $$\n\nwhere $\\mathbf{Q}_e$ are learnable event queries, and $\\mathbf{K}_e, V_e$ are computed from predicted future states $\\hat{\\mathbf{s}}_{t+k}^n$. $\\text{pad}(\\mathbf{\\cdot})$ zero-pads $\\mathbf{Z}_c$ to match the size of $V_e$, and $\\mathbf{W}_e$ is a $1 \\times 1$ convolution that fuses the concatenated literals. The output of the event decoder, $\\hat{E}_{t+k}^n \\in [0, 1]^\\alpha$, represents the probabilities of $\\alpha$ event types. Finally, for the future ego-state prediction, we decode the ego token $\\dot{\\mathbf{m}}_{t+k}^n \\subset \\hat{\\mathbf{s}}_{t+k}^n$ to obtain the speed and position $\\hat{\\mathbf{M}}_{t+k}^n$.\n\nGuided by the semantic attention map, the event decoder draws more attention to regions critical to specific events. For instance, when recognizing the vehicle collision event, the model focuses more on vehicle areas, improving the accuracy and reliability of event predictions.\n\n### **3.1.5 Action Selection and Predictive Control**\n\nGiven the decoder outputs, we perform predictive control by evaluating each candidate action sequence over the planning horizon *H* and selecting the one with the minimum predicted cost.\n\nSpecifically, for the *n*-th candidate $A_{t:t+H-1}^n$, we consider (i) progress toward the target and (ii) the risk of traffic-violation events. Let **p$$^\\star$$** be the target 3D position and $\\hat{\\mathbf{p}}_{t+k}^n \\subset \\hat{\\mathbf{M}}_{t+k}^n$ the predicted ego position at step $t+k$. We define the progress as the reduction in target distance:\n\n$$ \\hat{D}_{t+k}^{n} = \\| \\mathbf{p}^{\\star} - \\hat{\\mathbf{p}}_{t+k-1}^{n} \\|_{2} - \\| \\mathbf{p}^{\\star} - \\hat{\\mathbf{p}}_{t+k}^{n} \\|_{2}, \\quad (5) $$\n\n---\n\nWe then define the predictive-control objective as:\n\n$$ \n\\begin{equation} \n\\begin{split}\nC(\\hat{\\mathbf{s}}_{t+1:t+H}) &= \\sum_{k=1}^{H} \\eta_k (-\\hat{D}_{t+k}^{n} + \\sum_{j=1}^{\\alpha} \\lambda_j \\hat{E}_{t+k,j}^{n})\n\\end{split}\n\\tag{6}\n\\end{equation} \n$$\n\nwhere $\\eta_k = \\max(2^{-k+1}, 1/8)$ down-weights distant predictions to account for increasing uncertainty. We floor $\\eta_k$ at $1/8$ to avoid vanishing contributions from distant steps, which stabilizes planning when $H$ is moderately large. $\\lambda_j > 0$ reflects the severity of violation type $j$ (e.g., pedestrian/vehicle collisions receive larger weights). Finally, we select the action sequence that minimizes the horizon cost in Eq. (6), *i.e.*, a model-predictive control policy that favors faster progress while proactively reducing the probability of predicted violations.\n\n### 3.1.6 Overall Loss of RaWMPC\n\nThe RaWMPC framework is trained in an end-to-end manner with a world model loss $\\mathcal{L}_{\\text{world}}$, a segmentation loss $\\mathcal{L}_{\\text{seg}}$ from SegViT [88], ego-state loss $\\mathcal{L}_{\\text{ego}}$, and event loss $\\mathcal{L}_{\\text{event}}$:\n\n$$ \\mathcal{L} = \\mathcal{L}_{\\text{world}} + \\mathcal{L}_{\\text{seg}} + \\mathcal{L}_{\\text{ego}} + \\mathcal{L}_{\\text{event}} \\tag{7} $$\n\nFollowing SegViT [88], the segmentation term includes classification loss $\\mathcal{L}_{\\text{cls}}$ (cross-entropy) and the binary mask loss. The mask loss consists of a focal loss $\\mathcal{L}_{\\text{focal}}$ [43] and a dice loss $\\mathcal{L}_{\\text{dice}}$ [47] for optimizing the segmentation accuracy:\n\n$$ \\mathcal{L}_{\\text{seg}} = \\mathcal{L}_{\\text{cls}} + \\mathcal{L}_{\\text{focal}} + \\mathcal{L}_{\\text{dice}} \\tag{8} $$\n\nFor the other three losses, we use Mean Squared Error (MSE) to supervise the ego-state and world model supervision, and take Binary Cross Entropy (BCE) loss for the event decoder:\n\n$$ \\begin{align}\n\\mathcal{L}_{\\text{world}} &= \\frac{1}{H} \\sum_{k=1}^{H} \\|\\hat{\\mathbf{s}}_{t+k} - \\mathbf{s}_{t+k}\\|_{2}^{2}, \\\\\n\\mathcal{L}_{\\text{ego}} &= \\frac{1}{H} \\sum_{k=1}^{H} \\|\\hat{\\mathbf{M}}_{t+k} - \\mathbf{M}_{t+k}\\|_{2}^{2}, \n\\end{align} \\tag{9} $$\n\n$$ \\mathcal{L}_{\\text{event}} = \\frac{1}{H} \\sum_{k=1}^{H} BCE(\\hat{\\mathbf{E}}_{t+k}, \\mathbf{E}_{t+k}), $$\n\nwhere BCE(·) denotes the BCE loss. All the annotations $\\mathbf{s}_{t+k}$, $\\mathbf{M}_{t+k}$, $\\mathbf{E}_{t+k}$ along the executed rollout under $\\mathbf{A}_{t:t+H-1}$ are obtained from the simulator (e.g., CARLA).\n\n### 3.2 Risk-aware Interactive Training\n\nTo enable RaWMPC to evaluate diverse actions and identify risky scenarios, we propose a two-stage risk-aware interactive training scheme, as shown in Fig. 2. We first warm-start the world model from logged driving trajectories. Then, we refine it via online simulator interaction, intentionally collecting both good\n\nFigure 3 Different action-selection ranges under three driving modes in online simulator interaction.\n\nRed denotes high cost and green denotes low cost. **rand** samples uniformly from all candidates, **bad** samples from the high-cost region, and **good** samples from the low-cost one.\n\n(safe, goal-directed) and **bad** (hazardous) rollouts to improve generalization under out-of-distribution controls and to learn rare but safety-critical events. Notably, RaWMPC does not rely on expert action labels for policy learning. The optional warm-up stage, when used, serves solely to initialize the predictive world model from observed state transitions, rather than to imitate expert actions.\n\n#### 3.2.1 Offline World Model Warm-up\n\nWe bootstrap RaWMPC using a small set of logged trajectories to achieve simple and basic state-forecasting capability. Given state-action sequences $\\{(\\mathbf{s}_t, \\mathbf{a}_t)\\}_{t=1}^T$ from NAVSIM or CARLA, the world model predicts the next state $\\hat{\\mathbf{s}}_{t+1}$ and is trained with $\\mathcal{L}_{\\text{world}}$ to match the ground-truth $\\mathbf{s}_{t+1}$. We supervise the segmentation and ego-state decoders using the simulator-provided annotations. Since the warm-up trajectories contain no traffic violations, we train the event decoder with an all-zero target. In this way, only a small subset of training data (10%) is typically sufficient for warm-up, providing a reliable initialization for long-horizon rollouts and stabilizing subsequent world model optimization.\n\n#### 3.2.2 Online Simulator Interactive Training\n\nOffline warm-up data are mostly concentrated around human-like safe behaviors and thus provide limited coverage of hazardous or unconventional actions. To learn the consequences of risky behaviors, we perform world-model-guided exploration: selected simulator rollouts are fed back to refine the same world model, progressively improving prediction fidelity and risk sensitivity.\n\nSpecifically, to ensure temporal continuity and avoid unrealistic control jitter, we sample horizon-$H$ action sequences (segment-wise) rather than single-step actions (step-wise). The segment-wise sampling allows\n\n---\n\nsustained safe or risky behaviors to unfold and reveals\ntheir long-term consequences. At each training step,\nwe sample N<sub>s</sub> horizon-<i>H</i> candidate action sequences\n{A<sub>t:t+<i>H</i>-1</sub><sup>n</sup>}<sub>n=1</sub><sup>N<sub>s</sub></sup>, roll out future states s&#x0302;<sub>t+1:t+<i>H</i></sub><sup>n</sup> with\nthe current world model, evaluate their costs {C<sup>n</sup>}</sup>\nusing Eq. (6), and rank candidates by costs.\n\n**Modes for Interaction.** We define three patterns\nfor our risk-aware sampling strategy to select one\ncandidate from {$\\mathbf{A}_{t:t+H-1}^n$}$_{n=1}^{N_s}$ to execute, as shown\nin Fig. 3:\n\n* Rand samples uniformly from all candidates;\n\n• Bad samples from high-cost candidates;\n\n• Good samples from low-cost candidates.\n\nAt the start of segment *r*, the practical control mode\nis sampled according to probability:\n\n$$\nm_r = \\begin{cases}\n\\text{rand, } & \\text{w.p. } \\epsilon_1, \\\\\n\\text{bad, } & \\text{w.p. } (1 - \\epsilon_1)\\epsilon_2, \\\\\n\\text{good, } & \\text{w.p. } (1 - \\epsilon_1)(1 - \\epsilon_2),\n\\end{cases} \\qquad (10)\n$$\n\nwhere “w.p.” means “with probability”. $\\epsilon_1$ controls broad action-space exploration, and $\\epsilon_2$ controls the fraction of risk-seeking interaction within model-guided sampling.\n\nSoft **Candidates** Selection in Three Modes. Given\nsorted candidate actions with costs {$C^n$}, we con-\nstruct two cost-quantile sets: $\\mathcal{N}_{\\text{good}}$ as the bottom-$K$-\ncandidates and $\\mathcal{N}_{\\text{bad}}$ as the top-$K$ candidates. To\navoid low-information trajectories, we filter $\\mathcal{N}_{\\text{bad}}$ by\nremoving degenerate rollouts (e.g., those caused by\nunrealistic excessive control jumps), yielding $\\tilde{\\mathcal{N}}_{\\text{bad}}$.\nIn the **rand** mode, we randomly select the executing\naction sequence from all candidates.\n\nIn the good mode, rather than **deterministically** selecting the minimum-cost candidate, we sample from $\\mathcal{N}_{\\text{good}}$ using a **soft distribution** to preserve diversity among low-cost plans and mitigate bias from imperfect model predictions:\n\n$$\nP(n | \\mathbf{good}) \\propto \\exp(-C^n / \\tau_g), \\quad n \\in \\mathcal{N}_{good}. \\quad (11)\n$$\n\nwhere τ<sub>g</sub> is a temperature hyper-parameter that con-\ntrols the softness of the sampling distributions in\nthe **good** mode, trading off greediness for diversity.\nThis stochastic selection avoids repeatedly executing\na single estimated optimum and encourages broader\ncoverage of nominal behaviors.\n\nIn the **bad** mode, instead of always executing the\nmaximum-cost trajectory, we sample from high-cost\n\nFigure 4 Self-Evaluation Distillation for Policy Learn-\ning. A cVAE is trained with RaWMPC-scored actions in\na contrastive manner, pulling the condition prior toward\npositives and pushing it away from negatives. The well-\ntrained decoder serves as the test-time action proposer.\n\ncandidates to deliberately expose the model to a spec-\ntrum of risky outcomes that are under-represented\nin safe logs:\n\n$$\nP(n \\mid \\text{bad}) \\propto \\exp(C^n / \\tau_b), \\quad n \\in \\tilde{\\mathcal{N}}_{\\text{bad}}. \\quad (12)\n$$\n\nwhere τ_b is a temperature hyper-parameter. Compared to argmax selection, this soft sampling strategy prevents over-concentration on extreme or degenerate failures while still biasing interaction toward high-risk regions.\n\nIn this way, segment-wise interaction and soft cost-\nbased sampling bias exploration toward temporally\ncoherent and informative safe and hazardous tra-\njectories, enabling the world model to learn both\nreasonable dynamics and safety-critical consequences\nfor risk-aware decision making.\n\n---\n\n**3.3 Self-Evaluation Distillation for Policy**\n**Learning**\n\nAfter risk-aware interactive training, RaWMPC can\nreliably *score* candidate action sequences by predict-\ning their long-horizon consequences. To reduce the\ncost of online optimization at test time, we distill\nthis evaluation capability into a lightweight *action*\nproposal network, enabling efficient inference *without*\n*expert demonstrations*. The action proposal network\ncorresponds to the “Guidance” module illustrated in\nFig. 1(c), and is used to generate candidate action\nsequences for predictive control. Our key idea is\nto use RaWMPC as a self-evaluator to pseudo-label\nsampled actions and train a generative policy via\ncontrastive learning.\n\n**3.3.1 Action Sampling and Pseudo-labeling**\n\nGiven a state history $\\mathbf{s}_{1:t}$ (simplified as $\\mathbf{s}$), we\nrandomly sample $N_s$ horizon-$H$ action sequences\n$\\{ \\mathbf{A}_{t:t+H-1}^n \\}_{n=1}^{N_s}$ (simplified as $\\{ \\mathbf{A}^n \\}$) and compute\ntheir costs $\\{ C^n \\}$ with the pretrained RaWMPC. We\nthen form pseudo labels by ranking costs: the lowest-\ncost sequence is treated as a positive example $\\mathbf{A}^+$,\nand the top-$K$ highest-cost sequences are treated\nas negatives $\\{ \\bar{\\mathbf{A}}_j^K \\}_{j=1}^K$. This construction transfers\nRaWMPC's knowledge (low-risk / high-quality ac-\ntions) to the proposal network while avoiding any\nexternal supervision.\n\n3.3.2 Action Proposal Network\n\nFollowing [60, 87], we adopt a conditional VAE (cVAE) with an action encoder $q_{\\theta}(z|\\mathbf{A}, \\mathbf{s})$, a conditional prior $p_{\\gamma}(z|\\mathbf{s})$, and a decoder $p_{\\psi}(\\mathbf{A}|z, \\mathbf{s})$. The decoder serves as the proposal policy at inference.\n\nFor the positive action, we obtain a Gaussian pos-\nterior $q^+ = q_{\\theta}(z|\\mathbf{A}^{+}, \\mathbf{s}) = \\mathcal{N}(\\mu^{+}, \\text{diag}((\\sigma^{+})^{2}))$ and\ntrain the decoder to reconstruct $\\mathbf{A}^{+}$. For each neg-\native action $\\bar{\\mathbf{A}}_{j}$, we compute $q_{j} = q_{\\theta}(z|\\bar{\\mathbf{A}}_{j}, \\mathbf{s}) =$\n$\\mathcal{N}(\\bar{\\mu}_{j}, \\text{diag}((\\sigma_{j}^{+})^{2}))$, but **do not** reconstruct nega-\ntives to prevent the generator from imitating unsafe\nbehaviors. The conditional prior is $p^{c} = p_{\\gamma}(z|\\mathbf{s}) =$\n$\\mathcal{N}(\\bar{\\mu}^{c}, \\text{diag}((\\sigma^{c})^{2}))$.\n\n3.3.3 Contrastive Training Objective\n\nTo address the lack of expert supervision in policy\nlearning, we use an InfoNCE objective to make the\nconditional prior predictive of high-quality actions.\nConcretely, there are two potential contrastive for-\nmulations:\n\n• Using $p^c$ as the anchor, pulling $p^c$ toward $q^+$ while pushing it away from $\\{q^-\\}_{j=1}^K$.\n• Using $q^+$ as the anchor, pulling $q^+$ toward $p^c$ while pushing it away from $\\{q^-\\}_{j=1}^K$.\n\nWe empirically found that the former often produces under-optimized trajectories. One possible reason is that negative samples are far more numerous and broadly cover the latent space, therefore $p^c$ is easily driven to a region that is far from most negatives yet not sufficiently close to the positive. In contrast, the latter explicitly pulls $p^c$ toward the statistical center of the positive posterior and, via $q^+$, indirectly separates it from the negatives, leading to more stable learning and higher-quality trajectories. Therefore, we adopt the latter design choice to define our InfoNCE objective as:\n\n$$\n\\begin{align*}\n\\mathcal{L}_c &= -\\log \\frac{\\exp(\\ell^+)}{\\exp(\\ell^+) + \\sum_{j=1}^K \\exp(\\ell_j^-)}, \\\\\n\\ell^+ &= -\\mathcal{D}(q^+, p^c)/\\tau, \\\\\n\\ell_j^- &= -\\mathcal{D}(q^+, q_j^-)/\\tau,\n\\end{align*}\n\\tag{13}\n$$\n\nwhere $D(\\cdot, \\cdot)$ is the Wasserstein-$2$ distance between Gaussians and $\\tau$ is a temperature.\n\n3.3.4 Overall Loss of Action Proposal Network\n\nThe total loss of our cVAE combines reconstruction,\nKL regularization, and contrastive loss:\n\n$$\n\\mathcal{L}_{\\text{total}} = E_{z \\sim q^{+}} [-\\log p_{\\psi}(\\mathbf{A}^{+} | z, \\mathbf{s})] + \\beta D_{\\text{KL}}(q^{+} \\| p^{c}) + \\lambda \\mathcal{L}_{c}. \\quad (14)\n$$\n\nThis self-evaluation distillation trains a fast proposal\npolicy that generates candidate action sequences con-\nsistent with RaWMPC’s evaluations, eliminating the\nneed for expert demonstrations during policy learn-\ning.\n\n4 Experiments\n\nIn this section, we present a comprehensive perfor-\nmance comparison between the proposed framework\nand state-of-the-art methods. We also conduct ex-\ntensive ablation studies to assess the effectiveness of\nour predictive control approach.\n\n4.1 Benchmarks\n\nFollowing prior works [26, 40, 41], we evalu-\nate RaWMPC on two widely used benchmarks:\nBench2Drive [29] and Navsim [11]. They are com-\nplementary: Bench2Drive provides fully interactive\n\n---\n\n**Table 1** Comparison with SOTA approaches on the closed-loop Bench2Drive benchmark on CARLA simulator. ↑ means the higher the better. DS is taken as the primary metric in comparison and we rank all the methods accordingly, with bold indicating best performance.\n\n<table><thead><tr><th>Method</th><th>Venue</th><th>Scheme</th><th>DS↑</th><th>SR(%)↑</th><th>Efficiency↑</th><th>Comfortness↑</th></tr></thead><tbody><tr><td>VAD [31]</td><td>ICCV 2023</td><td>IL</td><td>42.35</td><td>15.00</td><td>157.94</td><td>46.01</td></tr><tr><td>SparseDrive [69]</td><td>ICRA 2025</td><td>IL</td><td>44.54</td><td>16.71</td><td>170.21</td><td>48.63</td></tr><tr><td>GenAD [99]</td><td>ECCV 2024</td><td>IL</td><td>44.81</td><td>15.90</td><td>-</td><td>-</td></tr><tr><td>UniAD [25]</td><td>CVPR 2023</td><td>IL</td><td>45.81</td><td>16.36</td><td>129.21</td><td>43.58</td></tr><tr><td>MomAD [68]</td><td>CVPR 2025</td><td>IL</td><td>47.91</td><td>18.11</td><td>174.91</td><td><u>51.20</u></td></tr><tr><td>UAD [16]</td><td>T-PAMI 2025</td><td>IL</td><td>49.22</td><td>20.45</td><td>189.53</td><td><b>52.71</b></td></tr><tr><td>BridgeAD [89]</td><td>CVPR 2025</td><td>IL</td><td>50.06</td><td>22.73</td><td>-</td><td>-</td></tr><tr><td>TCP [81]</td><td>NeurIPS 2022</td><td>IL</td><td>59.90</td><td>30.00</td><td>76.54</td><td>18.08</td></tr><tr><td>WoTE [39]</td><td>ICCV 2025</td><td>IL</td><td>61.71</td><td>31.36</td><td>-</td><td>-</td></tr><tr><td>DriveDPO [61]</td><td>NeurIPS 2025</td><td>IL &amp; RL</td><td>62.02</td><td>30.62</td><td>166.80</td><td>26.79</td></tr><tr><td>ThinkTwice [28]</td><td>CVPR 2023</td><td>IL</td><td>62.44</td><td>31.23</td><td>69.33</td><td>16.22</td></tr><tr><td>DriveTransformer [30]</td><td>ICLR 2025</td><td>IL</td><td>63.46</td><td>35.01</td><td>100.64</td><td>20.78</td></tr><tr><td>DriveAdapter [27]</td><td>ICCV 2023</td><td>IL</td><td>64.22</td><td>33.08</td><td>70.22</td><td>16.01</td></tr><tr><td>Raw2Drive [85]</td><td>NeurIPS 2025</td><td>RL</td><td>71.36</td><td>50.24</td><td><u>214.17</u></td><td>22.42</td></tr><tr><td>Hydra-NeXt [40]</td><td>ICCV 2025</td><td>IL</td><td>73.86</td><td>50.00</td><td>197.76</td><td>20.68</td></tr><tr><td>HiP-AD [73]</td><td>ICCV 2025</td><td>IL</td><td>86.77</td><td>69.09</td><td>203.12</td><td>19.36</td></tr><tr><td><b>RaWMPC w/o Warm-up</b></td><td></td><td>PC</td><td><u>87.34</u></td><td><u>69.62</u></td><td>203.25</td><td>30.95</td></tr><tr><td><b>RaWMPC</b></td><td></td><td>PC</td><td><b>88.31</b></td><td><b>70.48</b></td><td>206.85</td><td>32.65</td></tr><tr><td colspan=\"7\"><b>Pretrained VLM-based Approach</b></td></tr><tr><td>ReAL-AD [44]</td><td>ICCV 2025</td><td>IL</td><td>41.17</td><td>11.36</td><td>-</td><td>-</td></tr><tr><td>Dual-AEB [94]</td><td>ICRA 2025</td><td>IL</td><td>45.23</td><td>10.00</td><td>-</td><td>-</td></tr><tr><td>ETA [21]</td><td>ICCV 2025</td><td>IL</td><td>74.33</td><td>48.33</td><td>186.04</td><td>25.77</td></tr><tr><td>VLR-Drive [35]</td><td>ICCV 2025</td><td>IL</td><td>75.01</td><td>50.00</td><td>122.52</td><td>0.59</td></tr><tr><td>ORION [14]</td><td>ICCV 2025</td><td>IL</td><td>77.74</td><td>54.62</td><td>151.48</td><td>17.38</td></tr><tr><td>SimLingo [57]</td><td>CVPR 2025</td><td>IL</td><td>85.94</td><td>66.82</td><td><b>244.18</b></td><td>30.76</td></tr></tbody></table>\n\nclosed-loop evaluation in CARLA [12] with dense annotations, while NAVSIM evaluates large-scale real-world planning via a data-driven, non-reactive simulation-based short-horizon rollout with safety- and progress-aware metrics.\n\n**Bench2Drive.** Bench2Drive is a CARLA Leadboard v2 closed-loop benchmark for multi-ability stress testing under complex interactions (e.g., cut-ins, overtakes, detours, emergency braking, and give-way). Its official dataset contains ~2M fully annotated frames from short clips spanning 44 scenarios, 23 weather conditions, and 12 towns; the commonly used *Base* training set contains 1K clips. Closed-loop evaluation is performed on 220 short routes (each focused on a single scenario), enabling stable and fine-grained comparison. We report four official metrics: *Driving Score* (*DS*), *Success Rate* (*SR*), *Efficiency*, and *Comfortness*. *DS* is the primary aggregate score with penalties for safety and rule violations; *SR* measures successful completion; *Efficiency* reflects progress; and *Comfortness* captures motion smoothness.\n\n**NAVSIM.** NAVSIM benchmarks sensor-based planning on large-scale real-world data built on Open-Scene (a planning-oriented reprocessing of nuPlan logs). The task predicts a 4-second future ego trajectory (typically 8 waypoints) given a short history (e.g., 1.5 seconds) of observations. Following prior works [39, 61], we use the official splits: NAvtrain (~103K samples) and Navtest (~12K samples). We report NAVSIM metrics including *NC*, *DAC*, *EP*, *TTC*, *C*, and the primary score *PDMS*, where $\\text{PDMS} = \\text{NC} \\cdot \\text{DAC} \\cdot \\frac{(5\\text{EP} + 5\\text{TTC} + 2\\text{C})}{12}$. These metrics jointly capture safety, compliance, progress, and motion quality. All results are computed with the official toolkits and recommended splits.\n\nOur experiments combine offline warm-up with online simulator interaction, leveraging RaWMPC to learn predictive dynamics and risk-aware decision making. We adopt Bench2Drive for interactive closed-loop evaluation and NAVSIM for large-scale real-world generalization. We further conduct ablations to analyze key training components and strategies.\n\n---\n\n**Table 2** Comparison with the SOTA approaches on NAVSIM test set. ↑ means the higher the better. PDMS is taken as the primary metric in comparison and we rank all the methods accordingly, with bold indicating best performance.\n\n<table><thead><tr><th>Method</th><th>Venue</th><th>Scheme</th><th>NC↑</th><th>DAC↑</th><th>EP↑</th><th>TTC↑</th><th>C↑</th><th>PDMS↑</th></tr></thead><tbody><tr><td>Human</td><td>-</td><td>-</td><td>100</td><td>100</td><td>87.5</td><td>100</td><td>99.9</td><td>94.8</td></tr><tr><td>DrivingGPT [8]</td><td>ICCV 2025</td><td>IL</td><td>98.9</td><td>90.7</td><td>79.7</td><td>94.9</td><td>100.0</td><td>82.4</td></tr><tr><td>UniAD [25]</td><td>CVPR 2023</td><td>IL</td><td>97.8</td><td>91.9</td><td>78.8</td><td>92.9</td><td>100.0</td><td>83.4</td></tr><tr><td>Latent TransFuser [10]</td><td>T-PAMI 2023</td><td>IL</td><td>97.4</td><td>92.8</td><td>79.0</td><td>92.4</td><td>100.0</td><td>83.8</td></tr><tr><td>PARA-Drive [80]</td><td>CVPR 2024</td><td>IL</td><td>97.9</td><td>92.4</td><td>79.3</td><td>93.0</td><td>99.8</td><td>84.0</td></tr><tr><td>TransFuser [10]</td><td>T-PAMI 2023</td><td>IL</td><td>97.7</td><td>92.7</td><td>79.8</td><td>92.7</td><td>100.0</td><td>84.5</td></tr><tr><td>LAW [38]</td><td>ICLR 2025</td><td>IL</td><td>96.4</td><td>95.4</td><td>81.7</td><td>88.7</td><td>99.9</td><td>84.6</td></tr><tr><td>World4Drive [100]</td><td>ICCV 2025</td><td>IL</td><td>97.4</td><td>94.3</td><td>79.9</td><td>92.8</td><td>100.0</td><td>85.1</td></tr><tr><td>DiffusionDrive [42]</td><td>CVPR 2025</td><td>IL</td><td>98.2</td><td>96.2</td><td><b>88.2</b></td><td>94.7</td><td>100.0</td><td>88.1</td></tr><tr><td>WoTE [39]</td><td>ICCV 2025</td><td>IL</td><td>98.5</td><td>96.8</td><td>81.9</td><td>94.9</td><td>99.9</td><td>88.3</td></tr><tr><td>Hydra-NeXt [40]</td><td>ICCV 2025</td><td>IL</td><td>98.1</td><td>97.7</td><td>81.8</td><td>94.6</td><td>100.0</td><td>88.6</td></tr><tr><td>UAD [16]</td><td>T-PAMI 2025</td><td>IL</td><td><b>99.5</b></td><td>96.9</td><td>78.8</td><td><b>97.5</b></td><td>100.0</td><td>89.3</td></tr><tr><td>DriveDPO [61]</td><td>NeurIPS 2025</td><td>IL & RL</td><td>98.5</td><td>98.1</td><td>84.3</td><td>94.8</td><td>99.9</td><td>90.0</td></tr><tr><td>GoalFlow [82]</td><td>CVPR 2025</td><td>IL</td><td>98.4</td><td><b>98.3</b></td><td>85.0</td><td>94.6</td><td>100.0</td><td>90.3</td></tr><tr><td><b>RaWMPC w/o Warm-up</b></td><td></td><td>PC</td><td>98.3</td><td><u>98.2</u></td><td>85.3</td><td>94.5</td><td>99.9</td><td>90.5</td></tr><tr><td><b>RaWMPC</b></td><td></td><td>PC</td><td><u>98.9</u></td><td><b><u>98.3</u></b></td><td><u>86.1</u></td><td><u>95.6</u></td><td>99.9</td><td><b><u>91.3</u></b></td></tr></tbody></table>\n\n## 4.2 Implementation Details\n\n**Network architecture.** We employ a pretrained ViT [66] as the vision encoder and use the SegViT segmentation head [88] as the segmentation decoder. BEV features are extracted from multi-view images using the query-based view transformer [41]. Following previous works [10, 39], the input image resolution is $1024 \\times 256$, and the BEV feature map resolution is $256 \\times 256$. We use down-sampling factors of 32 (front-view) and 16 (BEV), resulting in 512 visual tokens $i_t$ (256 per branch). Measurement inputs are encoded through an MLP-based encoder into 4 measurement tokens $m_t$, and driving actions, represented as three scalar values (steer, throttle, brake), are transformed into 3 action tokens $a_t$ via a linear layer. We implement the world model as a transformer with 4 layers and 8 attention heads. RaWMPC predicts $H=10$ future steps conditioned on the past 5 observed steps, and evaluates $N=10$ candidate action sequences proposed by the distilled action proposal network (described in Section 3.3) using the cost in Eq. (6) during inference. During action selection, we use $\\eta_k = \\max(2^{-k+1}, 1/8)$ to downweight distant predictions, and set the event severity weights to $\\lambda_j = 10, 15, 30$ for off-lane driving, traffic-sign violations, and collisions, respectively.\n\n**Training.** RaWMPC is trained with the two-stage risk-aware interactive training strategy described in Section 3.2. We first perform an offline warm-up using\n\n10% of the training data (100 clips for Bench2Drive and 10K samples for NAVSIM), and then refine the model via online interaction using the proposed risk-aware training scheme. The random-sampling probability $\\epsilon_1$ is linearly annealed from 1 to 0, while the risk-sampling probability $\\epsilon_2$ is linearly increased from 0 to 0.3. We maintain a replay buffer of size 10K frames to store recent interaction data (e.g., RGB images, semantic segmentation, ego measurements, and traffic-event annotations). We use segment-wise sampling of horizon-$H=10$ action sequences to ensure temporal continuity. In good/bad modes, we rank the $N_s=50$ candidates by cost and sample from the bottom/top-$K=5$ sets using temperatures $\\tau_g = 0.5$ and $\\tau_b = 1.0$. An episode terminates when any of the following conditions is met: (1) the ego vehicle incurs 3 collisions, (2) the ego vehicle goes off-road or remains stuck for 100 consecutive steps, or (3) the ego vehicle successfully completes the route. Across offline warm-up and online interaction, we train RaWMPC using a total of 1K clips on Bench2Drive and 100K samples on NAVSIM (comparable in scale to the official training sets for fair comparison), on four NVIDIA A100 GPUs. We use Adam [33] with an initial learning rate of $10^{-4}$ decayed to $10^{-5}$, and a batch size of 16.\n\n**Self-evaluation distillation.** For self-evaluation distillation (Section 3.3), the action proposal network is implemented as a cVAE [67] with a 32-dimensional latent space. During contrastive training, we sample $N_s=50$ action sequences, treat the minimum-cost\n\n---\n\nsequence as the positive example $\\boldsymbol{A}^{+}$, and use the top-$K=5$ highest-cost sequences as negatives. We optimize the proposal network with the objective in Section 3.3, using temperature $\\tau=0.3$, a KL weight schedule $\\beta: 0 \\to 0.1$, and a contrastive weight $\\lambda=0.1$. At inference, we sample $N=10$ candidates from the cVAE decoder and select the final action by minimizing the predicted cost (Eq. (6)) under the world-model rollout.\n\n## 4.3 Comparison with the State-of-the-Art\n\nIn this section, we compare RaWMPC with state-of-the-art end-to-end driving methods on the closed-loop Bench2Drive benchmark in CARLA and the NAVSIM test set. To reflect our main claim, we report two training settings: (i) **w/o warm-up**, where RaWMPC is trained without using any offline logged video, and (ii) the **full** setting, where a small set of logged driving trajectories is used as an optional warm start. The warm start empirically accelerates convergence and further improves final performance, while RaWMPC already surpasses prior state-of-the-art even without it.\n\n### 4.3.1 Evaluation on Bench2Drive\n\nTable 1 illustrates the closed-loop results on Bench2Drive. RaWMPC achieves the best overall performance among all compared methods, reaching 88.31 DS and 70.48% SR in the full setting. More importantly, even **without warm-up** (i.e., without logged trajectories), RaWMPC still attains 87.34 DS and 69.62% SR, surpassing strong recent baselines such as HiP-AD [73] (86.77 DS / 69.09% SR) and the pretrained-VLM method SimLingo [57] (85.94 DS / 66.82% SR). In addition, RaWMPC maintains competitive efficiency and achieves higher comfortness than most high-performing closed-loop agents, indicating that the gains are not obtained by aggressive maneuvers but by more reliable decision-making.\n\n### 4.3.2 Evaluation on NAVSIM\n\nTable 2 summarizes the results on NAVSIM. RaWMPC achieves the highest **PDMS of 91.3** among all learning-based methods. Without warm-up, RaWMPC still reaches 90.5 PDMS, already outperforming previous best methods (e.g., GoalFlow [82]: 90.3). The warm start further improves PDMS (90.5→91.3), consistent with the observation that a small amount of logged trajectories can accelerate convergence and improve performance, while not being required to achieve state-of-the-art results.\n\n### 4.3.3 Generalization under weather-induced domain shift\n\nTo evaluate robustness beyond the training distribution, we conduct a weather-shift study where all methods are evaluated exclusively on *Rainy* scenarios while being trained on either *Sunny only* or *Sunny & Rainy* data (Table 3). A key observation is that imitation-based methods are sensitive to training-domain coverage: when rainy conditions are absent from training (Sunny-only), their performance drops notably, reflecting limited transfer to previously unseen environments. In contrast, RaWMPC achieves the best DS and SR under both training regimes, and notably still significantly outperforms strong IL baselines (LAW, WoTE) and SimLingo when trained on *Sunny only* (i.e., facing an unseen rainy target domain). Moreover, compared with SimLingo, RaWMPC exhibits substantially smaller degradation when rainy data is removed from training, indicating stronger robustness to previously unseen conditions.\n\nFigure 5 shows a qualitative example of this *Sunny-only* → *Rainy* shift. LAW fails to recognize the lead vehicle under the altered visual conditions and results in a high-severity frontal collision. WoTE and SimLingo attempt evasive maneuvers that reduce the impact relative to a direct frontal crash, yet the scene still ends in a side-swipe/rear collision. A plausible explanation is that the rainy shift degrades the reliability of the perception-decision stack, while the downstream policy does not explicitly optimize for minimum-risk clearance under uncertainty, yielding insufficient safety margins during close-proximity avoidance (e.g., inaccurate motion anticipation or mismatched ego response on wet roads). By contrast, RaWMPC explicitly evaluates the predicted consequences of candidate action sequences with a risk-aware world model and selects the minimum-risk predictive-control behavior, maintaining safe clearance under uncertainty.\n\nWe attribute this advantage to RaWMPC's risk-aware predictive-control formulation: instead of reproducing expert actions, RaWMPC learns *risk-awareness from interaction* and selects actions by minimizing predicted risk via the learned world model. Such an objective encourages transferable decision principles (e.g., maintaining safe margins and acting conservatively under uncertainty) that remain effective when appearance and dynamics change across domains. Thus, RaWMPC is less dependent on exhaustive expert action coverage for corner cases, which better matches the long-tail nature of real-world deployment where unseen scenarios are inevitable.\n\n---\n\nFigure 5 Qualitative comparison under weather-induced domain shift (Sunny-only → Rainy). All methods are trained on *Sunny-only* data and evaluated in *Rainy* conditions. LAW [38] misses the lead vehicle, causing a severe frontal collision. WoTE [39] and SimLingo [57] reduce severity by evasive maneuvers but still collide due to degraded perception-decision reliability and weak safety-margin enforcement. RaWMPC avoids collisions by selecting the minimum-risk predictive-control action under uncertainty.\n\n**Table 3** Performance comparison under domain shift. All methods are trained on either *Sunny only* or *Sunny & Rainy* data, and evaluated exclusively on *Rainy* scenarios. ↑ indicates higher is better.\n\n<table><thead><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Venue</th><th rowspan=\"2\">Scheme</th><th rowspan=\"2\">Training Data</th><th colspan=\"2\">Tested on Rainy</th></tr><tr><th>DS↑</th><th>SR(%)↑</th></tr></thead><tbody><tr><td rowspan=\"2\">LAW</td><td rowspan=\"2\">ICLR 2025</td><td rowspan=\"2\">IL</td><td>Sunny &amp; Rainy</td><td>34.54</td><td>7.09</td></tr><tr><td>Sunny only</td><td>23.58</td><td>3.56</td></tr><tr><td rowspan=\"2\">WoTE</td><td rowspan=\"2\">ICCV 2025</td><td rowspan=\"2\">IL</td><td>Sunny &amp; Rainy</td><td>36.54</td><td>7.85</td></tr><tr><td>Sunny only</td><td>28.65</td><td>5.21</td></tr><tr><td rowspan=\"2\">SimLingo (Pretrained-VLM)</td><td rowspan=\"2\">CVPR 2025</td><td rowspan=\"2\">IL</td><td>Sunny &amp; Rainy</td><td>51.69</td><td>13.68</td></tr><tr><td>Sunny only</td><td>33.49</td><td>8.97</td></tr><tr><td rowspan=\"2\">RaWMPC</td><td rowspan=\"2\">–</td><td rowspan=\"2\">PC</td><td>Sunny &amp; Rainy</td><td><strong>53.67</strong></td><td><strong>14.96</strong></td></tr><tr><td>Sunny only</td><td><strong>41.36</strong></td><td><strong>10.83</strong></td></tr></tbody></table>\n\n### 4.3.4 Qualitative visualization of predictive control\n\nWe provide some visualization results of the predictive control procedure of RaWMPC in Fig. 6. Given RGB observations and high-level navigation commands (e.g., keep going straight or merge left), our generative policy proposes a small set of candidate action sequences (e.g., keep going straight, detour, brake, and lane change). For each candidate, the risk-aware world model predicts the near-future semantic traffic state and the decoding module evaluates its consequence from both task progress and safety perspectives, including collision risk, off-lane/sidewalk intrusion risk, and progress-related penalties such as getting stuck in traffic. The final action is selected by comparing these predicted consequences and choosing the minimum-cost one under the navigation goal.\n\nIn the first case, going straight collides with a crossing\n\npedestrian, while detours either collide with an oncoming vehicle or drive onto the sidewalk; RaWMPC chooses *slow down, straight briefly, then stop* to safely stop in front of the pedestrian (instead of an overly conservative early brake). In the second case, going straight or merging left immediately causes a collision, steering right hits a parked vehicle, and stopping leads to a deadlock; thus RaWMPC selects *pause briefly, then merge left* for a collision-free merge. These cases demonstrate that RaWMPC can proactively avoid risky behaviors by explicitly forecasting and comparing action consequences, rather than merely following a single command or relying on a fixed fallback maneuver.\n\n## 4.4 Ablation Study\n\nIn this section, we provide comprehensive ablation studies of the proposed approach using the\n\n---\n\nFigure 6 Visualization of the predictive control procedure. At time t, we show the front-view and the BEV images (with segmentation). Dashed curves indicate candidate actions and highlighted agents denote key risks. Rollouts from t + 1 to t + 5 illustrate predicted consequences, and the bottom panel reports each action’s outcomes and costs (e.g., collision, sidewalk intrusion, stopping distance). **Scenario 1:** RaWMPC slows down, proceeds briefly, then stops for the pedestrian. **Scenario 2:** RaWMPC waits briefly, then turns left to avoid both the front-left vehicle and the parked car.\n\nBench2Drive dataset.\n\n4.4.1 Analysis of Framework\n\nTable 4 analyzes core components aligned with our\nmodel design (Sec. 3.1.1). *w/o Semantic Guidance*\nremoves semantic-guided event decoding, i.e., the\nfusion of semantic attention from the segmentation\ndecoder into the event decoder. This leads to a clear\ndrop (DS 88.31→82.36, SR 70.48%→62.69%), show-\ning that accurate safety-event prediction is crucial\nfor risk-aware cost evaluation. *w/o Segmentation*\n\n**Table 4** Ablation study on the proposed framework.\n\n<table>\n   <thead>\n    <tr>\n     <td>Method</td>\n     <td colspan=\"2\">Metrics</td>\n    </tr>\n    <tr>\n     <td></td>\n     <td>DS↑</td>\n     <td>SR(%)↑</td>\n    </tr>\n   </thead>\n   <tbody>\n    <tr>\n     <td>Entire RaWMPC (Ours)</td>\n     <td>88.31</td>\n     <td>70.48</td>\n    </tr>\n    <tr>\n     <td>w/o Semantic Guidance</td>\n     <td>82.36 -5.95</td>\n     <td>62.69 -7.79</td>\n    </tr>\n    <tr>\n     <td>w/o Segmentation Decoder</td>\n     <td>70.85 -17.46</td>\n     <td>48.95 -21.53</td>\n    </tr>\n    <tr>\n     <td>w/o Action Selection</td>\n     <td>61.35 -26.96</td>\n     <td>30.98 -39.50</td>\n    </tr>\n   </tbody>\n  </table>\n\nDecoder further removes the segmentation decoding branch and its supervision, resulting in a large degra-\n\n---\n\n**Table 5** Ablation study on the risk-aware training.\n\n<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">Metrics</th></tr><tr><th>DS↑</th><th>SR(%)↑</th></tr></thead><tbody><tr><td>Risk-aware Sampling (<strong>Ours</strong>)</td><td><strong>88.31</strong></td><td><strong>70.48</strong></td></tr><tr><td>ε-Greedy Sampling</td><td>83.86 <span style=\"color:red;\">-4.45</span></td><td>61.74 <span style=\"color:red;\">-8.74</span></td></tr><tr><td>Random Sampling</td><td>70.41 <span style=\"color:red;\">-17.90</span></td><td>46.82 <span style=\"color:red;\">-23.66</span></td></tr></tbody></table>\n\n**Table 6** Results obtained using different action supervision in policy learning.\n\n<table><thead><tr><th rowspan=\"2\">Policy Learning Data</th><th colspan=\"2\">Metrics</th></tr><tr><th>DS↑</th><th>SR(%)↑</th></tr></thead><tbody><tr><td>Pos. & Neg. Actions (<strong>Ours</strong>)</td><td><strong>88.31</strong></td><td><strong>70.48</strong></td></tr><tr><td>Expert Actions</td><td>86.75 <span style=\"color:red;\">-1.56</span></td><td>68.25 <span style=\"color:red;\">-2.23</span></td></tr><tr><td>Only Positive Actions</td><td>83.65 <span style=\"color:red;\">-4.66</span></td><td>66.52 <span style=\"color:red;\">-3.96</span></td></tr></tbody></table>\n\ndation (DS 70.85 / SR 48.95%), which indicates that forecasting high-level semantics is essential for reliable long-horizon rollouts. w/o Action Selection disables predictive control in Eq. (1) (bypassing cost-based ranking in Eq. (6)) and directly executes the proposal/guidance output, causing the most severe collapse (DS 61.35 / SR 30.98%). This confirms that selecting actions by explicitly evaluating predicted long-horizon consequences is the key to RaWMPC.\n\n### **4.4.2 Analysis of Risk-Aware Training**\n\nTable 5 evaluates the risk-aware interaction training strategy used to refine the world model. Our risk-aware sampling follows the design in Sec. 3.2: besides random exploration, it uses the current RaWMPC to score candidate action sequences and deliberately collects both good (low-cost) and bad (high-cost) rollouts, improving coverage of rare safety-critical outcomes. Replacing it with ϵ-greedy sampling (random with probability ϵ₁, otherwise only selecting low-cost rollouts) reduces performance (DS 83.86 / SR 61.74%), showing that excluding high-cost failures weakens learning of risky consequences. Pure random sampling further degrades results (DS 70.41 / SR 46.82%), indicating that unguided data collection is substantially less efficient for learning long-horizon consequences.\n\n### **4.4.3 Analysis of Self-Evaluation Distillation**\n\nTable 6 studies how to train the action proposal network in Sec. 3.3. Our default setting uses RaWMPC as a self-evaluator to pseudo-label actions: the lowest-cost sequence is treated as a positive, while high-cost sequences serve as negatives, which yields the best\n\n**Table 7** Results obtained using different amounts of prediction horizon.\n\n<table><thead><tr><th rowspan=\"2\">Prediction Horizon</th><th colspan=\"2\">Metrics</th></tr><tr><th>DS↑</th><th>SR(%)↑</th></tr></thead><tbody><tr><td>H=1</td><td>57.85 <span style=\"color:red;\">-30.46</span></td><td>28.64 <span style=\"color:red;\">-41.84</span></td></tr><tr><td>H=5</td><td>74.98 <span style=\"color:red;\">-13.33</span></td><td>49.52 <span style=\"color:red;\">-20.96</span></td></tr><tr><td>H=10 (<b>Ours</b>)</td><td><b>88.31</b></td><td><b>70.48</b></td></tr><tr><td>H=15</td><td>82.34 <span style=\"color:red;\">-5.97</span></td><td>62.38 <span style=\"color:red;\">-8.10</span></td></tr></tbody></table>\n\n**Table 8** Results obtained using different amounts of offline learning data in warm-up.\n\n<table><thead><tr><th rowspan=\"2\">Warm-up Data</th><th colspan=\"2\">Metrics</th></tr><tr><th>DS↑</th><th>SR(%)↑</th></tr></thead><tbody><tr><td>0%</td><td>87.34 <span style=\"color:red;\">-0.97</span></td><td>69.62 <span style=\"color:red;\">-0.86</span></td></tr><tr><td>10%</td><td><b>88.31</b></td><td><b>70.48</b></td></tr><tr><td>20%</td><td>88.09 <span style=\"color:red;\">-0.22</span></td><td>70.32 <span style=\"color:red;\">-0.16</span></td></tr><tr><td>30%</td><td>86.95 <span style=\"color:red;\">-1.36</span></td><td>68.52 <span style=\"color:red;\">-1.96</span></td></tr></tbody></table>\n\nperformance (DS 88.31 / SR 70.48%). Training the proposal network only with expert actions slightly degrades performance (DS 86.75 / SR 68.25%), suggesting that self-evaluated targets align better with the predictive-control objective than direct imitation targets. Using only positive actions further drops performance (DS 83.65 / SR 66.52%), indicating that explicitly contrasting against high-risk negatives is important for preventing unsafe candidates and improving downstream selection.\n\n### **4.4.4 Discussion on Prediction Horizon**\n\nTable 7 ablates the planning horizon *H* used in the world-model rollout and cost evaluation (Eq. (6)). Short horizons fail to capture delayed consequences, leading to poor performance (H=1: DS 57.85 / SR 28.64%; H=5: DS 74.98 / SR 49.52%). Increasing to H=10 yields the best results (DS 88.31 / SR 70.48%), as it provides sufficient look-ahead for risk assessment while keeping prediction uncertainty manageable. Further increasing to H=15 degrades performance (DS 82.34 / SR 62.38%), likely due to accumulated rollout errors that affect cost-based ranking.\n\n### **4.4.5 Discussion on Warm-up**\n\nTable 8 ablates the fraction of offline logged trajectories used for warm-up before interactive training. In this study, we keep the total number of training samples fixed and vary only the proportion allocated to offline warm-up data. Without warm-up (0%),\n\n---\n\n**Table 9** Ablation study on the control pattern with learned world model.\n\n<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">Metrics</th></tr><tr><th>DS↑</th><th>SR(%)↑</th></tr></thead><tbody><tr><td>Predictive Control (Ours)</td><td>88.31</td><td>70.48</td></tr><tr><td>Reinforcement Learning</td><td>73.58 <sup>-14.73</sup></td><td>51.85 <sup>-18.63</sup></td></tr></tbody></table>\n\n<table><tfoot><tr><th colspan=\"5\" style=\"text-align: center;\"> done not require</th></tr></tfoot></table>\n\nperformance drops (DS 87.34 / SR 69.62%), suggesting that training from scratch leads to less reliable rollouts and a less stable early optimization stage. Using a small amount of logged trajectories (10%) yields the best results (DS 88.31 / SR 70.48%), indicating that a light warm-up provides useful predictive priors (e.g., basic dynamics modeling and perception decoding) that improve long-horizon rollout quality and downstream control. However, further increasing the warm-up ratio begins to degrade performance (20%: DS 88.09 / SR 70.32%; 30%: DS 86.95 / SR 68.52%). We attribute this trend to the fact that offline logged trajectories are strongly biased toward safe, human-like behaviors and contain few hazardous events. Consequently, allocating too much data to offline warm-up reduces the opportunities for subsequent online interaction to explore unconventional actions and collect safety-critical failures—especially in dangerous scenarios—which are essential for learning robust risk awareness.\n\n**4.4.6 Discussion on Control Pattern**\n\nTable 9 compares predictive control to directly optimizing a policy with model-based reinforcement learning (RL). Predictive control achieves substantially higher performance (DS 88.31 / SR 70.48%) than model-based RL (DS 73.58 / SR 51.85%). This validates the benefit of evaluating candidate action sequences via decoded future outcomes (segmentation, events, ego-states) and selecting the minimum-cost one, rather than relying on end-to-end policy optimization alone.\n\n***4.4.7 World-Model Prediction Accuracy***\n\nTable 10 reports the event prediction quality used by\nthe cost function in Eq. (6). The predictor achieves\nhigh accuracy across event types (0.91–0.96) and\nstrong recall on collision-related events (e.g., pedes-\ntrian collision recall 0.99), providing reliable signals\nfor risk-aware evaluation. We observe lower precision\nfor some rare events (e.g., pedestrian collision pre-\ncision 0.52), reflecting a conservative tendency with\nmore false positives; in safety-critical driving, priori-\n\n**Table 10** Prediction accuracy of future traffic events with learned world model.\n\n<table>\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"4\">Collision</th>\n      <th rowspan=\"2\">Running Traffic Sign</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Pedestrian</th>\n      <th>Vehicle</th>\n      <th>Static</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Accuracy</td>\n      <td>0.96</td>\n      <td>0.91</td>\n      <td>0.93</td>\n      <td>0.91</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>Recall</td>\n      <td>0.99</td>\n      <td>0.84</td>\n      <td>0.89</td>\n      <td>0.84</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>Precision</td>\n      <td>0.52</td>\n      <td>0.62</td>\n      <td>0.63</td>\n      <td>0.68</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n\ntizing recall can be preferable to missing hazards.\n\n**5 Conclusion**\n\nIn this work, we proposed RaWMPC, a risk-aware\nworld-model predictive control framework for end-\nto-end autonomous driving that does not require\nexpert action supervision. RaWMPC learns an\naction-conditioned world model to roll out multiple\ncandidate behaviors, predicts future semantics and\nsafety-critical events, and selects actions by explicitly\nminimizing a risk-aware cost. To make rare-but-\ncatastrophic outcomes predictable and avoidable, we\nintroduced a risk-aware interaction strategy that\nintentionally collects both safe and hazardous roll-\nouts, and we further proposed self-evaluation dis-\ntillation to train an efficient action proposal policy\nusing RaWMPC as a self-evaluator. Extensive ex-\nperiments on Bench2Drive and NAVSIM show that\nRaWMPC achieves state-of-the-art performance and\nstronger robustness under domain shift, even without\noffline warm-up, showing the potential to significantly\nreduce the reliance on costly real-world expert demon-\nstrations. For future work, we will explore domain\nadaptation and more efficient planning to better sup-\nport real-world deployment and sim-to-real transfer.\n\n**Statements and Declarations**\n\n* Competing interests. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n* Data availability. This work does not propose any new dataset. The datasets (Bench2Drive [29] and NAVSIM [11]) that support the findings of this study are openly available at the URLs: Bench2Drive and NAVSIM.\n\n \"\"),\n  \"pre печать볕!\"\n\n---\n\n**References**\n\n[1] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In *Forty-first International Conference on Machine Learning*, 2024.\n[2] Jinkun Cao, Xin Wang, Trevor Darrell, and Fisher Yu. Instance-aware predictive navigation in multi-agent environments. In *IEEE International Conference on Robotics and Automation*, pages 5096-5102, 2021.\n[3] Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, et al. Pseudo-simulation for autonomous driving. *arXiv preprint arXiv:2506.04218*, 2025.\n[4] Raphael Chekroun, Marin Toromanoff, Sascha Hornauer, and Fabien Moutarde. Gri: General reinforced imitation and its application to vision-based autonomous driving. In *NeurIPS 2021, Machine Learning for Autonomous Driving Workshop*, 2021.\n[5] Dian Chen and Philipp Krähenbühl. Learning from all vehicles. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 17222-17231, 2022.\n[6] Dian Chen, Vladlen Koltun, and Philipp Krähenbühl. Learning to drive from a world on rails. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 15590-15599, 2021.\n[7] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-to-end autonomous driving: Challenges and frontiers. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2024.\n[8] Yuntao Chen, Yuqi Wang, and Zhaoxiang Zhang. Drivinggpt: Unifying driving world modeling and planning with multi-modal autoregressive transformers. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 26890-26900, October 2025.\n[9] Pranav Singh Chib and Pravendra Singh. Recent advancements in end-to-end autonomous driving using deep learning: A survey. *IEEE Transactions on Intelligent Vehicles*, 9(1):103-118, 2023.\n[10] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger. Transfuser: Imitation with transformer-based sensor fusion for autonomous driving. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2022.\n\n[11] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, et al. Navsim: Data-driven non-reactive autonomous vehicle simulation and benchmarking. *Advances in Neural Information Processing Systems*, 37:28706–28719, 2025.\n[12] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In *Conference on Robot Learning*, pages 1–16, 2017.\n[13] Lan Feng, Quanyi Li, Zhenghao Peng, Shuhan Tan, and Bolei Zhou. Trafficgen: Learning to generate diverse and realistic traffic scenarios. In *2023 IEEE international conference on robotics and automation (ICRA)*, pages 3567–3575. IEEE, 2023.\n[14] Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Dingkang Liang, Chong Zhang, Dingyuan Zhang, Hongwei Xie, Bing Wang, and Xiang Bai. Orion: A holistic end-to-end autonomous driving framework by vision-language instructed action generation. *arXiv preprint arXiv:2503.19755*, 2025.\n[15] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: A generalizable driving world model with high fidelity and versatile controllability. In *The Thirty-eighth Annual Conference on Neural Information Processing Systems*, 2024.\n[16] Mingzhe Guo, Zhipeng Zhang, Yuan He, Ke Wang, Liping Jing, and Haibin Ling. End-to-end autonomous driving without costly modularization and 3d manual annotation. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2025.\n[17] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. *Advances in neural information processing systems*, 31, 2018.\n[18] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In *International Conference on Learning Representations*, 2019.\n[19] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In *International conference on machine learning*, pages 2555–2565. PMLR, 2019.\n[20] Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In *International Conference on Learning Representations*, 2021.\n[21] Shadi Hamdan, Chonghao Sima, Zetong Yang, Hongyang Li, and Fatma Güney. Eta: Efficiency through thinking ahead, a dual approach to self-driving with large models. In *Proceedings of*\n\n---\n\nthe IEEE/CVF International Conference on Computer Vision, 2025.\n\n[22] Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive policy learning with uncertainty regularization for driving in dense traffic. In International Conference on Learning Representations, 2018.\n\n[23] Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary Mrez, Corina Gurau, Hudson Yeo, Alex Kendall, Roberto Cipolla, and Jamie Shotton. Model-based imitation learning for urban driving. Advances in Neural Information Processing Systems, 35:20703–20716, 2022.\n\n[24] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Mrez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023.\n\n[25] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n\n[26] Bernhard Jaeger, Kashyap Chitta, and Andreas Geiger. Hidden biases of end-to-end driving models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8240–8249, 2023.\n\n[27] Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, and Hongyang Li. Driveadapter: Breaking the coupling barrier of perception and planning in end-to-end autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7953–7963, 2023.\n\n[28] Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, and Hongyang Li. Think twice before driving: Towards scalable decoders for end-to-end autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21983–21994, 2023.\n\n[29] Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, and Junchi Yan. Bench2drive: Towards multi-ability benchmarking of closed-loop end-to-end autonomous driving. arXiv preprint arXiv:2406.03877, 2024.\n\n[30] Xiaosong Jia, Junqi You, Zhiyuan Zhang, and Junchi Yan. Drivetransformer: Unified transformer for scalable end-to-end autonomous driving. In International Conference on Learning Representations (ICLR), 2025.\n\n[31] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340–8350, 2023.\n\n[32] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning to drive in a day. In 2019 international conference on robotics and automation (ICRA), pages 8248–8254. IEEE, 2019.\n\n[33] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\n[34] Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Pathdreamer: A world model for indoor navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14738–14748, 2021.\n\n[35] Fanjie Kong, Yitong Li, Weihuang Chen, Chen Min, Yizhe Li, Zhiqiang Gao, Haoyang Li, Zhongyu Guo, and Hongbin Sun. Vlr-driver: Large vision-language-reasoning models for embodied autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 26966–26976, 2025.\n\n[36] Hanyang Kong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. Dreamdrone: Text-to-image diffusion models are zero-shot perpetual view generators. In European Conference on Computer Vision, pages 324–341. Springer, 2024.\n\n[37] Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan. Think2drive: Efficient reinforcement learning by thinking with latent world model for autonomous driving (in carla-v2). In European Conference on Computer Vision, pages 142–158. Springer, 2024.\n\n[38] Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yun-tao Chen, Zhaoxiang Zhang, and Tieniu Tan. Enhancing end-to-end autonomous driving with latent world model. In The Thirteenth International Conference on Learning Representations, 2025.\n\n[39] Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, Lue Fan, and Zhaoxiang Zhang. End-to-end driving with online trajectory evaluation via bev world model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 27137–27146, October 2025.\n\n[40] Zhenxin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Zuxuan Wu, and Jose M. Alvarez. Hydra-next: Robust closed-loop driving with open-loop training. In Proceedings of the IEEE/CVF International Conference onmploying occupancy selbst о p Forecasting reacen[\n\n---\n\nference on Computer Vision (ICCV), pages 27305-27314, October 2025.\n\n[41] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer: learning bird's-eye-view representation from lidar-camera via spatiotemporal transform-ers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.\n\n[42] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, and Xinggang Wang. Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12037-12047, June 2025.\n\n[43] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017.\n\n[44] Yuhang Lu, Jiadong Tu, Yuexin Ma, and Xinge Zhu. Real-ad: Towards human-like reasoning in end-to-end autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 27783-27793, 2025.\n\n[45] Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte, Prajwal Chi-dananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, et al. Lingoqa: Vis-sual question answering for autonomous driving. In European Conference on Computer Vision, pages 252-269. Springer, 2024.\n\n[46] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. In RSS, 2023.\n\n[47] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565-571. IEEE, 2016.\n\n[48] Michael Montemerlo, Jan Becker, Suhid Bhat, Hen-drik Dahlkamp, Dmitri Dolgov, Scott Ettinger, Dirk Haehnel, Tim Hilden, Gabe Hoffmann, Burkhard Huhnke, et al. Junior: The stanford entry in the urban challenge. Journal of field Robotics, 25(9): 569-597, 2008.\n\n[49] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, Yifei Zhan, Kun Zhan, Peng Jia, Xianpeng Lang, Xingang Wang, and Wenjun Mei. Recondreamer: Crafting world models for driving scene recon-\n\nstruction via online restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1559-1569, June 2025.\n\n[50] Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, and Zehuan Wu. Maskgwm: A generalizable driving world model with video mask reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22381-22391, June 2025.\n\n[51] Brian Paden, Michal Čáp, Sze Zheng Yong, Dmitry Yershov, and Emilio Frazzoli. A survey of motion planning and control techniques for self-driving urban vehicles. IEEE Transactions on Intelligent vehicles, 1(1):33-55, 2016.\n\n[52] Xinlei Pan, Xiangyu Chen, Qizhi Cai, John Canny, and Fisher Yu. Semantic predictive control for explainable and efficient policy learning. In International Conference on Robotics and Automation, pages 3203-3209, 2019.\n\n[53] Scott Drew Pendleton, Hans Andersen, Xinxin Du, Xiaotong Shen, Malika Meghjani, You Hong Eng, Daniela Rus, and Marcelo H Ang. Perception, planning, control, and coordination for autonomous vehicles. Machines, 5(1):6, 2017.\n\n[54] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-to-end autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7077-7087, 2021.\n\n[55] Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, A Sophia Koepke, Zeynep Akata, and Andreas Geiger. Plant: Explainable planning transformers via object-level representations. In Conference on Robot Learning, pages 459-470, 2023.\n\n[56] Katrin Renz, Long Chen, Ana-Maria Marcu, Jan Hünermann, Benoit Hanotte, Alice Karnsund, Jamie Shotton, Elahe Arani, and Oleg Sinavski. Carllava: Vision language models for camera-only closed-loop driving. arXiv preprint arXiv:2406.10165, 2024.\n\n[57] Katrin Renz, Long Chen, Elahe Arani, and Oleg Sinavski. Simlingo: Vision-only closed-loop autonomous driving with language-action alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11993-12003, June 2025.\n\n[58] Nicholas Rhinehart, Rowan McAllister, and Sergey Levine. Deep imitative models for flexible inference, planning, and control. In International Conference on Learning Representations, 2020.\n\n[59] Ahmad El Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. End-to-end deep re-\n\n---\n\n[{\"bbox\": [139, 151, 606, 1466], \"category\": \"Text\", \"text\": \"inforcement learning for lane keeping assist. *arXiv preprint arXiv:1612.04340*, 2016.\\n\\n[60] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data. In *European Conference on Computer Vision*, pages 683–700. Springer, 2020.\\n\\n[61] ShuYao Shang, Yuntao Chen, Yuqi Wang, Yingyan Li, and Zhaoxiang Zhang. Drivedpo: Policy learning via safety dpo for end-to-end autonomous driving. In *The Thirty-ninth Annual Conference on Neural Information Processing Systems*, 2025.\\n\\n[62] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and Yu Liu. Safety-enhanced autonomous driving using interpretable sensor fusion transformer. In *Conference on Robot Learning*, pages 726–737, 2023.\\n\\n[63] Hao Shao, Letian Wang, Ruobing Chen, Steven L Waslander, Hongsheng Li, and Yu Liu. Reason-net: End-to-end driving with temporal and global reasoning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 13723–13733, 2023.\\n\\n[64] Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven L Waslander, Yu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 15120–15130, 2024.\\n\\n[65] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In *European Conference on Computer Vision*, pages 256–274. Springer, 2024.\\n\\n[66] Oriane Siméoni, Huy V Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. *arXiv preprint arXiv:2508.10104*, 2025.\\n\\n[67] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. *Advances in neural information processing systems*, 28, 2015.\\n\\n[68] Ziying Song, Caiyan Jia, Lin Liu, Hongyu Pan, Yongchang Zhang, Junming Wang, Xingyu Zhang, Shaoqing Xu, Lei Yang, and Yadan Luo. Don’t shake the wheel: Momentum-aware planning in end-to-end autonomous driving. In *Proceedings of the Computer Vision and Pattern Recognition Conference*, pages 22432–22441, 2025.\\n\\n[69] Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, Haoran Wu, and Sifa Zheng. Sparsedrive: End-to-end autonomous driving via sparse scene representation. In *2025 IEEE International Conference on Robotics and Automation (ICRA)*, pages 8795–8801. IEEE, 2025.\"}, {\"bbox\": [632, 154, 1111, 1425], \"category\": \"Text\", \"text\": \"[70] Shuhan Tan, Kelvin Wong, Shenlong Wang, Siv-abalan Manivasagam, Mengye Ren, and Raquel Urtasun. Scenegen: Learning to generate realistic traffic scenes. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 892–901, 2021.\\n[71] Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Kraehenbuehl. Language con-ditioned traffic generation. In *Conference on Robot Learning*, pages 2714–2752. PMLR, 2023.\\n[72] Shuhan Tan, Boris Ivanovic, Yuxiao Chen, Boyi Li, Xinshuo Weng, Yulong Cao, Philipp Kraehen-buehl, and Marco Pavone. Promptable closed-loop traffic simulation. In *Annual Conference on Robot Learning*, 2024.\\n[73] Yingqi Tang, Zhuoran Xu, Zhaotie Meng, and Erkang Cheng. Hip-ad: Hierarchical and multi-granularity planning with deformable attention for autonomous driving in a single decoder. *arXiv preprint arXiv:2503.08612*, 2025.\\n[74] Sebastian Thrun, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James Diebel, Philip Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, et al. Stanley: The robot that won the darpa grand challenge. *Journal of field Robotics*, 23(9):661–692, 2006.\\n[75] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free reinforcement learning for urban driving using implicit affordances. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 7153–7162, 2020.\\n[76] Chris Urmson, Joshua Anhalt, Drew Bagnell, Christopher Baker, Robert Bittner, MN Clark, John Dolan, Dave Duggins, Tugrul Galatali, Chris Geyer, et al. Autonomous driving in urban envi-ronments: Boss and the urban challenge. *Journal of field Robotics*, 25(8):425–466, 2008.\\n[77] Hanqing Wang, Wei Liang, Luc Van Gool, and Wenguan Wang. Dream Walker: Mental planning for continuous vision-language navigation. In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 10873–10883, 2023.\\n[78] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yun-tao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In *Proceedings of the IEEE/CVF Conference on Computer\"}]\n\n---\n\nVision and Pattern Recognition, pages 14749–14759, 2024.\n\n[79] Kehan Wen, Yutong Hu, Yao Mu, and Lei Ke. M^ 3pc: Test-time model predictive control using pre-trained masked trajectory model. In *The Thirteenth International Conference on Learning Representations*, 2025.\n\n[80] Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Parallelized architecture for real-time autonomous driving. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2024.\n\n[81] Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, and Yu Qiao. Trajectory-guided control prediction for end-to-end autonomous driving: A simple yet strong baseline. *Advances in Neural Information Processing Systems*, 35:6119–6132, 2022.\n\n[82] Zebin Xing, Xingyu Zhang, Yang Hu, Bo Jiang, Tong He, Qian Zhang, Xiaoxiao Long, and Wei Yin. Goalflow: Goal-driven flow matching for multimodal trajectories generation in end-to-end autonomous driving. In *Proceedings of the Computer Vision and Pattern Recognition Conference*, pages 1602–1611, 2025.\n\n[83] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee K Wong, Zhenguo Li, and Hengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. *IEEE Robotics and Automation Letters*, 2024.\n\n[84] Xuemeng Yang, Licheng Wen, Tiantian Wei, Yukai Ma, Jianbiao Mei, Xin Li, Wenjie Lei, Daoyong Fu, Pinlong Cai, Min Dou, Liang He, Yong Liu, Botian Shi, and Yu Qiao. Drivearena: A closed-loop generative simulation platform for autonomous driving. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 26933–26943, October 2025.\n\n[85] Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, and Junchi Yan. Raw2drive: Reinforcement learning with aligned world models for end-to-end autonomous driving (in carla v2), 2025. Accepted by NeurIPS 2025.\n\n[86] Jianhua Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, and Matthew Gadd. Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model. arXiv preprint arXiv:2402.10828, 2024.\n\n[87] Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris M Kitani. Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting. In Proceed-\n\nings of the IEEE/CVF international conference on computer vision, pages 9813-9823, 2021.\n\n[88] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen, et al. Segvit: Semantic segmentation with plain vision transformers. In Advances in Neural Information Processing Systems, 2022.\n\n[89] Bozhou Zhang, Nan Song, Xin Jin, and Li Zhang. Bridging past and future: End-to-end autonomous driving with historical prediction and planning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 6854-6863, 2025.\n\n[90] Jimuyang Zhang, Zanming Huang, and Eshed Ohn-Bar. Coaching a teachable student. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7805–7815, 2023.\n\n[91] Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiao-Xiao Long, Xun Cao, and Wei Yin. Epona: Autoregressive diffusion world model for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 27220-27230, October 2025.\n\n[92] Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, and Raquel Urtasun. Copilot4d: Learning unsupervised world models for autonomous driving via discrete diffusion. In ICLR, 2024.\n\n[93] Peizhi Zhang, Lu Xiong, Zhuoping Yu, Peiyuan Fang, Senwei Yan, Jie Yao, and Yi Zhou. Reinforcement learning-based end-to-end parking for automatic parking system. Sensors, 19(18):3996, 2019.\n\n[94] Wei Zhang, Pengfei Li, Junli Wang, Bingchuan Sun, Qihao Jin, Guangjun Bao, Shibo Rui, Yang Yu, Wenchao Ding, Peng Li, et al. Dual-aeb: Synergizing rule-based and multimodal large language models for effective emergency braking. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 14888–14895. IEEE, 2025.\n\n[95] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. End-to-end urban driving by imitating a reinforcement learning coach. In Proceedings of the IEEE/CVF international conference on computer vision, pages 15222-15232, 2021.\n\n[96] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. Trafficbots: Towards world models for autonomous driving simulation and motion prediction. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1522-1529. IEEE, 2023.\n\n---\n\n[{\"bbox\": [132, 152, 612, 1109], \"category\": \"Text\", \"text\": \"[97] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, and Xingang Wang. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 12015–12026, June 2025.\\n\\n[98] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. Occ-world: Learning a 3d occupancy world model for autonomous driving. In *European conference on computer vision*, pages 55–72. Springer, 2024.\\n\\n[99] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chen-ming Zhang, and Long Chen. Genad: Generative end-to-end autonomous driving. In *European Conference on Computer Vision*, pages 87–104. Springer, 2024.\\n\\n[100] Yupeng Zheng, Pengxuan Yang, Zebin Xing, Qichao Zhang, Yuhang Zheng, Yinfeng Gao, Pengfei Li, Teng Zhang, Zhongpu Xia, Peng Jia, XianPeng Lang, and Dongbin Zhao. World4drive: End-to-end autonomous driving via intention-aware physical latent world model. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 28632–28642, October 2025.\\n\\n[101] Julius Ziegler, Philipp Bender, Markus Schreiber, Henning Lategahn, Tobias Strauss, Christoph Stiller, Thao Dang, Uwe Franke, Nils Appenrodt, Christoph G Keller, et al. Making bertha drive—an autonomous journey on a historic route. *IEEE Intelligent transportation systems magazine*, 6(2): 8–20, 2014.\\n\\n[102] Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, and Jiwen Lu. Gaussianworld: Gaussian world model for streaming 3d occupancy prediction. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 6772–6781, June 2025.\"}]",
  "figures": [
    {
      "name": "fig1.png",
      "caption": ""
    },
    {
      "name": "fig2.jpeg",
      "caption": ""
    },
    {
      "name": "fig3.jpeg",
      "caption": ""
    },
    {
      "name": "fig4.jpeg",
      "caption": ""
    },
    {
      "name": "fig5.jpeg",
      "caption": ""
    },
    {
      "name": "fig6.jpeg",
      "caption": ""
    },
    {
      "name": "fig7.jpeg",
      "caption": ""
    },
    {
      "name": "fig8.jpeg",
      "caption": ""
    },
    {
      "name": "fig9.jpeg",
      "caption": ""
    },
    {
      "name": "fig10.jpeg",
      "caption": ""
    },
    {
      "name": "fig11.png",
      "caption": ""
    },
    {
      "name": "fig12.png",
      "caption": ""
    },
    {
      "name": "fig13.jpeg",
      "caption": ""
    },
    {
      "name": "fig14.jpeg",
      "caption": ""
    },
    {
      "name": "fig15.jpeg",
      "caption": ""
    },
    {
      "name": "fig16.jpeg",
      "caption": ""
    },
    {
      "name": "fig17.jpeg",
      "caption": ""
    },
    {
      "name": "fig18.jpeg",
      "caption": ""
    },
    {
      "name": "fig19.jpeg",
      "caption": ""
    },
    {
      "name": "fig20.jpeg",
      "caption": ""
    },
    {
      "name": "fig21.jpeg",
      "caption": ""
    },
    {
      "name": "fig22.jpeg",
      "caption": ""
    },
    {
      "name": "fig23.jpeg",
      "caption": ""
    },
    {
      "name": "fig24.jpeg",
      "caption": ""
    },
    {
      "name": "fig25.jpeg",
      "caption": ""
    },
    {
      "name": "fig26.jpeg",
      "caption": ""
    },
    {
      "name": "fig27.jpeg",
      "caption": ""
    },
    {
      "name": "fig28.jpeg",
      "caption": ""
    },
    {
      "name": "fig29.jpeg",
      "caption": ""
    },
    {
      "name": "fig30.jpeg",
      "caption": ""
    },
    {
      "name": "fig31.jpeg",
      "caption": ""
    },
    {
      "name": "fig32.jpeg",
      "caption": ""
    },
    {
      "name": "fig33.png",
      "caption": ""
    },
    {
      "name": "fig34.png",
      "caption": ""
    },
    {
      "name": "fig35.jpeg",
      "caption": ""
    },
    {
      "name": "fig36.jpeg",
      "caption": ""
    },
    {
      "name": "fig37.jpeg",
      "caption": ""
    },
    {
      "name": "fig38.jpeg",
      "caption": ""
    },
    {
      "name": "fig39.jpeg",
      "caption": ""
    },
    {
      "name": "fig40.jpeg",
      "caption": ""
    },
    {
      "name": "fig41.jpeg",
      "caption": ""
    },
    {
      "name": "fig42.jpeg",
      "caption": ""
    },
    {
      "name": "fig43.jpeg",
      "caption": ""
    },
    {
      "name": "fig44.jpeg",
      "caption": ""
    },
    {
      "name": "fig45.jpeg",
      "caption": ""
    },
    {
      "name": "fig46.jpeg",
      "caption": ""
    },
    {
      "name": "fig47.jpeg",
      "caption": ""
    },
    {
      "name": "fig48.jpeg",
      "caption": ""
    },
    {
      "name": "fig49.jpeg",
      "caption": ""
    },
    {
      "name": "fig50.jpeg",
      "caption": ""
    },
    {
      "name": "fig51.jpeg",
      "caption": ""
    },
    {
      "name": "fig52.jpeg",
      "caption": ""
    },
    {
      "name": "fig53.jpeg",
      "caption": ""
    },
    {
      "name": "fig54.jpeg",
      "caption": ""
    },
    {
      "name": "fig55.png",
      "caption": ""
    },
    {
      "name": "fig56.png",
      "caption": ""
    },
    {
      "name": "fig57.png",
      "caption": ""
    },
    {
      "name": "fig58.jpeg",
      "caption": ""
    },
    {
      "name": "fig59.jpeg",
      "caption": ""
    },
    {
      "name": "fig60.jpeg",
      "caption": ""
    },
    {
      "name": "fig61.jpeg",
      "caption": ""
    },
    {
      "name": "fig62.jpeg",
      "caption": ""
    },
    {
      "name": "fig63.jpeg",
      "caption": ""
    },
    {
      "name": "fig64.jpeg",
      "caption": ""
    },
    {
      "name": "fig65.png",
      "caption": ""
    },
    {
      "name": "fig66.jpeg",
      "caption": ""
    },
    {
      "name": "fig67.jpeg",
      "caption": ""
    },
    {
      "name": "fig68.jpeg",
      "caption": ""
    },
    {
      "name": "fig69.jpeg",
      "caption": ""
    },
    {
      "name": "fig70.jpeg",
      "caption": ""
    },
    {
      "name": "fig71.png",
      "caption": ""
    },
    {
      "name": "fig72.png",
      "caption": ""
    },
    {
      "name": "fig73.png",
      "caption": ""
    },
    {
      "name": "fig74.png",
      "caption": ""
    },
    {
      "name": "fig75.jpeg",
      "caption": ""
    },
    {
      "name": "fig76.jpeg",
      "caption": ""
    },
    {
      "name": "fig77.jpeg",
      "caption": ""
    },
    {
      "name": "fig78.jpeg",
      "caption": ""
    },
    {
      "name": "fig79.jpeg",
      "caption": ""
    },
    {
      "name": "fig80.jpeg",
      "caption": ""
    },
    {
      "name": "fig81.png",
      "caption": ""
    },
    {
      "name": "fig82.jpeg",
      "caption": ""
    },
    {
      "name": "fig83.png",
      "caption": ""
    },
    {
      "name": "fig84.jpeg",
      "caption": ""
    },
    {
      "name": "fig85.jpeg",
      "caption": ""
    },
    {
      "name": "fig86.jpeg",
      "caption": ""
    },
    {
      "name": "fig87.jpeg",
      "caption": ""
    },
    {
      "name": "fig88.png",
      "caption": ""
    },
    {
      "name": "fig89.png",
      "caption": ""
    },
    {
      "name": "fig90.png",
      "caption": ""
    },
    {
      "name": "fig91.png",
      "caption": ""
    },
    {
      "name": "fig92.jpeg",
      "caption": ""
    },
    {
      "name": "fig93.jpeg",
      "caption": ""
    },
    {
      "name": "fig94.jpeg",
      "caption": ""
    },
    {
      "name": "fig95.jpeg",
      "caption": ""
    },
    {
      "name": "fig96.jpeg",
      "caption": ""
    },
    {
      "name": "fig97.jpeg",
      "caption": ""
    },
    {
      "name": "fig98.jpeg",
      "caption": ""
    },
    {
      "name": "fig99.jpeg",
      "caption": ""
    },
    {
      "name": "fig100.png",
      "caption": ""
    },
    {
      "name": "fig101.jpeg",
      "caption": ""
    },
    {
      "name": "fig102.jpeg",
      "caption": ""
    },
    {
      "name": "fig103.png",
      "caption": ""
    },
    {
      "name": "fig104.jpeg",
      "caption": ""
    },
    {
      "name": "fig105.jpeg",
      "caption": ""
    },
    {
      "name": "fig106.jpeg",
      "caption": ""
    },
    {
      "name": "fig107.jpeg",
      "caption": ""
    },
    {
      "name": "fig108.jpeg",
      "caption": ""
    },
    {
      "name": "fig109.jpeg",
      "caption": ""
    },
    {
      "name": "fig110.jpeg",
      "caption": ""
    },
    {
      "name": "fig111.jpeg",
      "caption": ""
    },
    {
      "name": "fig112.png",
      "caption": ""
    },
    {
      "name": "fig113.png",
      "caption": ""
    },
    {
      "name": "fig114.png",
      "caption": ""
    },
    {
      "name": "fig115.png",
      "caption": ""
    },
    {
      "name": "fig116.png",
      "caption": ""
    },
    {
      "name": "fig117.jpeg",
      "caption": ""
    },
    {
      "name": "fig118.png",
      "caption": ""
    },
    {
      "name": "fig119.jpeg",
      "caption": ""
    },
    {
      "name": "fig120.jpeg",
      "caption": ""
    },
    {
      "name": "fig121.jpeg",
      "caption": ""
    },
    {
      "name": "fig122.jpeg",
      "caption": ""
    },
    {
      "name": "fig123.jpeg",
      "caption": ""
    },
    {
      "name": "fig124.jpeg",
      "caption": ""
    },
    {
      "name": "fig125.jpeg",
      "caption": ""
    },
    {
      "name": "fig126.jpeg",
      "caption": ""
    },
    {
      "name": "fig127.jpeg",
      "caption": ""
    },
    {
      "name": "fig128.jpeg",
      "caption": ""
    },
    {
      "name": "fig129.jpeg",
      "caption": ""
    },
    {
      "name": "fig130.jpeg",
      "caption": ""
    },
    {
      "name": "fig131.jpeg",
      "caption": ""
    },
    {
      "name": "fig132.jpeg",
      "caption": ""
    },
    {
      "name": "fig133.jpeg",
      "caption": ""
    },
    {
      "name": "fig134.jpeg",
      "caption": ""
    },
    {
      "name": "fig135.jpeg",
      "caption": ""
    },
    {
      "name": "fig136.jpeg",
      "caption": ""
    },
    {
      "name": "fig137.jpeg",
      "caption": ""
    }
  ]
}