{
  "source": "html",
  "markdown": "[*]Equal Contribution \\contribution [‚Ä†]Corresponding authors\n\n# veScale-FSDP: Flexible and High-Performance FSDP at Scale\n\nFully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP‚Äôs fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today‚Äôs implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5 ‚àº \\sim 66% higher throughput and 16 ‚àº \\sim 30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.\n\nand \\projectpage https://github.com/volcengine/veScale\n\n## 1 Introduction\n\nLarge language models (LLMs) have become a transformative technology in everyday applications. Driven by the scaling law [ kaplan2020scaling ] , LLMs now reach billions of parameters and achieve human-level performance. Training such giant models requires parallelization techniques that distribute the model and optimizer states across thousands of GPUs [ jiang2024megascale ] . Among these, Deepspeed ZeRO [ rajbhandari2020zero ] or Fully Sharded Data Parallel (FSDP) [ zhao2023pytorch , pytorch2024fsdp2 , megatron_fsdp ] is one of the most fundamental techniques. FSDP is often the first choice because of its efficient yet flexible data-parallel programming paradigm and decoupling from model architecture. When additional scaling is needed [ smith2022using , ma2025veomni ] , FSDP can be combined with other parallelisms.\n\nHowever, existing FSDP/ZeRO systems struggle with modern structure-aware training. State-of-the-art models use non-element-wise optimizers such as Shampoo [ gupta2018shampoo ] and Muon [ jordan2024muon ] , and block-wise quantized training like DeepSeek-V3 [ liu2024deepseek ] , all of which require atomic tensor blocks.\nThe core limitation is that existing FSDP frameworks shard parameters, gradients, and optimizer states either element-wise [ rajbhandari2020zero , zhao2023pytorch ] or row-wise [ pytorch2024fsdp2 , megatron_fsdp ] , producing sharding boundaries that often misalign with the required block sizes. Consequently, model developers must intrusively modify their models or optimizers to match tensor boundaries, or system developers must handle complex boundary checks, padding, and additional communication logic.\n\nBeyond inflexibility, current FSDP systems fall short of our production throughput and memory targets, where we aim to extract every bit of hardware efficiency. GPU Memory is the tighter constraint: in shared clusters, jobs run out of memory or will operate at the memory limit incurring expensive device-side frees, prompting over-provisioning that leaves GPU resources wasted. These demands become even more critical when scaling training to over 10K GPUs and trillions of parameters. Few existing FSDP systems can scale to this level while maintaining efficiency. Deepspeed ZeRO [ rajbhandari2020zero ] pioneered the FSDP research but suffers from fragmented AllGather operations [ deepspeed_AG ] and inefficient memory management [ fsdp_record_stream ] . PyTorch FSDP1 [ zhao2023pytorch ] addresses some AllGather inefficiency, but incurs slow ReduceScatter [ fsdp1_reduce ] and does not solve memory overhead [ fsdp_record_stream ] . PyTorch FSDP2 [ pytorch2024fsdp2 ] improves memory management [ per_parameter_shard_rfc ] but introduces high tensor copy overhead. Meanwhile, both FSDP1 and FSDP2 suffer from slow collectives due to unaligned communication buffer [ wu2025terabyte , nccl16byte ] . Megatron-FSDP [ megatron_fsdp ] further improves performance but requires extra padding, increasing both communication and memory costs.\n\nTo this end, we reinvent PyTorch FSDP2 and present veScale-FSDP, combining both flexibility and performance at scale:\n\n‚ä≥ \\triangleright For flexibility, veScale-FSDP introduces a novel sharding format, RaggedShard , which supports arbitrary sharding granularity with custom block sizes for structure-aware training, while seamlessly composing with existing PyTorch DTensor sharding formats.\n\n‚ä≥ \\triangleright For performance, veScale-FSDP introduces a planning algorithm that rearranges RaggedShard tensors to maximize communication efficiency while respecting their desired sharding granularities. We formulate planning as a NP-hard optimization problem and use practical polynomial-time heuristics that achieve high-quality solutions in practice.\n\n‚ä≥ \\triangleright veScale-FSDP further provides a high-performance primitive, Distributed Buffer ( DBuffer ), that backs RaggedShard tensors with slices of a global buffer, not only enabling zero-copy access and minimal communication overhead but also reducing memory fragmentation via batched memory allocations.\n\nOur extensive evaluations demonstrate that veScale-FSDP outperforms all existing FSDP systems on both dense and sparse LLMs across different scales, achieving 5 ‚àº \\sim 66% higher throughput and 16 ‚àº \\sim 30% lower memory usage, while scaling efficiently to tens of thousands of GPUs.\nIn addition, case studies show that veScale-FSDP is able to natively accommodate both non-element-wise optimizers like Muon [ jordan2024muon ] and block-wise quantization methods like 8-bit Adam [ dettmers20218 ] .\nveScale-FSDP has been battle-tested in production and is portable without relying on internal infrastructure.\n\nRaggedShard code is open sourced at https://github.com/volcengine/veScale .\n\n## 2 Background and Motivation\n\n### 2.1 Structure‚ÄìAware Training\n\nStructure‚Äìaware training is the core technique behind the top-tier models such as Gemini [ team2024gemini ] and DeepSeek [ liu2024deepseek ] , and becomes increasingly important, including:\n\nMatrix Optimizers. Matrix-based\noptimizers such as Shampoo [ gupta2018shampoo ] and Muon [ jordan2024muon ] can deliver faster convergence rate.\nThe calculation is conducted on the matrix with the original 2D shape, requiring the full matrix to be present locally on each device and then be computed only on chosen devices.\n\nBlock-wise Quantization. Training with quantized model [ liu2024deepseek ] and optimizer states [ dettmers20218 ] is widely used to improve system efficiency.\nBlock-wise quantization is one of the prevailing techniques to preserve both quality and efficiency of training, but requires slicing the tensors into 2D blocks for the calculation of the scaling factors.\nSharding parameters without care would end up in sharded blocks across devices, incurring high complexity in either model design or system development.\n\n### 2.2 DTensor and JaggedTensor\n\n[FIGURE:x1.png] Figure 1 : The DTensor for flexible communication and computation. Here shows an example of DTensors executing a sharded matrix multiplication on a device. The darken part in each DTensor indicates the materialized local tensor on this device.\n\nDistributed Tensor (DTensor) [ torch-dtensor , li2025vescale ] is a promising primitive of PyTorch that provides the opportunity towards structure‚Äìaware training.\nIt represents a global tensor distributed across devices, where each device holds a local sharded tensor.\nDTensor supports three sharding formats (placement): Shard(dim) that evenly shards a global tensor along a tensor dimension, Replicate that replicates the global tensor, and Partial . It also enables users to switch between these placement via redistribute with implicit collective communications.\nAdditionally, DTensors can be computed directly by operators like matmul , as shown in Figure 1 .\nHowever, a fundamental limitation prevents structure-aware training: the Shard format cannot represent the block-wise sharding needed for quantization or the uneven sharding required by matrix optimizers.\n\nJaggedTensor/NestTensor on a single device [ jaggedtensor , nestedtensor , raggedtensors ] are PyTorch/TensorFlow primitives that represent single-device tensors whose last dimension is ‚Äújagged‚Äù. For example, a 2D tensor in which each row may have a different length.\nWhile these primitives still fail to represent the block-level granularity as the atomic unit, they offer a useful hint for how veScale-FSDP can support structure‚Äìawareness in distributed training setting.\n\n### 2.3 ZeRO and FSDPs\n\nDeepSpeed ZeRO [ rajbhandari2020zero ] pioneered this line of FSDP research.\nIts core idea is to concatenate a layer of tensors (parameters, gradients, and optimizer states) and then shard each concatenated tensors across devices, where tensors can be irregularly sharded across device boundary.\nZeRO only unshards a layer using AllGather before forward and backward pass, and reduces the layer gradients using ReduceScatter back to different devices.\nSuch sharding design is limited in element-wise plain tensor and cannot support structure-aware training.\n\nFullyShardedDataParallel (FSDP1) [ zhao2023pytorch ] is the first PyTorch-native ZeRO, following the same sharding format and limitation, but it is optimized in performance.\n\n[FIGURE_CAPTION] Table 1 : Interleaved copy time (ms) compared to their corresponding collectives in FSDP2 for GPT-OSS-120B on 64 GPUs. Shard(0) is the default parameter sharding mode and Shard(1) is used when Shard(0) incurs large padding.\n\nfully_shard (FSDP2) is the second PyTorch-native ZeRO, representing the state-of-the-art FSDP in the community.\nIt replaces the concatenated shard design with per-parameter sharding, representing each tensor as a Shard(0) DTensor. This exposes maximal DTensor flexibility for FSDP parameters in communication, computation, and model checkpointing.\nHowever, this even sharding format is still far from enabling structure-aware training.\nMoreover, this per-parameter design introduces performance overhead from copying parameters in interleaved memory addresses, as shown in Figure 2 and Table 1 .\n\n[FIGURE:x2.png] Figure 2 : The fundamental overhead in FSDP2. (AllGather is shown; ReduceScatter is a reverse process.)\n\nMegatron-FSDP [ megatron_fsdp ] is the most recent FSDP prototype that pursues speed.\nIt forgoes FSDP2‚Äôs design and rolls back to FSDP1‚Äôs concatenated sharding to avoid the copying overhead, while heavily optimizing performance.\nHowever, Megatron-FSDP develops a special mechanism to enforce a concatenation-sharded tensor become a Shard(0) DTensor, such that the model checkpointing can use DTensor. This mechanism inserts padding into the concatenation so that tensors are sharded row-wise along device boundaries rather than element-wise. Without careful padding planning, the concatenation size can grow significantly, increasing both memory usage and communication volume. Moreover, row-wise sharding still falls short of supporting structure-aware training.\n\n## 3 Overview\n\n[FIGURE:x3.png] Figure 3 : veScale-FSDP overview.\n\nTo address both challenges of flexibility and performance, we present veScale-FSDP, a novel FSDP that combines the best of worlds.\nFigure 3 provides the overview.\nModel developers are given the freedom to develop sophisticated large models (e.g., with sparse MoE structures) and structure-aware optimizers (e.g., with non-element-wise operators) for achieving unprecedented model quality.\nMeanwhile, the model/optimizer can be simply parallelized with PyTorch-native API fully_shard like FSDP2, without intrusively hacking model/optimizer code.\nDuring parallelization, complex operators of models/optimizers can still enjoy single-device semantics, thanks to a proposed sharding format, dubbed RaggedShard that offers the flexibility to express arbitrary sharding granularity and arbitrary distribution across devices for each DTensor (¬ß 4 ).\nUnder the hood, RaggedShard DTensors are grouped for bucketed communication.\nToward optimal performance, their layouts are rearranged via a proposed planning algorithm which is derived from a NP-hard optimization problem.\nThe planned layouts are then mapped to a Distributed Buffer ( DBuffer ), a new primitive that achieves zero-copy and minimal overhead (¬ß 5 ), enabling efficient scaling up to 10K GPUs in real production deployments.\n\n## 4 RaggedShard for Flexibility\n\nThis section proposes a novel and general sharding format RaggedShard to enable flexibility of FSDP for complex model and structure-aware optimizers.\n\n[FIGURE:x4.png] Figure 4 : Flexibility comparison of different sharding formats.\n\nExisting sharding formats. The second format is the Row-wise (Even) Shard , where a tensor is evenly partitioned along a dimension, with equal-sized shards assigned to each device. This design improves flexibility by enabling non-element-wise computations on sharded tensors and allowing dimension redistribution via All2All collectives. However, it still faces challenges with block-wise quantization, as evenly divided shards are not guaranteed to align with block boundaries. This row-wise sharding format serves as the foundation of FSDP2.\n\nThe RaggedShard format. Inspired by the JaggedTensor/NestedTensor on single device [ jaggedtensor , nestedtensor , raggedtensors ] , we propose RaggedShard format for DTensor to offer the flexibility to express arbitrary sharding granularity in contiguous memory (the atomic non-shardable block consisting of contiguous elements or rows or planes) and arbitrary sharding distribution (the number of blocks per device). For a simple example in Figure 4 , setting RaggedShard ‚Äôs granularity as one tensor row gives Row-wise RaggedShard with different number of rows across devices.\nA similar concept has been prototyped in the model checkpointing mechanism of Megatron-FSDP.\n\nThe most flexible sharding format is the Block-wise RaggedShard , where the sharding granularity is defined as a tensor block with a customizable shape. For example, a tensor may be partitioned into three 2D blocks, with one block placed on device 0 and two blocks on device 1 (see Figure 4 ). This format not only supports non-element-wise computation and efficient redistribution but also enables block-wise quantization with perfect alignment between quantization blocks and shard boundaries. In fact, the block-wise RaggedShard generalizes all previous sharding formats through different choices of block size.\n\n[FIGURE:x5.png] Figure 5 : Composability of RaggedShard with existing even Shard for 2D parallelism like FSDP √ó \\times EP (Expert Parallel).\n\nComposing with existing sharding formats. DTensor has been widely used to express tensor partitions in parallelization strategies such as tensor parallelism (TP) [ shoeybi2019megatron ] and expert parallelism (EP) [ lepikhin2020gshard ] . It allows tensors to be represented using replicated, partial-value, or evenly sharded placements along a selected dimension. RaggedShard extends this capability as an additional DTensor placement.\n\nTo support combinations of multiple parallelization strategies, RaggedShard needs to compose cleanly with existing DTensor placements. RaggedShard is orthogonal to both replicated and partial-value placements and veScale-FSDP specially handles the Shard placement. In practice, TP uses Shard(0) and Shard(1) for column- and row-wise tensor parallelism; EP can be encoded as Shard(0) along the expert dimension. By convention, EP/TP is applied before FSDP. In PyTorch, however, the DTensor placement list is organized in the opposite order of conceptual application (see Figure 5 ): a tensor shown with placements ( RaggedShard , Shard(0) ) is partitioned as Shard(0) followed by RaggedShard . veScale-FSDP reconciles this by: (i) for Shard(0) , introducing a dedicated placement StridedRaggedShard that carries reordering/stride metadata and performs\nMeanwhile, RaggedShard , as an extended DTensor placement, can offers the checkpointing capability by directly reusing DTensor-based checkpointing stacks (e.g., PyTorch Distributed Checkpoint [ dcp ] ) for failure recovery and also inheriting their optimizations such as communication-free sharded checkpointing.\n\n## 5 Grouped RaggedShard for Performance\n\nThis section discusses how to group RaggedShard DTensors for efficient communication, as well as its NP-hardness for optimality and the underlying optimization.\n\n[FIGURE:x6.png] Figure 6 : Grouped communication of RaggedShard DTensors.\n\nChallenges for efficient communication. As well known in the systems community, collective communication relies on tensor bucketing or grouping to maximize network utilization [ li2020pytorch , zhao2023pytorch ] . The same applies to RaggedShard DTensors in FSDP. However, efficiently grouping RaggedShard tensors is non-trivial and naively approaches can lead to significant inefficiencies.\nFigure 6 (a) illustrates three inefficient factors:\ni) Sharded block that can happen when tensors are just concatenated back-to-back and put into communication buffer, without realizing that sharding boundary is within a certain block, which breaks the abstraction of Block-wise RaggedShard and incurs extra communication for quantization;\nii) Non-contiguous tensor memory that can happen when two ends of communication buffers are padded (to align collective preferred unit size [ wu2025terabyte , nccl16byte ] or equal size across devices [ nccl2025collective ] ), without realizing that such padding is within a certain tensor, which breaks the memory contiguousness and incurs interleaved copy overhead (e.g., similar to the copy-out after AllGather in Figure 2 );\nand iii) Imbalanced load that can happen when different tensor sizes or block sizes or padding sizes are not aggregated to be equal across devices, which breaks the symmetry in collective communication (esp., Ring Algorithm) and ends up in underutilized networking.\n\nTowards efficient communication. To efficiently group RaggedShard DTensors, we propose a two-step approach that addresses the above challenges: first permute tensors, and then pad between them rather than padding within individual tensors, as illustrated in Figure 6 (b). The key idea is to balance tensor and block sizes across devices while aligning block boundaries in the sharded communication buffer so that blocks are placed contiguously. This approach inevitably introduces some padding overhead, which must be carefully minimized to reduce both memory usage and communication volume.\n\nOptimization problem formulation. Formally, the proposed approach can be formulated as an optimization problem.\nLet ùíØ = T 1 , T 2 , ‚Ä¶ , T n \\mathcal{T}={T_{1},T_{2},\\ldots,T_{n}} denote a set of RaggedShard DTensors, which\nare sharded across m m devices.\nEach DTensor t ‚àà ùíØ t\\in\\mathcal{T} has a block size of g t g_{t} , a total tensor size (in elements) e t e_{t} , and hence u t = e t / g t u_{t}=e_{t}/g_{t} sharding blocks.\nWe allocate a global communication buffer and place each t t as a contiguous memory interval [ ‚Ñì t , r t ) [\\ell_{t},r_{t}) .\nThe decision variables are the per-device buffer size S S and the interval endpoints { ‚Ñì t , r t } t ‚àà ùíØ \\{\\ell_{t},r_{t}\\}_{t\\in\\mathcal{T}} .\nOur goal is to minimize S S subject to the three factor constraints: Non-Sharded Block, Contiguous Tensor Memory, and strict Balanced Load (Figure 6 (b)):\n\n|  |  | min S , { ‚Ñì t , r t } t ‚àà ùíØ ‚Å° S \\displaystyle\\min_{S,\\{\\ell_{t},r_{t}\\}_{t\\in\\mathcal{T}}}\\ S |  |\n| --- | --- | --- | --- |\n|  | s.t. | r t ‚àí ‚Ñì t = e t ‚àß r t ‚â§ m ‚Äã S , ‚àÄ t ‚àà ùíØ , \\displaystyle r_{t}-\\ell_{t}=e_{t}\\ \\land\\ r_{t}\\leq mS,\\forall\\,t\\in\\mathcal{T}, |  |\n|  |  | r t ‚â§ ‚Ñì t ‚Ä≤ ‚à® r t ‚Ä≤ ‚â§ ‚Ñì t , ‚àÄ t ‚â† t ‚Ä≤ ‚àà ùíØ , \\displaystyle r_{t}\\leq\\ell_{t^{\\prime}}\\ \\lor\\ r_{t^{\\prime}}\\leq\\ell_{t},\\forall\\,t\\neq t^{\\prime}\\in\\mathcal{T}, |  |\n|  |  | k ‚Äã S ‚â§ ‚Ñì t ‚à® k ‚Äã S ‚â• r t ‚à® ( k ‚Äã S ‚àí ‚Ñì t ) ‚â° 0 ( mod g t ) , \\displaystyle kS\\leq\\ell_{t}\\ \\lor\\ kS\\geq r_{t}\\ \\lor\\ (kS-\\ell_{t})\\equiv 0\\pmod{g_{t}}, |  |\n|  |  | ‚àÄ t ‚àà ùíØ , ‚àÄ k = 1 , ‚Ä¶ , m \\displaystyle\\hskip 55.00008pt\\forall\\,t\\in\\mathcal{T},\\ \\forall\\,k=1,\\dots,m |  |\n\nThis optimization problem is NP-hard , as it can be reduced from the classic Partition problem [ garey1975complexity ] .\nAlthough it can be formulated as an Integer Linear Programming (ILP) problem and solved using off-the-shelf solvers, such methods are impractical at scale. In practice, ILP solvers often take tens of minutes to generate a plan and may even trigger system timeouts. Given that user-defined FSDP wrapping can yield hundreds of parameter groups with diverse sharding block sizes, and deployments may span up to hundreds of thousands of devices, we instead design a polynomial-time heuristic algorithm that achieves near-optimal efficiency in practice.\n\n[FIGURE_CAPTION] Algorithm 1 Structure-aware planning for grouped communication of RaggedShard DTensors.\n\nHeuristic-guided solution. veScale-FSDP proposes a polynomial-time dynamic-programming (DP) buffer-layout algorithm, guided by permutation heuristics that exploit the regularity of transformer models.\nThe optimization difficulty comes from tensor permutation: in principle, any permutation of tensors could be mapped into the buffer, and finding the global optimum would require exploring all permutations.\nFortunately, in practice, transformer parameters are highly structured: linear weights dominate the total parameter count, and sharding blocks are often consistent across layers.\nTo leverage this regularity, we explore three simple permutations: (i) default order of tensors; (ii) sorted order by sharding block sizes, and (iii) sorted order by tensor shapes.\nOur statistics show that these orders yield optimal or near-optimal results,\nso we adopt the default order for simplicity and ease of debugging.\n\nGiven the tensor order, the proposed algorithm applies a DP procedure to place tensors into a smallest global buffer while enforcing the aforementioned three factor constraints.\nIt enjoys a time complexity of O ‚Äã ( | ùíØ | 2 ‚Äã m ‚Äã log ‚Å° ( E ) ‚Äã log ‚Å° ( | ùíØ | ‚Äã m ) ) O(|\\mathcal{T}|^{2}m\\log(E)\\log(|\\mathcal{T}|m)) .\nAlgorithm 1 presents the detail.\n\nThe core idea is a case analysis of how each tensor aligns with shard boundaries in any valid layout: (1) it lies entirely within a single local shard; (2) it straddles two adjacent shards, but doesn‚Äôt contain a full shard; (3) it fully contains at least one shard. If every tensor falls into cases (1)‚Äì(2), feasibility is monotone in the shard size S S : whenever a layout exists for S S , it also exists for S + Œî S+\\Delta (where Œî \\Delta is the base alignment quantum). Because every shard includes an inter-tensor boundary, we can always absorb additional Œî \\Delta as padding. If any tensor is in case (3), the feasible shard sizes must be multiples of L = LCM ‚Å° { g t ‚à£ t is in case (3) } L=\\operatorname{LCM}\\{\\,g_{t}\\mid\\text{$t$ is in case (3)}\\,\\} . In this regime, feasibility is monotone over multiples: if k ‚Äã L kL is feasible, then ( k + 1 ) ‚Äã L (k{+}1)L is also feasible.\nWe therefore enumerate the case-(3) set and binary-search the minimal S S over the corresponding multiples, as shown in Line 17-21. To avoid exponentially enumerating all Case-(3) subsets, we sort tensors by element count and enumerate only the granularity prefixes, yielding a 2-approximation. Inside the feasibility checker, we define d ‚Äã p ‚Äã ( t , i ) dp(t,i) as the minimum number of devices (shards) required to store all atomic units up to the i i -th unit of tensor t t . Although the index space is as large as numerical space, d ‚Äã p ‚Äã ( t , i ) dp(t,i) is monotone within a tensor: d ‚Äã p ‚Äã ( t , i ) ‚â§ d ‚Äã p ‚Äã ( t , i + 1 ) dp(t,i)\\leq dp(t,i{+}1) . So within each tensor there are at most m m distinct values. In Line 8-9, we exploit this by batching contiguous indices into segments and skipping d ‚Äã p ‚Äã ( t , ‚àó ) dp(t,*) evaluations at intermediate indices, achieving the stated time complexity.\n\n[FIGURE:x7.png] Figure 7 : Distributed Buffer ( DBuffer ) for high performance communication. A 2D DBuffer for AllGather‚Äôing parameters is shown; Reversely, a 2D DBuffer redistributing from ( Partial , Partial ) to ( Replicate , Shard ) implements 2D gradient reduction with ReduceScatter and AllReduce.\n\nDistributed Buffer ( DBuffer ) Beyond grouping RaggedShard DTensors, the underlying communication buffer also plays a vital role in achieving high performance of communication, computation, and memory efficiency. To this end, veScale-FSDP proposes a new primitive, Distributed Buffer ( DBuffer ), to optimize the performance of grouped DTensors. Figure 7 shows the design.\nFirst, inspired by DTensor, DBuffer provides global buffer semantics over an N N -dimensional device topology, with a sharding specification along each dimension, abstracting away the complexity of N N -D communication and operations.\nSecond, DBuffer takes a group of tensors and executes group-level operators rather than per-tensor operators.\nFor example, before communication, each tensor may need to launch its own CUDA kernels for add, scale, zero, or copy (which may differ across tensors), incurring fragmented compute overhead and blocking communication.\nWith DBuffer , identical kernels across tensors are fused before communication, reducing blocking time.\nThird, DBuffer offers zero-copy before and after communication by leveraging RaggedShard ‚Äôs planning algorithm and providing a persistent address mapping to each tensor‚Äôs data pointer, minimizing memory footprint and fragmentation.\nLastly, DBuffer uses in-place communication and computation.\n\n## 6 Evaluation\n\nOur evaluation answers the following questions:\n\n- ‚Ä¢ How much does veScale-FSDP improve end-to-end training performance over all baseline systems (¬ß 6.1 )?\n- ‚Ä¢ How well does veScale-FSDP scale to large device counts (¬ß 6.2 ), in terms of weak scaling, strong scaling, and model size scaling?\n- ‚Ä¢ How are 8-bit Adam and Muon optimizer enabled by veScale-FSDP‚Äôs customizable sharding granularity and RaggedShard DTensor, in both performance and development velocity (¬ß 6.3 )?\n- ‚Ä¢ How does veScale-FSDP planner minimize padding, and what is the algorithm overhead (¬ß 6.4 )?\n- ‚Ä¢ How much does each component of veScale-FSDP contribute to the training performance (¬ß 6.5 )?\n\nHow much does veScale-FSDP improve end-to-end training performance over all baseline systems (¬ß 6.1 )?\n\nHow well does veScale-FSDP scale to large device counts (¬ß 6.2 ), in terms of weak scaling, strong scaling, and model size scaling?\n\nHow are 8-bit Adam and Muon optimizer enabled by veScale-FSDP‚Äôs customizable sharding granularity and RaggedShard DTensor, in both performance and development velocity (¬ß 6.3 )?\n\nHow does veScale-FSDP planner minimize padding, and what is the algorithm overhead (¬ß 6.4 )?\n\nHow much does each component of veScale-FSDP contribute to the training performance (¬ß 6.5 )?\n\nHardware: We ran all experiments on a GPU cluster; each node contains 8 √ó \\times GPUs and connected with proprietary high-speed interconnect.\n\nImplementation: veScale-FSDP is implemented with 7.6 K lines of code (LoC) in Python, transparently replacing the backend of FSDP2 while using the same PyTorch-native fully_shard API.\nveScale-FSDP serves as a plug-and-play Python module, compatible with standard PyTorch distributed runtimes and a wide range of PyTorch versions.\n\nBaselines: We compare veScale-FSDP against state-of-the-art open-source frameworks: DeepSpeed ZeRO v0.17.6 [ rajbhandari2020zero ] , PyTorch 2.7.1 FullyShardedDataParallel (FSDP1) [ zhao2023pytorch ] , PyTorch 2.7.1 fully_shard (FSDP2) [ pytorch2024fsdp2 ] , and Megatron-FSDP. For fairness, all frameworks are configured to use ZeRO-3 with mixed precision (i.e., FP32 master weights and BF16 forward/backward). Unless otherwise specified, veScale-FSDP employs element-wise sharding granularity and is compatible with standard training workflows.\n\nWorkloads: For the end-to-end comparison with the baselines (¬ß 6.1 ), we evaluate two state-of-the-art open-source models, LLaMA-3-70B [ dubey2024llama ] and GPT-OSS-120B [ agarwal2025gpt ] , as well as an internal MoE model. Under weak scaling, each device is statically assigned one batch; the sequence length is 4096 for the dense LLaMA model and 8192 for the MoE models. We use the AdamW optimizer by default. To avoid out-of-memory (OOM) errors for the baselines on GPT-OSS, we also report results using the SGD optimizer.\n\n### 6.1 End-to-End Performance\n\n[FIGURE:x8.png] Figure 8 : FSDP training performance.\nTop row: normalized aggregate throughput (tokens/s). Bottom row: peak per-GPU memory (GB). We sweep FSDP (ZeRO-3) at 128/256 GPUs and HSDP with 2- and 4-way replication (2*256, 4*256 GPUs).\n\nFigure 8 compares the performance of veScale-FSDP to baselines on three representative models on 1024 GPUs.\n\nThroughput: on the MoE models, veScale-FSDP is 11 ‚àº \\sim 66% faster than all baselines. On LLaMA-3-70B veScale-FSDP is 5% faster than DeepSpeed, FSDP1, and FSDP2, and slightly ahead of Megatron-FSDP. The higher throughput arises from optimized communication overlapping, DBuffer -based zero-copy collectives, and flexible sharding granularities that avoid padding overhead. In contrast, DeepSpeed emits fragmented collectives [ deepspeed_AG ] , while FSDP1 exhibits communication bubbles where data movement operations block NCCL progress, under-utilizing the network in both systems. FSDP2 relies on the per-parameter DTensor even-sharding format that introduces interleaved copy-out after all-gather and interleaved copy-in before reduce-scatter; together these copies can consume up to 14% of a training iteration and hence reduce throughput. In addition, FSDP1 and FSDP2 overlook NCCL address alignment caveat, leading to substantial degenerate communication performance in certain cases [ wu2025terabyte ] . Although Megatron optimized for zero-copy collectives, its fixed Stride(0) sharding granularity, where to remain consistent with upstream DTensor Shard(0) semantic for distributed checkpointing, induces 33% buffer padding inflation in MOE models thus slows the collectives [ megatron_fsdp ] . Our experiments show that veScale-FSDP achieves linear scalability; detailed analysis appears in ¬ß 6.2 .\n\nMemory: Across benchmarks, veScale-FSDP reduces peak reserved memory by 16‚Äì30%. The memory saving stems from deterministic, batched DBuffer memory management: we explicitly manage stream dependencies for predictable memory deallocation, and we batch allocations to reduce fragmentation. By contrast, DeepSpeed and FSDP1 inherit non-deterministic deallocations from implicit record_stream [ per_parameter_shard_rfc ] , which often prevents the caching allocator from reusing buffers, inflating peak reserved memory by\n20%. Relative to FSDP2 ‚Äôs per-parameter eager allocation, our batched policy yields a further 12% reduction. Megatron‚Äôs padding-inflated buffers not only degrade collective efficiency but also raise peak memory by 33% in MoE experiments; its mixed-precision support persists low-precision buffers, consuming 24% more memory than veScale-FSDP in the LLaMA-3 experiments. Lower reserved memory translates directly into higher end-to-end efficiency: under high memory pressure, the PyTorch caching allocator issues device frees that synchronize with the driver and stall training. In terms of scalability, veScale-FSDP ‚Äôs memory footprint decreases monotonically as the FSDP group size increases and grows only marginally with the replication factor, matching scaling expectations. A notable exception appears with GPT-OSS: FSDP2 trains at 128 devices but OOMs at 256. The per-parameter sharding design in DTensor requires padding to enforce even splits along the sharded dimension; with 128 experts spread over 256 devices, the all-gather buffer effectively doubles, exhausting memory.\nWhile FSDP2 allows custom sharding along other dimensions, it requires manual padding and thus doubles the interleaved-copy overhead, making it prohibitively expensive (recall Table 1 ).\n\n### 6.2 Scalability and Composability\n\n[FIGURE:x9.png] (a)\n\nThe flexibility of RaggedShard also enables seamless integration with complementary parallelization strategies such as expert parallelism (EP) [ lepikhin2020gshard ] .\nCombining these techniques allows veScale-FSDP to efficiently scale training to internal models with up to 2.4T parameters on as many as 10K GPUs, as shown in Figure 9 . Note that we evaluate scalability of MoE, because MoE workloads are often more challenging to scale under FSDP: sparse expert computation lowers per-GPU compute while requires substantial all-gather/reduce-scatter traffic, making communication and padding overheads more significant.\n\nWeak scaling: Figure LABEL:fig:weak_scale_10k presents the weak scaling performance of veScale-FSDP. We train an 800B-parameter MoE internal model on 1K to 8K GPUs while keeping the input size fixed at 2K‚Äì16K tokens per GPU. Across all input sizes, veScale-FSDP demonstrates near-linear scalability as the GPU count increases. This is expected since the communication cost of FSDP and the computation cost per GPU remain constant with respect to the number of GPUs, depending only on the model and input sizes. These results confirm the efficiency of veScale-FSDP on large-scale GPU clusters.\n\nStrong scaling: We further evaluate the strong scaling performance of veScale-FSDP by fixing the global batch size to 16M‚Äì128M tokens and tuning expert and sequence parallelism configurations for each setting. Figures LABEL:fig:strong_scale_10k show the resulting throughput across different GPU numbers. veScale-FSDP scales linearly with a 128M-token global batch up to 10K GPUs, while still delivering a 3.4 √ó \\times throughput gain from 1K to 8K GPUs at a 16M-token global batch. When number of GPUs is small, each GPU processes enough tokens to fully overlap communication with computation, yielding near-linear scaling. However, as the GPU count continues to increase, fewer tokens are assigned per GPU per iteration, causing FSDP communication‚Äì‚Äìincluding parameter all-gather and gradient reduce-scatter‚Äì‚Äìto dominate runtime. To mitigate this overhead, we adopt cross-node expert parallelism, which further reduces FSDP communication time. This optimization introduces higher computation cost due to token exchange and reduced kernel efficiency, resulting in the performance drop at very large scales.\n\nModel scaling: We also evaluate model scaling by fixing the GPU count to 1K and increasing the model size from 400B to 2.4T parameters. With model sparsity constant and 8K training tokens per GPU, we scale both depth (number of layers) and width (intermediate dimensions) proportionally. Figure LABEL:fig:model_scale reports the effective Model FLOPS Utilization (MFU) per GPU as model size grows. Enabled by efficient memory management of DBuffer (¬ß 5 ), veScale-FSDP can train 2.4T-parameter models on only 1K GPUs without any performance degradation. In fact, MFU slightly improves with larger models due to the increased compute intensity and better utilization of GPU resources.\n\n### 6.3 8-bit Adam and Muon Optimizer\n\nWe show the flexibility of RaggedShard DTensor using two examples: 8-bit Adam and distributed Muon optimizer.\n\n8-bit Adam optimizer. 8-bit Adam [ dettmers8 ] applies block-wise INT8 quantization to the gradient statistics, substantially reducing optimizer-state memory. To enable 8-bit Adam, veScale-FSDP exposes orig_param_policy interface that lets users set the quantization granularity per parameter. In our setup, we use 32 √ó 32 32\\times 32 blocks and assign matrix parameters to 32-row block granularity. With this layout, each device quantizes its local shard independently without any communication, and block boundaries are perfectly preserved by RaggedShard . In contrast, existing FSDP systems do not natively track such block boundaries, so enabling block-wise 8-bit Adam often requires intrusive system changes or manual collectives to exchange quantization metadata, incurring both complexity and overhead.\n\n[FIGURE:x12.png] (a)\n\nWe implement the 8-bit Adam using veScale-FSDP with few lines of code and provide the evaluation in Figure LABEL:fig:adam_loss . We compare 8-bit Adam under distributed data parallelism (DDP) and veScale-FSDP. The loss curves track closely, with occasional spikes characteristic of reduced-precision optimizer states. The small difference stems from the gradient-reduction schedule: DDP uses bucketed all-reduce, whereas veScale-FSDP performs layer-wise reduce-scatter. (Note that the loss curves in Figure 10 are not directly comparable: for 8-bit Adam we use a smaller learning rate to mitigate overflow/underflow in reduced precision.)\n\n[FIGURE_CAPTION] Algorithm 2 RaggedShard Distributed Muon\n\nDistributed Muon optimizer. The matrix-sign preconditioner (e.g., Newton‚ÄìSchulz) of Muon requires the full 2D parameter matrix with its original shape. Algorithm 2 sketches the distributed Muon optimizer enabled by RaggedShard . Thanks to RaggedShard ‚Äôs capability to support uneven sharding, users can write Muon‚Äôs parameter-gather step in a clean SPMD way: after redistribution, only the root rank holds the full 2D parameter, so the Newton‚ÄìSchulz update becomes a no-op on other ranks. As lines 4‚Äì7 show, the algorithm selects a root via load balancing and unshards to it using the standard DTensor redistribute with RaggedShard placement. Lines 8‚Äì9 run the Muon matrix iteration only on the root that holds the full tensor. Finally, lines 10‚Äì12 redistribute the update back to the original device and apply it. Therefore, users do not need to handle the complex logic of communication and can further overlap communication with computation via asynchronous redistribute . In addition, our optimized Muon reaches 47.3% MFU on 256 GPUs by exploiting the communication-computation overlapping and using torch.compile to further increase compute density.\n\nWe implement distributed Muon using veScale-FSDP with few lines of code and provide the evaluation in Figure LABEL:fig:muon_loss .\nWe compare the loss curves of Muon with AdamW: the two Muon runs (veScale-FSDP and DDP) match closely, and Muon converges faster than AdamW, stabilizing around 0.01 lower loss after training ‚àº \\sim 80B tokens, which is consistent with prior results [ wen2025fantastic ] .\n\n### 6.4 Planning Quality\n\nA major design objective of the planning algorithm (Algorithm 1 ) is to enable arbitrary granularity of RaggedShard , while minimizing padding overhead and thus reducing communication volume.\nThe quality of planning algorithm can be directly evaluated by padding size.\nWe evaluate it by benchmarking DeepSeek-v3-671B and GPT-OSS-120B across varying device counts. Following the DeepSeek-style quantization scheme, we quantize only the FFN weights (most parameters) and sweep the row granularity of expert-MLP matrices over 128, 16, 1. The 128-row setting reproduces DeepSeek‚Äôs 128 √ó \\times 128 tiling (i.e., weights can be sliced into 128 √ó \\times 128 blocks). We then report the resulting relative padding ratios and analyze the root cause of the extra padding.\n\n[FIGURE:x14.png] (a)\n\nFigure LABEL:fig:deepseek_v3_padding and LABEL:fig:gpt_oss_padding show that with 1 √ó \\times and 16 √ó \\times row granularities, veScale-FSDP keep padding overhead less than 3% across all FSDP sizes for both models. With 128 √ó \\times rows, DeepSeek-v3 remains mostly below 3% with mild growth, whereas GPT-OSS exhibits step-like fluctuations with spikes up to 18%. GPT-OSS fuses all experts into a single parameter tensor, whereas DeepSeek-V3 materializes each expert as a separate parameter; this enables per-expert padding between MLPs and thus relaxes the global padding constraint. The fluctuation behavior is expected: each matrix must be partitioned across the shard group in discrete quanta determined by (i) the granularity unit (e.g., rows) and (ii) NCCL‚Äôs even-input alignment for high-performance collectives. Effective shard sizes are therefore rounded up to the least common multiple of these granularities; when the group size crosses a multiple, the per-device shard size jumps, producing the observed spikes.\n\nLastly, we also evaluate the overhead of planning algorithm itself: the algorithm runtime is less than 0.3 seconds across all experiments, which is one-time and negligible in distributed training initialization.\n\n### 6.5 Performance Breakdown\n\n[FIGURE_CAPTION] Table 2 : Component ablation for 8-bit Adam, in terms of normalized throughput when disabling each component. N/A means impossible without intrusively changing model/optimizer code or managing custom collectives.\n\nTo quantify the benefit of each component, we ablate veScale-FSDP by disabling one component at a time and report the resulting throughput, normalized to the full system. We run this study on 32 GPUs when training a GPT-OSS-style model with 8-bit Adam.\n\nTable 2 shows that DBuffer and the Planning Algorithm account for most of the realized speedups: disabling them reduces throughput to 92.8% and 65.4%, respectively. In contrast, RaggedShard is not just an optimization; it is the abstraction that makes block-wise 8-bit Adam usable without intrusive model/optimizer changes or hand-written collectives. Specifically:\n\n- ‚Ä¢ DBuffer . Disabling DBuffer drops throughput by 7.2%, reflecting the copy-in/copy-out overhead around collectives when communication buffers require copy.\n- ‚Ä¢ Planning Algorithm. Disabling the planning drops throughput by 34.6% because quantization blocks are no longer guaranteed to be fully contained within a device‚Äôs local shard. The system then falls back to DTensor redistribution to assemble the required optimizer states before per-block quantization, incurring substantial extra communication overhead.\n- ‚Ä¢ RaggedShard DTensor. Disabling RaggedShard makes it effectively non-runnable: users must either (i) carefully change every model and optimizer tensor so that 32 √ó 32 32\\times 32 block boundaries align with shard boundaries, or (ii) manually implement complex collectives (e.g., per-block metadata exchange and state gathering) to recover block-wise semantics. We therefore report this setting as N/A to indicate it‚Äôs not meaningfully usable.\n\nDBuffer . Disabling DBuffer drops throughput by 7.2%, reflecting the copy-in/copy-out overhead around collectives when communication buffers require copy.\n\nPlanning Algorithm. Disabling the planning drops throughput by 34.6% because quantization blocks are no longer guaranteed to be fully contained within a device‚Äôs local shard. The system then falls back to DTensor redistribution to assemble the required optimizer states before per-block quantization, incurring substantial extra communication overhead.\n\nRaggedShard DTensor. Disabling RaggedShard makes it effectively non-runnable: users must either (i) carefully change every model and optimizer tensor so that 32 √ó 32 32\\times 32 block boundaries align with shard boundaries, or (ii) manually implement complex collectives (e.g., per-block metadata exchange and state gathering) to recover block-wise semantics. We therefore report this setting as N/A to indicate it‚Äôs not meaningfully usable.\n\n## 7 Lessons Learned\n\nDuring the deployment of veScale-FSDP for real industrial workloads that use more than 10K GPUs, we summarize the key lessons we have learned in the following.\n\nLesson-1: Small-scale workloads can predict large-scale performance. The performance of FSDP-based workloads can be accurately estimated using each layer‚Äôs computation time and FSDP communication time. Computation occurs entirely within each GPU, and FSDP communication time remains largely unchanged when the number of GPUs increases. This observation is validated by our weak scaling experiments (¬ß 6 ). In practice, we profile the performance of veScale-FSDP on around 64 GPUs and extrapolate to thousands of GPUs, achieving similar results.\n\nThis extrapolation assumes that the profiling run exercises network behavior similar to the target scale: comparable network topology, identical collective algorithms/protocols, and a sufficiently large workload to reach bandwidth saturation. To further improve predictability at large scales, we use additional parallelization (e.g., HSDP/EP) to cap the collective group size, preventing excessively large collectives whose latency can vary more.\n\nLesson-2: Design system abstractions on the shoulders of giants. DTensor provides a powerful abstraction that already supports a wide range of parallelization techniques. By designing new abstractions on top of DTensor, we can seamlessly integrate existing parallelization strategies. In our work, RaggedShard placement is implemented as an optional placement on DTensor, enabling easy collaboration of established infrastructure such as tensor and expert parallelism, as well as mature training tools like distributed checkpointing [ dcp ] . This approach minimizes engineering effort while contributing to a shared ecosystem that benefits the broader community. In fact, RaggedShard has already appeared as a planned feature on the official roadmap [ pytorch_2026_h1_roadmap ] of PyTorch.\n\nLesson-3: Decoupled model definition with system optimization matters. The rapid evolution of model architectures demands frequent updates to model definitions. However, existing frameworks such as Megatron-LM tightly couple system-level parallelization optimizations with model code, making it difficult for researchers to modify or extend architectures. In veScale-FSDP, we decouple model definition from the system framework, allowing researchers to focus on model design while maintaining linear scalability across up to 10K GPUs. This separation greatly simplifies model development and accelerates architectural innovation.\n\n## 8 Conclusion\n\nveScale-FSDP is a scalable training system that combines high flexibility with high performance through the RaggedShard abstraction and a structure-aware planning algorithm that maximizes GPU utilization. Experiments demonstrate that veScale-FSDP seamlessly integrates with emerging techniques such as Muon optimizers and significantly outperforms existing systems, achieving 5 ‚àº \\sim 66% higher throughput and 16 ‚àº \\sim 30% lower memory usage, while scaling efficiently to tens of thousands of GPUs.\n\n## 9 Acknowledgments\n\nveScale-FSDP would not have been possible without the tremendous support and collaboration of our teammates and colleagues. We sincerely thank them (in no particular order; this list is not exhaustive):\n\n- ‚Ä¢ veScale members: Hongrui Zhan, Ziyi Zhang, Hao Feng\n- ‚Ä¢ ByteDance teammates: Jianyu Jiang, Chenyuan Wang, Cesar Andres Stuardo Moraga, Juntao Zhao, Bin Jia, Chengye Li, Zhongkai Zhao, Shixiong Zhao, Tiantian Fan, Hanshi Sun, Wenlei Bao, Shixun Wu, Zhekun Zhang, Yanbo Liang, Li-wen Chang, Jun Wang, Cheng Li, Li Han, Heng Zhang, Zhenbo Sun, Bo Liu, Xiaonan Nie, Ru Zhang, Hao Gong, Zuquan Song, Yucheng Nie, Jiawei Wu, Hongpeng Guo, Xinyi Di\n\nveScale members: Hongrui Zhan, Ziyi Zhang, Hao Feng\n\nByteDance teammates: Jianyu Jiang, Chenyuan Wang, Cesar Andres Stuardo Moraga, Juntao Zhao, Bin Jia, Chengye Li, Zhongkai Zhao, Shixiong Zhao, Tiantian Fan, Hanshi Sun, Wenlei Bao, Shixun Wu, Zhekun Zhang, Yanbo Liang, Li-wen Chang, Jun Wang, Cheng Li, Li Han, Heng Zhang, Zhenbo Sun, Bo Liu, Xiaonan Nie, Ru Zhang, Hao Gong, Zuquan Song, Yucheng Nie, Jiawei Wu, Hongpeng Guo, Xinyi Di\n\nEqually important, we thank everyone on the TorchTitan team and Edward Z. Yang for the insightful discussions and collaboration within the open-source community.\n\n## References",
  "figures": []
}