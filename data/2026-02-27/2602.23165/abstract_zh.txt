# 論文摘要翻譯

生成逼真的對話手勢對於實現與數位人類的自然、社交互動至關重要。然而，現有方法通常將單一音訊流映射到單一說話者的動作，而未考慮社交背景或建模兩人進行對話之間的相互動態。我們提出 DyaDiT，一個多模態擴散 Transformer，從二人組音訊信號生成情境相適應的人類動作。在 Seamless Interaction Dataset 上進行訓練，DyaDiT 接收二人組音訊及可選的社交背景標記，以產生情境相適應的動作。它融合來自兩位說話者的信息以捕捉交互動態，使用動作字典來編碼動作先驗，並可選擇性地利用對話夥伴的手勢來產生更具回應性的動作。我們在標準動作生成指標上評估 DyaDiT，並進行定量用戶研究，證明它不僅在客觀指標上超越現有方法，而且受到用戶的強烈偏好，突出了其穩健性和社交有利的動作生成。程式碼和模型將在接收後發布。