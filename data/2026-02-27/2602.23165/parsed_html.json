{
  "source": "html",
  "markdown": "# DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation\n\nGenerating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker‚Äôs motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset [ 1 ] , DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner‚Äôs gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.\n\n## 1 Introduction\n\nBuilding synthetic agents (also known as digital humans, AI agents, avatars or androids) that can interact naturally with people, is essential for the future of human‚Äìcomputer interfaces. Recent language models such as GPT-4.5 [ 33 ] and LLaMA-3.1 [ 12 ] already demonstrate impressive conversational ability, and many users even feel as if they are speaking with another person [ 15 ] . However, currently this illusion still remains confined to a text window. In reality, human interaction involves more than just spoken words. People gesture, respond to each other, and express subtle social cues through body motion. In order for humans to truly feel that a synthetic agent is interactive, the agent must accompany its speech with gestures that evolve naturally with the conversation.\n\nHowever, generating such gestures is challenging. They are strongly shaped by social factors such as personality, the relationship between speakers, and their conversational roles. These cues influence how people move, how they react, and how they coordinate with one another, yet most existing gesture generation models do not explicitly model them [ 42 , 26 , 27 ] . As a result, their gestures often appear generic or fake. In addition to social context, dyadic conversation, which happens between two interacting individuals, exhibits speech dynamics that are difficult for gesture generation models to handle. Two people can speak at the same time, interrupt one another, or rapidly switch between speaking and listening [ 8 , 28 ] . Their audio streams merge together, making it difficult to separate who is speaking and who is responding [ 39 , 30 , 19 ] . Most existing dyadic gesture generation models [ 32 , 29 ] , however, simply fuse the two speech streams into one, making the contribution of each speaker blurry and therefore weakening the correspondence between the generated gestures and the underlying interaction.\n\nTo tackle these limitations, we introduce DyaDiT, a diffusion based transformer that generates socially aware gestures from dyadic audio. Unlike previous work [ 22 , 23 , 36 , 10 ] that focuses only on the alignment between audio and generated motion, DyaDiT conditions its generation on explicit social cues such as relationship and personality. Furthermore, in order to address the strong entanglement of two overlapping audio streams during conversation, we propose Orthogonalization Cross Attention (ORCA), a simple yet effective module that disambiguates the two audio streams, resulting in a cleaner audio representation for better gesture generation. Lastly, since human motion in a dyadic setting is often affected by the partner‚Äôs gestures, DyaDiT can optionally take the partner‚Äôs movements as an additional input, allowing the model to generate gestures that are more coordinated, responsive, and natural.\n\nOur experiments show that DyaDiT consistently outperforms existing dyadic gesture generation methods across standard quantitative metrics, achieving clear improvements in gesture quality and distribution alignment. In addition, we conduct extensive user studies to assess human perceptual preference. The results show that participants strongly favor gestures generated by DyaDiT, indicating that our generated motion appears more socially aware and better suited for real conversational settings.\n\nIn summary, our main contributions are as follows:\n\n- ‚Ä¢ We propose DyaDiT, a diffusion transformer (DiT) that generates social context aware gestures in dyadic conversations.\n- ‚Ä¢ We introduce an Orthogonalization Cross Attention (ORCA) module that reduces interference between two individual‚Äôs audio streams, enabling a cleaner audio representation for better gesture generation in dyadic conversation.\n- ‚Ä¢ Through extensive quantitative evaluations and user studies, we show that DyaDiT consistently outperforms existing methods on standard metrics and is preferred by users in terms of perceived realism and social consistency.\n\nWe propose DyaDiT, a diffusion transformer (DiT) that generates social context aware gestures in dyadic conversations.\n\nWe introduce an Orthogonalization Cross Attention (ORCA) module that reduces interference between two individual‚Äôs audio streams, enabling a cleaner audio representation for better gesture generation in dyadic conversation.\n\nThrough extensive quantitative evaluations and user studies, we show that DyaDiT consistently outperforms existing methods on standard metrics and is preferred by users in terms of perceived realism and social consistency.\n\n[FIGURE:x2.png] Figure 2 : Overview of DyaDiT. DyaDiT conditions on multiple input modalities, including audio, partner motion, relationship type, and personality scores. It employs an Audio Orthogonalization Cross Attention (ORCA) module to obtain cleaner audio representations and a motion dictionary to guide style aware gesture generation.\n\n## 2 Related Work\n\n### 2.1 Co-Speech Gesture Generation\n\nCo-speech gesture generation focuses on synthesizing body movements aligned with a single speaker‚Äôs speech. Early approaches formulate gesture generation as a translation problem from multimodal speech cues, combining text, audio, and speaker identity through recurrent or adversarial models, such as the trimodal framework of Yoon et al. [ 49 ] , which treated gesture generation as a translation problem, combining speech transcripts, audio, and speaker identity through an adversarial recurrent framework. Liu et al. [ 23 ] introduced the BEAT dataset and CaMN, a cascaded multimodal adversarial network capable of generating synchronized body and hand gestures. Other works, such as EMAGE [ 22 ] , TalkSHOW [ 47 ] , MECo [ 5 ] , etc. [ 34 , 24 , 11 , 21 , 45 ] enhanced realism by disentangling rhythmic and semantic motion features or integrating discrete latent spaces via VQ-VAEs.\n\nAlthough these advances are notable, most of them focuses on single-speaker co-speech gestures and overlooks the interactive nature of human communication. More complex settings such as dyadic gesture generation, which require modeling both participants‚Äô behaviors and their interpersonal dynamics, remain largely unexplored.\n\n### 2.2 Dyadic Gesture and Reaction Generation\n\nUnlike co-speech gesture synthesis, dyadic gesture generation must model the coordinated behavior between two participants, including interpersonal timing, mutual attention, and responsiveness. Most prior works in this domain stem from facial reaction generation (FRG), where the goal is to predict non-deterministic listener facial responses to a speaker‚Äôs behavior [ 50 , 16 , 26 , 27 , 25 , 38 , 31 ] .\nAlthough FRG provides a basis for modeling two-person interactions, it primarily focuses only on facial or head movements rather than full-body gestures. Due to this limitation, several recent works have begun exploring body gesture generation in dyadic settings.\nA few efforts such as Audio2Photoreal [ 32 ] , ConvoFusion [ 29 ] , and TAG2G [ 9 ] extend single-speaker frameworks to two-party settings.\nYet, these models often either (1) overlook the social context between the two individuals or (2) treat dyadic audio as a single blended signal without explicitly modeling cross-person dynamics, which often leads to ambiguity in the roles and interaction patterns presented to the model. These limitations highlight the need for explicit feature disentanglement and better support for socially contextual reasoning.\n\n### 2.3 Diffusion-based Gesture Generation\n\nDiffusion models have emerged as powerful generative tools for human motion due to their ability to model multi-modal and many-to-many distributions.\nSeveral works have adapted diffusion frameworks originally designed for text-conditioned motion generation, such as MotionDiffuse [ 51 ] , FineMoGen [ 52 ] , and MDM [ 41 ] , etc. [ 37 , 40 , 6 , 3 ] , to the speech‚Äìgesture domain. Alexanderson et al. [ 2 ] reformulated DiffWave for co-speech gestures, while Zhu et al. [ 55 ] proposed DiffGesture, integrating noisy gesture sequences with contextual embeddings for temporal modeling. DiffuseStyleGesture+ [ 46 ] further introduced conditioning on audio, text, style, and seed gestures, leveraging transformer-based denoising with attention control. Other works, such as UnifiedGesture [ 44 ] , LivelySpeaker [ 53 ] , and AMUSE [ 7 ] , emphasized semantic and rhythmic consistency, disentangling emotional and stylistic latent factors. Considering the success of diffusion-based approaches in gesture generation, and the inherently nondeterministic nature of dyadic conversation, we build our method upon a diffusion model to better capture the variability and dynamics present in two-person interactions.\n\n[FIGURE:x3.png] Figure 3 : ORCA reduces ambiguity between the two audio streams, allowing DyaDiT to generate realistic motion even when one person interrupts the other during the conversation. The example demonstrates the generated motions adjusts naturally as the conversation shifts.\n\n## 3 DyaDiT\n\nWe introduce DyaDiT , a diffusion transformer (DiT) with multi-modal input for dyadic gesture generation. Given the conversational audio streams from two individuals (denoted as self and other ), our objective is to generate plausible upper-body gestures for the other speaker in accordance with the conversational context. To further enhance the realism of the generated gestures, DyaDiT optionally incorporates the relationship, personality traits, and gesture sequence of self as auxiliary conditioning signals during inference. We train our model on a subset of the Seamless Interaction dataset [ 1 ] , with dataset specifications and preprocessing procedures detailed in Section 3.1 . An overview of the full architecture is shown in Fig. 2 . Details of the Audio Orthogonalization Cross Attention (ORCA) module are provided in Section 3.3 , while the motion dictionary and motion tokenizer are presented in Sections 3.4 and 3.5 , respectively.\n\n### 3.1 Seamless Interaction Dataset\n\nThe Seamless Interaction dataset [ 1 ] is a large-scale corpus of dyadic conversation containing synchronized audio, full-body motion, and facial expressions. In this work, we use a curated subset of 3,000 clips (approximately 182 hours) from the dataset‚Äôs naturalistic scenario collection, which contains rich and spontaneous dyadic conversations suited for modeling conversational gestures. We focus on generating upper-body gestures, represented as g ‚àà ‚Ñù T √ó J √ó 6 g\\in\\mathbb{R}^{T\\times J\\times 6} using the 6D rotation representation [ 54 ] , where J J denotes the number of upper-body joints. In addition to motion data, we incorporate two high level social annotations provided in the dataset: a relationship type label f r ‚Äã s ‚àà { 0 , 1 } 4 f_{rs}\\in\\{0,1\\}^{4} indicating whether the speakers are friends, strangers, family members, or dating partners; and a personality score vector f p ‚Äã s ‚àà ‚Ñù 5 f_{ps}\\in\\mathbb{R}^{5} that quantifies five major personality traits, which are extraversion, agreeableness, conscientiousness, neuroticism, and openness. For audio inputs, we extract the dyadic speech signals from both individuals and process them with a pretrained Wav2Vec2 encoder [ 4 ] to obtain audio feature embeddings for conditioning in our model.\n\n### 3.2 DiT Backbone\n\nOur model adopts a diffusion transformer (DiT) backbone that follows the Denoising Diffusion Probabilistic Model (DDPM) [ 13 ] framework. The network takes a noisy latent pose ùê± t \\mathbf{x}_{t} and predicts the added Gaussian noise according to time step t t , œµ Œ∏ ‚Äã ( ùê± t , t , ùêú ) \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{c}) conditioned on multiple contextual inputs ùêú \\mathbf{c} ,\nwhere ùêú = ( O ‚Äã R ‚Äã C ‚Äã A ‚Äã ( a self , a other ) , p self , f relat , f ps ) \\mathbf{c}=(ORCA(a_{\\text{self}},a_{\\text{other}}),p_{\\text{self}},f_{\\text{relat}},f_{\\text{ps}}) including audio, partner‚Äôs motion, relationship type, and personality scores. The training objective follows the standard œµ \\epsilon -prediction loss:\n\n|  | ‚Ñí diff = ùîº ùê± 0 , t , œµ ‚Äã [ ‚Äñ œµ ‚àí œµ Œ∏ ‚Äã ( ùê± t , t , ùêú ) ‚Äñ 2 2 ] , \\mathcal{L}_{\\text{diff}}=\\mathbb{E}_{\\mathbf{x}_{0},t,\\boldsymbol{\\epsilon}}\\left[\\left\\|\\boldsymbol{\\epsilon}-\\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{c})\\right\\|_{2}^{2}\\right], |  | (1) |\n| --- | --- | --- | --- |\n\nwhere ùê± t = Œ± ¬Ø t ‚Äã ùê± 0 + 1 ‚àí Œ± ¬Ø t ‚Äã œµ \\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\boldsymbol{\\epsilon} .\n\nAs shown in Figure 2 (a), each DiT block consists of a self-attention layer for modeling temporal dependencies within the latent pose sequence and a cross-attention layer for integrating contextual information from multimodal cues. In addition, the relationship and personality embeddings are injected through both FiLM-based [ 35 ] modulation and cross-attention, enabling DyaDiT to jointly capture social attributes and individual expressive styles in gesture generation.\n\n### 3.3 Audio Orthogonalization Cross Attention (ORCA)\n\nTo effectively capture conversational dynamics between two speakers, we introduce a orthogonalization cross attention module for audio fusion (Figure 2 (b)). Given the audio features from both speakers a self a_{\\text{self}} and a other a_{\\text{other}} , encoded by Wav2Vec2, our goal is to separate redundant factors while aligning complementary information into a joint representation.\n\nWe first apply an orthogonalization process to filter out redundant components between the two audio streams:\n\n|  | a self ‚üÇ = a self ‚àí Proj a other ‚Äã ( a self ) , a_{\\text{self}}^{\\perp}=a_{\\text{self}}-\\mathrm{Proj}_{a_{\\text{other}}}(a_{\\text{self}}), |  | (2) |\n| --- | --- | --- | --- |\n\nwhere Proj a other ‚Äã ( ‚ãÖ ) \\mathrm{Proj}_{a_{\\text{other}}}(\\cdot) denotes the projection of the self audio feature a s ‚Äã e ‚Äã l ‚Äã f a_{self} onto the subspace of the other‚Äôs audio feature. This operation enforces complementary conditioning and reduces correlated information across dyadic audio.\n\nSubsequently, we employ two symmetric cross-attention modules to achieve bidirectional information exchange. The first module uses a other a_{\\text{other}} as query and attends to a self ‚üÇ a_{\\text{self}}^{\\perp} , capturing the speaker‚Äôs response to the partner‚Äôs utterance. Conversely, the second module takes a self ‚üÇ a_{\\text{self}}^{\\perp} as query and attends to a other a_{\\text{other}} , modeling the listener‚Äôs reactive cues. The outputs of the two cross-attention streams are then adaptively fused through a learnable gating mechanism:\n\n|  | f audio = œÉ ‚Äã ( ùêñ g ) ‚ãÖ h self ‚Üí other + ( 1 ‚àí œÉ ‚Äã ( ùêñ g ) ) ‚ãÖ h other ‚Üí self , f_{\\text{audio}}=\\sigma(\\mathbf{W}_{g})\\cdot h_{\\text{self}\\rightarrow\\text{other}}+(1-\\sigma(\\mathbf{W}_{g}))\\cdot h_{\\text{other}\\rightarrow\\text{self}}, |  | (3) |\n| --- | --- | --- | --- |\n\nwhere œÉ ‚Äã ( ‚ãÖ ) \\sigma(\\cdot) is a sigmoid function and ùêñ g \\mathbf{W}_{g} is a learnable gate parameter.\n\nThe resulting fused token f audio f_{\\text{audio}} is used as the final audio conditioning input to the DyaDiT, providing a plausible acoustic embedding that reflects both interlocutors‚Äô vocal behaviors. As shown in Figure 3 , with ORCA, DyaDiT have the capability to generate either speaker or listener gestures.\n\n### 3.4 Motion Dictionary (MD)\n\nInspired by prior work LIA [ 43 ] which learns a set of orthogonal motion-directions by enforcing orthonormality at initialization, we similarly introduce a learnable orthogonal motion dictionary to modulate the partner‚Äôs audio feature a o ‚Äã t ‚Äã h ‚Äã e ‚Äã r a_{other} according to the current style motion. The motion dictionary is a module designed to incorporate motion style conditioning when user requires motion style control. As shown in Figure 2 (c), it consists of a set of learnable motion bases { d 0 , d 1 , ‚Ä¶ , d n } \\{d_{0},d_{1},\\dots,d_{n}\\} that encode representative gesture primitives. During training, we include ground truth motion style features f motion = [ m 0 , m 1 , ‚Ä¶ , m n ] f_{\\text{motion}}=[m_{0},m_{1},\\dots,m_{n}] to guide the model in learning style-aware correspondences between audio cues and motion patterns.\n\nGiven the other-speaker audio feature a other a_{\\text{other}} , the dictionary is integrated through a cross-attention (CA) operation followed by a weighted combination with the motion bases:\n\n|  | a other ‚Ä≤ = CA ‚Äã ( a other , ‚àë k = 0 n m k ‚Äã d k ) + a other , a_{\\text{other}}^{\\prime}=\\mathrm{CA}(a_{\\text{other}},\\sum_{k=0}^{n}m_{k}d_{k})+a_{\\text{other}}, |  | (4) |\n| --- | --- | --- | --- |\n\nwhere Dict = [ d 0 , d 1 , ‚Ä¶ , d n ] \\mathrm{Dict}=[d_{0},d_{1},\\dots,d_{n}] and m k m_{k} denotes the style-dependent modulation weight derived from f motion f_{\\text{motion}} .\n\nAt inference, the motion dictionary can be optionally activated. With classifier-free guidance (CFG) [ 14 ] , the style conditioning can be strengthened to generate gestures with distinct stylistic patterns, or completely dropped to produce an unconditional, style-agnostic motion driven purely by the learned audio‚Äìmotion prior.\n\n### 3.5 Motion Tokenizer\n\nInstead of directly training our DiT model using the raw gesture sequences x 0 ‚Ä≤ ‚àà ‚Ñù T √ó J x_{0}^{\\prime}\\in\\mathbb{R}^{T\\times J} , we first train a VQ-VAE to discretize the continuous motion space into compact latent representations. Our implementation follows the residual vector quantization framework introduced in prior works [ 18 ] , where multiple codebooks are cascaded to progressively refine quantization errors. Specifically, we adopt a residual VQ-VAE with a residual length of 4, utilizing four hierarchical codebooks to capture progressively finer-grained motion details. In addition, to reduce temporal redundancy, we downsample the input gesture sequence by a factor of 4 using 1D convolution layers in the encoder, resulting in one latent token for every four frames. During training of DyaDiT, the VQ-VAE encoder compresses gesture sequences into a sequence of quantized tokens, x t ‚Ä≤ ‚àà ‚Ñù T / 4 √ó d x_{t}^{\\prime}\\in\\mathbb{R}^{T/4\\times d} , where d = 64 d=64 in our experiment. At inference time, the VQ-VAE decoder then reconstructs continuous motions from the generated quantized embeddings. Note that since our diffusion model operates directly in the latent space, the encoder is not used during inference, and the decoder is not used during training. This hierarchical and temporally compact quantization allows the diffusion model to capture long-range dependencies while maintaining high-fidelity motion reconstruction.\n\n## 4 Experiments\n\nWe evaluate the effectiveness of DyaDiT for dyadic gesture generation through both quantitative and qualitative experiments. For the quantitative evaluation, we compute the Fr√©chet Distance (FD) and Diversity metrics to compare DyaDiT with two existing dyadic gesture generation models, ConvoFusion [ 29 ] and Audio2PhotoReal [ 32 ] . In addition, we report scores with respect to the ground truth gestures, which serve as a reference for the underlying motion distribution in the dataset. For the qualitative evaluation, we conduct a user preference study to evaluate both the overall perceived quality of the generated gestures and their alignment with the social relationship and personality characteristics.\n\n### 4.1 Evaluation Metrics\n\n[FIGURE_CAPTION] Table 1: Quantitative comparison of DyaDiT and baselines in terms of Fr√©chet Distance (FD) and Diversity. Lower FD indicates higher realism, and higher diversity values indicate more varied motion generation.\n\n[FIGURE:x4.png] Figure 4 : Qualitative Results. Comparison of visualization results between DyaDiT , ConvoFusion [ 29 ] , and Audio2PhotoReal [ 3 ] . The gestures generated by DyaDiT exhibit higher diversity and greater realism compared to the other methods.\n\n[FIGURE:x5.png] Figure 5 : Visualization results under different personality score conditionings. All samples are generated using classifier-free guidance with CFG = 2.5.\n\nFollowing prior works on gesture generation [ 32 , 3 , 2 ] , we adopt both distribution and diversity metrics for quantitative evaluation. Specifically, we report the Fr√©chet Distance (FD) to measure the realism of generated motions, Diversity [ 20 ] to quantify motion variability across samples\nFor reference, we also compute all metrics on ground truth samples from the Seamless Interaction dataset [ 1 ] to provide as baseline for comparison. Below, we provide the definitions of the quantitative metrics used in our evaluation.\n\n- ‚Ä¢ FD (Static) measures the pose realism for individual frames in the pose space ‚Ñù 3 √ó j \\mathbb{R}^{3\\times j} , where j = 43 j{=}43 (including finger joints).\n- ‚Ä¢ FD (Kinetic) evaluates the realism of temporal dynamics by computing the FD over gesture velocities in ‚Ñù T √ó 3 √ó j \\mathbb{R}^{T\\times 3\\times j} , where T = 300 T{=}300 frames in our experiments.\n- ‚Ä¢ Diversity (Static) quantifies the variation of single-frame poses by computing the average mean squared error (MSE) between clips.\n- ‚Ä¢ Diversity (Kinetic) measures temporal motion diversity by averaging the pairwise velocity differences across generated gesture sequences.\n\nFD (Static) measures the pose realism for individual frames in the pose space ‚Ñù 3 √ó j \\mathbb{R}^{3\\times j} , where j = 43 j{=}43 (including finger joints).\n\nFD (Kinetic) evaluates the realism of temporal dynamics by computing the FD over gesture velocities in ‚Ñù T √ó 3 √ó j \\mathbb{R}^{T\\times 3\\times j} , where T = 300 T{=}300 frames in our experiments.\n\nDiversity (Static) quantifies the variation of single-frame poses by computing the average mean squared error (MSE) between clips.\n\nDiversity (Kinetic) measures temporal motion diversity by averaging the pairwise velocity differences across generated gesture sequences.\n\n### 4.2 Baseline Methods\n\nWe compare DyaDiT with two representative dyadic gesture generation models:\n\n- ‚Ä¢ ConvoFusion [ 29 ] implement a diffusion-based framework with multiple layers of cross-attention to fuse multimodal inputs, including audio and visual features. For a fair comparison, we retrain their model on our training subset of the Seamless Interaction Dataset [ 1 ] to match our experimental domain and output representation.\n- ‚Ä¢ Audio2PhotoReal [ 32 ] is a full-body gesture generation model that jointly synthesizes body and facial motion. We extract and adapt only the body gesture generation branch for comparison. Specifically, the model first predicts keyframe poses from audio using a causal transformer, and then employs a diffusion transformer to interpolate between keyframes to produce temporally coherent motion sequences. Likewise, we retrain the model on our dataset to ensure domain consistency with our evaluation study setup.\n\nConvoFusion [ 29 ] implement a diffusion-based framework with multiple layers of cross-attention to fuse multimodal inputs, including audio and visual features. For a fair comparison, we retrain their model on our training subset of the Seamless Interaction Dataset [ 1 ] to match our experimental domain and output representation.\n\nAudio2PhotoReal [ 32 ] is a full-body gesture generation model that jointly synthesizes body and facial motion. We extract and adapt only the body gesture generation branch for comparison. Specifically, the model first predicts keyframe poses from audio using a causal transformer, and then employs a diffusion transformer to interpolate between keyframes to produce temporally coherent motion sequences. Likewise, we retrain the model on our dataset to ensure domain consistency with our evaluation study setup.\n\nDuring inference, both methods use a DDIM scheduler with 50 denoising steps (CFG = 2) to generate 300-frame gesture sequences (10 seconds).\n\n### 4.3 Quantitative Results\n\nThe quantitative comparison between DyaDiT and the baseline models is shown in Table 1 .\nFor interpretability, the GT diversity reflects the dataset‚Äôs upper bound, while the GT Random FD serves as its lower bound. Across all metrics, DyaDiT consistently outperforms existing baselines, achieving substantially lower FD(static) and FD(kinetic) scores while preserving high motion diversity. These results demonstrate that the proposed social-context-aware DiT framework is more effective than the multi-branch fusion design in ConvoFusion [ 29 ] and the keyframe interpolation approach used in Audio2PhotoReal [ 20 ] .\n\n### 4.4 Ablation Studies\n\nWe further conduct several ablation studies to analyze how different components and conditioning signals contribute to DyaDiT‚Äôs performance. Below, we explicitly explain the difference between each variant.\n\n- ‚Ä¢ DyaDiT (w/o ORCA) removes the ORCA module and directly concatenate the raw dyadic audio features.\n- ‚Ä¢ DyaDiT (w/o MD) removes the motion dictionary.\n- ‚Ä¢ DyaDiT (Uncond) generates gestures without any conditioning.\n- ‚Ä¢ DyaDiT (Random) performs inference with randomly assigned relationship and personality labels.\n\nDyaDiT (w/o ORCA) removes the ORCA module and directly concatenate the raw dyadic audio features.\n\nDyaDiT (w/o MD) removes the motion dictionary.\n\nDyaDiT (Uncond) generates gestures without any conditioning.\n\nDyaDiT (Random) performs inference with randomly assigned relationship and personality labels.\n\nAs expected, we observe that removing the ORCA module leads to a noticeable degradation in FD performance, confirming its contribution to motion realism. Likewise, removing the Motion Dictionary also reduces overall performance, especially in Diversity (Static) , which drops by 9.12, suggesting that the learned motion bases play an important role in encouraging style variation.\n\nIn addition, we observe that DyaDiT outperforms both DyaDiT (Uncond) and DyaDiT (Random) in terms of generation quality. This demonstrates that social context is essential for producing more realistic gesture generation. Furthermore, we find that the Diversity (Static) metric drops by around 6 points when social context is removed or randomly assigned. We believe this is because the audio signal is positively correlated with both relationship and personality traits, and mismatched or missing social context leads to a more restricted motion distribution.\n\nLastly, we observe that DyaDiT (Uncond) does not exhibit higher diversity than the fully conditioned DyaDiT model. We believe this is due to the nature of dyadic conversations, where a substantial portion of the data consists of listener behavior, which naturally shows less variation in both static and kinetic gestures compared to speaker behavior. As a result, additional conditioning signals help to guide the model to produce a wider spread of gesture variations, leading to greater overall diversity.\n\n## 5 User Study\n\nWe conduct an A/B preference study with sixteen participants to assess user preference on human motion in dyadic conversation settings. We compare our method with ConvoFusion [ 29 ] as well as with ground truth motion. The user study evaluates three aspects of the generated gestures: overall quality, relationship consistency, and personality consistency. Participants are shown paired clips and select their preferred gesture sequence, with the video pairs presented in randomized order to avoid ordering bias. An example video pair is shown in Figure 6 .\n\nWe randomly selected 56 ten-second sequences from the validation set of the Seamless Interaction dataset to conduct the experiment. Below we show sample questions presented to the participants in each section.\n\n- ‚Ä¢ overall quality : ‚ÄúWhich gesture of the orange character looks more human-like?‚Äù\n- ‚Ä¢ relationship consistency : ‚ÄúWhich pair seems more likely to be friends (or strangers, or family members, or dating partners)?‚Äù\n- ‚Ä¢ personality consistency : ‚ÄúWhich gesture better reflects the personality trait agreeable (or conscientious, or extraverted, or neurotic)?‚Äù\n\noverall quality : ‚ÄúWhich gesture of the orange character looks more human-like?‚Äù\n\nrelationship consistency : ‚ÄúWhich pair seems more likely to be friends (or strangers, or family members, or dating partners)?‚Äù\n\npersonality consistency : ‚ÄúWhich gesture better reflects the personality trait agreeable (or conscientious, or extraverted, or neurotic)?‚Äù\n\nFor each video pair, participants were asked to provide both their preference and their confidence level (strongly or slightly). For the full questionnaire and interface design used in the experiment, please refer to the supplementary material.\n\n[FIGURE:x6.png] Figure 6 : Example video pairs used in the user study for evaluating participant preference in conversational gesture generation.\n\n[FIGURE:x7.png] Figure 7 : A/B subjective evaluation percentages comparing our method with ConvoFusion [ 29 ] and with ground truth. Participants preferred our generated motion due to its more natural and socially aware conversational behavior.\n\nAs shown in Figure 7 , DyaDiT achieves notably higher user preference scores than ConvoFusion [ 29 ] , and interestingly, even slightly outperforms the ground truth gestures from the Seamless Interaction dataset.\nSpecifically, DyaDiT is preferred by 73.9%, 69.8%, and 66.7% of participants in the overall gesture quality, relationship consistency, and personality consistency sections, respectively. These results demonstrate that DyaDiT not only produces higher-quality gestures, but also generates motions that are more socially appropriate in dyadic conversational settings. Furthermore, our generated gestures are preferred over the ground-truth gestures by 1% and 1.7% in the two comparison settings. We hypothesize this is mainly because (1) the ground-truth motion data occasionally contains jittering artifacts, whereas the diffusion process implicitly regularizes motion and produces smoother, more temporally consistent gestures; and\n(2) conditioning on social context encourage the model to generate slightly more expressive or exaggerated motions, which, although less common in real interactions, tend to be more visually engaging to our participants. Lastly, It is worth noting that we do not compare our generated gestures with the ground truth in the personality consistency section. This is because the personality annotations in the dataset are provided as continuous values rather than discrete categories, making it difficult for participants to reliably judge whether a gesture aligns more strongly with a specific personality trait.\n\n## 6 Conclusion\n\nIn this work, we present DyaDiT , a multi-modal dyadic gesture generation model designed to produce socially consistent and natural conversational behaviors between two speakers. By jointly conditioning on dyadic audio, social attributes, and the partner‚Äôs motion, DyaDiT effectively captures interpersonal dynamics in dyadic conversation. Our orthogonalization cross-attention (ORCA) module helps clarify the contribution of each speaker‚Äôs audio, and the motion dictionary enhances expressive richness by providing style-aware motion priors. Both quantitative and subjective evaluations demonstrate that DyaDiT achieves superior performance compared to existing methods in terms of realism, diversity, and social coherence. We believe this work takes an important step toward socially favorable synthetic agents and opens up future research directions, such as socially aware dual-agent gesture generation.\n\n## 7 Limitation & Future Work\n\nCurrently, our model only generates upper-body gestures due to the limitations of the dataset. However, we believe that the proposed framework can be naturally extended to full-body gesture and facial expression generation. Furthermore, although our model can generate socially favorable gestures according to the given relationship or personality conditioning, some personality cues may be unintentionally contained in the audio input, leading to conditioning conflicts and reduced diversity in the generated motions. To further improve controllability and stabilize diversity, our future work will explore audio neutralization techniques to enable more diverse gesture generation.\n\n## References\n\n- [1] V. Agrawal, A. Akinyemi, K. Alvero, M. Behrooz, and et al. (2025) Seamless interaction: dyadic audiovisual motion modeling and large-scale dataset . External Links: Link Cited by: ¬ß3.1 , ¬ß3 , 1st item , ¬ß4.1 , ¬ß9 , DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation .\n- [2] S. Alexanderson, R. Nagy, J. Beskow, and G. E. Henter (2023-07) Listen, denoise, action! audio-driven motion synthesis with diffusion models . ACM Trans. Graph. 42 ( 4 ). External Links: ISSN 0730-0301 , Link , Document Cited by: ¬ß2.3 , ¬ß4.1 .\n- [3] T. Ao, Z. Zhang, and L. Liu GestureDiffuCLIP: gesture diffusion model with clip latents . ACM Trans. Graph. . External Links: Document Cited by: ¬ß2.3 , Figure 4 , Figure 4 , ¬ß4.1 .\n- [4] A. Baevski, H. Zhou, A. Mohamed, and M. Auli (2020) Wav2vec 2.0: a framework for self-supervised learning of speech representations . In Proceedings of the 34th International Conference on Neural Information Processing Systems , NIPS ‚Äô20 , Red Hook, NY, USA . External Links: ISBN 9781713829546 Cited by: ¬ß3.1 , ¬ß9 .\n- [5] B. Chen, Y. Li, Y. Zheng, Y. Ding, and K. Zhou (2025) Motion-example-controlled co-speech gesture generation leveraging large language models . In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers , SIGGRAPH Conference Papers ‚Äô25 , New York, NY, USA . External Links: ISBN 9798400715402 , Link , Document Cited by: ¬ß2.1 .\n- [6] X. Chen, B. Jiang, W. Liu, Z. Huang, B. Fu, T. Chen, and G. Yu (2023) Executing your commands via motion diffusion in latent space . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†18000‚Äì18010 . Cited by: ¬ß2.3 .\n- [7] K. Chhatre, R. Danƒõƒçek, N. Athanasiou, G. Becherini, C. Peters, M. J. Black, and T. Bolkart (2024-06) AMUSE: emotional speech-driven 3D body animation via disentangled latent diffusion . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp.¬†1942‚Äì1953 . External Links: Link Cited by: ¬ß2.3 .\n- [8] M. Dunne and S. H. Ng (1994) Simultaneous speech in small group conversation: all-together-now and one-at-a-time? . Journal of Language and Social Psychology 13 ( 1 ), pp.¬†45‚Äì71 . Cited by: ¬ß1 .\n- [9] F. Favali, V. Schmuck, V. Villani, and O. Celiktutan (2024) TAG2G: a diffusion-based approach to interlocutor-aware co-speech gesture generation . Electronics 13 ( 17 ). External Links: Link , ISSN 2079-9292 , Document Cited by: ¬ß2.2 .\n- [10] C. Fu, Y. Wang, J. Zhang, Z. Jiang, X. Mao, J. Wu, W. Cao, C. Wang, Y. Ge, and Y. Liu (2024) MambaGesture: enhancing co-speech gesture generation with mamba and disentangled multi-modality fusion . In Proceedings of the 32nd ACM International Conference on Multimedia , MM ‚Äô24 , New York, NY, USA , pp.¬†10794‚Äì10803 . External Links: ISBN 9798400706868 , Link , Document Cited by: ¬ß1 .\n- [11] S. Ginosar, A. Bar, G. Kohavi, C. Chan, A. Owens, and J. Malik (2019) Learning individual styles of conversational gesture . In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Vol. , pp.¬†3492‚Äì3501 . External Links: Document Cited by: ¬ß2.1 .\n- [12] A. Grattafiori, A. Dubey, and A. J. et al. (2024) The llama 3 herd of models . External Links: 2407.21783 , Link Cited by: ¬ß1 .\n- [13] J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probabilistic models . arXiv preprint arxiv:2006.11239 . Cited by: ¬ß3.2 .\n- [14] J. Ho and T. Salimans (2021) Classifier-free diffusion guidance . In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications , External Links: Link Cited by: ¬ß3.4 .\n- [15] C. R. Jones and B. K. Bergen (2025) Large language models pass the turing test . External Links: 2503.23674 , Link Cited by: ¬ß1 .\n- [16] S. Jung and T. Kim (2025) DiffListener: discrete diffusion model for listener generation . In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Vol. , pp.¬†1‚Äì5 . External Links: Document Cited by: ¬ß2.2 .\n- [17] R. Khirodkar, J. Song, J. Cao, Z. Luo, and K. Kitani (2024) Harmony4D: a video dataset for in-the-wild close human interactions . In Advances in Neural Information Processing Systems , A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.) , Vol. 37 , pp.¬†107270‚Äì107285 . External Links: Document , Link Cited by: ¬ß9 .\n- [18] D. Lee, C. Kim, S. Kim, M. Cho, and W. Han (2022) Autoregressive image generation using residual quantization . External Links: 2203.01941 , Link Cited by: ¬ß3.5 , ¬ß9 .\n- [19] Y. Lee, S. Choi, B. Kim, Z. Wang, and S. Watanabe (2024) Boosting unknown-number speaker separation with transformer decoder-based attractor . In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp.¬†446‚Äì450 . Cited by: ¬ß1 .\n- [20] J. Li, D. Kang, W. Pei, X. Zhe, Y. Zhang, L. Bao, and Z. He (2024-08) Audio2Gestures: generating diverse gestures from audio . IEEE Transactions on Visualization and Computer Graphics 30 ( 8 ), pp.¬†4752‚Äì4766 . External Links: ISSN 1077-2626 , Link , Document Cited by: ¬ß4.1 , ¬ß4.3 , Table 1 .\n- [21] H. Liu, N. Iwamoto, Z. Zhu, Z. Li, Y. Zhou, E. Bozkurt, and B. Zheng (2022) DisCo: disentangled implicit content and rhythm learning for diverse co-speech gestures synthesis . In Proceedings of the 30th ACM International Conference on Multimedia , MM ‚Äô22 , New York, NY, USA , pp.¬†3764‚Äì3773 . External Links: ISBN 9781450392037 , Link , Document Cited by: ¬ß2.1 .\n- [22] H. Liu, Z. Zhu, G. Becherini, Y. Peng, M. Su, Y. Zhou, X. Zhe, N. Iwamoto, B. Zheng, and M. J. Black (2024-06) EMAGE: towards unified holistic co-speech gesture generation via expressive masked audio gesture modeling . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp.¬†1144‚Äì1154 . Cited by: ¬ß1 , ¬ß2.1 .\n- [23] H. Liu, Z. Zhu, N. Iwamoto, Y. Peng, Z. Li, Y. Zhou, E. Bozkurt, and B. Zheng (2022) BEAT: a large-scale semantic and emotional multi-modal dataset for conversational gestures synthesis . In Computer Vision ‚Äì ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part VII , Berlin, Heidelberg , pp.¬†612‚Äì630 . External Links: ISBN 978-3-031-20070-0 , Link , Document Cited by: ¬ß1 , ¬ß2.1 .\n- [24] X. Liu, Q. Wu, H. Zhou, Y. Xu, R. Qian, X. Lin, X. Zhou, W. Wu, B. Dai, and B. Zhou (2022) Learning hierarchical cross-modal association for co-speech gesture generation . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†10462‚Äì10472 . Cited by: ¬ß2.1 .\n- [25] C. Luo, S. Song, W. Xie, M. Spitale, Z. Ge, L. Shen, and H. Gunes (2025) ReactFace: online multiple appropriate facial reaction generation in dyadic interactions . IEEE Transactions on Visualization and Computer Graphics 31 ( 9 ), pp.¬†6190‚Äì6207 . External Links: Document Cited by: ¬ß2.2 .\n- [26] C. Luo, S. Song, S. Yan, Z. Yu, and Z. Ge (2025) ReactDiff: fundamental multiple appropriate facial reaction diffusion model . In Proceedings of the 33rd ACM International Conference on Multimedia , MM ‚Äô25 , New York, NY, USA , pp.¬†5607‚Äì5616 . External Links: ISBN 9798400720352 , Link , Document Cited by: ¬ß1 , ¬ß2.2 .\n- [27] C. Luo, J. Wang, B. Li, S. Song, and B. Ghanem (2025) OmniResponse: online multimodal conversational response generation in dyadic interactions . arXiv preprint arXiv:2505.21724 . Cited by: ¬ß1 , ¬ß2.2 .\n- [28] A. S. Meyer (2023) Timing in conversation . Journal of Cognition 6 ( 1 ), pp.¬†20 . Cited by: ¬ß1 .\n- [29] M. H. Mughal, R. Dabral, I. Habibie, L. Donatelli, M. Habermann, and C. Theobalt (2024-06) ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Vol. , Los Alamitos, CA, USA , pp.¬†1388‚Äì1398 . External Links: ISSN , Document , Link Cited by: ¬ß1 , ¬ß2.2 , Figure 4 , Figure 4 , 1st item , ¬ß4.3 , Table 1 , ¬ß4 , Figure 7 , Figure 7 , ¬ß5 , ¬ß5 .\n- [30] J. Neri and S. Braun (2023) Towards real-time single-channel speech separation in noisy and reverberant environments . External Links: 2303.07569 , Link Cited by: ¬ß1 .\n- [31] E. Ng, H. Joo, L. Hu, H. Li, T. Darrell, A. Kanazawa, and S. Ginosar (2022-06) Learning to listen: modeling non-deterministic dyadic facial motion . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp.¬†20395‚Äì20405 . Cited by: ¬ß2.2 .\n- [32] E. Ng, J. Romero, T. Bagautdinov, S. Bai, T. Darrell, A. Kanazawa, and A. Richard (2024-06) From audio to photoreal embodiment: synthesizing humans in conversations . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp.¬†1001‚Äì1010 . Cited by: ¬ß1 , ¬ß2.2 , 2nd item , ¬ß4.1 , ¬ß4 .\n- [33] OpenAI, J. Achiam, and S. A. et al. (2024) GPT-4 technical report . External Links: 2303.08774 , Link Cited by: ¬ß1 .\n- [34] K. Pang, D. Qin, Y. Fan, J. Habekost, T. Shiratori, J. Yamagishi, and T. Komura (2023-07) Bodyformer: semantics-guided 3d body gesture synthesis with transformer . ACM Trans. Graph. 42 ( 4 ). External Links: ISSN 0730-0301 , Link , Document Cited by: ¬ß2.1 .\n- [35] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. Courville (2018) FiLM: visual reasoning with a general conditioning layer . In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence , AAAI‚Äô18/IAAI‚Äô18/EAAI‚Äô18 . External Links: ISBN 978-1-57735-800-8 Cited by: ¬ß3.2 , ¬ß9 .\n- [36] X. Qi, C. Liu, L. Li, J. Hou, H. Xin, and X. Yu (2024-01) EmotionGesture: audio-driven diverse emotional co-speech 3d gesture generation . Trans. Multi. 26 , pp.¬†10420‚Äì10430 . External Links: ISSN 1520-9210 , Link , Document Cited by: ¬ß1 .\n- [37] Y. Shafir, G. Tevet, R. Kapon, and A. H. Bermano (2024) Human motion diffusion as a generative prior . In The Twelfth International Conference on Learning Representations , Cited by: ¬ß2.3 .\n- [38] S. Song, M. Spitale, X. Kong, H. Zhu, C. Luo, C. Palmero, G. Barquero, S. Escalera, M. Valstar, M. Daoudi, et al. (2025) REACT 2025: the third multiple appropriate facial reaction generation challenge . In Proceedings of the ACM International Conference on Multimedia , Cited by: ¬ß2.2 .\n- [39] H. Taherian and D. Wang (2024) Multi-channel conversational speaker separation via neural diarization . IEEE/ACM Transactions on Audio, Speech, and Language Processing 32 , pp.¬†2467‚Äì2476 . Cited by: ¬ß1 .\n- [40] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or (2022) Motionclip: exposing human motion generation to clip space . In Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XXII , pp.¬†358‚Äì374 . Cited by: ¬ß2.3 .\n- [41] G. Tevet, S. Raab, B. Gordon, Y. Shafir, D. Cohen-or, and A. H. Bermano (2023) Human motion diffusion model . In The Eleventh International Conference on Learning Representations , External Links: Link Cited by: ¬ß2.3 .\n- [42] M. Tran, D. Chang, M. Siniukov, and M. Soleymani (2024) Dyadic interaction modeling for social behavior generation . arXiv preprint arXiv:2403.09069 . Cited by: ¬ß1 .\n- [43] Y. Wang, D. Yang, F. Bremond, and A. Dantcheva (2024) LIA: latent image animator . IEEE Transactions on Pattern Analysis and Machine Intelligence , pp.¬†1‚Äì16 . Cited by: ¬ß3.4 .\n- [44] S. Yang, Z. Wang, Z. Wu, M. Li, Z. Zhang, Q. Huang, L. Hao, S. Xu, X. Wu, C. Yang, and Z. Dai (2023) UnifiedGesture: a unified gesture synthesis model for multiple skeletons . In Proceedings of the 31st ACM International Conference on Multimedia , MM ‚Äô23 , New York, NY, USA , pp.¬†1033‚Äì1044 . External Links: ISBN 9798400701085 , Link , Document Cited by: ¬ß2.3 .\n- [45] S. Yang, Z. Xu, H. Xue, Y. Cheng, S. Huang, M. Gong, and Z. Wu (2024) Freetalker: controllable speech and text-driven gesture generation based on diffusion models for enhanced speaker naturalness . In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Cited by: ¬ß2.1 .\n- [46] S. Yang, H. Xue, Z. Zhang, M. Li, Z. Wu, X. Wu, S. Xu, and Z. Dai (2023) The diffusestylegesture+ entry to the genea challenge 2023 . In Proceedings of the 25th International Conference on Multimodal Interaction , ICMI ‚Äô23 , New York, NY, USA , pp.¬†779‚Äì785 . External Links: ISBN 9798400700552 , Link , Document Cited by: ¬ß2.3 .\n- [47] H. Yi, H. Liang, Y. Liu, Q. Cao, Y. Wen, T. Bolkart, D. Tao, and M. J. Black (2023-06) Generating holistic 3d human motion from speech . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.¬†469‚Äì480 . Cited by: ¬ß2.1 .\n- [48] W. Yin, Z. Cai, R. Wang, A. Zeng, C. Wei, Q. Sun, H. Mei, Y. Wang, H. E. Pang, M. Zhang, L. Zhang, C. C. Loy, A. Yamashita, L. Yang, and Z. Liu (2025) SMPLest-x: ultimate scaling for expressive human pose and shape estimation . arXiv preprint arXiv:2501.09782 . Cited by: ¬ß9 .\n- [49] Y. Yoon, B. Cha, J. Lee, M. Jang, J. Lee, J. Kim, and G. Lee (2020-11) Speech gesture generation from the trimodal context of text, audio, and speaker identity . ACM Trans. Graph. 39 ( 6 ). External Links: ISSN 0730-0301 , Link , Document Cited by: ¬ß2.1 .\n- [50] J. Zhang, L. Liang, Z. Xue, and Y. Liu (2020-05) APB2FACE: audio-guided face reenactment with auxiliary pose and blink signals . pp.¬†4402‚Äì4406 . External Links: Document Cited by: ¬ß2.2 .\n- [51] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu (2024-06) MotionDiffuse: text-driven human motion generation with diffusion model . IEEE Trans. Pattern Anal. Mach. Intell. 46 ( 6 ), pp.¬†4115‚Äì4128 . External Links: ISSN 0162-8828 , Link , Document Cited by: ¬ß2.3 .\n- [52] M. Zhang, H. Li, Z. Cai, J. Ren, L. Yang, and Z. Liu (2023) FineMoGen: fine-grained spatio-temporal motion generation and editing . NeurIPS . Cited by: ¬ß2.3 .\n- [53] Y. Zhi, X. Cun, X. Chen, X. Shen, W. Guo, S. Huang, and S. Gao (2023-10) LivelySpeaker: towards semantic-aware co-speech gesture generation . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp.¬†20807‚Äì20817 . Cited by: ¬ß2.3 .\n- [54] Y. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li (2018) On the continuity of rotation representations in neural networks . CoRR abs/1812.07035 . External Links: Link , 1812.07035 Cited by: ¬ß3.1 , ¬ß9 .\n- [55] L. Zhu, X. Liu, X. Liu, R. Qian, Z. Liu, and L. Yu (2023) Taming diffusion models for audio-driven co-speech gesture generation . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†10544‚Äì10553 . Cited by: ¬ß2.3 .\n\nSupplementary Material\n\nThis supplementary material contains sections below:\n\n- ‚Ä¢ 1. Relationships & Personality Clustering of Generated Results\n- ‚Ä¢ 2. Implementation Details of DyaDiT\n- ‚Ä¢ 3. Details of A/B Test Questionnaire\n\n1. Relationships & Personality Clustering of Generated Results\n\n2. Implementation Details of DyaDiT\n\n3. Details of A/B Test Questionnaire\n\nIn addition to this supplementary.pdf , we also include a narration_video.mp4 , which provides a brief overview of the paper along with several qualitative gesture generation examples.\nWe further provide our implementation in dyadit_code.zip ; please refer to the included README.md for instructions on running the code.\nThe trained models will be released upon acceptance.\n\n## 8 Clustering of Generated Gestures\n\nIn the main paper, we conduct an A/B test to evaluate the relationship and personality consistency of the generated gestures. To further assess the controllability of the conditional inputs in DyaDiT, we perform a t-SNE clustering analysis on the generated motion embeddings.\n\nFigure 8 visualizes the t-SNE embeddings of the generated gestures under different conditioning signals.\nOn the left, we generate gestures using various relationship types while fixing the personality scores.\nOn the right, we discretize the continuous personality score features into five ‚Äúone-hot‚Äù vectors and generate gestures for each vector to examine personality controllability.\n\nWe observe that the personality clusters form clearly separable groups, indicating that DyaDiT effectively captures the global behavioral tendencies associated with different personality traits.\nIn contrast, the relationship clusters are less clearly separated. We consider this to be consistent with the nature of dyadic conversational gestures: the styles between Friend , Family , and Dating share some overlap. As a result, the generated gestures also show a more continuous manifold across these categories rather than sharp cluster boundaries.\n\n[FIGURE:x8.png] Figure 8 : t-SNE clustering results of Relationships (left), Personality Scores (right).\n\n## 9 Implementation Details\n\n#### Diffusion Transformer\n\nThe input pose sequence is encoded into a latent space aligned with the VQ-VAE representation, resulting in a latent embedding in ‚Ñù 64 \\mathbb{R}^{64} , instead of the original ‚Ñù 43 √ó 6 \\mathbb{R}^{43\\times 6} 6D rotation matrix [ 54 ] joint representation.\nA linear layer projects the noisy latent input from ‚Ñù 64 \\mathbb{R}^{64} to a hidden space in ‚Ñù 512 \\mathbb{R}^{512} , followed by a symmetric projection back to ‚Ñù 64 \\mathbb{R}^{64} at the output.\n\nThe model contains 4 Transformer blocks, each equipped with 4-head multi-head attention ( ‚Ñù 128 \\mathbb{R}^{128} ) and a ‚Ñù 2048 \\mathbb{R}^{2048} feed-forward network.\nWe employ a ‚Ñù 512 \\mathbb{R}^{512} sinusoidal time embedding, which is injected into each block through FiLM [ 35 ] modulation.\n\nTwo independent Wav2Vec2 [ 4 ] processors extract high-level audio features from the conversational speech of both speakers.\nThese yield feature sequences denoted as a self , a other ‚àà ‚Ñù T √ó 768 a_{\\text{self}},a_{\\text{other}}\\in\\mathbb{R}^{T\\times 768} , where T T denotes the number of audio frames.\nEach of these is projected to ‚Ñù T √ó 512 \\mathbb{R}^{T\\times 512} via a linear transformation, followed by LayerNorm and a gated fusion mechanism to combine self and other speaker cues.\n\nA learnable motion bank contains 1000 prototype vectors in ‚Ñù 512 \\mathbb{R}^{512} , providing contextual priors via cross-attention.\nsimilar to the time embedding, relationship and personality embeddings are projected into ‚Ñù 512 \\mathbb{R}^{512} .\nThese vectors are injected into the DiT blocks via FiLM-style adaptive scaling.\nAll contextual cues are concatenated into a unified sequence in ‚Ñù 512 \\mathbb{R}^{512} and injected into each DiT block via cross-attention.\n\n#### Motion Tokenizer (VQ-VAE).\n\nWe implement a temporal VQ-VAE [ 18 ] to discretize pose sequences before diffusion.\nGiven an input sequence of joint features X ‚àà ‚Ñù T √ó 6 √ó 43 X\\in\\mathbb{R}^{T\\times 6\\times 43} , the encoder is a 1D CNN consisting of three Conv1d layers, with LeakyReLU applied after the first two layers, and an overall temporal downsampling factor of 4, producing a latent sequence in ‚Ñù ( T / 4 ) √ó 64 \\mathbb{R}^{(T/4)\\times 64} .\nThis continuous latent is quantized by a residual vector quantizer with depth 4, each equipped with a 512-entry codebook, which maps each time step to a stack of discrete code indices.\nThe decoder is a 1D CNN consisting of an initial Conv1d-LeakyReLU layer, two upsampling blocks (linear interpolation followed by Conv1d and LeakyReLU) interleaved with additional Conv1d-LeakyReLU refinement layers, and a final Conv1d projection, achieving an overall temporal upsampling factor of 4 and recovering the original temporal resolution to reconstruct poses in ‚Ñù T √ó 6 √ó 43 \\mathbb{R}^{T\\times 6\\times 43} .\nThe final latent representation used by the DiT denoiser is obtained from the quantized codes as a compact 64-dimensional embedding per 4 √ó T 4\\times T frames in ‚Ñù 64 \\mathbb{R}^{64} .\n\n#### Seamless Interactive Dataset.\n\nWe conduct our experiments on a part of the Seamless Interaction dataset [ 1 ] . In particular, we adopt the naturalistic split of the dataset. For training, we utilize the first 10 official training archives (provided as zip files), which contain approximately 182 hours of naturalistic interactions and 3000 paired motion-audio samples. For testing, we select the first archive from the official test split to ensure a consistent evaluation setting.\n\nWe observe that the SMPL-H parameters provided in the dataset exhibit noticeable inaccuracies in lower body estimation, likely due to limited camera views and body occlusions during data capture. To avoid introducing artifacts into our motion modeling, we discard lower-body joints and only retain the upper body comprising 43 joints, including fingers. For visualization, all unused joints, along with global orientation and root translation, are set to zero.\n\nIn addition to pose data, the dataset includes high level annotations such as relationship and personality scores . While the dataset provides Interpersonal Communicative Dynamic (IPC) tags for social dynamics, we found the annotations to be too noisy and ambiguous where it is unclear which speaker they apply to. Consequently, we do not employ IPC tag supervision in our current study and instead focus on the cleaner relationship and personality cues. We note that once the IPC annotations are refined in future dataset releases, we plan to extend our framework with an IPC-awared conditioning module to further capture communicative intent in dyadic gestures.\n\nIn the future, we plan to re-annotate the video data using advanced human pose estimation tools such as SMPLest-x [ 48 ] , Harmony4D [ 17 ] or recent state-of-the-art models, with the aim of obtaining more reliable full body motion supervision.\n\n## 10 Questionnaire\n\nWe provide a reconstructed version of the A/B test questionnaire used in our user study.\nTo view the questionnaire, please first extract the questionnaire_video.zip file inside the questionnaire folder. After extraction, open questionnaire.html in any modern web browser.\n\nThe original questionnaire was conducted through Google Forms (see Figure 9 ). It consists of 28 √ó 2 28\\times 2 questions in total, including 10 questions on overall gesture quality,\n8 questions on relationship consistency, and 10 questions on personality consistency.\nEach question presents paired gesture videos for comparison under two settings: DyaDiT vs. ConvoFusion and DyaDiT vs. Ground Truth .\n\nFor an accurate viewing experience, please wear headphones.\nThe left audio channel corresponds to the partner‚Äôs speech, while the right audio channel\ncorresponds to the target speaker‚Äôs speech.\n\nThe reconstructed interface allows reviewers to browse all questions and play the\ncorresponding videos to experience the same evaluation procedure as our participants.\n\n[FIGURE:x9.jpg] Figure 9 : Example of Questionnaires in GoogleForm",
  "figures": []
}