{
  "source": "html",
  "markdown": "# G e o W o r l d :\nGeometric World Models\n\nEnergy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: ( i ) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and ( ii ) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts.\nTo address these challenges, we introduce GeoWorld , a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA , which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA¬†2.\n\n## 1 Introduction\n\nAutoregressive (AR) next-token prediction has endowed large language models (LLMs) [ 79 ] and vision-language models (VLMs) [ 53 , 20 ] with extensive world knowledge and reasoning capability, enabling them to effectively tackle complex tasks involving searching [ 78 ] , reasoning [ 32 , 33 , 44 ] , and planning [ 8 , 35 , 82 , 65 ] . Although the success of LLMs stems from modeling within language space, which serves as a shortcut toward human level knowledge [ 48 ] , they still fail to fully represent the rich information of the real world, such as its physical and geometric properties [ 3 ] . In the real world, human and biological cognition often acquire knowledge primarily through visual information rather than relying solely on language, as vision offers a higher bandwidth of information than language [ 62 ] . For example, human infants learn mainly from visual perception during the first few months before developing a language system [ 38 ] , and some animals do not possess language at all [ 18 ] . Therefore, there are world models [ 46 , 59 , 63 , 4 , 3 ] that learn solely from visual input, such as videos, and perform planning with either a generative or a predictive approach. Generative world models [ 46 , 59 , 63 ] explicitly generate pixels or latent visual tokens that decode into pixels in order to predict only one step at a time [ 67 ] . As a result, they lack awareness of the full trajectory structure or the energy landscape over multiple steps. In contrast, predictive world models [ 2 , 29 , 4 , 3 ] such as JPEA [ 41 ] do not generate pixels. Instead, they learn an energy landscape in latent space that measures the compatibility between current and target states. This enables multi-step hierarchical planning, where high-level reasoning minimizes energy in latent space, while lower-level modules fill in the physical details.\n\nHowever, existing energy-based predictive world models face two significant challenges:\n\n(1) Geometric neglect. Although predictive world models perform multi-step hierarchical planning in latent space, their representations are typically learned in a Euclidean space without preserving the underlying geometric relations among states.\nAs a result, the learned energy landscape fails to capture meaningful geodesic distances or hierarchical embeddings between latent states [ 51 ] , which weakens the model‚Äôs ability to perform geometry-consistent planning over long horizons.\n\n(2) Multi-step shortcoming. Multi-step videos are limited and expensive to acquire, so existing predictive world models are primarily trained on one-step video transitions [ 31 , 40 , 66 , 12 , 37 , 47 ] . Although learning an energy landscape over entire trajectories conceptually enables long-horizon planning, their performance degrades rapidly as the planning horizon increases, exposing a weakness in modeling long-term temporal dependencies.\n\nOur motivation is to address these problems from a geometric perspective. For the first challenge, a geometry-aware world model is required to preserve geometric properties when learning the energy landscape for hierarchical planning.\nFor the second challenge, reinforcement learning (RL) has proven effective in adjusting a pretrained foundation model when its outputs are unsatisfactory in certain aspects [ 54 , 60 ] .\nTherefore, a geometry-aware RL method is required to obtain optimal trajectories on the latent manifold, improving the model‚Äôs multi-step planning capability.\n\nHence, we introduce the Geometric World Model ( GeoWorld ), a method that enhances energy-based predictive world models by preserving geometric structure and hierarchical awareness in latent space, as shown in Figure LABEL:fig:teaser .\nTo address the first challenge, we propose Hyperbolic JEPA ( H-JEPA ), which maps latent representations from Euclidean space ‚Ñù n \\mathbb{R}^{n} onto a hyperbolic manifold ‚Ñç n \\mathbb{H}^{n} , where geodesic distances naturally encode hierarchical relations among states.\nBy learning dynamics along hyperbolic geodesics, H-JEPA preserves latent geometry during multi-step prediction, ensuring that the learned energy landscape aligns with the underlying structure of the physical world and supports geometry-consistent planning, as shown in 1 .\n\nTo address the second challenge, we design a Geometric Reinforcement Learning ( GRL ) that reformulates multi-step planning as the optimization of an energy-based value function, where lower hyperbolic energy corresponds to higher cumulative reward. GRL directly optimizes the predictor of the world model without training an additional policy or reward model.\nBy adjusting the predictor‚Äôs energy-based value representation through hyperbolic geodesics minimization and triangle inequality regularization, GRL enforces geodesic-consistent rollouts on the latent manifold, effectively improving long-horizon stability and planning performance.\n\n[FIGURE:x1.png] (a) V-JEPA 2 [ 3 ] Energy Landscape\n\nTo verify our method‚Äôs capability on long-horizon planning, we evaluate multi-step goal-conditioned visual planning on standard benchmarks, including CrossTask [ 88 ] and COIN [ 71 ] . Our GeoWorld achieves consistent improvements over the previous state-of-the-art predictive world model V-JEPA¬†2, including improvements of around 3% SR in 3-step planning and 2% SR in 4-step planning across both datasets.\n\nThe contributions of our work can be summarized as follows:\n\n- ‚Ä¢ We introduce the Geometric World Model ( GeoWorld ) with a Hyperbolic JEPA ( H-JEPA ), which preserves geometric structure and hierarchical relations by mapping latent representations onto a hyperbolic manifold and learning dynamics along hyperbolic geodesics, resulting in a geometry-consistent energy landscape for multi-step prediction and planning.\n- ‚Ä¢ We propose Geometric Reinforcement Learning ( GRL ), an energy-based optimization framework that directly refines the predictor through hyperbolic energy minimization and triangle-inequality regularization, enabling geodesic-consistent rollouts and improving long-horizon planning stability.\n- ‚Ä¢ We demonstrate strong performance on long-horizon goal-conditioned visual planning across CrossTask and COIN, achieving around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to V-JEPA¬†2.\n\nWe introduce the Geometric World Model ( GeoWorld ) with a Hyperbolic JEPA ( H-JEPA ), which preserves geometric structure and hierarchical relations by mapping latent representations onto a hyperbolic manifold and learning dynamics along hyperbolic geodesics, resulting in a geometry-consistent energy landscape for multi-step prediction and planning.\n\nWe propose Geometric Reinforcement Learning ( GRL ), an energy-based optimization framework that directly refines the predictor through hyperbolic energy minimization and triangle-inequality regularization, enabling geodesic-consistent rollouts and improving long-horizon planning stability.\n\nWe demonstrate strong performance on long-horizon goal-conditioned visual planning across CrossTask and COIN, achieving around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to V-JEPA¬†2.\n\n## 2 Related Works\n\nVideo World Models\n\nThere are two primary approaches for video world modeling: generative world models [ 46 , 59 , 63 ] and predictive world models [ 41 , 2 , 29 , 4 , 3 ] . Generative world models typically build upon autoregressive [ 24 , 77 , 43 ] or semi-autoregressive [ 58 , 34 , 22 , 72 , 81 , 17 ] architectures that observe the visual context and explicitly generate the next frame or its latent representation. These models often incorporate an inverse dynamics module [ 67 ] trained to infer actions from consecutive observations, enabling one-step reactive control but preventing multi-step reasoning because the model lacks access to the global trajectory structure and cannot capture long-range dynamics. Moreover, generative approaches must decode visual tokens or pixels during planning, which introduces unnecessary noise and computational overhead and limits their ability to model abstract energy landscapes for hierarchical planning [ 59 ] . In contrast, predictive world models do not generate pixels. Instead, they learn an energy landscape in latent space that quantifies the compatibility between current and target states [ 41 ] . This design allows for multi-step trajectory optimization using sampling-based planners such as the cross-entropy method (CEM) [ 23 ] , enabling long-horizon planning without explicit pixel decoding.\n\nGoal-Conditioned Visual Planning\n\nGoal conditioned visual planning aims to produce a sequence of actions that achieves a given goal based on visual observations. Prior works have evolved into three independent setups depending on the modalities of the observation and the goal, which may be images, videos, or language. (1) In visual planning for assistance (VPA) [ 56 ] , observations are videos and goals are described in natural language. This typically requires models built on LLMs with multimodal processing capability [ 36 , 84 , 16 ] . (2) In procedural planning (PP) [ 15 ] , both observations and goals are specified as images without any language involved, which limits the model‚Äôs ability to capture temporal information in the physical world [ 42 , 36 , 52 , 7 , 86 , 75 , 50 , 61 , 87 , 69 , 74 ] . (3) In visual planning with videos, both observations and goals are given as videos, which aligns more naturally with temporal dynamics in the real world and is commonly addressed using video LLMs [ 76 , 79 , 20 , 53 ] , generative world models [ 59 ] , and predictive world models [ 3 ] .\n\n[FIGURE:x3.png] Figure 2 : Overview of GeoWorld. Our geometric world model integrates Hyperbolic JEPA for geometry-preserving latent dynamics and Geometric Reinforcement Learning for geodesic-consistent multi-step refinement. Together with energy-based planning using CEM, GeoWorld enables stable and geometry-aware long-horizon visual planning.\n\n## 3 Method\n\n### 3.1 Overview\n\nWe introduce GeoWorld , a geometric world model designed to enhance long-horizon visual planning by preserving geometric structure and hierarchical awareness in latent space. To address the limitation of Euclidean latent representations, GeoWorld incorporates Hyperbolic JEPA (H-JEPA), which maps encoder outputs from Euclidean space onto a hyperbolic manifold where geodesic distances naturally encode hierarchical relations among states. By learning latent dynamics along hyperbolic geodesics, H-JEPA enforces geometry-consistent transitions that better reflect the structure of real-world trajectories. To further improve stability in multi-step prediction, we develop Geometric Reinforcement Learning (GRL), an energy-based optimization framework that treats planning as minimizing a hyperbolic value function without training an additional policy or reward model. GRL refines the predictor through hyperbolic energy minimization and triangle-inequality regularization, encouraging geodesic-consistent rollouts and improving long-horizon temporal coherence. Leveraging energy-based planning with the Cross-Entropy Method (CEM) [ 23 ] further enables efficient trajectory optimization by searching for action sequences that follow geodesic paths in hyperbolic latent space. Together, H-JEPA and GRL form the core of GeoWorld, enabling geometry-aware multi-step planning in predictive world models, as shown in Figure 2 .\n\nFor preliminaries on JEPA [ 41 ] , hyperbolic geometry, and the value function in RL, see Appendix 1 .\n\n### 3.2 Hyperbolic JEPA\n\nFrom a representation perspective, we aim to learn a mapping from states onto a hyperbolic space ‚Ñç n \\mathbb{H}^{n} such that the optimal plan corresponds to a geodesic in hyperbolic space.\nHence, we propose Hyperbolic JEPA ( H-JEPA ), which models latent dynamics on the hyperbolic manifold to preserve hierarchical relations and underlying geometric coherence during multi-step planning.\n\nWe define the observation at time t t as x t x_{t} . E Œ∏ ‚Äã ( ‚ãÖ ) E_{\\theta}(\\cdot) denotes the pretrained encoder [ 3 ] , which encodes the observation x t x_{t} into the latent state s t x s_{t}^{x} :\n\n|  | s t x = E Œ∏ ‚Äã ( x t ) ‚àà ‚Ñù n . s_{t}^{x}=E_{\\theta}(x_{t})\\in\\mathbb{R}^{n}. |  | (1) |\n| --- | --- | --- | --- |\n\nTo effectively map the encoder output from Euclidean space ‚Ñù n \\mathbb{R}^{n} to hyperbolic space ‚Ñç n \\mathbb{H}^{n} ,\nwe interpret the Euclidean embedding s t x s_{t}^{x} as a tangent vector in the tangent space ùêì 0 ‚Äã ‚Ñç n \\mathbf{T}_{0}\\mathbb{H}^{n} at the origin.\nWe then apply the exponential map at the origin of the Poincar√© ball model ùîπ c n \\mathbb{B}_{c}^{n} with curvature K = ‚àí c K=-c ,\nwhich projects the tangent vector onto the hyperbolic manifold, as detailed in Appendix 1.5.1 .\n\nFormally, the hyperbolic latent state is obtained as\n\n|  | s t , ‚Ñç x = exp 0 ‚Å° ( s t x ) = tanh ‚Å° ( c ‚Äã ‚Äñ s t x ‚Äñ ) ‚Äã s t x c ‚Äã ‚Äñ s t x ‚Äñ , s t , ‚Ñç x ‚àà ùîπ c n . s_{t,\\mathbb{H}}^{x}=\\exp_{0}(s_{t}^{x})=\\tanh\\!\\big(\\sqrt{c}\\|s_{t}^{x}\\|\\big)\\frac{s_{t}^{x}}{\\sqrt{c}\\|s_{t}^{x}\\|},\\quad s_{t,\\mathbb{H}}^{x}\\in\\mathbb{B}_{c}^{n}. |  | (2) |\n| --- | --- | --- | --- |\n\nThen the action-conditioned predictor P œï ‚Äã ( ‚ãÖ ) P_{\\phi}(\\cdot) takes a sequence of hyperbolic latent states ( s t , ‚Ñç x ) t = 1 T (s_{t,\\mathbb{H}}^{x})_{t=1}^{T} and a corresponding sequence of actions ( a t ) t = 1 T (a_{t})_{t=1}^{T} as input, and predicts the sequence of next-state representations ( s ^ t + 1 , ‚Ñç x ) t = 1 T (\\hat{s}_{t+1,\\mathbb{H}}^{x})_{t=1}^{T} over a planning horizon T T :\n\n|  | ( s ^ t + 1 , ‚Ñç x ) t = 1 T = P œï ‚Äã ( ( s t , ‚Ñç x , a t ) t = 1 T ) . (\\hat{s}_{t+1,\\mathbb{H}}^{x})_{t=1}^{T}=P_{\\phi}\\!\\big((s_{t,\\mathbb{H}}^{x},a_{t})_{t=1}^{T}\\big). |  | (3) |\n| --- | --- | --- | --- |\n\nand Œ∏ \\theta and œï \\phi denote the parameters (weights) of the encoder and predictor networks, respectively.\n\n### 3.3 Training Objective\n\nThe supervised training objective of H-JEPA is to learn a predictive world model that follows the geodesic path of minimum energy cost between the current and target latent states in hyperbolic space.\nSpecifically, the model minimizes the Poincar√©-ball hyperbolic distance d ‚Ñç d_{\\mathbb{H}} between the predicted and true latent representations, as defined in Eq. 68 of Appendix 1.5.1 , ensuring that each transition aligns with the lowest-energy trajectory on the manifold.\n\nThe objective consists of a joint loss combining a teacher-forcing loss and a rollout loss.\nThe teacher-forcing loss encourages accurate one-step prediction by aligning the predicted next-state representation with the ground-truth latent embedding, while the rollout loss recursively feeds the model‚Äôs own predictions as inputs to enforce temporal consistency across multiple future steps.\n\nTeacher Forcing\n\nThe teacher-forcing loss trains the model to accurately perform one-step future prediction by minimizing the hyperbolic geodesic distance d ‚Ñç d_{\\mathbb{H}} between the predicted latent representation s ^ t + 1 , ‚Ñç x \\hat{s}_{t+1,\\mathbb{H}}^{x} and the encoded ground-truth latent s t + 1 , ‚Ñç x s_{t+1,\\mathbb{H}}^{x} at each time step t t :\n\n|  | ‚Ñí TF ‚Äã ( Œ∏ , œï ) \\displaystyle\\mathcal{L}_{\\text{TF}}(\\theta,\\phi) | = 1 T ‚àë t = 1 T d ‚Ñç ( P œï ( exp 0 ( E Œ∏ ( x t ) ) , a t ) , \\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}d_{\\mathbb{H}}\\!\\left(P_{\\phi}\\!\\big(\\exp_{0}\\!\\big(E_{\\theta}(x_{t})\\big),a_{t}\\right), |  |\n| --- | --- | --- | --- |\n|  |  | exp 0 ( E Œ∏ ( x t + 1 ) ) ) \\displaystyle\\qquad\\left.\\exp_{0}\\!\\big(E_{\\theta}(x_{t+1})\\big)\\right) |  | (4) |\n|  |  | = 1 T ‚Äã ‚àë t = 1 T d ‚Ñç ‚Äã ( s ^ t + 1 , ‚Ñç x , s t + 1 , ‚Ñç x ) \\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}d_{\\mathbb{H}}\\!\\left(\\hat{s}_{t+1,\\mathbb{H}}^{x},s_{t+1,\\mathbb{H}}^{x}\\right) |  | (5) |\n|  |  | = 1 T ‚àë t = 1 T 1 c arcosh ( 1 \\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}\\frac{1}{\\sqrt{c}}\\,\\operatorname{arcosh}\\!\\left(1\\right. |  |\n|  |  | + 2 c ‚Äñ s ^ t + 1 , ‚Ñç x ‚àí s t + 1 , ‚Ñç x ‚Äñ 2 ( 1 ‚àí c ‚Äã ‚Äñ s ^ t + 1 , ‚Ñç x ‚Äñ 2 ) ‚Äã ( 1 ‚àí c ‚Äã ‚Äñ s t + 1 , ‚Ñç x ‚Äñ 2 ) ) \\displaystyle\\qquad\\left.{}+2c\\frac{\\|\\hat{s}_{t+1,\\mathbb{H}}^{x}-s_{t+1,\\mathbb{H}}^{x}\\|^{2}}{(1-c\\|\\hat{s}_{t+1,\\mathbb{H}}^{x}\\|^{2})(1-c\\|s_{t+1,\\mathbb{H}}^{x}\\|^{2})}\\right) |  | (6) |\n\nRollout\n\nThe rollout loss feeds the predictor‚Äôs output back as input, enabling the model to learn multi-step future prediction.\nIn this case, we design a two-step rollout loss to enhance the model‚Äôs capability for long-horizon planning:\n\n|  | ‚Ñí rollout ‚Äã ( Œ∏ , œï ) \\displaystyle\\mathcal{L}_{\\text{rollout}}(\\theta,\\phi) | = 1 T ‚àë t = 1 T d ‚Ñç ( P œï ( exp 0 ( E Œ∏ ( x t ) ) , a t , a t + 1 ) , \\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}d_{\\mathbb{H}}\\!\\left(P_{\\phi}\\!\\big(\\exp_{0}\\!\\big(E_{\\theta}(x_{t})\\big),a_{t},a_{t+1}\\big)\\right., |  |\n| --- | --- | --- | --- |\n|  |  | exp 0 ( E Œ∏ ( x t + 2 ) ) ) \\displaystyle\\qquad\\left.\\exp_{0}\\!\\big(E_{\\theta}(x_{t+2})\\big)\\right) |  | (7) |\n|  |  | = 1 T ‚Äã ‚àë t = 1 T d ‚Ñç ‚Äã ( s ^ t + 2 , ‚Ñç x , s t + 2 , ‚Ñç x ) \\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}d_{\\mathbb{H}}\\!\\left(\\hat{s}_{t+2,\\mathbb{H}}^{x},s_{t+2,\\mathbb{H}}^{x}\\right) |  | (8) |\n|  |  | = 1 T ‚àë t = 1 T 1 c arcosh ( 1 \\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}\\frac{1}{\\sqrt{c}}\\,\\operatorname{arcosh}\\!\\left(1\\right. |  |\n|  |  | + 2 c ‚Äñ s ^ t + 2 , ‚Ñç x ‚àí s t + 2 , ‚Ñç x ‚Äñ 2 ( 1 ‚àí c ‚Äã ‚Äñ s ^ t + 2 , ‚Ñç x ‚Äñ 2 ) ‚Äã ( 1 ‚àí c ‚Äã ‚Äñ s t + 2 , ‚Ñç x ‚Äñ 2 ) ) , \\displaystyle\\qquad\\left.{}+2c\\frac{\\|\\hat{s}_{t+2,\\mathbb{H}}^{x}-s_{t+2,\\mathbb{H}}^{x}\\|^{2}}{(1-c\\|\\hat{s}_{t+2,\\mathbb{H}}^{x}\\|^{2})(1-c\\|s_{t+2,\\mathbb{H}}^{x}\\|^{2})}\\right), |  | (9) |\n\nTotal Loss\n\nHence, the total loss in the supervised stage is defined as\n\n|  | ‚Ñí SFT ‚Äã ( Œ∏ , œï ) = Œª ‚Äã ‚Ñí TF ‚Äã ( Œ∏ , œï ) + ( 1 ‚àí Œª ) ‚Äã ‚Ñí rollout ‚Äã ( Œ∏ , œï ) , \\mathcal{L}_{\\text{SFT}}(\\theta,\\phi)=\\lambda\\,\\mathcal{L}_{\\text{TF}}(\\theta,\\phi)+(1-\\lambda)\\,\\mathcal{L}_{\\text{rollout}}(\\theta,\\phi), |  | (10) |\n| --- | --- | --- | --- |\n\nwhere Œª \\lambda is a loss weighting hyperparameter.\n\nTogether, these two components train the predictor to learn smooth, geodesically consistent trajectories that capture both short-term accuracy and long-horizon stability within the hyperbolic latent space.\n\n### 3.4 Geometric Reinforcement Learning\n\nWe propose a Geometric Reinforcement Learning ( GRL ) approach that improves the predictor in multi-step planning by adjusting its energy-based value representation, aligning lower energy with higher expected reward.\n\nEnergy Cost\n\nGiven a frozen encoder E E and a trainable predictor P œï P_{\\phi} , we define the energy cost of moving from state s t , ‚Ñç x s_{t,\\mathbb{H}}^{x} to state s t + 1 , ‚Ñç x s_{t+1,\\mathbb{H}}^{x} as\n\n|  | c t ‚Äã ( s t , ‚Ñç x , s t + 1 , ‚Ñç x ) \\displaystyle c_{t}(s_{t,\\mathbb{H}}^{x},s_{t+1,\\mathbb{H}}^{x}) | = d ‚Ñç ‚Äã ( P œï ‚Äã ( exp 0 ‚Å° ( E ‚Äã ( x t ) ) , a t ) , exp 0 ‚Å° ( E ‚Äã ( x t + 1 ) ) ) \\displaystyle=d_{\\mathbb{H}}\\!\\left(P_{\\phi}\\!\\big(\\exp_{0}\\!\\big(E(x_{t})\\big),a_{t}\\big),\\exp_{0}\\!\\big(E(x_{t+1})\\big)\\right) |  | (11) |\n| --- | --- | --- | --- | --- |\n|  |  | = d ‚Ñç ‚Äã ( s ^ t + 1 , ‚Ñç x , s t + 1 , ‚Ñç x ) . \\displaystyle=d_{\\mathbb{H}}(\\hat{s}_{t+1,\\mathbb{H}}^{x},s_{t+1,\\mathbb{H}}^{x}). |  | (12) |\n\nwhich ideally indicates that we aim to minimize the energy cost of moving from state s t , ‚Ñç x s_{t,\\mathbb{H}}^{x} to state s t + 1 , ‚Ñç x s_{t+1,\\mathbb{H}}^{x} , which is identical to minimizing the geodesic distance between the predicted state s ^ t + 1 , ‚Ñç x \\hat{s}_{t+1,\\mathbb{H}}^{x} and the target state s t + 1 , ‚Ñç x s_{t+1,\\mathbb{H}}^{x} .\n\nReward\n\nWe then define the reward as the negative energy cost of moving between states:\n\n|  | r t ‚Äã ( s t , ‚Ñç x , a t , s t + 1 , ‚Ñç x ) = ‚àí c t ‚Äã ( s t , ‚Ñç x , s t + 1 , ‚Ñç x ) . r_{t}(s_{t,\\mathbb{H}}^{x},a_{t},s_{t+1,\\mathbb{H}}^{x})=-c_{t}(s_{t,\\mathbb{H}}^{x},s_{t+1,\\mathbb{H}}^{x}). |  | (13) |\n| --- | --- | --- | --- |\n\nPath Value Function\n\nAs mentioned in Appendix 1.6 , the value function V V is a mathematical object that quantifies the amount of energy required for an agent to reach optimality from a given state to a target state, where lower energy corresponds to a higher expected cumulative reward.\n\nHence, the path value function between the current and goal latent states, given a planning horizon T T , is defined as the expected cumulative reward:\n\n|  | V ‚Äã ( s 1 , ‚Ñç x , s 1 + T , ‚Ñç x ) = ùîº a 1 : T ‚àº œï ‚Äã [ ‚àë t = 1 T Œ≥ t ‚àí 1 ‚Äã r t ‚Äã ( s t , ‚Ñç x , a t , s t + 1 , ‚Ñç x ) ] , V(s_{1,\\mathbb{H}}^{x},s_{1+T,\\mathbb{H}}^{x})=\\mathbb{E}_{a_{1:T}\\sim\\phi}\\!\\left[\\sum_{t=1}^{T}\\gamma^{t-1}r_{t}(s_{t,\\mathbb{H}}^{x},a_{t},s_{t+1,\\mathbb{H}}^{x})\\right], |  | (14) |\n| --- | --- | --- | --- |\n\nwhere Œ≥ ‚àà [ 0 , 1 ) \\gamma\\in[0,1) is the discount factor.\n\nOur objective is to maximize the total reward (i.e., maximize the return) such that P œï P_{\\phi} follows the geodesics. Therefore, the optimal path value function maximizes the expected cumulative reward:\n\n|  | V ‚àó ‚Äã ( s 1 , ‚Ñç x , s 1 + T , ‚Ñç x ) \\displaystyle V^{*}(s_{1,\\mathbb{H}}^{x},s_{1+T,\\mathbb{H}}^{x}) | = max œï ùîº a 1 : T ‚àº œï [ ‚àë t = 1 T Œ≥ t ‚àí 1 r t ( s t , ‚Ñç x , a t , \\displaystyle=\\max_{\\phi}\\,\\mathbb{E}_{a_{1:T}\\sim\\phi}\\!\\left[\\sum_{t=1}^{T}\\gamma^{t-1}\\,r_{t}(s_{t,\\mathbb{H}}^{x},a_{t},\\right. |  |\n| --- | --- | --- | --- |\n|  |  | s t + 1 , ‚Ñç x ) ] . \\displaystyle\\qquad\\qquad\\qquad\\qquad\\left.s_{t+1,\\mathbb{H}}^{x})\\right]. |  |\n|  |  | = min œï ùîº a 1 : T ‚àº œï [ ‚àë t = 1 T Œ≥ t ‚àí 1 d ‚Ñç ( s ^ t + 1 , ‚Ñç x , \\displaystyle=\\min_{\\phi}\\,\\mathbb{E}_{a_{1:T}\\sim\\phi}\\!\\left[\\sum_{t=1}^{T}\\gamma^{t-1}\\,d_{\\mathbb{H}}(\\hat{s}_{t+1,\\mathbb{H}}^{x},\\right. |  |\n|  |  | s t + 1 , ‚Ñç x ) ] . \\displaystyle\\qquad\\qquad\\qquad\\qquad\\left.s_{t+1,\\mathbb{H}}^{x})\\right]. |  | (15) |\n\nwhich is equivalent to minimizing the total hyperbolic distance between the predicted and target states.\n\nTriangle Inequality Regularization\n\nThe hyperbolic geodesic distance d ‚Ñç d_{\\mathbb{H}} satisfies the triangle inequality. Therefore, for any consecutive triplet in the predictor‚Äôs rollouts:\n\n|  | d ‚Ñç ‚Äã ( s ^ t , ‚Ñç x , s ^ t + 2 , ‚Ñç x ) ‚â§ d ‚Ñç ‚Äã ( s ^ t , ‚Ñç x , s ^ t + 1 , ‚Ñç x ) + d ‚Ñç ‚Äã ( s ^ t + 1 , ‚Ñç x , s ^ t + 2 , ‚Ñç x ) . d_{\\mathbb{H}}(\\hat{s}_{t,\\mathbb{H}}^{x},\\hat{s}_{t+2,\\mathbb{H}}^{x})\\leq d_{\\mathbb{H}}(\\hat{s}_{t,\\mathbb{H}}^{x},\\hat{s}_{t+1,\\mathbb{H}}^{x})+d_{\\mathbb{H}}(\\hat{s}_{t+1,\\mathbb{H}}^{x},\\hat{s}_{t+2,\\mathbb{H}}^{x}). |  | (16) |\n| --- | --- | --- | --- |\n\nThis indicates that minimizing the sum of consecutive step distances encourages the predicted trajectory to align with the geodesic path. Hence, we introduce a regularization term:\n\n|  | ‚Ñí Œî = 1 T ‚àí 2 ‚àë t = 1 T ‚àí 2 [ d ‚Ñç ( s ^ t , s ^ t + 2 ) ‚àí d ‚Ñç ( s ^ t , s ^ t + 1 ) \\displaystyle\\mathcal{L}_{\\Delta}=\\frac{1}{T-2}\\sum_{t=1}^{T-2}\\Big[d_{\\mathbb{H}}(\\hat{s}_{t},\\hat{s}_{t+2})-d_{\\mathbb{H}}(\\hat{s}_{t},\\hat{s}_{t+1}) |  |\n| --- | --- | --- |\n|  | ‚àí d ‚Ñç ( s ^ t + 1 , s ^ t + 2 ) ] + . \\displaystyle\\qquad\\qquad-d_{\\mathbb{H}}(\\hat{s}_{t+1},\\hat{s}_{t+2})\\Big]_{+}. |  | (17) |\n\nThis term enforces multi-step rollout consistency by encouraging predicted trajectories to satisfy hyperbolic geodesic properties.\n\nTotal Loss\n\nHence, the total loss in Geometric Reinforcement Learning can be expressed as\n\n|  | ‚Ñí GRL ‚Äã ( œï ) = ùîº a 1 : T ‚àº œï ‚Äã [ ‚àë t = 1 T Œ≥ t ‚àí 1 ‚Äã d ‚Ñç ‚Äã ( s ^ t + 1 , ‚Ñç x , s t + 1 , ‚Ñç x ) ] + Œ≤ ‚Äã ‚Ñí Œî . \\mathcal{L}_{\\mathrm{GRL}}(\\phi)=\\mathbb{E}_{a_{1:T}\\sim\\phi}\\!\\left[\\sum_{t=1}^{T}\\gamma^{t-1}d_{\\mathbb{H}}(\\hat{s}_{t+1,\\mathbb{H}}^{x},s_{t+1,\\mathbb{H}}^{x})\\right]+\\beta\\mathcal{L}_{\\Delta}. |  | (18) |\n| --- | --- | --- | --- |\n\nwhere Œ≤ \\beta is the regularization factor.\n\n### 3.5 Energy-Based Planning\n\nWe then perform energy-based planning after training, with the frozen encoder E E and predictor P P .\nThe predictor serves as a world model, capable of predicting how latent representations evolve when an action sequence is applied.\nDuring planning, we search for an optimal action sequence that follows the geodesic path between the current and goal latent states, effectively minimizing a goal-conditioned energy cost defined in the hyperbolic latent space.\n\nGiven the current observation x 1 x_{1} , the future target x 1 + T x_{1+T} , and the planning horizon T T ,\nwe encode the current and goal observations as\n\n|  | s 1 , ‚Ñç x = exp 0 ‚Å° ( E ‚Äã ( x 1 ) ) , s 1 + T , ‚Ñç x = exp 0 ‚Å° ( E ‚Äã ( x 1 + T ) ) . s_{1,\\mathbb{H}}^{x}=\\exp_{0}(E(x_{1})),\\qquad s_{1+T,\\mathbb{H}}^{x}=\\exp_{0}(E(x_{1+T})). |  | (19) |\n| --- | --- | --- | --- |\n\nWe then define the energy cost function C C based on the Poincar√© geodesic distance, which measures the hyperbolic energy between the predicted and goal latent states over the planning horizon:\n\n|  | C ‚Äã ( ( a ^ t ) t = 1 T ; s 1 , ‚Ñç x , s 1 + T , ‚Ñç x ) = d ‚Ñç ‚Äã ( P ‚Äã ( ( a ^ t ) t = 1 T ; s 1 , ‚Ñç x ) , s 1 + T , ‚Ñç x ) , C((\\hat{a}_{t})_{t=1}^{T};s_{1,\\mathbb{H}}^{x},s_{1+T,\\mathbb{H}}^{x})=d_{\\mathbb{H}}\\!\\left(P((\\hat{a}_{t})_{t=1}^{T};s_{1,\\mathbb{H}}^{x}),\\,s_{1+T,\\mathbb{H}}^{x}\\right), |  | (20) |\n| --- | --- | --- | --- |\n\nHence, the optimal action sequence ( a t ‚àó ) t = 1 T (a_{t}^{*})_{t=1}^{T} is obtained by minimizing this hyperbolic energy cost:\n\n|  | ( a t ‚àó ) t = 1 T = arg ‚Å° min ( a ^ t ) t = 1 T ‚Å° d ‚Ñç ‚Äã ( P ‚Äã ( ( a ^ t ) t = 1 T ; s 1 , ‚Ñç x ) , s 1 + T , ‚Ñç x ) . (a_{t}^{*})_{t=1}^{T}=\\arg\\min_{(\\hat{a}_{t})_{t=1}^{T}}d_{\\mathbb{H}}\\!\\left(P((\\hat{a}_{t})_{t=1}^{T};s_{1,\\mathbb{H}}^{x}),\\,s_{1+T,\\mathbb{H}}^{x}\\right). |  | (21) |\n| --- | --- | --- | --- |\n\nThe optimization is performed with the Cross-Entropy Method (CEM) [ 23 ] ,\nas detailed in Algorithm 1 of Appendix 1.3.3\n\n|  | ( a t ‚àó ) t = 1 T = CEM ‚Å° ( x 1 , x 1 + T , P ‚Äã ( ‚ãÖ ) , E ‚Äã ( ‚ãÖ ) , T , N , K , I , Œº 0 , Œ£ 0 ) (a_{t}^{*})_{t=1}^{T}=\\operatorname{CEM}\\!\\left(x_{1},\\,\\,x_{1+T},\\,P(\\cdot),\\,E(\\cdot),\\,T,\\,N,\\,K,\\,I,\\,\\mu_{0},\\,\\Sigma_{0}\\right) |  | (22) |\n| --- | --- | --- | --- |\n\nwhere x 1 x_{1} is the current observation, x 1 + T x_{1+T} is the goal observation, P œï P_{\\phi} is the predictor, E ‚Äã ( ‚ãÖ ) E(\\cdot) is the encoder, T T is the planning horizon, N N is the number of samples, K K is the number of elites, I I is the number of iterations,\nand ( Œº 0 , Œ£ 0 ) (\\mu_{0},\\Sigma_{0}) denote the initial mean and covariance of the action distribution.\n\n## 4 Experiments\n\n### 4.1 Benchmarks and Evaluation Metrics\n\nBenchmarks\n\nFor evaluating our world model‚Äôs capability in multi-step goal-conditioned planning, we adapt two standard goal-conditioned visual planning datasets, CrossTask [ 88 ] and COIN [ 71 ] , which contain diverse fine-grained action labels and timestamps of human daily activities.\n\nCrossTask consists of 4.7K videos across 83 tasks, covering 105 actions, with an average of 8 actions per video. The total duration is 375h.\n\nCOIN consists of 11,287 videos across 180 tasks, covering 778 actions, with an average of 3.9 actions per video. The total duration is 476h.\n\nMetrics\n\nFollowing previous works in goal-conditioned visual planning [ 7 ] , we adopt three metrics for evaluation: (1) Success Rate (SR) computes whether the predicted action sequence exactly matches the ground truth sequence. (2) Mean Accuracy (mAcc) computes the average accuracy of the predicted actions at each time step. (3) Mean Intersection over Union (mIoU) quantifies the overlap between the predicted procedure and the ground truth.\n\n### 4.2 Baseline and Evaluation Protocol\n\nWe follow previous works [ 15 , 7 , 59 , 3 ] and evaluate goal-conditioned visual planning in two setups based on the modality of the observation and the target, as discussed in Section 2 . For procedural planning [ 15 ] , both observations and goals are specified as images, which is more aligned with the traditional visual planning setup. For visual planning with videos [ 59 ] , both observations and goals are specified as video clips, which more faithfully reflect the temporal‚Äìspatial information in the real world.\n\nFor both setups, evaluation is conducted over a planning horizon T T , where the model outputs a sequence of T T actions given the observation and the goal.\n\nIn both setups, we include three categories of baselines. LLM-based methods leverage LLMs or VLMs for reasoning and planning [ 42 , 36 , 52 , 76 , 79 , 20 , 53 ] . Generative (world) models explicitly generate pixels or latent visual tokens that decode into pixels for planning [ 15 , 7 , 86 , 75 , 50 , 61 , 87 , 59 ] . Predictive (world) models predict a sequence of actions without relying on pixel generation [ 27 , 1 , 68 , 69 , 74 , 3 ] .\n\nThere are two extra baselines in the procedural planning setup. Random randomly selects an action from all actions and serves as the empirical lower bound of performance [ 15 ] . The Retrieval-Based approach retrieves the nearest neighbor by minimizing the visual feature distance within the training dataset, and the action sequence associated with the retrieved neighbor is then used as the plan [ 87 ] .\n\nBesides, for both V-JEPA 2 [ 3 ] and our GeoWorld, we adopt frozen encoders, while for VideoWorld [ 59 ] we perform full finetuning. For general VLMs [ 76 , 79 , 20 , 53 ] in visual planning with videos, all evaluations are conducted in a zero-shot setting. For more details on the baselines, see Appendix 3 .\n\n[FIGURE_CAPTION] Table 1: Goal-conditioned visual planning with images on CrossTask [ 88 ] and COIN [ 71 ] datasets. We evaluate multi-step planning over a horizon T T under the procedural planning setup [ 15 ] , where both observations and goals are specified as images.\n\n### 4.3 Implementation Details\n\nFor a fair comparison, both the V-JEPA 2 [ 3 ] baseline and our GeoWorld adopt frozen encoders pretrained on VideoMix22M.\nThe exponential map exp 0 ‚Å° ( ‚ãÖ ) \\exp_{0}(\\cdot) is implemented and trained as a differentiable hyperbolic projection layer, where the curvature c c is treated as a learnable parameter [ 14 ] .\nThe predictor network P œï ‚Äã ( ‚ãÖ ) P_{\\phi}(\\cdot) is a ‚àº 300 \\sim\\!300 M-parameter transformer with 24 layers, 16 heads, a 1024-dimensional hidden size, and GELU activations.\n\nWe conduct a two-stage training procedure for both V-JEPA¬†2 and our GeoWorld, consisting of supervised post-training followed by geometric reinforcement learning.\n\nIn the supervised post-training stage, both V-JEPA¬†2 and GeoWorld are trained with the AdamW optimizer [ 45 ] using a warmup‚Äìconstant‚Äìdecay learning rate schedule and a constant weight decay of 0.04. We linearly warm up the learning rate from 7.5 √ó 10 ‚àí 5 7.5\\times 10^{-5} to 4.25 √ó 10 ‚àí 4 4.25\\times 10^{-4} over 4500 iterations, hold it constant for 85,500 iterations, and then decay it to 0 over the final 4500 iterations, with a batch size of 256.\n\nFor geometric reinforcement learning, we keep the same AdamW optimizer and weight decay as in the supervised post-training stage, but adopt a smaller learning rate and a shorter schedule due to the higher variance of the RL objective. Specifically, we linearly warm up the learning rate from 5.0 √ó 10 ‚àí 5 5.0\\times 10^{-5} to 2.0 √ó 10 ‚àí 4 2.0\\times 10^{-4} over 2,000 iterations, hold it constant for 18,000 iterations, and then linearly decay it to 0 over the final 5,000 iterations, with a batch size of 128. Unless otherwise specified, we set the discount factor to Œ≥ = 0.99 \\gamma=0.99 and the triangle-inequality regularization weight to Œ≤ = 0.1 \\beta=0.1 .\n\nFor energy-based planning with CEM [ 23 ] , we adopt a sample size of N = 800 N=800 , an elite set size of K = 80 K=80 , and I = 10 I=10 refinement iterations.\n\nThe entire training is conducted on 4 nodes, each equipped with 8 NVIDIA H100 GPUs, 48-core Intel Xeon Platinum 8469C CPUs, and 230‚ÄâGB of RAM. We use only a single H100 GPU for inference.\n\n[FIGURE_CAPTION] Table 2: Goal-conditioned visual planning with videos on CrossTask [ 88 ] and COIN [ 71 ] datasets. We evaluate multi-step planning over a horizon T T under the visual planning with videos [ 59 ] setup, where both observations and goals are specified as video clips.\n\n[FIGURE_CAPTION] Table 3: Long horzion planning on CrossTask [ 88 ] .\n\n### 4.4 Main Results\n\nAs shown in Table 1 and 2 , GeoWorld consistently improves multi-step goal-conditioned visual planning across both CrossTask and COIN. Under the procedural planning setup, GeoWorld yields notable gains over prior predictive world models, especially in long-horizon settings, achieving higher SR, mAcc, and mIoU for both T = 3 T{=}3 and T = 4 T{=}4 . In the video-based planning setup, GeoWorld continues to outperform V-JEPA¬†2 across all model scales, with the ViT-g 384 variant achieving the best overall results and surpassing strong LLM-based planners. These improvements highlight the effectiveness of geometry-aware latent dynamics and geometric reinforcement learning in enhancing long-horizon stability and planning accuracy.\n\nFor ablation study, please refer to Appendix 5 .\n\n### 4.5 Long-Horizon Planning\n\nTable 3 highlights GeoWorld‚Äôs strength in long-horizon planning. As the horizon increases from T = 3 T=3 to T = 6 T=6 , the performance of existing predictive and generative world models consistently degrades due to accumulated geometric drift in Euclidean latent space. In contrast, GeoWorld maintains higher stability and achieves the best Success Rate across all horizons.\n\n## 5 Conclusion\n\nWe introduced GeoWorld , a geometric world model designed to improve long-horizon visual planning by preserving geometric structure and hierarchical relations in latent space. Through Hyperbolic JEPA , GeoWorld maps Euclidean latent representations onto a hyperbolic manifold, enabling geodesic-aware latent dynamics that produce a more structured and physically meaningful energy landscape. Building on this representation, Geometric Reinforcement Learning refines the predictor via hyperbolic energy optimization and triangle-inequality regularization, yielding geodesic-consistent rollouts and reducing error accumulation across extended horizons. Extensive experiments on CrossTask and COIN demonstrate that GeoWorld consistently improves long-horizon performance over strong predictive world models such as V-JEPA¬†2, achieving higher success rates across T = 3 T=3 to T = 6 T=6 planning. These results highlight the importance of incorporating geometric principles into predictive world models and reinforce the value of geometry-aware reinforcement learning for stable and effective multi-step planning.\n\n## References\n\n- [1] Y. Abu Farha and J. Gall (2019) Uncertainty-aware anticipation of activities . In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops , pp.¬†0‚Äì0 . Cited by: ¬ß3 , ¬ß4.2 , Table 1 .\n- [2] M. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas (2023) Self-supervised learning from images with a joint-embedding predictive architecture . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†15619‚Äì15629 . Cited by: ¬ß1.1 , ¬ß1.3.2 , ¬ß1.4 , ¬ß1 , ¬ß2 .\n- [3] M. Assran, A. Bardes, D. Fan, Q. Garrido, R. Howes, M. Muckley, A. Rizvi, C. Roberts, K. Sinha, A. Zholus, et al. (2025) V-jepa 2: self-supervised video models enable understanding, prediction and planning . arXiv preprint arXiv:2506.09985 . Cited by: Figure 1 , Figure 1 , 1(a) , 1(a) , ¬ß1.1 , ¬ß1.3.3 , ¬ß1.4 , ¬ß1 , ¬ß2 , ¬ß2 , ¬ß3 , ¬ß3.2 , ¬ß4 , ¬ß4 , ¬ß4.2 , ¬ß4.2 , ¬ß4.2 , ¬ß4.3 , Table 1 , Table 1 , Table 1 , Table 1 , Table 2 , Table 2 , Table 2 , Table 2 , Table 3 , Table 3 , Table 2 , Table 3 , Table 4 .\n- [4] A. Bardes, Q. Garrido, J. Ponce, X. Chen, M. Rabbat, Y. LeCun, M. Assran, and N. Ballas (2024) Revisiting feature prediction for learning visual representations from video . Transactions on Machine Learning Research . Cited by: ¬ß1.1 , ¬ß1.3.3 , ¬ß1.4 , ¬ß1 , ¬ß2 .\n- [5] A. Bardes, J. Ponce, and Y. Lecun (2022) VICReg: variance-invariance-covariance regularization for self-supervised learning . In ICLR 2022-International Conference on Learning Representations , Cited by: ¬ß1.3.1 .\n- [6] R. Bellman (1966) Dynamic programming . science 153 ( 3731 ), pp.¬†34‚Äì37 . Cited by: ¬ß1.6 .\n- [7] J. Bi, J. Luo, and C. Xu (2021) Procedure planning in instructional videos via contextual modeling and model-based policy learning . In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp.¬†15611‚Äì15620 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.1 , ¬ß4.2 , ¬ß4.2 , Table 1 , Table 1 .\n- [8] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, et al. (2024) \\textpi 0 : A Vision‚ÄìLanguage‚ÄìAction Flow Model for General Robot Control . arXiv preprint arXiv:2410.24164 . Cited by: ¬ß1 .\n- [9] R. Boney, J. Kannala, and A. Ilin (2020) Regularizing model-based planning with energy-based models . In Conference on Robot Learning , pp.¬†182‚Äì191 . Cited by: ¬ß1.1 .\n- [10] D. Brandfonbrener, O. Nachum, and J. Bruna (2023) Inverse dynamics pretraining learns good representations for multitask imitation . Advances in Neural Information Processing Systems 36 , pp.¬†66953‚Äì66978 . Cited by: ¬ß1.4 .\n- [11] M. R. Bridson and A. Haefliger (2013) Metric spaces of non-positive curvature . Vol. 319 , Springer Science & Business Media . Cited by: ¬ß1.5 .\n- [12] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles (2015) Activitynet: a large-scale video benchmark for human activity understanding . In Proceedings of the ieee conference on computer vision and pattern recognition , pp.¬†961‚Äì970 . Cited by: ¬ß1 .\n- [13] E. Cetin, B. P. Chamberlain, M. M. Bronstein, and J. J. Hunt (2023) Hyperbolic deep reinforcement learning . In The Eleventh International Conference on Learning Representations , Cited by: ¬ß1.5.1 , ¬ß1.5.1 , ¬ß1.5 .\n- [14] I. Chami, Z. Ying, C. R√©, and J. Leskovec (2019) Hyperbolic graph convolutional neural networks . Advances in neural information processing systems 32 . Cited by: ¬ß4.3 , ¬ß5 .\n- [15] C. Chang, D. Huang, D. Xu, E. Adeli, L. Fei-Fei, and J. C. Niebles (2020) Procedure planning in instructional videos . In European Conference on Computer Vision , pp.¬†334‚Äì350 . Cited by: ¬ß2 , ¬ß3 , ¬ß3 , ¬ß4.2 , ¬ß4.2 , ¬ß4.2 , Table 1 , Table 1 , Table 1 , Table 3 , Table 3 .\n- [16] D. Chen, T. Moutakanni, W. Chung, Y. Bang, Z. Ji, A. Bolourchi, and P. Fung (2025) Planning with reasoning using vision language world model . arXiv preprint arXiv:2509.02722 . Cited by: ¬ß2 .\n- [17] G. Chen, D. Lin, J. Yang, C. Lin, J. Zhu, M. Fan, H. Zhang, S. Chen, Z. Chen, C. Ma, et al. (2025) Skyreels-v2: infinite-length film generative model . arXiv preprint arXiv:2504.13074 . Cited by: ¬ß2 .\n- [18] D. L. Cheney and R. M. Seyfarth (1998) Why animals don‚Äôt have language . Tanner lectures on human values 19 , pp.¬†173‚Äì210 . Cited by: ¬ß1 .\n- [19] K. C. Ciesielski, A. X. Falc√£o, and P. A. Miranda (2018) Path-value functions for which dijkstra‚Äôs algorithm returns optimal mapping . Journal of Mathematical Imaging and Vision 60 ( 7 ), pp.¬†1025‚Äì1036 . Cited by: ¬ß1.6 .\n- [20] G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al. (2025) Gemini 2.5: pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities . arXiv preprint arXiv:2507.06261 . Cited by: ¬ß1 , ¬ß2 , ¬ß3 , ¬ß4.2 , ¬ß4.2 , Table 2 , Table 3 .\n- [21] B. C. Cs√°ji and L. Monostori (2008) Value function based reinforcement learning in changing markovian environments. . Journal of Machine Learning Research 9 ( 8 ). Cited by: ¬ß1.6 .\n- [22] J. Cui, J. Wu, M. Li, T. Yang, X. Li, R. Wang, A. Bai, Y. Ban, and C. Hsieh (2025) Self-forcing++: towards minute-scale high-quality video generation . arXiv preprint arXiv:2510.02283 . Cited by: ¬ß2 .\n- [23] P. De Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein (2005) A tutorial on the cross-entropy method . Annals of operations research 134 ( 1 ), pp.¬†19‚Äì67 . Cited by: ¬ß1.3.3 , ¬ß2 , ¬ß3.1 , ¬ß3.5 , ¬ß4.3 .\n- [24] H. Deng, T. Pan, H. Diao, Z. Luo, Y. Cui, H. Lu, S. Shan, Y. Qi, and X. Wang (2024) Autoregressive video generation without vector quantization . arXiv preprint arXiv:2412.14169 . Cited by: ¬ß2 .\n- [25] K. Desai, M. Nickel, T. Rajpurohit, J. Johnson, and S. R. Vedantam (2023) Hyperbolic image-text representations . In International Conference on Machine Learning , pp.¬†7694‚Äì7731 . Cited by: ¬ß1.5 , ¬ß1.5 , ¬ß5 .\n- [26] Y. Du, S. Li, J. Tenenbaum, and I. Mordatch (2022) Learning iterative reasoning through energy minimization . In International Conference on Machine Learning , pp.¬†5570‚Äì5582 . Cited by: ¬ß1.1 .\n- [27] K. Ehsani, H. Bagherinezhad, J. Redmon, R. Mottaghi, and A. Farhadi (2018) Who let the dogs out? modeling dog behavior from visual data . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp.¬†4051‚Äì4060 . Cited by: ¬ß3 , ¬ß4.2 , Table 1 .\n- [28] O. Ganea, G. B√©cigneul, and T. Hofmann (2018) Hyperbolic neural networks . Advances in neural information processing systems 31 . Cited by: ¬ß1.5.1 , ¬ß1.5.1 .\n- [29] Q. Garrido, M. Assran, N. Ballas, A. Bardes, L. Najman, and Y. LeCun (2024) Learning and leveraging world models in visual representation learning . arXiv preprint arXiv:2403.00504 . Cited by: ¬ß1.4 , ¬ß1 , ¬ß2 .\n- [30] S. Ge, S. Mishra, S. Kornblith, C. Li, and D. Jacobs (2023) Hyperbolic contrastive learning for visual representations beyond objects . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp.¬†6840‚Äì6849 . Cited by: ¬ß1.5 , ¬ß1.5 .\n- [31] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag, et al. (2017) The\" something something\" video database for learning and evaluating visual common sense . In Proceedings of the IEEE international conference on computer vision , pp.¬†5842‚Äì5850 . Cited by: ¬ß1 .\n- [32] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. (2025) Deepseek-r1: incentivizing reasoning capability in llms via reinforcement learning . arXiv preprint arXiv:2501.12948 . Cited by: ¬ß1 .\n- [33] T. Huang, Z. Zhang, and H. Tang (2025) 3d-r1: enhancing reasoning in 3d vlms for unified scene understanding . arXiv preprint arXiv:2507.23478 . Cited by: ¬ß1 .\n- [34] X. Huang, Z. Li, G. He, M. Zhou, and E. Shechtman (2025) Self forcing: bridging the train-test gap in autoregressive video diffusion . arXiv preprint arXiv:2506.08009 . Cited by: ¬ß2 .\n- [35] P. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, et al. (2025) \\textpi 0.5 : A Vision‚ÄìLanguage‚ÄìAction Model with Open-World Generalization . arXiv preprint arXiv:2504.16054 . Cited by: ¬ß1 .\n- [36] M. M. Islam, T. Nagarajan, H. Wang, F. Chu, K. Kitani, G. Bertasius, and X. Yang (2024) Propose, assess, search: harnessing llms for goal-oriented planning in instructional videos . In European Conference on Computer Vision , pp.¬†436‚Äì452 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , Table 1 , Table 1 .\n- [37] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, et al. (2017) The kinetics human action video dataset . arXiv preprint arXiv:1705.06950 . Cited by: ¬ß1 .\n- [38] P. J. Kellman, M. Arterberry, W. Damon, R. Lerner, D. Kuhn, R. Siegler, et al. (2006) Infant visual perception . Cited by: ¬ß1 .\n- [39] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, et al. DROID: a large-scale in-the-wild robot manipulation dataset . In RSS 2024 Workshop: Data Generation for Robotics , Cited by: ¬ß1.3.3 .\n- [40] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre (2011) HMDB: a large video database for human motion recognition . In 2011 International conference on computer vision , pp.¬†2556‚Äì2563 . Cited by: ¬ß1 .\n- [41] Y. LeCun (2022) A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27 . Open Review 62 ( 1 ), pp.¬†1‚Äì62 . Cited by: ¬ß1.1 , ¬ß1.2 , ¬ß1.3 , ¬ß1.4 , ¬ß1.4 , ¬ß1 , ¬ß2 , ¬ß3.1 , ¬ß7 .\n- [42] J. Liu, S. Li, Z. Wang, M. Li, and H. Ji (2023) A language-first approach for procedure planning . In Findings of the Association for Computational Linguistics: ACL 2023 , pp.¬†1941‚Äì1954 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , Table 1 .\n- [43] J. Liu, J. Han, B. Yan, H. Wu, F. Zhu, X. Wang, Y. Jiang, B. Peng, and Z. Yuan (2025) InfinityStar: unified spacetime autoregressive modeling for visual generation . arXiv preprint arXiv:2511.04675 . Cited by: ¬ß2 .\n- [44] Q. Liu, T. Huang, Z. Zhang, and H. Tang (2025) Nav-r1: reasoning and navigation in embodied scenes . arXiv preprint arXiv:2509.10884 . Cited by: ¬ß1 .\n- [45] I. Loshchilov and F. Hutter (2017) Decoupled weight decay regularization . arXiv preprint arXiv:1711.05101 . Cited by: ¬ß4.3 .\n- [46] R. Mendonca, S. Bahl, and D. Pathak (2023) Structured world models from human videos . Cited by: ¬ß1.4 , ¬ß1 , ¬ß2 .\n- [47] A. Miech, D. Zhukov, J. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic (2019) Howto100m: learning a text-video embedding by watching hundred million narrated video clips . In Proceedings of the IEEE/CVF international conference on computer vision , pp.¬†2630‚Äì2640 . Cited by: ¬ß1 .\n- [48] M. Mitchell and D. C. Krakauer (2023) The debate over understanding in ai‚Äôs large language models . Proceedings of the National Academy of Sciences 120 ( 13 ), pp.¬†e2215907120 . Cited by: ¬ß1 .\n- [49] S. Mitsuhashi and S. Ishii (2023) Triangle inequality for inverse optimal control . IEEE Access 11 , pp.¬†119187‚Äì119199 . Cited by: ¬ß1.6 .\n- [50] K. R. Y. Nagasinghe, H. Zhou, M. Gunawardhana, M. R. Min, D. Harari, and M. H. Khan (2024) Why not use your textbook? knowledge-enhanced procedure planning of instructional videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†18816‚Äì18826 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , Table 1 , Table 3 .\n- [51] M. Nickel and D. Kiela (2017) Poincar√© embeddings for learning hierarchical representations . Advances in neural information processing systems 30 . Cited by: ¬ß1.5.1 , ¬ß1.5 , ¬ß1.5 , ¬ß1 .\n- [52] Y. Niu, W. Guo, L. Chen, X. Lin, and S. Chang (2024) Schema: state changes matter for procedure planning in instructional videos . arXiv preprint arXiv:2403.01599 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , Table 1 , Table 3 .\n- [53] OpenAI (2025) GPT-5 system card, version 1.0, 2025-08-13 . Note: https://cdn.openai.com/gpt-5-system-card.pdf Cited by: ¬ß1 , ¬ß2 , ¬ß3 , ¬ß4.2 , ¬ß4.2 , Table 2 , Table 3 .\n- [54] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. (2022) Training language models to follow instructions with human feedback . Advances in neural information processing systems 35 , pp.¬†27730‚Äì27744 . Cited by: ¬ß1 .\n- [55] A. Pal, M. van Spengler, G. M. D. di Melendugno, A. Flaborea, F. Galasso, and P. Mettes (2024) Compositional entailment learning for hyperbolic vision-language models . arXiv preprint arXiv:2410.06912 . Cited by: ¬ß1.5 , ¬ß1.5 .\n- [56] D. Patel, H. Eghbalzadeh, N. Kamra, M. L. Iuzzolino, U. Jain, and R. Desai (2023) Pretrained language models as visual planners for human assistance . In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp.¬†15302‚Äì15314 . Cited by: ¬ß2 .\n- [57] S. Pitis, H. Chan, K. Jamali, and J. Ba (2020) An inductive bias for distances: neural nets that respect the triangle inequality . In International Conference on Learning Representations , Cited by: ¬ß1.6 .\n- [58] S. Ren, C. Chen, Z. Wang, L. Song, X. Zhu, A. Yuille, Y. Yang, and J. Lu (2025) Autoregressive video generation beyond next frames prediction . arXiv preprint arXiv:2509.24081 . Cited by: ¬ß2 .\n- [59] Z. Ren, Y. Wei, X. Guo, Y. Zhao, B. Kang, J. Feng, and X. Jin (2025) Videoworld: exploring knowledge learning from unlabeled videos . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.¬†29029‚Äì29039 . Cited by: ¬ß1.4 , ¬ß1 , ¬ß2 , ¬ß2 , ¬ß3 , ¬ß4.2 , ¬ß4.2 , ¬ß4.2 , Table 2 , Table 2 , Table 3 .\n- [60] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Proximal policy optimization algorithms . arXiv preprint arXiv:1707.06347 . Cited by: ¬ß1 .\n- [61] L. Shi, P. B√ºrkner, and A. Bulling (2025) Actiondiffusion: an action-aware diffusion model for procedure planning in instructional videos . In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) , pp.¬†8816‚Äì8825 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , Table 1 .\n- [62] M. Sigman and S. Dehaene (2008) Brain mechanisms of serial and parallel processing during dual-task performance . Journal of Neuroscience 28 ( 30 ), pp.¬†7585‚Äì7598 . Cited by: ¬ß1 .\n- [63] H. G. Singh, A. Loquercio, C. Sferrazza, J. Wu, H. Qi, P. Abbeel, and J. Malik (2025) Hand-object interaction pretraining from videos . In 2025 IEEE International Conference on Robotics and Automation (ICRA) , pp.¬†3352‚Äì3360 . Cited by: ¬ß1.4 , ¬ß1 , ¬ß2 .\n- [64] G. Skenderi, H. Li, J. Tang, and M. Cristani (2025) Graph-level representation learning with joint-embedding predictive architectures . Transactions on Machine Learning Research . Cited by: ¬ß2 .\n- [65] Z. Song, G. Ouyang, M. Li, Y. Ji, C. Wang, Z. Xu, Z. Zhang, X. Zhang, Q. Jiang, Z. Chen, et al. (2025) Maniplvm-r1: reinforcement learning for reasoning in embodied manipulation with large vision-language models . arXiv preprint arXiv:2505.16517 . Cited by: ¬ß1 .\n- [66] K. Soomro, A. R. Zamir, and M. Shah (2012) Ucf101: a dataset of 101 human actions classes from videos in the wild . arXiv preprint arXiv:1212.0402 . Cited by: ¬ß1 .\n- [67] M. W. Spong and R. Ortega (2002) On adaptive inverse dynamics control of rigid robots . IEEE Transactions on Automatic Control 35 ( 1 ), pp.¬†92‚Äì95 . Cited by: ¬ß1.4 , ¬ß1 , ¬ß2 .\n- [68] A. Srinivas, A. Jabri, P. Abbeel, S. Levine, and C. Finn (2018) Universal planning networks: learning generalizable representations for visuomotor control . In International conference on machine learning , pp.¬†4732‚Äì4741 . Cited by: ¬ß3 , ¬ß4.2 , Table 1 .\n- [69] J. Sun, D. Huang, B. Lu, Y. Liu, B. Zhou, and A. Garg (2022) Plate: visually-grounded planning with transformers in procedural tasks . IEEE Robotics and Automation Letters 7 ( 2 ), pp.¬†4924‚Äì4930 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , Table 1 .\n- [70] R. S. Sutton, A. G. Barto, et al. (1998) Reinforcement learning: an introduction . Vol. 1 , MIT press Cambridge . Cited by: ¬ß1.6 .\n- [71] Y. Tang, D. Ding, Y. Rao, Y. Zheng, D. Zhang, L. Zhao, J. Lu, and J. Zhou (2019) Coin: a large-scale dataset for comprehensive instructional video analysis . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†1207‚Äì1216 . Cited by: ¬ß1 , ¬ß4 , ¬ß4.1 , Table 1 , Table 1 , Table 1 , Table 2 , Table 2 , Table 2 , ¬ß7 .\n- [72] H. Teng, H. Jia, L. Sun, L. Li, M. Li, M. Tang, S. Han, T. Zhang, W. Zhang, W. Luo, et al. (2025) MAGI-1: autoregressive video generation at scale . arXiv preprint arXiv:2505.13211 . Cited by: ¬ß2 .\n- [73] Y. Tian, S. Yang, J. Zeng, P. Wang, D. Lin, H. Dong, and J. Pang (2025) Predictive inverse dynamics models are scalable learners for robotic manipulation . In The Thirteenth International Conference on Learning Representations , Cited by: ¬ß1.4 .\n- [74] A. Wang, K. Lin, J. Du, J. Meng, and W. Zheng (2023) Event-guided procedure planning from instructional videos with text supervision . In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp.¬†13565‚Äì13575 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , Table 1 , Table 3 .\n- [75] H. Wang, Y. Wu, S. Guo, and L. Wang (2023) Pdpp: projected diffusion for procedure planning in instructional videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†14836‚Äì14845 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , Table 1 , Table 3 .\n- [76] W. Wang, Z. Gao, L. Gu, H. Pu, L. Cui, X. Wei, Z. Liu, L. Jing, S. Ye, J. Shao, et al. (2025) InternVL3.5: advancing open-source multimodal models in versatility, reasoning, and efficiency . arXiv preprint arXiv:2508.18265 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , ¬ß4.2 , Table 2 , Table 3 .\n- [77] Y. Wang, T. Xiong, D. Zhou, Z. Lin, Y. Zhao, B. Kang, J. Feng, and X. Liu (2024) Loong: generating minute-level long videos with autoregressive language models . arXiv preprint arXiv:2410.02757 . Cited by: ¬ß2 .\n- [78] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. (2022) Chain-of-thought prompting elicits reasoning in large language models . Advances in neural information processing systems 35 , pp.¬†24824‚Äì24837 . Cited by: ¬ß1 .\n- [79] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, et al. (2025) Qwen3 technical report . arXiv preprint arXiv:2505.09388 . Cited by: ¬ß1 , ¬ß2 , ¬ß3 , ¬ß4.2 , ¬ß4.2 , Table 2 , Table 3 .\n- [80] C. Yang, X. Ma, W. Huang, F. Sun, H. Liu, J. Huang, and C. Gan (2019) Imitation learning from observations by minimizing inverse dynamics disagreement . Advances in neural information processing systems 32 . Cited by: ¬ß1.4 .\n- [81] S. Yang, W. Huang, R. Chu, Y. Xiao, Y. Zhao, X. Wang, M. Li, E. Xie, Y. Chen, Y. Lu, et al. (2025) Longlive: real-time interactive long video generation . arXiv preprint arXiv:2509.22622 . Cited by: ¬ß2 .\n- [82] A. Ye, Z. Zhang, B. Wang, X. Wang, D. Zhang, and Z. Zhu (2025) Vla-r1: enhancing reasoning in vision-language-action models . arXiv preprint arXiv:2510.01623 . Cited by: ¬ß1 .\n- [83] Y. Yue, F. Lin, K. D. Yamada, and Z. Zhang (2023) Hyperbolic contrastive learning . arXiv preprint arXiv:2302.01409 . Cited by: ¬ß1.5 , ¬ß1.5 .\n- [84] C. Zhang, Y. Song, R. Desai, M. L. Iuzzolino, J. Tighe, G. Bertasius, and S. Kottur (2025) Enhancing visual planning with auxiliary tasks and multi-token prediction . arXiv preprint arXiv:2507.15130 . Cited by: ¬ß2 .\n- [85] Z. Zhang, Y. Wang, D. Li, D. Gong, I. Reid, and R. Hartley (2025) FlashMo: geometric interpolants and frequency-aware sparsity for scalable efficient motion generation . In The Thirty-ninth Annual Conference on Neural Information Processing Systems , Cited by: ¬ß1.5.1 .\n- [86] H. Zhao, I. Hadji, N. Dvornik, K. G. Derpanis, R. P. Wildes, and A. D. Jepson (2022) P3iv: probabilistic procedure planning from instructional videos with weak supervision . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†2938‚Äì2948 . Cited by: ¬ß2 , ¬ß3 , ¬ß4.2 , Table 1 , Table 3 .\n- [87] Y. Zhou, Z. Qi, L. Lin, J. Jing, T. Chai, B. Zhang, S. Wang, and W. Zhang (2025) Masked temporal interpolation diffusion for procedure planning in instructional videos . arXiv preprint arXiv:2507.03393 . Cited by: ¬ß2 , ¬ß3 , ¬ß3 , ¬ß4.2 , ¬ß4.2 , Table 1 , Table 1 , Table 3 , Table 3 , ¬ß5 .\n- [88] D. Zhukov, J. Alayrac, R. G. Cinbis, D. Fouhey, I. Laptev, and J. Sivic (2019) Cross-task weakly supervised learning from instructional videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†3537‚Äì3545 . Cited by: ¬ß1 , Figure 1 , Figure 1 , ¬ß4.1 , Table 1 , Table 1 , Table 1 , Table 2 , Table 2 , Table 2 , Table 3 , ¬ß5 , Table 1 , Table 2 , Table 3 , Table 4 , Table 5 , ¬ß7 .\n\nSupplementary Material\n\n## 1 Preliminaries\n\n### 1.1 Energy-Based World Models\n\nEnergy-Based World Models (EBWM) [ 41 , 2 , 4 , 3 ] are derived from Energy-Based Models (EBM) [ 9 , 26 ] , which define a scalar energy function F ‚Äã ( x , y ) F(x,y) that measures how compatible two variables are, such as a current world state s x s_{x} and a possible future state s y s_{y} . A low energy value corresponds to a plausible scenario, while a high energy value indicates an implausible one. Instead of predicting a single future, the energy landscape implicitly represents all plausible futures as valleys of low energy. Because energy replaces probability, the model can naturally handle multi-modal or uncertain worlds without the need for explicit sampling or normalization. Reasoning and planning are therefore formulated as energy minimization, where the goal is to find the configuration (actions, latents, or next states) that minimizes the expected energy:\n\n|  | Plan = arg ‚Å° min actions ‚Å° F ‚Äã ( s t , s t + 1 : T ) . \\text{Plan}=\\arg\\min_{\\text{actions}}F(s_{t},s_{t+1:T}). |  | (23) |\n| --- | --- | --- | --- |\n\n### 1.2 Hierarchical Planning\n\nHierarchical planning often consists of two levels: high-level planning computes trajectories in abstract latent space that minimize energy (i.e., the most plausible and least costly transitions), while lower levels fill in the physical details [ 41 ] .\n\nThe world model learns hierarchical latent abstractions, where low-level modules predict short-term fine details and higher-level modules capture long-term abstract dynamics.\n\nSpecifically, planning becomes energy minimization in latent space. High-level modules operate on abstract latent states s ( 2 ) s^{(2)} that evolve slowly, where planning corresponds to finding a geodesic of minimum energy between the abstract states s A ( 2 ) s_{A}^{(2)} and s B ( 2 ) s_{B}^{(2)} , yielding a coarse trajectory that serves as the overall plan. Lower levels then refine this trajectory into fine-grained predictions s t ( 1 ) s_{t}^{(1)} , minimizing sub-energies conditioned on the higher-level plan. As a result, the overall behavior emerges through hierarchical energy descent, where each layer enforces consistency between its predictions and the layer above. In other words, high-level planning computes trajectories in abstract latent space that minimize energy (i.e., the most plausible and least costly transitions), while lower levels fill in the physical details.\n\nFormally, for a hierarchical world model F ( L ) F^{(L)} , the optimal plan is defined as:\n\n|  | Optimal Plan: min { a t , z t } ‚Äã ‚àë l = 1 L ‚àë t F ( l ) ‚Äã ( s t ( l ) , s t + 1 ( l ) , z t ( l ) ) . \\text{Optimal Plan:}\\qquad\\min_{\\{a_{t},z_{t}\\}}\\sum_{l=1}^{L}\\sum_{t}F^{(l)}\\big(s_{t}^{(l)},s_{t+1}^{(l)},z_{t}^{(l)}\\big). |  | (24) |\n| --- | --- | --- | --- |\n\nEach F ( l ) F^{(l)} expresses the energy cost of moving between abstract states at level l l , and gradients through this hierarchy yield a coherent plan across scales.\n\n### 1.3 Joint-Embedding Predictive Architecture\n\nJoint-Embedding Predictive Architectures (JEPA) [ 41 ] learn a predictive world model directly in latent space rather than generating pixels. A JEPA encodes observations into a compact representation space and predicts future latent states by minimizing an energy or similarity objective between encoded targets and predicted embeddings. This joint-embedding formulation bypasses the need for autoregressive pixel generation, which is computationally expensive and prone to error accumulation over long horizons. Learning in latent space instead focuses the model on high-level structure, semantics, and temporal dependencies rather than low-level appearance details, enabling more stable and efficient multi-step prediction. By operating on representations rather than images, JEPA captures the underlying dynamics of the environment while avoiding the challenges of modeling raw pixel distributions.\n\n#### 1.3.1 JEPA\n\nWe define the current observation x x and target y y . E Œ∏ ‚Äã ( ‚ãÖ ) E_{\\theta}(\\cdot) denotes the observation encoder that maps raw visual inputs into the latent representation space, E ¬Ø Œ∏ ‚Äã ( ‚ãÖ ) \\bar{E}_{\\theta}(\\cdot) denotes the target encoder with exponential moving average (EMA) weights, s x s_{x} and s y s_{y} are the latent representation of the current and target states obtained from the encoders, and z z is a latent variable capturing uncertainty. P œï ‚Äã ( ‚ãÖ ) P_{\\phi}(\\cdot) denotes the predictor,\nand Œ∏ \\theta and œï \\phi denote the parameters (weights) of the encoder and predictor networks, respectively.\n\nFor a unified one-step observation‚Äìtarget formulation, we first encode the current observation:\n\n|  | s x = E Œ∏ ‚Äã ( x ) s_{x}=E_{\\theta}(x) |  | (25) |\n| --- | --- | --- | --- |\n\nThen we perform latent prediction, which takes the current state s x s_{x} and the uncertainty z z as input and predicts the latent representation of the target state s ^ y \\hat{s}_{y} :\n\n|  | s ^ y = P œï ‚Äã ( s x , z ) \\hat{s}_{y}=P_{\\phi}(s_{x},z) |  | (26) |\n| --- | --- | --- | --- |\n\nSimilarly, we encode the target representation:\n\n|  | s y = E ¬Ø Œ∏ ‚Äã ( y ) s_{y}=\\bar{E}_{\\theta}(y) |  | (27) |\n| --- | --- | --- | --- |\n\nFor planning, the training objective becomes an energy minimization in latent space, which involves finding a geodesic of minimum energy between s x s_{x} and s y s_{y} to obtain the optimal plan.\n\n|  | min z ‚Å° C ‚Äã ( s x , s y , z ) , where ‚Äã s x = E Œ∏ ‚Äã ( x ) , s y = E ¬Ø Œ∏ ‚Äã ( y ) . \\min_{z}\\;C(s_{x},s_{y},z),\\qquad\\text{where }s_{x}=E_{\\theta}(x),\\;s_{y}=\\bar{E}_{\\theta}(y). |  | (28) |\n| --- | --- | --- | --- |\n\nHere, C C represents the energy cost of moving between the abstract states s x s_{x} and s y s_{y} . It learns two encoders (for past and future) and a predictor that maps s x s_{x} to s y s_{y} , where the energy is defined as the representation mismatch between the predicted and true embeddings.\nThe JEPA is non-generative and is trained non-contrastively to ensure that the embeddings remain both informative and predictable [ 5 ] .\nAs a result, it forms a predictive world model that learns latent abstractions.\n\nHierarchical-JEPA\n\nIf we extend to hierarchical planning, we can develop a model that learns hierarchical latent abstractions (Hierarchical-JEPA), in which low-level modules predict short-term fine details, while higher-level modules capture long-term abstract dynamics. As an example, consider a two-level model where high-level JEPA layers operate on abstract latent states s ( 2 ) s^{(2)} that evolve slowly.\nPlanning in this context corresponds to finding a geodesic of minimum energy between the abstract states s x ( 2 ) s_{x}^{(2)} and s y ( 2 ) s_{y}^{(2)} , yielding a coarse trajectory, i.e., the plan.\nMeanwhile, lower levels refine this trajectory into fine-grained predictions s t ( 1 ) s_{t}^{(1)} , minimizing sub-energies conditioned on the higher-level plan.\nOverall behavior emerges through hierarchical energy descent, where each layer enforces consistency between its predictions and the layer above.\n\nIn other words, high-level planning computes trajectories in abstract latent space that minimize energy (i.e., most plausible and least costly transitions), while lower levels fill in the physical details.\n\nFormally, the training objective of a Hierarchical-JEPA is given by:\n\n|  | min { z t } ‚Äã ‚àë l = 1 L ‚àë t C ( l ) ‚Äã ( s t ( l ) , s t + 1 ( l ) , z t ( l ) ) , \\min_{\\{z_{t}\\}}\\sum_{l=1}^{L}\\sum_{t}C^{(l)}\\big(s_{t}^{(l)},s_{t+1}^{(l)},z_{t}^{(l)}\\big), |  | (29) |\n| --- | --- | --- | --- |\n\nwhere each C ( l ) C^{(l)} represents the energy cost of moving between abstract states at level l l , and gradients through this hierarchy yield a coherent plan across scales.\n\n#### 1.3.2 I-JEPA\n\nImage-JEPA (I-JEPA) [ 2 ] extends JEPA to learn semantic image representations by predicting latent features of masked image regions from visible context patches.\n\nIn I-JEPA, an image y y is divided into non-overlapping patches, from which a single large block is sampled as the context and several smaller blocks are sampled as targets.\nThe context block x x is fed to the context encoder E Œ∏ ‚Äã ( ‚ãÖ ) E_{\\theta}(\\cdot) to obtain patch-level latent representations s x s_{x} , while the target blocks are processed by a target encoder with EMA weights, E ¬Ø Œ∏ ‚Äã ( ‚ãÖ ) \\bar{E}_{\\theta}(\\cdot) , to produce target embeddings s y s_{y} .\nThe predictor network P œï ‚Äã ( ‚ãÖ ) P_{\\phi}(\\cdot) takes the context representation s x s_{x} along with positional mask tokens { m j } j ‚àà B i \\{m_{j}\\}_{j\\in B_{i}} indicating the spatial locations of each target block, and predicts the corresponding feature vectors s ^ y = P œï ‚Äã ( s x , { m j } j ‚àà B i ) \\hat{s}_{y}=P_{\\phi}(s_{x},\\{m_{j}\\}_{j\\in B_{i}}) for those regions.\n\nThe training objective minimizes the average squared distance between the predicted and target representations of the target blocks, averaged over all sampled blocks and training samples in the dataset ùíü \\mathcal{D} :\n\n|  | min Œ∏ , œï ùîº ( x , y ) ‚àº ùíü [ 1 M ‚àë i = 1 M ‚àë j ‚àà B i ‚à• P œï ( E Œ∏ ( x ) , { m j } j ‚àà B i ) ‚àí E ¬Ø Œ∏ ( y ) j ‚à• 2 2 ] . \\min_{\\theta,\\phi}\\;\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\Bigg[\\frac{1}{M}\\sum_{i=1}^{M}\\sum_{j\\in B_{i}}\\Big\\|P_{\\phi}\\big(E_{\\theta}(x),\\{m_{j}\\}_{j\\in B_{i}}\\big)\\\\\n-\\bar{E}_{\\theta}(y)_{j}\\Big\\|_{2}^{2}\\Bigg]. |  | (30) |\n| --- | --- | --- | --- |\n\nHere, M M denotes the number of target blocks, ùîº ‚Äã [ ‚ãÖ ] \\mathbb{E}[\\cdot] denotes the expectation (average) over all training samples ( x , y ) (x,y) , and the loss measures the representation-level prediction error rather than pixel-level reconstruction, allowing I-JEPA to learn highly semantic, non-generative representations.\n\n#### 1.3.3 V-JEPA\n\nVideo-JEPA (V-JEPA) [ 4 ] and V-JEPA 2 [ 3 ] extend JEPA to learn spatio-temporal video representations by predicting masked tubelet features from visible context regions.\n\nIn V-JEPA and V-JEPA 2, a video clip y y is tokenized into spatial‚Äìtemporal patches (tubelets), and a subset of these patches is masked (replaced with mask tokens).\nThe remaining unmasked patches form the context view, while the masked patches form the target view.\nThe observation (context) encoder E Œ∏ ‚Äã ( ‚ãÖ ) E_{\\theta}(\\cdot) processes the masked version x x (containing both visible and mask tokens) and produces latent embeddings for all positions, including the masked ones.\nHowever, the predictor P œï ‚Äã ( ‚ãÖ ) P_{\\phi}(\\cdot) focuses only on the masked positions.\nA target encoder with EMA weights, E ¬Ø Œ∏ ‚Äã ( ‚ãÖ ) \\bar{E}_{\\theta}(\\cdot) , computes the target embeddings s y s_{y} of the full (unmasked) input.\nThe predictor takes the context representation s x = E Œ∏ ‚Äã ( x ) s_{x}=E_{\\theta}(x) along with the mask token indicators Œî y \\Delta_{y} and predicts the feature vectors s ^ y \\hat{s}_{y} for each masked patch position.\n\nThe training objective is to minimize the L 1 L_{1} distance between the predicted representations of the masked regions and the target representations from the target encoder, averaged over the dataset ùíü \\mathcal{D} :\n\n|  | min Œ∏ , œï ‚Å° ùîº ( x , Œî y , y ) ‚àº ùíü ‚Äã ‚Äñ P œï ‚Äã ( E Œ∏ ‚Äã ( x ) , Œî y ) ‚àí E ¬Ø Œ∏ ‚Äã ( y ) ‚Äñ 1 \\min_{\\theta,\\phi}\\;\\mathbb{E}_{(x,\\Delta_{y},y)\\sim\\mathcal{D}}\\left\\|P_{\\phi}\\big(E_{\\theta}(x),\\Delta_{y}\\big)-\\bar{E}_{\\theta}(y)\\right\\|_{1} |  | (31) |\n| --- | --- | --- | --- |\n\nwhere ùîº ‚Äã [ ‚ãÖ ] \\mathbb{E}[\\cdot] denotes the expectation (average) over all training samples ( x , Œî y , y ) (x,\\Delta_{y},y) .\n\nThat is,\n\n|  | min Œ∏ , œï ‚Å° ùîº ( s x , Œî y , s y ) ‚àº ùíü ‚Äã [ ‚Äñ s ^ y ‚àí s y ‚Äñ 1 ] , where ‚Äã s ^ y = P œï ‚Äã ( s x , Œî y ) . \\min_{\\theta,\\phi}\\;\\mathbb{E}_{(s_{x},\\Delta_{y},s_{y})\\sim\\mathcal{D}}\\left[\\left\\|\\hat{s}_{y}-s_{y}\\right\\|_{1}\\right],~\\text{where }\\hat{s}_{y}=P_{\\phi}(s_{x},\\Delta_{y}). |  | (32) |\n| --- | --- | --- | --- |\n\nHence the loss function ‚Ñí \\mathcal{L} measuring the L 1 L_{1} distance between the s ^ y \\hat{s}_{y} and s y s_{y} :\n\n|  | ‚Ñí ‚Äã ( Œ∏ , œï ) = ‚Äñ P œï ‚Äã ( E Œ∏ ‚Äã ( x ) , Œî y ) ‚àí E ¬Ø Œ∏ ‚Äã ( y ) ‚Äñ 1 \\mathcal{L}(\\theta,\\phi)=\\left\\|P_{\\phi}\\big(E_{\\theta}(x),\\Delta_{y})-\\bar{E}_{\\theta}(y)\\right\\|_{1} |  | (33) |\n| --- | --- | --- | --- |\n\nV-JEPA 2-AC\n\nThe action-conditioned variation (V-JEPA 2-AC) serves as a downstream extension of V-JEPA 2 that predicts future latent representations conditioned on agent actions.\n\nAs a step-by-step (1-step prediction) formulation, V-JEPA 2-AC adapts the frozen encoder E ‚Äã ( ‚ãÖ ) E(\\cdot) from V-JEPA 2, pretrained on unlabeled videos, to encode the current observation x t x_{t} into the latent representation s t x s_{t}^{x} and the future target x t + 1 x_{t+1} into s t + 1 x s_{t+1}^{x} .\nThe action-conditioned predictor P œï ‚Äã ( ‚ãÖ ) P_{\\phi}(\\cdot) takes the current latent s t x s_{t}^{x} and the action a t a_{t} as input to predict the next-state latent representation s ^ t + 1 x \\hat{s}_{t+1}^{x} .\nThe L 1 L_{1} loss in latent space then trains the predictor to align its predicted next-state representations with the encoded future representations:\n\n|  | ‚Ñí AC ‚Äã ( œï ) \\displaystyle\\mathcal{L}_{\\text{AC}}(\\phi) | = ‚Äñ P œï ‚Äã ( E ‚Äã ( x t ) , a t ) ‚àí E ‚Äã ( x t + 1 ) ‚Äñ 1 \\displaystyle=\\left\\|P_{\\phi}\\big(E(x_{t}),a_{t}\\big)-E(x_{t+1})\\right\\|_{1} |  | (34) |\n| --- | --- | --- | --- | --- |\n|  |  | = ‚Äñ s ^ t + 1 x ‚àí s t + 1 x ‚Äñ 1 . \\displaystyle=\\left\\|\\hat{s}_{t+1}^{x}-s_{t+1}^{x}\\right\\|_{1}. |  | (35) |\n\nIn the multi-step rollout setting, V-JEPA 2-AC extends the one-step formulation to predict a sequence of future latent representations over a time horizon T T conditioned on a sequence of actions.\n\nWe randomly sample a mini-batch of 4-second video clips from the Droid dataset [ 39 ] and, for simplicity, discard any videos shorter than 4 seconds, leaving us with a smaller subset of the dataset comprising under 62 hours of video. The video clips are sampled with a resolution of 256 √ó 256 256\\times 256 and a frame rate of 4 fps, yielding 16-frame clips ( x t ) t = 1 16 (x_{t})_{t=1}^{16} , where\neach x t x_{t} represents a single video frame. The robot‚Äôs end-effector state in each observation is denoted by the sequence ( s t e ) t = 1 16 (s_{t}^{e})_{t=1}^{16} , where s t e s_{t}^{e} is a real-valued 7D vector defined relative to the base of the robot. We construct a sequence of actions ( a t ) t = 1 15 (a_{t})_{t=1}^{15} by computing the change in end-effector state between adjacent frames. We use the V-JEPA¬†2 encoder E ‚Äã ( ‚ãÖ ) E(\\cdot) as an image encoder and encode each frame independently in a given clip to obtain a sequence of feature maps ( s t x ) t = 1 16 (s_{t}^{x})_{t=1}^{16} . The sequence of observed feature maps, end-effector states, and actions is temporally interleaved as ( s t x , s t e , a t ) t = 1 15 (s_{t}^{x},s_{t}^{e},a_{t})_{t=1}^{15} and processed with the transformer predictor network P œï ‚Äã ( ‚ãÖ ) P_{\\phi}(\\cdot) to obtain a sequence of next-state representation predictions ( s t + 1 x ) t = 1 15 (s_{t+1}^{x})_{t=1}^{15} :\n\n|  | ( s ^ t + 1 x ) t = 1 15 = P œï ‚Äã ( ( s t x , s t e , a t ) t = 1 15 ) . (\\hat{s}_{t+1}^{x})_{t=1}^{15}=P_{\\phi}\\!\\big((s_{t}^{x},s_{t}^{e},a_{t})_{t=1}^{15}\\big). |  | (36) |\n| --- | --- | --- | --- |\n\nThe teacher-forcing loss trains the predictor P œï ‚Äã ( ‚ãÖ ) P_{\\phi}(\\cdot) to accurately perform one-step future prediction by minimizing the L 1 L_{1} distance between the predicted latent representation s ^ t + 1 x \\hat{s}_{t+1}^{x} and the encoded ground-truth latent s t + 1 x s_{t+1}^{x} at each time step t t :\n\n|  | ‚Ñí TF ‚Äã ( œï ) \\displaystyle\\mathcal{L}_{\\text{TF}}(\\phi) | = 1 T ‚Äã ‚àë t = 1 T ‚Äñ P œï ‚Äã ( E ‚Äã ( x t ) , s t e , a t ) ‚àí E ‚Äã ( x t + 1 ) ‚Äñ 1 \\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}\\left\\|P_{\\phi}(E(x_{t}),s_{t}^{e},a_{t})-E(x_{t+1})\\right\\|_{1} |  | (37) |\n| --- | --- | --- | --- | --- |\n|  |  | = 1 T ‚Äã ‚àë t = 1 T ‚Äñ s ^ t + 1 x ‚àí s t + 1 x ‚Äñ 1 , where ‚Äã T = 15 . \\displaystyle=\\frac{1}{T}\\sum_{t=1}^{T}\\left\\|\\hat{s}_{t+1}^{x}-s_{t+1}^{x}\\right\\|_{1},~\\text{where }T=15. |  | (38) |\n\nThe rollout loss involves feeding the predictor‚Äôs output back as input, allowing the model to be trained to predict several timesteps ahead. In this case, we design a two-step rollout loss to improve the model‚Äôs ability to perform autoregressive rollouts during inference. We can now denote the rollout loss as\n\n|  | ‚Ñí rollout ‚Äã ( œï ) \\displaystyle\\mathcal{L}_{\\text{rollout}}(\\phi) | = ‚Äñ P œï ‚Äã ( E ‚Äã ( x t ) , s t e , ( a t ) t t + n ) ‚àí E ‚Äã ( x t + n + 1 ) ‚Äñ 1 , \\displaystyle=\\left\\|P_{\\phi}(E(x_{t}),s_{t}^{e},(a_{t})_{t}^{t+n})-E(x_{t+n+1})\\right\\|_{1}, |  | (39) |\n| --- | --- | --- | --- | --- |\n|  |  | = ‚à• P œï ( s t x , s t e , a t , a t + 1 ) ‚àí s t + 2 x ) ‚à• 1 , \\displaystyle=\\left\\|P_{\\phi}(s_{t}^{x},s_{t}^{e},a_{t},a_{t+1})-s_{t+2}^{x})\\right\\|_{1}, |  | (40) |\n|  |  | = ‚Äñ s ^ t + 2 x ‚àí s t + 2 x ‚Äñ 1 , where ‚Äã n = 1 . \\displaystyle=\\left\\|\\hat{s}_{t+2}^{x}-s_{t+2}^{x}\\right\\|_{1},~\\text{where }n=1. |  | (41) |\n\nHence, the total loss is\n\n|  | ‚Ñí AC ‚Äã ( œï ) = ‚Ñí TF ‚Äã ( œï ) + ‚Ñí rollout ‚Äã ( œï ) . \\mathcal{L}_{\\text{AC}}(\\phi)=\\mathcal{L}_{\\text{TF}}(\\phi)+\\mathcal{L}_{\\text{rollout}}(\\phi). |  | (42) |\n| --- | --- | --- | --- |\n\nInference\n\nWe can then perform energy-based planning after training, using the frozen encoder E E and predictor P P . This predictor acts as the world model, capable of imagining how latent representations evolve when an action sequence is applied. At test time, no weights are trained, instead, we search for an action sequence that minimizes a goal-conditioned energy cost between the imagined future and the goal latent representation.\n\nGiven the current observation x 1 x_{1} , current end-effector state s 1 e s_{1}^{e} , target goal image x 1 + T x_{1+T} , and planning horizon T T , we encode the current and goal observations as:\n\n|  | s 1 x = E ‚Äã ( x 1 ) , s 1 + T x = E ‚Äã ( x 1 + T ) . s_{1}^{x}=E(x_{1}),\\hskip 28.80008pts_{1+T}^{x}=E(x_{1+T}). |  | (43) |\n| --- | --- | --- | --- |\n\nWe then define the L 1 L_{1} energy cost function:\n\n|  | C ‚Äã ( ( a ^ t ) t = 1 T ; s 1 e , s 1 x , s 1 + T x ) = ‚Äñ P ‚Äã ( ( a ^ t ) t = 1 T ; s 1 e , s 1 x ) ‚àí s 1 + T x ‚Äñ 1 . C((\\hat{a}_{t})_{t=1}^{T};s_{1}^{e},s_{1}^{x},s_{1+T}^{x})=\\left\\|P((\\hat{a}_{t})_{t=1}^{T};s_{1}^{e},s_{1}^{x})-s_{1+T}^{x}\\right\\|_{1}. |  | (44) |\n| --- | --- | --- | --- |\n\nThe optimal action sequence is obtained by minimizing this cost:\n\n|  | ( a t ‚àó ) t = 1 T = arg ‚Å° min ( a ^ t ) t = 1 T ‚Å° C ‚Äã ( ( a ^ t ) t = 1 T ; s 1 e , s 1 x , s 1 + T x ) . (a_{t}^{*})_{t=1}^{T}=\\arg\\min_{(\\hat{a}_{t})_{t=1}^{T}}C((\\hat{a}_{t})_{t=1}^{T};s_{1}^{e},s_{1}^{x},s_{1+T}^{x}). |  | (45) |\n| --- | --- | --- | --- |\n\nThus, the predictor is used to imagine the future latent trajectory, and planning reduces to finding the action sequence that minimizes the latent L1 distance to the goal embedding.\n\nV-JEPA 2-AC uses the Cross-Entropy Method (CEM) [ 23 ] with 800 samples and 10 iterations to efficiently minimize C C at each planning step, as shown in Algorithm 1 . It executes only the first action on the robot before re-planning, as in receding horizon control, and is tested only on a horizon of T = 1 T=1 .\n\n[FIGURE_CAPTION] Algorithm 1 Energy-Based Planning with Cross-Entropy Method (CEM)\n\nIn other words, during inference, the predictor P P serves as a world model for energy-based planning , where at each step the CEM searches for an action sequence that minimizes the latent-space cost C C between the imagined future and the goal representation, executing only the first action a 1 ‚àó a_{1}^{*} before re-planning.\n\n### 1.4 World Modeling Paradigms\n\nThere are two typical approaches for goal-conditional world modeling: generative world models [ 46 , 59 , 63 ] and predictive world models [ 41 , 2 , 29 , 4 , 3 ] .\n\nGenerative world models.\n\nGenerative world models typically build upon autoregressive (AR) transformers or semi-AR (autoregressive diffusion) models œÅ \\rho that observe the visual context x t x_{t} and autoregressively predict the latent z t z_{t} and the next frame x ^ t + 1 \\hat{x}_{t+1} :\n\n|  | ( z t , x ^ t + 1 ) = œÅ ‚Äã ( x t ) . (z_{t},\\hat{x}_{t+1})=\\rho(x_{t}). |  | (46) |\n| --- | --- | --- | --- |\n\nThey often rely on an inverse dynamics model (IDM) œÄ \\pi [ 67 ] , trained separately, which maps the pair ( x t , z t , x ^ t + 1 ) (x_{t},z_{t},\\hat{x}_{t+1}) to an explicit action [ 80 , 10 , 73 ] :\n\n|  | a t = œÄ ‚Äã ( x t , z t , x ^ t + 1 ) . a_{t}=\\pi(x_{t},z_{t},\\hat{x}_{t+1}). |  | (47) |\n| --- | --- | --- | --- |\n\nIn other words, it‚Äôs a one-step inverse mapping of the environment‚Äôs dynamics. Hence, it can only predict one step at a time, because it does not know the full trajectory structure or the energy landscape over multiple steps.\n\nMoreover, it must explicitly generate or reconstruct the next frame (or latent visual tokens that decode into pixels), and perform planning by predicting how the world will look after an implicit action, which connects to the challenge mentioned in 1.3 .\n\nPredictive world models\n\nPredictive world models are not generative, they do not model pixel distributions. Instead, they learn an energy landscape in latent space that measures compatibility between current and target states.\n\nHowever, predictive world models require an explicit goal observation x t + 1 x_{t+1} to compute their goal-conditioned energy, as shown in Equation 44 , and minimize the energy cost with the CEM for planning.\n\nThis design trade-off is intentional, not accidental. As mentioned in 1.3 , we need to avoid pixel prediction during planning since pixels are noisy, unimportant, and computationally expensive [ 41 ] . Therefore, predictive world models are not intended to be self-contained simulators. Moreover, unlike the IDM, which predicts only a single action given consecutive states, CEM performs multi-step trajectory optimization by searching over candidate action sequences to minimize the latent-space energy cost, enabling long-horizon planning rather than one-step reactive control.\n\n### 1.5 Hyperbolic Learning\n\nHyperbolic space, denoted as ‚Ñç n \\mathbb{H}^{n} , is a negatively curved Riemannian manifold characterized by exponential volume growth and a saddle-shaped geometry [ 11 ] . Unlike Euclidean space, where parallel lines remain equidistant, lines in hyperbolic space diverge, and the volume expands exponentially with radius. This property makes hyperbolic geometry naturally suited for representing hierarchical or tree-like data structures [ 51 , 55 ] , such as hierarchical planning for world models, where the number of nodes grows exponentially with depth. In deep learning, hyperbolic space enables exponentially efficient representations of hierarchies by compressing large-scale differences while maintaining fine-grained local relationships. As a result, it has been widely applied in representation learning [ 51 , 83 , 25 , 55 ] , computer vision and graphics [ 30 ] that require modeling multi-level, non-Euclidean structures.\n\nHyperbolic space ‚Ñç n \\mathbb{H}^{n} is an abstract Riemannian manifold of constant negative curvature that does not depend on any coordinate system. In order to represent points in this curved space for computation, coordinate models are introduced to map ‚Ñç n \\mathbb{H}^{n} into Euclidean space while preserving its geometric structure. Two of the most common models are the Poincar√© ball model ùîπ n \\mathbb{B}^{n} [ 51 , 83 , 30 , 13 ] and the Lorentz (or hyperboloid) model ùïÉ n \\mathbb{L}^{n} [ 25 , 55 ] , both providing isometric representations of the same manifold but differing in their coordinate systems and numerical properties.\n\n#### 1.5.1 Poincar√© Ball Model.\n\nThe Poincar√© ball model ùîπ n \\mathbb{B}^{n} represents hyperbolic space as an open unit ball embedded in Euclidean space, defined as\n\n|  | ùîπ n = { z ‚àà ‚Ñù n : ‚Äñ z ‚Äñ < 1 } . \\mathbb{B}^{n}=\\{z\\in\\mathbb{R}^{n}:\\|z\\|<1\\}. |  | (48) |\n| --- | --- | --- | --- |\n\nIt is endowed with a Riemannian metric that encodes constant negative curvature, ensuring that Euclidean distances are reweighted to reflect hyperbolic geometry. Each point z z lies strictly inside the ball, and distances are measured using the hyperbolic metric rather than Euclidean norms. This bounded representation makes the geometry intuitive and well-suited for visualization, as tree-like hierarchies naturally fit inside a finite domain where the boundary corresponds to infinite distance.\nIt constrains embeddings to normalized radii, but note that in the Poincar√© model ‚Äúnormalization‚Äù means keeping points inside the unit ball, not unit-norm on a sphere (that corresponds to the hyperspherical case).\n\nGeodesics\n\nThe geodesic, or the Poincar√©-ball hyperbolic distance, between two points u , v ‚àà ùîπ n u,v\\in\\mathbb{B}^{n} is a circular arc perpendicular to the boundary of the ball.\nIts length is given by the hyperbolic distance function [ 51 , 28 ]\n\n|  | d ‚Ñç ‚Äã ( u , v ) = arcosh ‚Å° ( 1 + 2 ‚Äã ‚Äñ u ‚àí v ‚Äñ 2 ( 1 ‚àí ‚Äñ u ‚Äñ 2 ) ‚Äã ( 1 ‚àí ‚Äñ v ‚Äñ 2 ) ) . d_{\\mathbb{H}}(u,v)=\\operatorname{arcosh}\\!\\left(1+2\\frac{\\|u-v\\|^{2}}{(1-\\|u\\|^{2})(1-\\|v\\|^{2})}\\right). |  | (49) |\n| --- | --- | --- | --- |\n\nThis metric measures the shortest path along the curved manifold rather than in Euclidean space, capturing the exponential growth of distances as points approach the boundary of the Poincar√© ball.\n\nExponential Map\n\nIn Riemannian geometry, the exponential map\n\n|  | exp x : ùêì x ‚Äã ‚Ñç n ‚Üí ‚Ñç n \\exp_{x}:\\mathbf{T}_{x}\\mathbb{H}^{n}\\rightarrow\\mathbb{H}^{n} |  | (50) |\n| --- | --- | --- | --- |\n\ntakes a tangent vector v ‚àà ùêì x ‚Äã ‚Ñç n v\\in\\mathbf{T}_{x}\\mathbb{H}^{n} (the tangent space at point x x ) and moves it along the geodesic starting from x x in direction v v , traveling a distance equal to ‚Äñ v ‚Äñ \\|v\\| under the hyperbolic metric. In other words, exp x ‚Å° ( v ) \\exp_{x}(v) can be interpreted as starting at x x and walking along the manifold in the direction of v v for a distance ‚Äñ v ‚Äñ \\|v\\| . This operation maps local Euclidean updates v v into global manifold coordinates, ensuring that updates remain consistent with the geometry of hyperbolic space. In hyperbolic space, the mapping from the tangent space to the manifold through the exponential map is one-to-one. Although there exist manifolds equipped with hyperbolic metrics where this mapping is not one-to-one [ 85 ] , the Poincar√© model of hyperbolic space preserves this one-to-one correspondence, ensuring a well-defined relationship between the tangent space and the manifold.\n\nThe exponential map from the origin of the Poincar√© ball, denoted as exp 0 ‚Å° ( v ) \\exp_{0}(v) , maps Euclidean vectors directly into hyperbolic space, which is particularly useful for initialization. It is defined as\n\n|  | exp 0 ‚Å° ( v ) = tanh ‚Å° ( ‚Äñ v ‚Äñ ) ‚Äã v ‚Äñ v ‚Äñ . \\exp_{0}(v)=\\tanh(\\|v\\|)\\frac{v}{\\|v\\|}. |  | (51) |\n| --- | --- | --- | --- |\n\nThis formulation is simple because the tangent space at the origin aligns perfectly with the Euclidean space, making the mapping between Euclidean and hyperbolic representations straightforward.\n\nThe exponential map from a general point x ‚àà ùîπ n x\\in\\mathbb{B}^{n} must account for the curvature around x x . It moves the point x x along the geodesic in the direction of the tangent vector v v . The key difference from the origin case is that x x is not the origin, so we need a way to ‚Äúadd‚Äù v v to x x under hyperbolic geometry. In Euclidean space, moving from a point x x by a vector v v is simply computed as x + v x+v . However, in hyperbolic geometry, vector addition is replaced by M√∂bius addition , denoted as x ‚äï v x\\oplus v . Therefore, the general exponential map is expressed as\n\n|  | exp x ‚Å° ( v ) = x ‚äï ( tanh ‚Å° ( Œª x ‚Äã ‚Äñ v ‚Äñ 2 ) ‚Äã v ‚Äñ v ‚Äñ ) , \\exp_{x}(v)=x\\oplus\\left(\\tanh\\!\\left(\\frac{\\lambda_{x}\\|v\\|}{2}\\right)\\frac{v}{\\|v\\|}\\right), |  | (52) |\n| --- | --- | --- | --- |\n\nwhere\n\n|  | Œª x = 2 1 ‚àí ‚Äñ x ‚Äñ 2 \\lambda_{x}=\\frac{2}{1-\\|x\\|^{2}} |  | (53) |\n| --- | --- | --- | --- |\n\nis the conformal factor that rescales distances locally. This formulation ensures that the operation respects hyperbolic curvature instead of Euclidean linearity.\n\nThe M√∂bius addition of two vectors x x and y y is defined as\n\n|  | x ‚äï y = ( 1 + 2 ‚Äã ‚ü® x , y ‚ü© + ‚Äñ y ‚Äñ 2 ) ‚Äã x + ( 1 ‚àí ‚Äñ x ‚Äñ 2 ) ‚Äã y 1 + 2 ‚Äã ‚ü® x , y ‚ü© + ‚Äñ x ‚Äñ 2 ‚Äã ‚Äñ y ‚Äñ 2 . x\\oplus y=\\frac{(1+2\\langle x,y\\rangle+\\|y\\|^{2})x+(1-\\|x\\|^{2})y}{1+2\\langle x,y\\rangle+\\|x\\|^{2}\\|y\\|^{2}}. |  | (54) |\n| --- | --- | --- | --- |\n\nSubstituting y = tanh ‚Å° ( Œª x ‚Äã ‚Äñ v ‚Äñ 2 ) ‚Äã v ‚Äñ v ‚Äñ y=\\tanh\\!\\left(\\frac{\\lambda_{x}\\|v\\|}{2}\\right)\\frac{v}{\\|v\\|} , we obtain an explicit expression for exp x ‚Å° ( v ) \\exp_{x}(v) in hyperbolic space\n\n|  | exp x ‚Å° ( v ) = ( 1 + 2 ‚Äã Œ± ‚Äã ‚ü® x , v ‚Äñ v ‚Äñ ‚ü© + Œ± 2 ) ‚Äã x + ( 1 ‚àí ‚Äñ x ‚Äñ 2 ) ‚Äã Œ± ‚Äã v ‚Äñ v ‚Äñ 1 + 2 ‚Äã Œ± ‚Äã ‚ü® x , v ‚Äñ v ‚Äñ ‚ü© + Œ± 2 ‚Äã ‚Äñ x ‚Äñ 2 , \\exp_{x}(v)=\\frac{\\big(1+2\\alpha\\langle x,\\tfrac{v}{\\|v\\|}\\rangle+\\alpha^{2}\\big)x+(1-\\|x\\|^{2})\\alpha\\tfrac{v}{\\|v\\|}}{1+2\\alpha\\langle x,\\tfrac{v}{\\|v\\|}\\rangle+\\alpha^{2}\\|x\\|^{2}}, |  | (55) |\n| --- | --- | --- | --- |\n\nwhere\n\n|  | Œ± = tanh ‚Å° ( Œª x ‚Äã ‚Äñ v ‚Äñ 2 ) , Œª x = 2 1 ‚àí ‚Äñ x ‚Äñ 2 . \\alpha=\\tanh\\!\\left(\\frac{\\lambda_{x}\\|v\\|}{2}\\right),\\qquad\\lambda_{x}=\\frac{2}{1-\\|x\\|^{2}}. |  | (56) |\n| --- | --- | --- | --- |\n\nThis formulation explicitly shows how the exponential map combines the curvature-adjusted scaling (via Œª x \\lambda_{x} and tanh \\tanh ) with the non-linear composition of x x and v v under M√∂bius addition, ensuring consistency with hyperbolic geometry.\n\nLogarithmic Map\n\nThe logarithmic map serves as the inverse of the exponential map, mapping points from the manifold back to the tangent space at a given point x ‚àà ùîπ n x\\in\\mathbb{B}^{n} . Formally, it is defined as\n\n|  | log x : ùîπ n ‚Üí ùêì x ‚Äã ‚Ñç n , \\log_{x}:\\mathbb{B}^{n}\\rightarrow\\mathbf{T}_{x}\\mathbb{H}^{n}, |  | (57) |\n| --- | --- | --- | --- |\n\nwhich takes a point y ‚àà ùîπ n y\\in\\mathbb{B}^{n} and returns a tangent vector v ‚àà ùêì x ‚Äã ‚Ñç n v\\in\\mathbf{T}_{x}\\mathbb{H}^{n} that, when re-projected through the exponential map, satisfies exp x ‚Å° ( v ) = y \\exp_{x}(v)=y . This operation locally linearizes the manifold around x x , allowing differential computations such as gradient-based optimization to be performed in the tangent space.\n\nThe logarithmic map from the origin of the Poincar√© ball, denoted as log 0 ‚Å° ( y ) \\log_{0}(y) , converts a point y ‚àà ùîπ n y\\in\\mathbb{B}^{n} back to its Euclidean tangent vector and is defined as\n\n|  | log 0 ‚Å° ( y ) = arctanh ‚Å° ( ‚Äñ y ‚Äñ ) ‚Äã y ‚Äñ y ‚Äñ . \\log_{0}(y)=\\operatorname{arctanh}(\\|y\\|)\\frac{y}{\\|y\\|}. |  | (58) |\n| --- | --- | --- | --- |\n\nThis formulation is the inverse of the exponential map at the origin, satisfying exp 0 ‚Å° ( log 0 ‚Å° ( y ) ) = y \\exp_{0}(\\log_{0}(y))=y , and is computationally simple since the tangent space at the origin coincides with ‚Ñù n \\mathbb{R}^{n} .\n\nFor a general point x ‚àà ùîπ n x\\in\\mathbb{B}^{n} , the logarithmic map must account for the local curvature around x x . It can be expressed using the inverse of the M√∂bius addition:\n\n|  | log x ‚Å° ( y ) = 2 Œª x ‚Äã arctanh ‚Å° ( ‚Äñ ‚àí x ‚äï y ‚Äñ ) ‚Äã ‚àí x ‚äï y ‚Äñ ‚àí x ‚äï y ‚Äñ , \\log_{x}(y)=\\frac{2}{\\lambda_{x}}\\operatorname{arctanh}\\!\\big(\\|{-x}\\oplus y\\|\\big)\\frac{{-x}\\oplus y}{\\|{-x}\\oplus y\\|}, |  | (59) |\n| --- | --- | --- | --- |\n\nwhere\n\n|  | Œª x = 2 1 ‚àí ‚Äñ x ‚Äñ 2 \\lambda_{x}=\\frac{2}{1-\\|x\\|^{2}} |  | (60) |\n| --- | --- | --- | --- |\n\nis the same conformal factor as in the exponential map, and ‚àí x ‚äï y {-x}\\oplus y denotes M√∂bius addition with the inverse of x x .\n\nExpanding the definition of the M√∂bius addition into the logarithmic map expression, we obtain an explicit formulation of log x ‚Å° ( y ) \\log_{x}(y) in the Poincar√© ball model.\nSubstituting Eq. 54 into Eq. 59 , we first compute the intermediate term ‚àí x ‚äï y {-x}\\oplus y as\n\n|  | ‚àí x ‚äï y = ( 1 ‚àí 2 ‚Äã ‚ü® x , y ‚ü© + ‚Äñ y ‚Äñ 2 ) ‚Äã ( ‚àí x ) + ( 1 ‚àí ‚Äñ x ‚Äñ 2 ) ‚Äã y 1 ‚àí 2 ‚Äã ‚ü® x , y ‚ü© + ‚Äñ x ‚Äñ 2 ‚Äã ‚Äñ y ‚Äñ 2 . {-x}\\oplus y=\\frac{(1-2\\langle x,y\\rangle+\\|y\\|^{2})(-x)+(1-\\|x\\|^{2})y}{1-2\\langle x,y\\rangle+\\|x\\|^{2}\\|y\\|^{2}}. |  | (61) |\n| --- | --- | --- | --- |\n\nThen, the logarithmic map can be written explicitly as\n\n|  | log x ‚Å° ( y ) = 2 Œª x ‚Äã arctanh ‚Å° ( ‚Äñ N ‚Äã ( x , y ) ‚Äñ D ‚Äã ( x , y ) ) ‚Äã N ‚Äã ( x , y ) ‚Äñ N ‚Äã ( x , y ) ‚Äñ , \\log_{x}(y)=\\frac{2}{\\lambda_{x}}\\,\\operatorname{arctanh}\\!\\left(\\frac{\\|N(x,y)\\|}{D(x,y)}\\right)\\,\\frac{N(x,y)}{\\|N(x,y)\\|}, |  | (62) |\n| --- | --- | --- | --- |\n\nwhere\n\n|  | N ‚Äã ( x , y ) \\displaystyle N(x,y) | = ( 1 ‚àí 2 ‚Äã ‚ü® x , y ‚ü© + ‚Äñ y ‚Äñ 2 ) ‚Äã ( ‚àí x ) + ( 1 ‚àí ‚Äñ x ‚Äñ 2 ) ‚Äã y , \\displaystyle=\\big(1-2\\langle x,y\\rangle+\\|y\\|^{2}\\big)(-x)+\\big(1-\\|x\\|^{2}\\big)\\,y, |  | (63) |\n| --- | --- | --- | --- | --- |\n|  | D ‚Äã ( x , y ) \\displaystyle D(x,y) | = 1 ‚àí 2 ‚Äã ‚ü® x , y ‚ü© + ‚Äñ x ‚Äñ 2 ‚Äã ‚Äñ y ‚Äñ 2 , Œª x = 2 1 ‚àí ‚Äñ x ‚Äñ 2 . \\displaystyle=1-2\\langle x,y\\rangle+\\|x\\|^{2}\\|y\\|^{2},\\quad\\lambda_{x}=\\frac{2}{1-\\|x\\|^{2}}. |  | (64) |\n\nThis expanded form explicitly expresses the logarithmic map in terms of x x and y y , showing how the non-linear geometry of the Poincar√© ball modifies vector displacement through the M√∂bius addition. It provides the tangent vector at x x that points toward y y with a magnitude corresponding to the hyperbolic distance between them.\n\nThis formulation ensures that log x ‚Å° ( y ) \\log_{x}(y) returns a tangent vector at x x whose exponential map precisely recovers y y , i.e., exp x ‚Å° ( log x ‚Å° ( y ) ) = y \\exp_{x}(\\log_{x}(y))=y . Together, the exponential and logarithmic maps establish a smooth and invertible correspondence between the Euclidean tangent space and the curved manifold, enabling consistent optimization and representation learning in hyperbolic space.\n\nCurvature\n\nThe above formulation corresponds to the unit-curvature case, where the curvature is fixed as K = ‚àí 1 K=-1 .\nIn general, hyperbolic space has a constant negative curvature usually written as K = ‚àí c K=-c , where c > 0 c>0 .\nWhen c = 1 c=1 , the space has curvature ‚àí 1 -1 , which is the normalized convention adopted by most works.\nThe general form instead keeps c c as a free curvature parameter, so the ball‚Äôs radius becomes 1 / c 1/\\sqrt{c} .\nThis allows different degrees of curvature ‚Äî flatter when c ‚Üí 0 c\\to 0 , and more curved when c ‚Üí ‚àû c\\to\\infty .\n\n|  | ùîπ n = { z ‚àà ‚Ñù n : c ‚Äã ‚Äñ z ‚Äñ 2 < 1 } , \\mathbb{B}^{n}=\\{z\\in\\mathbb{R}^{n}:c\\|z\\|^{2}<1\\}, |  | (65) |\n| --- | --- | --- | --- |\n\nwhere the radius is 1 / c 1/\\sqrt{c} and curvature K = ‚àí c K=-c .\nThe Riemannian metric is scaled by the conformal factor\n\n|  | Œª x = 2 1 ‚àí c ‚Äã ‚Äñ x ‚Äñ 2 , \\lambda_{x}=\\frac{2}{1-c\\|x\\|^{2}}, |  | (66) |\n| --- | --- | --- | --- |\n\nwhich defines the metric tensor as\n\n|  | ùêÜ x = Œª x 2 ‚Äã ùêà . \\mathbf{G}_{x}=\\lambda_{x}^{2}\\mathbf{I}. |  | (67) |\n| --- | --- | --- | --- |\n\nThe general form of the Poincar√©-ball hyperbolic distance under curvature K = ‚àí c K=-c ( c > 0 ) (c>0) between two points u , v ‚àà ùîπ n u,v\\in\\mathbb{B}^{n} is given by [ 13 ]\n\n|  | d ‚Ñç ‚Äã ( u , v ) = 1 c ‚Äã arcosh ‚Å° ( 1 + 2 ‚Äã c ‚Äã ‚Äñ u ‚àí v ‚Äñ 2 ( 1 ‚àí c ‚Äã ‚Äñ u ‚Äñ 2 ) ‚Äã ( 1 ‚àí c ‚Äã ‚Äñ v ‚Äñ 2 ) ) . d_{\\mathbb{H}}(u,v)=\\frac{1}{\\sqrt{c}}\\operatorname{arcosh}\\!\\left(1+2c\\frac{\\|u-v\\|^{2}}{(1-c\\|u\\|^{2})(1-c\\|v\\|^{2})}\\right). |  | (68) |\n| --- | --- | --- | --- |\n\nThe exponential map with the curvature c c is expressed as\n\n|  | exp x ‚Å° ( v ) = x ‚äï c ( tanh ‚Å° ( c ‚Äã Œª x ‚Äã ‚Äñ v ‚Äñ 2 ) ‚Äã v c ‚Äã ‚Äñ v ‚Äñ ) , \\exp_{x}(v)=x\\oplus_{c}\\left(\\tanh\\!\\left(\\frac{\\sqrt{c}\\,\\lambda_{x}\\|v\\|}{2}\\right)\\frac{v}{\\sqrt{c}\\,\\|v\\|}\\right), |  | (69) |\n| --- | --- | --- | --- |\n\nwhere\n\n|  | Œª x = 2 1 ‚àí c ‚Äã ‚Äñ x ‚Äñ 2 , \\lambda_{x}=\\frac{2}{1-c\\|x\\|^{2}}, |  | (70) |\n| --- | --- | --- | --- |\n\nand ‚äï c \\oplus_{c} denotes the M√∂bius addition under curvature c c .\n\nThe M√∂bius addition of two vectors x x and y y under curvature c c is defined as [ 28 , 13 ]\n\n|  | x ‚äï c y = ( 1 + 2 ‚Äã c ‚Äã ‚ü® x , y ‚ü© + c ‚Äã ‚Äñ y ‚Äñ 2 ) ‚Äã x + ( 1 ‚àí c ‚Äã ‚Äñ x ‚Äñ 2 ) ‚Äã y 1 + 2 ‚Äã c ‚Äã ‚ü® x , y ‚ü© + c 2 ‚Äã ‚Äñ x ‚Äñ 2 ‚Äã ‚Äñ y ‚Äñ 2 . x\\oplus_{c}y=\\frac{(1+2c\\langle x,y\\rangle+c\\|y\\|^{2})x+(1-c\\|x\\|^{2})y}{1+2c\\langle x,y\\rangle+c^{2}\\|x\\|^{2}\\|y\\|^{2}}. |  | (71) |\n| --- | --- | --- | --- |\n\nSo the explicit expression for exp x ‚Å° ( v ) \\exp_{x}(v) in hyperbolic space with curvature c c is given by\n\n|  | exp x ‚Å° ( v ) = ( 1 + 2 ‚Äã c ‚Äã Œ± ‚Äã ‚ü® x , v ‚Äñ v ‚Äñ ‚ü© + c ‚Äã Œ± 2 ‚Äã ‚Äñ v ‚Äñ 2 ) ‚Äã x + ( 1 ‚àí c ‚Äã ‚Äñ x ‚Äñ 2 ) ‚Äã Œ± ‚Äã v 1 + 2 ‚Äã c ‚Äã Œ± ‚Äã ‚ü® x , v ‚Äñ v ‚Äñ ‚ü© + c 2 ‚Äã Œ± 2 ‚Äã ‚Äñ x ‚Äñ 2 ‚Äã ‚Äñ v ‚Äñ 2 , \\exp_{x}(v)=\\frac{\\big(1+2c\\alpha\\langle x,\\tfrac{v}{\\|v\\|}\\rangle+c\\alpha^{2}\\|v\\|^{2}\\big)x+(1-c\\|x\\|^{2})\\alpha v}{1+2c\\alpha\\langle x,\\tfrac{v}{\\|v\\|}\\rangle+c^{2}\\alpha^{2}\\|x\\|^{2}\\|v\\|^{2}}, |  | (72) |\n| --- | --- | --- | --- |\n\nwhere\n\n|  | Œ± = tanh ‚Å° ( c ‚Äã Œª x ‚Äã ‚Äñ v ‚Äñ 2 ) , Œª x = 2 1 ‚àí c ‚Äã ‚Äñ x ‚Äñ 2 . \\alpha=\\tanh\\!\\left(\\frac{\\sqrt{c}\\,\\lambda_{x}\\|v\\|}{2}\\right),\\qquad\\lambda_{x}=\\frac{2}{1-c\\|x\\|^{2}}. |  | (73) |\n| --- | --- | --- | --- |\n\nFor a general curvature K = ‚àí c K=-c ( c > 0 ) (c>0) , the logarithmic map must account for the local curvature around x x . It can be expressed using the inverse of the M√∂bius addition as\n\n|  | log x ‚Å° ( y ) = 2 Œª x ‚Äã c ‚Äã arctanh ‚Å° ( c ‚Äã ‚Äñ ‚àí x ‚äï c y ‚Äñ ) ‚Äã ‚àí x ‚äï c y ‚Äñ ‚àí x ‚äï c y ‚Äñ , \\log_{x}(y)=\\frac{2}{\\lambda_{x}\\sqrt{c}}\\,\\operatorname{arctanh}\\!\\big(\\sqrt{c}\\|{-x}\\oplus_{c}y\\|\\big)\\frac{{-x}\\oplus_{c}y}{\\|{-x}\\oplus_{c}y\\|}, |  | (74) |\n| --- | --- | --- | --- |\n\nwhere\n\n|  | Œª x = 2 1 ‚àí c ‚Äã ‚Äñ x ‚Äñ 2 \\lambda_{x}=\\frac{2}{1-c\\|x\\|^{2}} |  | (75) |\n| --- | --- | --- | --- |\n\nis the conformal factor, and ‚àí x ‚äï c y {-x}\\oplus_{c}y denotes the M√∂bius addition under curvature ‚àí c -c .\n\nExpanding the definition of M√∂bius addition into the logarithmic map expression, we obtain an explicit formulation of log x ‚Å° ( y ) \\log_{x}(y) in the general Poincar√© ball model with curvature ‚àí c -c . Substituting the c c -dependent M√∂bius addition (Eq. 71 )\ninto Eq. 74 , we first compute the intermediate term ‚àí x ‚äï c y {-x}\\oplus_{c}y as\n\n|  | ‚àí x ‚äï c y = ( 1 ‚àí 2 ‚Äã c ‚Äã ‚ü® x , y ‚ü© + c ‚Äã ‚Äñ y ‚Äñ 2 ) ‚Äã ( ‚àí x ) + ( 1 ‚àí c ‚Äã ‚Äñ x ‚Äñ 2 ) ‚Äã y 1 ‚àí 2 ‚Äã c ‚Äã ‚ü® x , y ‚ü© + c 2 ‚Äã ‚Äñ x ‚Äñ 2 ‚Äã ‚Äñ y ‚Äñ 2 . {-x}\\oplus_{c}y=\\frac{(1-2c\\langle x,y\\rangle+c\\|y\\|^{2})(-x)+(1-c\\|x\\|^{2})y}{1-2c\\langle x,y\\rangle+c^{2}\\|x\\|^{2}\\|y\\|^{2}}. |  | (76) |\n| --- | --- | --- | --- |\n\nThen, the logarithmic map can be written explicitly as\n\n|  | log x ‚Å° ( y ) = 2 Œª x ‚Äã c ‚Äã arctanh ‚Å° ( c ‚Äã ‚Äñ N c ‚Äã ( x , y ) ‚Äñ D c ‚Äã ( x , y ) ) ‚Äã N c ‚Äã ( x , y ) ‚Äñ N c ‚Äã ( x , y ) ‚Äñ , \\log_{x}(y)=\\frac{2}{\\lambda_{x}\\sqrt{c}}\\,\\operatorname{arctanh}\\!\\left(\\sqrt{c}\\frac{\\|N_{c}(x,y)\\|}{D_{c}(x,y)}\\right)\\,\\frac{N_{c}(x,y)}{\\|N_{c}(x,y)\\|}, |  | (77) |\n| --- | --- | --- | --- |\n\nwhere\n\n|  | N c ‚Äã ( x , y ) \\displaystyle N_{c}(x,y) | = ( 1 ‚àí 2 ‚Äã c ‚Äã ‚ü® x , y ‚ü© + c ‚Äã ‚Äñ y ‚Äñ 2 ) ‚Äã ( ‚àí x ) + ( 1 ‚àí c ‚Äã ‚Äñ x ‚Äñ 2 ) ‚Äã y , \\displaystyle=\\big(1-2c\\langle x,y\\rangle+c\\|y\\|^{2}\\big)(-x)+\\big(1-c\\|x\\|^{2}\\big)\\,y, |  | (78) |\n| --- | --- | --- | --- | --- |\n|  | D c ‚Äã ( x , y ) \\displaystyle D_{c}(x,y) | = 1 ‚àí 2 ‚Äã c ‚Äã ‚ü® x , y ‚ü© + c 2 ‚Äã ‚Äñ x ‚Äñ 2 ‚Äã ‚Äñ y ‚Äñ 2 , Œª x = 2 1 ‚àí c ‚Äã ‚Äñ x ‚Äñ 2 . \\displaystyle=1-2c\\langle x,y\\rangle+c^{2}\\|x\\|^{2}\\|y\\|^{2},\\quad\\lambda_{x}=\\frac{2}{1-c\\|x\\|^{2}}. |  | (79) |\n\nWhen c = 1 c=1 , this expression reduces to the unit-curvature form of the logarithmic map given in Eq. 59 .\n\nLorentz Model.\n\nThe Lorentz model ùïÉ n \\mathbb{L}^{n} represents hyperbolic space as the upper sheet of a two-sheeted hyperboloid embedded in Minkowski space ‚Ñù n + 1 \\mathbb{R}^{n+1} , defined as\n\n|  | ùïÉ n = { p ‚àà ‚Ñù n + 1 : ‚ü® p , p ‚ü© L = ‚àí 1 / Œ∫ , p 0 > 0 } , \\mathbb{L}^{n}=\\{p\\in\\mathbb{R}^{n+1}:\\langle p,p\\rangle_{L}=-1/\\kappa,\\,p_{0}>0\\}, |  | (80) |\n| --- | --- | --- | --- |\n\nwhere ‚ü® p , q ‚ü© L = ‚àí p 0 ‚Äã q 0 + ‚àë i = 1 n p i ‚Äã q i \\langle p,q\\rangle_{L}=-p_{0}q_{0}+\\sum_{i=1}^{n}p_{i}q_{i} is the Lorentzian inner product. This formulation is unbounded and algebraically convenient, allowing closed-form computation of geodesic distances and stable gradient optimization. Because of its numerical robustness and simple analytical expressions for exponential and logarithmic maps, the Lorentz model is widely adopted in hyperbolic representation learning, particularly in entailment-based and hierarchical vision-language models.\n\n### 1.6 Value Function\n\nReinforcement learning (RL) is about learning to act in an environment so as to maximize future reward.\nThe value function in RL estimates the expected cumulative future reward that an agent can obtain from a particular state or state-action pair [ 70 ] . It helps the agent decide which actions are more desirable in the long run, guiding it towards making decisions that maximize its total reward over time.\nThere are two main types: state-value function V ‚Äã ( s ) V(s) , which predicts the value of a given state, and state‚Äìaction value function V ‚Äã ( s , a ) V(s,a) , which predicts the value of taking a specific action in a given state.\n\nState Value Function.\n\nThe value function V ‚Äã ( s ) V(s) represents the expected return (cumulative reward) starting from a specific state s s and following a particular policy, which defines the agent‚Äôs strategy for choosing actions. It indicates how good it is for the agent to be in a certain state. For example, if an agent is located at a particular position in a maze, the value function V ‚Äã ( s ) V(s) represents the expected total reward it will obtain from that point until it reaches the target, assuming it continues to follow its current policy.\n\nState-Action Value Function.\n\nThe value function V ‚Äã ( s , a ) V(s,a) or Q ‚Äã ( s , a ) Q(s,a) , also known as the action value function, represents the expected return (cumulative reward) starting from a specific state s s , taking an action a a , and subsequently following a particular policy. It indicates how good it is for the agent to take a specific action in a given state. For example, if an agent is at a particular position in a maze, the function V ‚Äã ( s , a ) V(s,a) represents the expected total reward it will obtain by choosing a particular action at that point and then following its current policy until it reaches the target.\n\nFormal Definition.\n\nGiven a state s t s_{t} , an action a t a_{t} , a policy œÄ ‚Äã ( a ‚à£ s ) \\pi(a\\mid s) that defines how the agent acts, a reward function r ‚Äã ( s , a ) r(s,a) , and a discount factor Œ≥ ‚àà [ 0 , 1 ) \\gamma\\in[0,1) , the state-value function under policy œÄ \\pi is formally defined as\n\n|  | V œÄ ( s t ) = ùîº œÄ [ ‚àë k = 0 ‚àû Œ≥ k r ( s t + k , a t + k ) | s t ] . V^{\\pi}(s_{t})=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}\\,r(s_{t+k},a_{t+k})\\,\\middle|\\,s_{t}\\right]. |  | (81) |\n| --- | --- | --- | --- |\n\nIt measures the expected cumulative reward that an agent will receive when starting from state s t s_{t} and following policy œÄ \\pi thereafter.\n\nOptimal Value Function.\n\nIf the agent follows the best possible policy that maximizes the expected reward, we obtain the optimal value function :\n\n|  | V ‚àó ( s t ) = max œÄ ùîº œÄ [ ‚àë k = 0 ‚àû Œ≥ k r ( s t + k , a t + k ) | s t ] . V^{*}(s_{t})=\\max_{\\pi}\\,\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}\\,r(s_{t+k},a_{t+k})\\,\\middle|\\,s_{t}\\right]. |  | (82) |\n| --- | --- | --- | --- |\n\nIntuitively, V ‚àó ‚Äã ( s t ) V^{*}(s_{t}) quantifies the maximum expected cumulative reward that an agent can achieve when starting from state s t s_{t} and following an optimal policy thereafter.\n\nBellman Optimality Equation.\n\nThe value function encodes the long-term consequences of actions and forms the foundation of reasoning in reinforcement learning [ 6 ] . Once V ‚àó ‚Äã ( s ) V^{*}(s) is known, the optimal policy can be derived if the one-step transition dynamics P ‚Äã ( s ‚Ä≤ ‚à£ s , a ) P(s^{\\prime}\\mid s,a) are available. Specifically, the optimal policy œÄ ‚àó \\pi^{*} satisfies the Bellman optimality equation:\n\n|  | œÄ ‚àó ‚Äã ( s ) = arg ‚Å° max a ‚Å° [ r ‚Äã ( s , a ) + Œ≥ ‚Äã ‚àë s ‚Ä≤ P ‚Äã ( s ‚Ä≤ ‚à£ s , a ) ‚Äã V ‚àó ‚Äã ( s ‚Ä≤ ) ] . \\pi^{*}(s)=\\arg\\max_{a}\\left[r(s,a)+\\gamma\\sum_{s^{\\prime}}P(s^{\\prime}\\mid s,a)\\,V^{*}(s^{\\prime})\\right]. |  | (83) |\n| --- | --- | --- | --- |\n\nThis recursive relationship expresses how the value of a state depends on the values of its successor states, thereby capturing the essence of sequential decision-making.\n\nInterpretation.\n\nThe optimal value function V ‚àó ‚Äã ( s ) V^{*}(s) can be viewed as a potential field or energy map over the state space. States with high value correspond to desirable or low-energy configurations that are closer to reward, whereas states with low value represent undesirable or high-energy configurations that are further away. From this perspective, acting optimally can be interpreted as following the gradient of the value landscape toward regions of higher value (or lower energy). This analogy bridges reinforcement learning with energy-based modeling, suggesting that value functions implicitly define an energy surface that guides the agent toward optimal behavior.\n\nPath Value Function.\n\nIn generic RL settings above, the value depends on future rewards under the optimal policy. But if we redefine reward as the negative energy cost ‚àí c -c of moving between states:\n\n|  | r ‚Äã ( s , a , s ‚Ä≤ ) = ‚àí c ‚Äã ( s , s ‚Ä≤ ) , r(s,a,s^{\\prime})=-c(s,s^{\\prime}), |  | (84) |\n| --- | --- | --- | --- |\n\nand the cumulative reward becomes the total negative cost, then the optimal path value function V ‚àó ‚Äã ( s , s ‚Ä≤ ) V^{*}(s,s^{\\prime}) corresponds to the negative of the minimum accumulated cost from s s to s ‚Ä≤ s^{\\prime} [ 19 ] .\n\nIn this case, the optimal path value function V ‚àó ‚Äã ( s , s ‚Ä≤ ) V^{*}(s,s^{\\prime}) obeys triangle inequality [ 57 , 21 , 49 ] :\n\n|  | c ‚Äã ( s 1 , s 3 ) \\displaystyle c(s_{1},s_{3}) | ‚â§ c ‚Äã ( s 1 , s 2 ) + c ‚Äã ( s 2 , s 3 ) \\displaystyle\\leq c(s_{1},s_{2})+c(s_{2},s_{3}) |  |\n| --- | --- | --- | --- |\n|  | ‚áí V ‚àó ‚Äã ( s 1 , s 3 ) \\displaystyle\\Rightarrow\\quad V^{*}(s_{1},s_{3}) | ‚â• V ‚àó ‚Äã ( s 1 , s 2 ) + V ‚àó ‚Äã ( s 2 , s 3 ) . \\displaystyle\\geq V^{*}(s_{1},s_{2})+V^{*}(s_{2},s_{3}). |  | (85) |\n\n## 2 Motivation\n\nWorld state transitions (from video observations) naturally form a hierarchical structure that is suitable represented in hyperbolic space. Let s t s_{t} denote the state at time t t and ùíú \\mathcal{A} be a discrete action set with cardinality | ùíú | = B |\\mathcal{A}|=B . The world evolves according to the transition s t + 1 = f ‚Äã ( s t , a t ) s_{t+1}=f(s_{t},a_{t}) , where a t ‚àà ùíú a_{t}\\in\\mathcal{A} . When predicting d d steps into the future, each action choice produces a distinct future trajectory, resulting in N d = B d N_{d}=B^{d} possible future states. These futures form a exponentially branching tree, where the depth corresponds to the prediction horizon and the branching factor is determined by the action space. As a result, future world states are naturally organized hierarchically: states at smaller depths represent coarse, high-level abstractions, while states at larger depths correspond to finer, more detailed futures. Similar motivations are also supported by [ 64 ] .\n\n## 3 Baseline Details\n\nAs mentioned in Section 4.2 of main content, in both Procedural Planning (PP) and Visual Planning with Videos setup, we evaluate against three categories of baselines. LLM-based approaches rely on large language or vision-language models for reasoning, instruction following, and multi-step planning. Generative (world) models perform planning by generating pixels or latent video tokens and using visual rollouts to guide decision-making. Predictive (world) models focus purely on action prediction, estimating future action sequences directly without generating visual frames.\n\nRandom Selection. Following prior work [ 15 ] , actions are sampled uniformly at random from the available action set to form a plan, without considering the task context.\n\nRetrieval-Based. Following prior work [ 87 ] , given the start and goal observations, this method retrieves the most similar trajectory from the training set by minimizing visual feature distance. The corresponding action sequence from the retrieved example is then used as the predicted plan.\n\nLLM-based\n\nLFP (Language-First Planning) [ 42 ] . This method first converts both the start and goal observations into text and then prompts a large language model to infer the missing steps. The LLM predicts a sequence of intermediate actions based solely on language reasoning rather than visual planning.\n\nVidAssist [ 36 ] . This method uses a vision-language model to extract temporal and spatial cues from the video, then queries a large language model to interpret these cues and generate an action sequence. The LLM refines and structures the predicted steps into a coherent plan, combining visual grounding with language-based reasoning.\n\nSCHEMA [ 52 ] . This method performs procedure planning by modeling how states evolve over time. It aligns visual observations with textual state descriptions through cross-modal contrastive learning and uses a transformer backbone to represent state transitions. A large language model is then used to reason over these inferred intermediate states and generate the next actions, enabling structured step-by-step planning in instructional video settings.\n\nOther VLMs. We also evaluate several large vision-language models, including InternVL3.5-241B [ 76 ] , Qwen3-VL-Max [ 79 ] , Gemini 2.5 Pro [ 20 ] , and GPT-5 [ 53 ] , using them in a zero-shot setting to perform visual reasoning and planning directly from video observations without task-specific training.\n\nGenerative (World) Models\n\nDDN [ 15 ] . This approach uses an autoregressive structure with two coordinated branches: one learns a compact representation of action steps, while the other predicts transitions in the latent feature space. By forecasting the next visual state rather than directly selecting actions, DDN models procedural progression through iterative frame prediction.\n\nInt-MGAIL and Ext-MGAIL [ 7 ] . These generative models perform procedure planning by jointly learning a latent world model and an action policy through adversarial training, enabling multi-step action synthesis conditioned on visual goal states.\n\nP 3 IV [ 86 ] . This transformer-based model uses a learnable memory module together with an adversarial generation setup, and, similar to our method, outputs all action steps in a single forward pass rather than generating them sequentially.\n\nPDPP [ 75 ] . This two-branch diffusion-based framework models temporal dependencies and action transitions, generating the full action sequence in parallel and progressively refining it over multiple denoising stages to improve coherence and logical structure.\n\nKEPP [ 50 ] . This method incorporates structured procedural knowledge through a probabilistic knowledge graph learned from training plans, which serves as external guidance for step ordering. KEPP predicts the full action sequence in a single pass with limited supervision, producing strong performance in instructional video planning.\n\nActionDiffusion [ 61 ] . This diffusion-based approach generates the full action sequence by iteratively denoising a latent representation, allowing the model to refine predictions over multiple steps and capture long-term dependencies in instructional procedures.\n\nMTID [ 87 ] . This model treats procedure planning as a multimodal trajectory generation problem, using a diffusion-based latent policy to synthesize complete action sequences conditioned on video observations while modeling long-term dependencies through iterative denoising.\n\nVideoWorld [ 59 ] . This autoregressive framework generates future video frames step by step to model procedural progression, using predicted visual states to implicitly guide the unfolding action sequence.\n\nPredictive (World) Models\n\nWLTDO [ 27 ] . This recurrent neural network model generates action sequences directly from paired observations, using temporal reasoning over the encoded features to predict ordered procedural steps.\n\nUAAA [ 1 ] . This two-stage method predicts action steps autoregressively by combining an RNN with a hidden Markov model to model temporal uncertainty and step transitions in procedural tasks.\n\nUPN [ 68 ] . This method learns a differentiable latent space suitable for planning by predicting trajectories in feature space, and a softmax output layer is used to convert the continuous plan representation into discrete action steps.\n\nPlaTe [ 69 ] . This model builds on DDN by introducing transformer modules into its dual-branch architecture for action and state prediction, but follows a distinct evaluation protocol compared to other procedure planning methods.\n\nE3P [ 74 ] . This method adopts an event-centric formulation, inferring latent events from visual observations and using them to guide intermediate action prediction. Through event-aware prompting and action relation modeling, E3P improves the logical structure of predicted steps and achieves strong performance on procedural planning benchmarks.\n\nV-JEPA 2 [ 3 ] . A large-scale predictive world model pretrained on masked latent feature prediction over one million hours of unlabeled video. Action-conditioned post-training enables autoregressive rollouts for planning without pixel generation.\n\n## 4 Energy Landscape\n\nTo better illustrate the difference between Euclidean predictive world models and our\nhyperbolic formulation, we visualize the energy landscape around a given latent\nstate.\n\nŒî ‚Äã x \\Delta x and Œî ‚Äã y \\Delta y .\n\nIn the original V-JEPA 2-AC setup [ 3 ] , Œî ‚Äã x \\Delta x and Œî ‚Äã y \\Delta y represent physical end-effector offsets in Cartesian coordinates. The visualization shows how the model‚Äôs energy changes as the end-effector‚Äôs target position varies along the Œî ‚Äã x \\Delta x and Œî ‚Äã y \\Delta y axes while keeping the vertical displacement fixed ( Œî ‚Äã z = 0 \\Delta z=0 ).\n\nFormally, the plotted quantity is:\n\n|  | s t + 1 hyp = s t + ( Œî ‚Äã x , Œî ‚Äã y , 0 ) , \\displaystyle s_{t+1}^{\\mathrm{hyp}}=s_{t}+(\\Delta x,\\Delta y,0), |  | (86) |\n| --- | --- | --- | --- |\n|  | Energy ‚Äã ( Œî ‚Äã x , Œî ‚Äã y ) = c ‚Äã ( s t , s t + 1 hyp ) . \\displaystyle\\text{Energy}(\\Delta x,\\Delta y)=c(s_{t},s_{t+1}^{\\mathrm{hyp}}). |  | (87) |\n\nwhere c c denotes the energy cost defined in Eq. 11 .\n\nIn visual planning, Œî ‚Äã x \\Delta x and Œî ‚Äã y \\Delta y are no longer physical displacements. Instead, they represent latent displacements that probe the local geometry of the world model around a visual state.\n\nIn the Euclidean space, the encoder maps an observation x t x_{t} into a latent vector s t x ‚àà ‚Ñù n s_{t}^{x}\\in\\mathbb{R}^{n} . To visualize how the model evaluates hypothetical future\nstates, V-JEPA 2 [ 3 ] perturbs the latent representation along two Euclidean axes.\nWe choose two orthonormal directions in latent space, u 1 , u 2 ‚àà ‚Ñù n u_{1},u_{2}\\in\\mathbb{R}^{n} . A natural, semantically aligned choice is:\n\n|  | u 1 = E Œ∏ ‚Äã ( x t + T ) ‚àí E Œ∏ ‚Äã ( x t ) ‚Äñ E Œ∏ ‚Äã ( x t + T ) ‚àí E Œ∏ ‚Äã ( x t ) ‚Äñ , u_{1}=\\frac{E_{\\theta}(x_{t+T})-E_{\\theta}(x_{t})}{\\left\\lVert E_{\\theta}(x_{t+T})-E_{\\theta}(x_{t})\\right\\rVert}, |  | (88) |\n| --- | --- | --- | --- |\n\nwhich represents the direction from the current state toward the goal (i.e., progress along the procedure).\n\nThe second direction, u 2 u_{2} , spans variations orthogonal to this progress direction (i.e., sampled from another trajectory at the same step and then orthonormalized against u 1 u_{1} ).\n\nThen, a hypothetical next latent state is defined as\n\n|  | s t + 1 hyp = s t x + Œî ‚Äã x ‚Äã u 1 + Œî ‚Äã y ‚Äã u 2 , s^{\\text{hyp}}_{t+1}=s^{x}_{t}+\\Delta x\\,u_{1}+\\Delta y\\,u_{2}, |  | (89) |\n| --- | --- | --- | --- |\n\nand the corresponding energy landscape is\n\n|  | Energy ‚Äã ( Œî ‚Äã x , Œî ‚Äã y ) = ‚Äñ s t + 1 x ‚àí s t + 1 hyp ‚Äñ . \\text{Energy}(\\Delta x,\\Delta y)=\\left\\lVert s^{x}_{t+1}-s^{\\text{hyp}}_{t+1}\\right\\rVert. |  | (90) |\n| --- | --- | --- | --- |\n\nIn GeoWorld, the encoder maps each observation x t x_{t} to a latent representation s t , ‚Ñç x s_{t,\\mathbb{H}}^{x} on the hyperbolic manifold. To probe the local geometry around\nthis latent state, we sweep two orthonormal directions in the tangent space ùêì 0 ‚Äã ‚Ñç n \\mathbf{T}_{0}\\mathbb{H}^{n} , denoted as ( Œî ‚Äã x , Œî ‚Äã y ) (\\Delta x,\\Delta y) .\nEach coordinate pair ( Œî ‚Äã x , Œî ‚Äã y ) (\\Delta x,\\Delta y) corresponds to a small displacement applied at the tangent space before projection onto the manifold via the exponential map:\n\n|  | s t + 1 , ‚Ñç hyp = exp 0 ‚Å° ( s t x + Œî ‚Äã x ‚Äã u 1 + Œî ‚Äã y ‚Äã u 2 ) , s^{\\text{hyp}}_{t+1,\\mathbb{H}}=\\exp_{0}\\!\\big(s_{t}^{x}+\\Delta x\\,u_{1}+\\Delta y\\,u_{2}\\big), |  | (91) |\n| --- | --- | --- | --- |\n\nwhere u 1 u_{1} and u 2 u_{2} form an orthonormal basis in ùêì 0 ‚Äã ‚Ñç n \\mathbf{T}_{0}\\mathbb{H}^{n} .\nThus, ( Œî ‚Äã x , Œî ‚Äã y ) (\\Delta x,\\Delta y) describes local perturbations of the latent state ,\nnot pixel space offsets.\n\nAnd the energy landscape is\n\n|  | Energy ‚Ñç ‚Äã ( Œî ‚Äã x , Œî ‚Äã y ) = d ‚Ñç ‚Äã ( s t + 1 , ‚Ñç x , s t + 1 , ‚Ñç hyp ) . \\text{Energy}_{\\mathbb{H}}(\\Delta x,\\Delta y)=d_{\\mathbb{H}}\\!\\left(s^{x}_{t+1,\\mathbb{H}},\\;s^{\\text{hyp}}_{t+1,\\mathbb{H}}\\right). |  | (92) |\n| --- | --- | --- | --- |\n\nVisualization.\n\nIn Figure 1 , we select a reference latent state s t s_{t} from the initial step of the Replace Memory Chip task in the COIN dataset [ 71 ] , and visualize the local energy geometry by sweeping two orthonormal tangent-space directions ( Œî ‚Äã x , Œî ‚Äã y ) (\\Delta x,\\Delta y) around this state.\nFigure 1 compares the Euclidean (left) and\nhyperbolic (right) landscapes. The Euclidean surface shows a smooth, nearly symmetric\nparaboloid with weak directional structure, indicating that V-JEPA¬†2 treats perturbations\nhomogeneously. In contrast, the hyperbolic surface in GeoWorld forms a sharper,\ncurvature-aware basin with more pronounced directional variation. This reflects the\nability of H-JEPA to encode hierarchical structure: states positioned higher in the task\nhierarchy lie at hyperbolically greater distances, creating more informative energy\ngradients during planning.\n\nSuch curvature-aware energy landscapes promote more stable long-horizon planning:\nCEM naturally follows the hyperbolic geodesics shaped by GeoWorld, resulting in more\naccurate multi-step trajectory optimization.\n\n[FIGURE:hyperbolicity.png] Figure 1 : Gromov Œ¥ \\delta -hyperbolicity on CrossTask [ 88 ] .\n\n## 5 Ablation Study\n\n[FIGURE:x4.png] (a) Curvature and geodesics.\n\nCurvature\n\nAs discussed in Section 4.3 , the curvature K = ‚àí c K=-c is learned in the logarithmic space by optimizing log ‚Å° ( c ) \\log(c) , which is initialized at c = 1 c=1 and treated as a learnable scalar. This formulation ensures that c c remains positive and stabilizes the gradients of both the hyperbolic distance and the exponential map [ 14 , 25 ] . The learned curvature is further clamped to the range [ 0.1 , 10.0 ] [0.1,\\,10.0] to prevent training instability.\n\nWe analyze how the learnable curvature evolves during training and how it influences geometric planning quality. As shown in Fig. 2 (d), the curvature parameter c c in GeoWorld typically starts near 1 1 and gradually decreases to a stable value around 0.3 0.3 , indicating that the model learns a flatter yet still hyperbolic latent geometry. A smaller curvature reduces distortion in the exponential map and leads to more stable multi-step planning, especially for larger backbone encoders. The geometric effect of curvature is further visualized in Fig. 2 (a)‚Äì(c): as c c decreases, geodesic paths bend less aggressively toward the origin (Fig. 2 (a)), boundary-anchored geodesic patterns become flatter (Fig. 2 (b)), and the hyperbolic distance between x x and y y contracts smoothly as curvature approaches zero (Fig. 2 (c)). This suggests that moderate negative curvature is sufficient to capture hierarchical structure while preserving stable value propagation across long planning horizons.\n\nGromov Œ¥ \\delta -Hyperbolicity\n\nWe visualize Gromov Œ¥ \\delta -hyperbolicity by sampling latent quadruples in CrossTask [ 88 ] and evaluating the four-point condition under each model‚Äôs intrinsic metric (hyperbolic geodesic distance for GeoWorld and Euclidean distance for V-JEPA 2). As shown in Fig. 1 , GeoWorld exhibits a substantially more concentrated distribution of near-zero Œ¥ \\delta values, indicating a stronger tree-like hierarchical geometry in its learned representation space.\n\nFrozen Encoder vs. Fully Fine-Tuned\n\n[FIGURE_CAPTION] Table 1: Ablation of frozen encoder vs. fully fine-tuned model for visual planning with videos on CrossTask [ 88 ] .\n\nAs shown in Table 1 , we evaluate the impact of Fully Fine-Tuning (FFT) the encoder during the supervised finetuning stage, compared to the original configuration where the encoder remains frozen and only a lightweight exponential projection layer is trainable. Fully fine-tuning yields consistent yet modest improvements across all metrics and model scales, with gains of approximately 0.3 ‚àí 0.8 % 0.3{-}0.8\\% in SR and 0.5 ‚àí 1.2 % 0.5{-}1.2\\% in mAcc and mIoU for both T = 3 T{=}3 and T = 4 T{=}4 planning horizons. While these improvements indicate that the encoder can still adapt beneficially to downstream visual planning objectives, the gains come at the cost of significantly increased trainable parameters and slower optimization. Moreover, the relative performance margin narrows as model size increases, suggesting diminishing returns for larger backbones. These results imply that the frozen-encoder design already captures task-relevant structure effectively, and full encoder finetuning provides only incremental benefit relative to the additional computation and memory overhead introduced.\n\n[FIGURE_CAPTION] Table 2: Ablation of Supervised Fine-Tuning (SFT) vs. Geometric Reinforcement Learning (GRL) for visual planning with videos on CrossTask [ 88 ] .\n\n[FIGURE_CAPTION] Table 3: Ablation of weighting hyperparameter Œª \\lambda in Supervised Fine-Tuning (SFT) Only for visual planning with videos on CrossTask [ 88 ] .\n\nEffectiveness of GRL\n\nAs shown in Table 2 , incorporating Geometric Reinforcement Learning (GRL) leads to clear and consistent improvements over the supervised fine-tuning (SFT) baseline. While SFT alone yields marginal gains over the pretrained V-JEPA 2 model, applying GRL independently further boosts SR, mAcc, and mIoU across both planning horizons, suggesting that GRL better aligns the learned energy landscape with multi-step planning objectives. The combination of SFT and GRL achieves the strongest performance, indicating that SFT provides a strong initialization while GRL refines the latent dynamics toward energy-minimizing trajectories required for long-horizon reasoning. These findings highlight the complementary nature of supervised learning and reinforcement-based value shaping in predictive world models.\n\nSFT Hyperparameters\n\nAs shown in Table 3 , incorporating the rollout loss into SFT consistently improves visual planning performance over the pure one-step objective ( Œª = 1 \\lambda=1 ). Once 1 ‚àí Œª > 0 1-\\lambda>0 , all metrics exhibit steady gains, indicating that multi-step rollout supervision provides additional temporal consistency beyond standard single-step training.\nAs the rollout weight increases (i.e., smaller Œª \\lambda ), improvements become more pronounced, particularly for the longer planning horizon ( T = 4 T=4 ). For example, SR and mIoU steadily increase as Œª \\lambda decreases from 1 1 to 0.5 0.5 , suggesting that stronger rollout supervision effectively mitigates error accumulation over longer sequences. This trend aligns with the intuition that longer-horizon prediction requires explicit multi-step consistency constraints rather than relying solely on local one-step accuracy.\nA balanced weighting around Œª = 0.5 \\lambda=0.5 achieves the strongest overall performance across metrics, demonstrating that equal emphasis on one-step prediction and rollout consistency yields the best trade-off. Further increasing the rollout weight (e.g., Œª = 0.3 \\lambda=0.3 ) leads to negligible changes for the shorter horizon ( T = 3 T=3 ), while yielding slight yet consistent gains for the longer horizon ( T = 4 T=4 ). This behavior indicates that stronger rollout supervision primarily benefits long-horizon planning, especially under the hyperbolic structure where multi-step geodesic consistency becomes more critical. In contrast, short-horizon planning does not induce strong hierarchical structure, limiting the advantage of hyperbolic geometry and GRL. The primary benefit of GeoWorld emerges as the planning horizon increases, where exponential branching and long-term abstraction become critical, as shown in Table 5 .\n\nGRL Hyperparameters\n\nAs shown in Table 4 , both the discount factor Œ≥ \\gamma and the regularization weight Œ≤ \\beta play important roles in shaping the learning dynamics in GRL. Increasing Œ≥ \\gamma strengthens long-horizon supervision by assigning greater weight to later predicted steps, which benefits multi-step rollout consistency and improves SR, mAcc, and mIoU as the planning horizon increases from T = 3 T{=}3 to T = 4 T{=}4 . Meanwhile, introducing the triangle inequality regularization term through Œ≤ > 0 \\beta>0 consistently boosts performance compared to the Œ≤ = 0 \\beta=0 setting, demonstrating that enforcing hyperbolic geodesic constraints helps stabilize the predictor and prevents degenerate shortcuts in latent space. Moderate regularization ( Œ≤ = 0.1 \\beta=0.1 ) paired with a large discount factor ( Œ≥ = 0.99 \\gamma=0.99 ) achieves the strongest results, indicating that encouraging long-horizon consistency while softly enforcing geodesic structure yields the most effective balance. These results validate the effectiveness of GRL as both a geometric constraint mechanism and a planning-aligned training signal.\n\nHyperbolic Geometry vs. GRL in Long-Horizon Planning\n\n[FIGURE_CAPTION] Table 4: Ablation of discount factor Œ≥ \\gamma and the regularization weight b ‚Äã e ‚Äã t ‚Äã a beta in Geometric Reinforcement Learning (GRL) for visual planning with videos on CrossTask [ 88 ] .\n\nSection 4.5 in main paper reports results up to T = 6 T{=}6 , following the long-horizon setting in [ 87 ] . Table 5 further extends the evaluation to T = 8 T{=}8 to stress-test planning stability under increasingly long rollouts. As the horizon grows, the vanilla V-JEPA¬†2 baseline exhibits rapid performance degradation, with SR dropping sharply from 50.16 50.16 at T = 3 T{=}3 to 4.95 4.95 at T = 8 T{=}8 , highlighting severe error accumulation in long-horizon prediction.\nIntroducing hyperbolic geometry substantially mitigates this collapse. SFT in hyperbolic space already improves stability at longer horizons, maintaining significantly higher SR at T ‚â• 7 T{\\geq}7 . Applying GRL in Euclidean space further strengthens multi-step consistency and consistently outperforms the baseline, demonstrating that rollout-based geometric regularization alone contributes meaningful gains even without hyperbolic modeling.\nWhen GRL is implemented in hyperbolic space, the advantage becomes more pronounced, particularly for T ‚â• 6 T{\\geq}6 , suggesting that enforcing geodesic consistency in a curvature-aware latent space better preserves long-range structural dependencies. The full model (SFT + GRL) achieves the strongest results across all horizons, with the performance gap widening as T T increases. This trend indicates that SFT and GRL play complementary roles: SFT stabilizes short-term prediction, while GRL enhances long-horizon rollout consistency, together yielding a clear advantage in extended planning scenarios.\n\n[FIGURE_CAPTION] Table 5: SR of long horzion planning on CrossTask [ 88 ] videos.\n\n## 6 Error Accumulation in Long-Horizon Planning\n\nAutoregressive (AR) methods inevitably lead to error accumulation in long-horizon planning, which is why many existing works focus on mitigating this issue through rollout loss. However, our claim is not that hierarchy replaces this effect, but that geometry shapes how errors accumulate. In Euclidean latent spaces, small prediction errors cause unconstrained drift that compounds uniformly over time, whereas hyperbolic geometry imposes a hierarchical structure on the latent space that constrains long-horizon trajectories along geodesically meaningful directions. In this sense, error accumulation and geometric drift are closely related: hierarchical geometry mitigates how errors propagate, while rollout loss and GRL help eliminate them.\n\n## 7 Limitation and Future Work\n\nOur intuition for hierarchical structure arises from state transitions in multi-step planning over futures. Therefore, even when the action sequences annotated in CrossTask [ 88 ] and COIN [ 71 ] appear linear, predicting d d -step futures from a state induces an exponentially branching set of possible trajectories ( B d B^{d} ), forming an implicit tree underlying a hierarchical structure. We must clarify that, as mentioned in Section 1.2 , sub-task hierarchies involving multi-level planning are the intuition of the original JEPA [ 41 ] . However, the hierarchical structure in GeoWorld arises from multi-step future expansion, rather than from explicit high-level planning and low-level execution.\n\nFuture work may involve sub-task hierarchies, such as high-level task labels, mid-level actions, and low-level end-effectors. Moreover, our framework is compatible with embodied planning. As this is a computer vision conference, we plan to extend our work to embodied settings in the future.",
  "figures": []
}