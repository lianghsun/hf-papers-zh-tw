# æŽ¢ç´¢æ€§è¨˜æ†¶å¢žå¼·åž‹ LLM Agentï¼šæ··åˆåœ¨ç·šèˆ‡é›¢ç·šç­–ç•¥å„ªåŒ–

æŽ¢ç´¢ä»ç„¶æ˜¯ä½¿ç”¨å¼·åŒ–å­¸ç¿’è¨“ç·´çš„å¤§åž‹èªžè¨€æ¨¡åž‹ Agent çš„é—œéµç“¶é ¸ã€‚é›–ç„¶å…ˆå‰çš„æ–¹æ³•åˆ©ç”¨é è¨“ç·´çŸ¥è­˜ï¼Œä½†åœ¨éœ€è¦ç™¼ç¾æ–°ç‹€æ…‹çš„ç’°å¢ƒä¸­å»è¡¨ç¾ä¸ä½³ã€‚æˆ‘å€‘æå‡ºæŽ¢ç´¢æ€§è¨˜æ†¶å¢žå¼·åž‹åœ¨ç·šèˆ‡é›¢ç·šç­–ç•¥å„ªåŒ–ï¼ˆEMPOÂ²ï¼‰ï¼Œä¸€å€‹æ··åˆ RL æ¡†æž¶ï¼Œåˆ©ç”¨è¨˜æ†¶é€²è¡ŒæŽ¢ç´¢ï¼Œä¸¦çµåˆåœ¨ç·šå’Œé›¢ç·šç­–ç•¥æ›´æ–°ï¼Œä½¿ LLM åœ¨ä½¿ç”¨è¨˜æ†¶æ™‚è¡¨ç¾è‰¯å¥½ï¼ŒåŒæ™‚ä¹Ÿç¢ºä¿åœ¨æ²’æœ‰è¨˜æ†¶æƒ…æ³ä¸‹çš„é­¯æ£’æ€§ã€‚åœ¨ ScienceWorld å’Œ WebShop ä¸Šï¼ŒEMPOÂ² åˆ†åˆ¥ç›¸å°æ–¼ GRPO é”åˆ°äº† 128.6% å’Œ 11.3% çš„æ”¹é€²ã€‚æ­¤å¤–ï¼Œåœ¨åˆ†ä½ˆå¤–æ¸¬è©¦ä¸­ï¼ŒEMPOÂ² å±•ç¤ºäº†å°æ–°ä»»å‹™çš„å„ªè¶Šé©æ‡‰æ€§ï¼Œåªéœ€å°‘é‡è¨˜æ†¶è©¦é©—ä¸”ç„¡éœ€åƒæ•¸æ›´æ–°ã€‚é€™äº›çµæžœçªé¡¯ EMPOÂ² æ˜¯æ§‹å»ºæ›´å…·æŽ¢ç´¢æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„åŸºæ–¼ LLM Agent çš„æœ‰å‰æ™¯æ¡†æž¶ã€‚

## 1 ä»‹ç´¹

[FIGURE:25_graph.png] (a)

å¤§èªžè¨€æ¨¡åž‹ (LLMs) æœ€è¿‘å·²æˆç‚ºå…·æœ‰æŽ¨ç†ã€è¦åŠƒä»¥åŠèˆ‡å¤–éƒ¨ç’°å¢ƒäº’å‹•èƒ½åŠ›çš„å¼·å¤§ä»£ç† (Achiam et al., 2023; Park et al., 2023; Yao et al., 2023; Kim et al., 2025)ã€‚ç•¶çµåˆå¼·åŒ–å­¸ç¿’ (RL) æ™‚ï¼Œæ­¤é¡žä»£ç†å¯æ ¹æ“šç¶“é©—å’Œåé¥‹èª¿æ•´å…¶è¡Œç‚ºï¼Œä½¿å…¶è¶…è¶Šéœæ…‹æç¤ºæˆ–ç›£ç£å¾®èª¿ (Guo et al., 2025; Tan et al., 2024)ã€‚æ­¤ç¯„ä¾‹é©…å‹•äº†äº’å‹•å¼æ±ºç­–ã€å·¥å…·ä½¿ç”¨å’Œå…·èº«äººå·¥æ™ºæ…§ç­‰é ˜åŸŸçš„æœ€è¿‘é€²å±• (Feng et al., 2025b; Lu et al., 2025b; Feng et al., 2025a; Dong et al., 2025; Luo et al., 2025)ã€‚

ç„¶è€Œï¼Œç•¶å‰åŸºæ–¼ LLM çš„ä»£ç†çš„ä¸€å€‹é—œéµé™åˆ¶åœ¨æ–¼å®ƒå€‘ä¾è³´æ–¼åˆ©ç”¨å…ˆé©—çŸ¥è­˜ï¼Œè€Œéžé€²è¡Œç³»çµ±æ€§çš„æŽ¢ç´¢ã€‚é›–ç„¶ RL æ¡†æž¶å¼·èª¿å¹³è¡¡æŽ¢ç´¢èˆ‡åˆ©ç”¨ï¼Œä½†è¨±å¤š LLM ä»£ç†ç³»çµ±ä¸»è¦åˆ©ç”¨é è¨“ç·´çŸ¥è­˜ï¼Œä¸¦ä¸”åªåœ¨ç†Ÿæ‚‰çš„åˆ†ä½ˆå…§é€²è¡Œæœ‰é™æœç´¢ã€‚å› æ­¤ï¼Œé€™äº›ä»£ç†åœ¨éœ€è¦ç™¼ç¾æ–°ç‹€æ…‹æˆ–ä¸»å‹•ç²å–æ–°ä¿¡æ¯çš„ç’°å¢ƒä¸­å¾€å¾€è¡¨ç¾ä¸ä½³ï¼Œè€Œä¸æ˜¯é‡è¤‡ä½¿ç”¨å·²çŸ¥çš„å…§å®¹ã€‚

ç‚ºäº†æ‡‰å°é€™ä¸€æŒ‘æˆ°ï¼Œæœ€è¿‘çš„ç ”ç©¶å·²å°‡å¤–éƒ¨è¨˜æ†¶æ¨¡çµ„ç´å…¥ LLMsï¼Œä½œç‚ºé•·æœŸè¨˜æ†¶çš„å½¢å¼ã€‚é€™ä½¿æ¨¡åž‹èƒ½å¤ åˆ©ç”¨éŽåŽ»çš„ç¶“é©—ä¾†ä¿®æ­£å¤±æ•—çš„å˜—è©¦ï¼Œå¾žè€Œåœ¨å¾ŒçºŒè©¦é©—ä¸­æ”¹é€²æ±ºç­–ï¼Œè€Œç„¡éœ€åƒæ•¸æ›´æ–° (Shinn et al., 2023; Zhang et al., 2023)ã€‚ç„¶è€Œï¼Œå¦‚ Zhang et al. (2023) æ‰€æŒ‡å‡ºçš„ï¼Œæ­¤é¡žæ–¹æ³•çš„æ€§èƒ½å¾€å¾€æœƒè¿…é€Ÿé£½å’Œï¼Œå› ç‚ºä½¿ç”¨éœæ…‹åƒæ•¸æ”¶é›†ç¶“é©—ç„¡æ³•å®Œå…¨æ•æ‰é€£çºŒæ”¹é€²æ‰€éœ€çš„å¤šæ¨£æ€§ã€‚

[FIGURE:concept.png] åœ– 2ï¼šéžåƒæ•¸åŒ–æ›´æ–°å¯ä»¥é¼“å‹µæŽ¢ç´¢ï¼Œå¼•å°Žåƒæ•¸åŒ–æ›´æ–°ã€‚

åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘æå‡ºäº†ä¸€å€‹çµ±ä¸€æ¡†æž¶ï¼Œä½¿ LLM ä»£ç†èƒ½å¤ é€šéŽå»£æ³›æŽ¢ç´¢æ›´æœ‰æ•ˆåœ°å­¸ç¿’ï¼Œæ–¹æ³•æ˜¯ä½¿ç”¨ RL è¯åˆæ›´æ–°å…¶åƒæ•¸åŒ–ç­–ç•¥åƒæ•¸ï¼Œä¸¦é€šéŽäº’å‹•æ›´æ–°å…¶éžåƒæ•¸åŒ–è¨˜æ†¶æ¨¡çµ„ã€‚è‡³é—œé‡è¦çš„æ˜¯ï¼Œéžåƒæ•¸åŒ–æ›´æ–°ä¸åƒ…è£œå……ï¼Œè€Œä¸”å¢žå¼·äº†åƒæ•¸åŒ–å­¸ç¿’çš„æ•ˆçŽ‡ï¼Œå¾žè€Œå¯¦ç¾æ›´æœ‰æ•ˆçš„æŽ¢ç´¢å’Œé©æ‡‰ã€‚æ­¤é›™æ›´æ–°ç¯„ä¾‹å……ç•¶äº†åƒæ•¸ç´šå„ªåŒ–å’Œè¨˜æ†¶æ“´å¢žæŽ¨ç†ä¹‹é–“çš„æ©‹æ¨‘ã€‚é›–ç„¶åœ¨å­¸ç¿’æœŸé–“åˆ©ç”¨è¨˜æ†¶ï¼Œä½†è¦å¯¦ç¾æ›´å¯æ¦‚æ‹¬çš„æ™ºæ…§ï¼Œéœ€è¦æ¸›å°‘å°å¤–éƒ¨è¨˜æ†¶çš„ä¾è³´ï¼Œè€Œæ˜¯å°‡å…¶å¥½è™•ç›´æŽ¥åµŒå…¥åˆ°æ¨¡åž‹çš„åƒæ•¸ä¸­ã€‚ç‚ºæ­¤ï¼Œæˆ‘å€‘æå‡º Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPOÂ²)ï¼Œä¸€ç¨®æ–°çš„æ··åˆ RL ç®—æ³•ï¼Œåœ¨æŽ¨å»£éšŽæ®µçµåˆäº†å…©ç¨®æ¨¡å¼â€”â€”å–æ±ºæ–¼æ˜¯å¦ä½¿ç”¨è¨˜æ†¶â€”â€”ä»¥åŠåœ¨æ›´æ–°éšŽæ®µçš„å…©ç¨®æ¨¡å¼â€”â€”åœ¨ç·šå’Œé›¢ç·šå­¸ç¿’â€”â€”å¾žè€Œä½¿ä»£ç†èƒ½å¤ åœ¨æœ‰è¨˜æ†¶æ™‚åˆ©ç”¨è¨˜æ†¶ï¼ŒåŒæ™‚åœ¨æ²’æœ‰è¨˜æ†¶æ™‚ä¿æŒç©©å¥ã€‚

åœ¨æˆ‘å€‘çš„å¯¦é©—ä¸­ï¼Œæˆ‘å€‘åœ¨å…©å€‹å»£æ³›ä½¿ç”¨çš„å¤šæ­¥å…·èº«æŽ¨ç†ç’°å¢ƒä¸Šè©•ä¼° EMPOÂ²ï¼Œé€™äº›ç’°å¢ƒéœ€è¦æŽ¢ç´¢ä¾†è§£æ±ºè¤‡é›œä»»å‹™ï¼šScienceWorld (Wang et al., 2022) å’Œ WebShop (Yao et al., 2022)ã€‚æˆ‘å€‘å°‡å…¶æ€§èƒ½èˆ‡ä¸€ç³»åˆ—éžåƒæ•¸åŒ–å’Œåƒæ•¸åŒ–ï¼ˆé›¢ç·šå’Œåœ¨ç·šï¼‰RL æ–¹æ³•é€²è¡Œæ¯”è¼ƒã€‚å¦‚åœ– 1 æ‰€ç¸½çµçš„ï¼ŒEMPOÂ² å¤§å¹…è¶…è¶Šå…ˆå‰çš„ç®—æ³•ï¼Œåœ¨ ScienceWorld ä¸Šç›¸è¼ƒå¼·å¤§çš„åœ¨ç·š RL åŸºç·š GRPO å¯¦ç¾äº† 128.6% çš„æ”¹é€²ï¼Œåœ¨ WebShop ä¸Šå¯¦ç¾äº† 11.3% çš„æ”¹é€²ã€‚åœ– 1 (a) ä¸­çš„è¨“ç·´æ›²ç·šé€²ä¸€æ­¥é¡¯ç¤ºï¼Œèˆ‡æ”¶æ–‚åˆ°æ¬¡å„ªè§£çš„ GRPO ä¸åŒï¼ŒEMPOÂ² åˆ©ç”¨æŒçºŒæŽ¢ç´¢ä¸¦æˆåŠŸè§£æ±ºä»»å‹™ã€‚æ­¤å¤–ï¼Œé‡å° OOD å¯¦é©—ï¼ˆåœ– 1ï¼Œæœ€å³å´ï¼‰ï¼Œæ¨¡åž‹ä¹Ÿèƒ½ä»¥åƒ…æœ‰çš„å¹¾æ¬¡è©¦é©—å’Œç„¡æ¬Šé‡æ›´æ–°çš„æƒ…æ³ä¸‹é”åˆ°è‰¯å¥½åˆ†æ•¸ï¼Œè¡¨æ˜Žæ›´æ–°å¾Œçš„æ¨¡åž‹å·²ç²å¾—ä½¿ç”¨è¨˜æ†¶æŽ¢ç´¢æœªè¦‹æˆ–ä¸ç†Ÿæ‚‰ç’°å¢ƒçš„èƒ½åŠ›ã€‚é€™äº›çµæžœçªå‡ºäº† EMPOÂ² æ˜¯æ§‹å»ºæ›´é©æ‡‰æ€§å’Œæ›´å¯æ¦‚æ‹¬çš„å…·èº«ä»£ç†çš„æœ‰å‰æ™¯çš„æ–¹å‘ã€‚

## 2 é å‚™çŸ¥è­˜

åœ¨ç·š RL åŒ…æ‹¬åœ¨ rollout éšŽæ®µå’Œæ›´æ–°éšŽæ®µä¹‹é–“äº¤æ›¿é€²è¡Œï¼Œåœ¨ rollout éšŽæ®µä¸­ä½¿ç”¨ç”± Î¸ \theta åƒæ•¸åŒ–çš„ç•¶å‰ç­–ç•¥ Ï€ \pi ç”Ÿæˆè»Œè·¡ï¼Œåœ¨æ›´æ–°éšŽæ®µä¸­æ ¹æ“šé€™äº› rollout å„ªåŒ–ç­–ç•¥ã€‚

**ç­–ç•¥ Rolloutã€‚** æˆ‘å€‘è€ƒæ…®ä¸€å€‹è¨­ç½®ï¼Œå…¶ä¸­çµ¦å®šå¾ž p â€‹ ( ð’° ) p(\mathcal{U}) æŽ¡æ¨£çš„ä»»å‹™ u u ï¼Œä¸€å€‹ LLM agent é€šéŽèˆ‡ç’°å¢ƒçš„å¤šæ­¥äº¤äº’ä¾†è§£æ±ºä»»å‹™ã€‚å¾žä»»å‹™ u u é–‹å§‹ï¼ŒLLM Ï€ Î¸ \pi_{\theta} ç”Ÿæˆç¬¬ä¸€å€‹è‡ªç„¶èªžè¨€å‹•ä½œ a 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ u ) âˆˆ ð’œ a_{1}\sim\pi_{\theta}(\cdot\mid u)\in\mathcal{A} ã€‚åŸ·è¡Œæ­¤å‹•ä½œå¾Œï¼Œç’°å¢ƒè¿”å›žçŽå‹µ r 1 r_{1} å’Œä¸‹ä¸€å€‹ç‹€æ…‹ s 1 s_{1} ã€‚åœ¨ä¸€èˆ¬æ™‚åˆ» t t ï¼Œä»¥ç•¶å‰ç‹€æ…‹ s t s_{t} å’Œä»»å‹™ u u ç‚ºæ¢ä»¶ï¼Œç­–ç•¥ç”¢ç”Ÿä¸‹ä¸€å€‹å‹•ä½œ a t + 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ s t , u ) a_{t+1}\sim\pi_{\theta}(\cdot\mid s_{t},u) ã€‚æ­¤äº¤äº’å¾ªç’°æŒçºŒé€²è¡Œï¼Œç›´åˆ°ä»»å‹™å®Œæˆæˆ–é”åˆ°æœ€å¤§æ­¥æ•¸ã€‚rollout è»Œè·¡å› æ­¤å®šç¾©ç‚ºç‹€æ…‹ã€å‹•ä½œå’ŒçŽå‹µçš„åºåˆ—ï¼ŒÏ„ = ( u , a 1 , r 1 , s 1 , a 2 , r 2 , â€¦ , s T ) . \tau=\big(u,a_{1},r_{1},s_{1},a_{2},r_{2},\ldots,s_{T}\big).

**ç¾¤çµ„ç›¸å°ç­–ç•¥å„ªåŒ–ã€‚** ç¾¤çµ„ç›¸å°ç­–ç•¥å„ªåŒ– (Group Relative Policy Optimization, GRPO) (Shao et al. , 2024 ) é€šéŽæ¯”è¼ƒåŒä¸€ä»»å‹™ u u çš„å¤šå€‹ rollout ä¾†æ›´æ–°ç­–ç•¥ï¼Œæ¶ˆé™¤äº† PPO (Schulman et al. , 2017 ) ä¸­å°åƒ¹å€¼å‡½æ•¸çš„éœ€æ±‚ã€‚çµ¦å®šä»»å‹™ u u ï¼Œç­–ç•¥ Ï€ Î¸ \pi_{\theta} ç”Ÿæˆ N N æ¢ rollout è»Œè·¡ { Ï„ ( 1 ) , â€¦ , Ï„ ( N ) } \{\tau^{(1)},\ldots,\tau^{(N)}\} ã€‚æ¯æ¢è»Œè·¡ç²å¾—å›žå ± { R ( 1 ) , â€¦ , R ( N ) } \{R^{(1)},\ldots,R^{(N)}\} ï¼Œå®šç¾©ç‚ºæ²¿è‘—è»Œè·¡çš„çŽå‹µä¹‹å’Œï¼šR ( i ) = âˆ‘ t = 1 T r t ( i ) . R^{(i)}=\sum_{t=1}^{T}r_{t}^{(i)}. ã€‚å°æ–¼åœ¨è»Œè·¡ Ï„ ( i ) \tau^{(i)} ä¸­æŽ¡å–çš„æ¯å€‹å‹•ä½œ a t ( i ) a_{t}^{(i)} ï¼Œæˆ‘å€‘å°‡å…¶ç›¸å°å„ªå‹¢å®šç¾©ç‚ºï¼šA â€‹ ( a t ( i ) ) = R ( i ) âˆ’ 1 N â€‹ âˆ‘ j = 1 N R ( j ) Ïƒ â€‹ ( R ) , A(a_{t}^{(i)})=\frac{R^{(i)}-\frac{1}{N}\sum_{j=1}^{N}R^{(j)}}{\sigma(R)}, å…¶ä¸­ä¾†è‡ªçŽå‹µé«˜æ–¼å¹³å‡å€¼è»Œè·¡çš„å‹•ä½œç²å¾—æ­£å„ªå‹¢ï¼Œè€Œä¾†è‡ªè¡¨ç¾è¼ƒå·®è»Œè·¡çš„å‹•ä½œç²å¾—è² å„ªå‹¢ã€‚GRPO æå¤±å‰‡ç‚ºï¼š

|  | ð”¼ u âˆ¼ p â€‹ ( ð’° ) { Ï„ ( i ) } i = 1 N âˆ¼ Ï€ Î¸ old \displaystyle\mathbb{E}_{\begin{subarray}{c}u\sim p(\mathcal{U})\\
\{\tau^{(i)}\}_{i=1}^{N}\sim\pi_{\theta_{\text{old}}}\end{subarray}} | [ 1 N â€‹ T â€‹ âˆ‘ i = 1 N âˆ‘ t = 1 T min â¡ ( Ï Î¸ â€‹ ( a t ( i ) ) â€‹ A â€‹ ( a t ( i ) ) , clip â€‹ ( Ï Î¸ â€‹ ( a t ( i ) ) , 1 âˆ’ Ïµ , 1 + Ïµ ) â€‹ A â€‹ ( a t ( i ) ) ) ] \displaystyle\Bigg[\frac{1}{NT}\sum_{i=1}^{N}\sum_{t=1}^{T}\min\Big(\rho_{\theta}(a_{t}^{(i)})A(a_{t}^{(i)}),\text{clip}\big(\rho_{\theta}(a_{t}^{(i)}),1-\epsilon,1+\epsilon\big)A(a_{t}^{(i)})\Big)\Bigg] |  |
| --- | --- | --- | --- |
|  |  | âˆ’ Î² D KL ( Ï€ Î¸ ( â‹… | u ) âˆ¥ Ï€ ref ( â‹… | u ) ) , \displaystyle\quad-\beta\,D_{\text{KL}}\!\big(\pi_{\theta}(\cdot|u)\,\|\;\pi_{\text{ref}}(\cdot|u)\big), |  | (1) |

å…¶ä¸­ Ï Î¸ â€‹ ( a t ( i ) ) = Ï€ Î¸ â€‹ ( a t ( i ) | s t ( i ) , u ) Ï€ Î¸ old â€‹ ( a t ( i ) | s t ( i ) , u ) , \rho_{\theta}(a_{t}^{(i)})=\frac{\pi_{\theta}(a_{t}^{(i)}|s_{t}^{(i)},u)}{\pi_{\theta_{\text{old}}}(a_{t}^{(i)}|s_{t}^{(i)},u)}, å…¶ä¸­ Î² â‰¥ 0 \beta\geq 0 æŽ§åˆ¶æœå‘åƒè€ƒç­–ç•¥ Ï€ ref \pi_{\text{ref}} çš„æ­£å‰‡åŒ–å¼·åº¦ã€‚

## 3 The Exploration Problem of LLM Agents

LLMs encode rich prior knowledge, but such priors often fail to reflect the actual rules or dynamics of a given environment. Blind reliance on these priors can lead to erroneous behaviors, making it necessary for agents to adapt through direct interaction and trial-and-error. A key requirement for such adaptation is exploration , which involves seeking information beyond pre-training, sometimes by taking atypical or counterintuitive actions. However, current LLM-based agents struggle with this (Qiao et al. , 2024 ; Zhou et al. , 2024 ) , as it demands stepping outside the distribution of behaviors where the model feels most confident.

Consequently, many prior studies have sought to align agents with new environments through warm-start supervised fine-tuning (SFT) using numerous golden trajectories (Song et al. , 2024 ; Qiao et al. , 2024 ; Xiang et al. , 2024 ) , leveraging large-scale models such as GPT-4 (Tang et al. , 2024 ; Lin et al. , 2023 ) , or employing human engineering or well-established simulation information (Choudhury and Sodhi, 2025 ) . While these methods achieve strong results in constrained settings, their effectiveness is limited to cases where such external support is available, and they generalize poorly to unseen scenarios without it.

[FIGURE:exploration_problem.png] Figure 3: When training LLM with GRPO in ScienceWorld, the agent struggles because of insufficient exploration. For instance, in the task â€œturn on the red light bulb,â€ the agent must first find the red light bulb before activating it. However, the agent fails to locate it and, as a result, cannot complete the task. Rather than analyzing the cause of failure and exploring alternative actions, the agent proceeds unchanged, so its score stagnates even as additional training steps are taken.

Therefore, we focus on how to efficiently train agents in online RL through trial and error, without any prior embedding of the environmentâ€™s rules. The key challenge is that, without intrinsic exploration capability, online RL struggles to optimize effectively. As illustrated in Figure 3 , in ScienceWorld (Wang et al. , 2022 ) environment the agent is given the mission â€œturn on the red light bulb.â€ The instructions specify that the agent should first focus on the light bulb and then build a circuit to activate it, based on the current room observation. However, since no red light bulb is present in the observation, the agent must search the environment to locate it. Instead, the agent follows the instruction literally, attempts to focus on the red light bulb, and fails because it does not exist in the room. Ideally, when an agent fails to reach its goal, it should analyze the reasons for failure and broaden its action space to discover successful strategies. Yet in representative online RL algorithms GRPO (Shao et al. , 2024 ) , prior trajectory rollouts provide no continuity beyond a scalar reward signal, thereby restricting exploration and ultimately limiting learning.

## 4 Method

In this section, we present Exploratory Memory-augmented On- and Off-Policy Optimization (EMPO 2 ), a novel algorithm aimed at tackling the exploration challenges in online RL. EMPO 2 operates in two modes for both rollout phase and update phase. During rollout, actions can be generated either through (1) prompting without memory , where no retrieved information is used, or (2) memory-augmented prompting , conditioned on tips retrieved from memory. In the update phase, rollouts with memory-augmented prompting are used in two ways: (a) on-policy , where tips are retained and the update is performed with the original prompt, and (b) off-policy , where tips are removed during update. Notably, tips are generated not by a separate model but by the policy Ï€ Î¸ \pi_{\theta} itself, which is continually updated during training. The full algorithm is provided in Appendix A .

### 4.1 ä½¿ç”¨è‡ªç”Ÿæˆè¨˜æ†¶ä¿ƒé€²æŽ¢ç´¢

EMPO 2 çš„ä¸€å€‹é—œéµçµ„ä»¶æ˜¯å…¶ä½¿ç”¨è¨˜æ†¶ä¾†ç¶­æŒè·¨æ»¾å‹•è»Œè·¡ï¼ˆrolloutsï¼‰çš„é€£çºŒæ€§ã€‚å¾žæ™ºèƒ½é«”çš„äº’å‹•ä¸­ç²å¾—çš„ä¿¡æ¯å¯ä»¥é€šéŽç­–ç•¥å„ªåŒ–ç·¨ç¢¼åˆ°åƒæ•¸ä¸­ï¼Œä½†ä¹Ÿå¯ä»¥è¨˜éŒ„åœ¨æ™ºèƒ½é«”æŒçºŒæŸ¥é–±çš„å¤–éƒ¨è¨˜æ†¶ä¸­ã€‚ç”±æ–¼æˆ‘å€‘çš„ç­–ç•¥å¾žé è¨“ç·´çš„ LLM åˆå§‹åŒ–ï¼Œè©² LLM å…·æœ‰å›ºæœ‰çš„ç¸½çµå’Œåæ€èƒ½åŠ›ï¼Œé€™äº›èƒ½åŠ›å¯ä»¥ä½œç‚ºè¼”åŠ©ä¿¡è™Ÿåˆ©ç”¨ï¼Œé™¤äº†æ¨™é‡çŽå‹µå¤–ï¼Œå¾žè€Œæ›´æœ‰æ•ˆåœ°å¼•å°ŽæŽ¢ç´¢ã€‚ç‚ºäº†å¯¦ç¾é€™ä¸€é»žï¼ŒEMPO 2 æ•´åˆäº†åƒæ•¸åŒ–ï¼ˆLLM å…§çš„åƒæ•¸æ›´æ–°ï¼‰å’Œéžåƒæ•¸åŒ–ï¼ˆå¤–éƒ¨è¨˜æ†¶ï¼‰æ›´æ–°ï¼Œå¼·åŒ–äº†æ»¾å‹•è»Œè·¡ä¹‹é–“çš„è¯ç¹«ä¸¦ä¿ƒé€²æŽ¢ç´¢ï¼Œæ‰€æœ‰æ•¸æ“šå’ŒæŒ‡å°Žéƒ½ç”±æ™ºèƒ½é«”è‡ªä¸»ç”Ÿæˆã€‚

[FIGURE:motivation3.png] åœ– 4ï¼šåœ¨ EMPO 2 ä¸­ï¼Œç•¶å‰çš„ç­–ç•¥åƒæ•¸ $\pi_{\theta}$ ç”¨æ–¼å¯©æŸ¥éŽåŽ»çš„æ»¾å‹•è»Œè·¡ï¼Œæ‰€ç”¢ç”Ÿçš„è¦‹è§£è¢«æ·»åŠ åˆ°è¨˜æ†¶ä¸­ã€‚é€™å€‹æ›´æ–°çš„è¨˜æ†¶å½±éŸ¿å¾ŒçºŒçš„æ»¾å‹•è»Œè·¡ä¸¦ä¿ƒé€²æŽ¢ç´¢ã€‚

åœ¨éžåƒæ•¸åŒ–æ›´æ–°ä¸­ï¼Œé¡žä¼¼æ–¼ Reflexionï¼ˆShinn et al., 2023ï¼‰ï¼Œæ™ºèƒ½é«”å¯©æŸ¥éŽåŽ»çš„æ»¾å‹•è»Œè·¡ï¼Œç”Ÿæˆè‡ªæˆ‘æŒ‡å°Žæç¤ºï¼Œä¸¦å°‡å…¶å­˜å„²åœ¨è¨˜æ†¶ä¸­ã€‚é€™äº›æç¤ºå¹«åŠ©æ™ºèƒ½é«”é¿å…é‡è¤‡çŠ¯éŒ¯ä¸¦æŽ¢ç´¢æ–°ç­–ç•¥ã€‚èˆ‡ Reflexion ä¸åŒï¼ŒReflexion èšç„¦æ–¼è¿­ä»£çš„èªžè¨€æŒ‡å°Žä»¥åœ¨ä¸‹ä¸€æ¬¡è©¦é©—ä¸­ç²å¾—æ›´é«˜çš„çŽå‹µï¼Œè€Œæˆ‘å€‘çš„æ–¹æ³•æ—¨åœ¨è®“é€™äº›æç¤ºå°Žè‡´å¢žå¼·çš„æŽ¢ç´¢ï¼Œæœ€çµ‚é€šéŽåƒæ•¸åŒ–æ›´æ–°ä¾†éžå›ºã€‚

è‡ªç”Ÿæˆè¨˜æ†¶å’Œæç¤ºã€‚æˆ‘å€‘å®šç¾©ä¸€å€‹è¨˜æ†¶ç·©è¡å€ $\mathcal{M}=\{\text{tip}_{1},\text{tip}_{2},\ldots\}$ï¼Œå…¶å­˜å„²ç”±ç­–ç•¥ $\pi_{\theta}$ åœ¨è»Œè·¡åæ€æœŸé–“ç”Ÿæˆçš„åæ€æç¤ºã€‚æ­£å¼åœ°èªªï¼Œç•¶ä»»å‹™ $u$ çš„ç¬¬ $i$ å€‹ç–‡ï¼ˆepisodeï¼‰åœ¨æ™‚é–“æ­¥ $t$ çµ‚æ­¢æ™‚ï¼Œç­–ç•¥ä»¥æœ€çµ‚ç‹€æ…‹ $s_{t}$ å’Œæç¤ºç”Ÿæˆæç¤ºè©žï¼ˆtip-generation promptï¼‰ä½œç‚ºè¼¸å…¥ï¼Œä¸¦ç”Ÿæˆä¸€å€‹æç¤ºï¼Œå…¶ä¸­ $\text{tip}_{i}\sim\pi_{\theta}(s_{t},u,\text{tip-generation prompt})$ã€‚ä¸‹é¢æä¾›äº†ä¸€çµ„èªªæ˜Žæ€§ç¤ºä¾‹ï¼Œè€Œæç¤ºç”Ÿæˆæç¤ºè©žå‘ˆç¾åœ¨é™„éŒ„ B ä¸­ï¼Œå…¶ä»–ç¤ºä¾‹åŒ…å«åœ¨é™„éŒ„ E.1 ä¸­ã€‚

### 4.2 Parameterize non-parametric updates via hybrid policy optimization

Agents can use memory to improve exploration and learning efficiency, but the acquired knowledge needs be internalized into model parameters to enhance inherent capabilities. To this end, we propose two modes for the rollout and update phases, whose combinations yield three hybrid learning modes (Figure 5 ).

[FIGURE:EMPO.png] Figure 5: EMPO 2 mode combinations. By combining the two rollout modes and update modes, three EMPO mode configurations are possible: on-policy learning without memory, on-policy learning with memory and off-policy learning.

Rollout Modes. During rollouts, the agent samples between the two modes, selecting one mode at each step: mode (2) with memory rollout probability p p and mode (1) with probability 1 âˆ’ p 1-p . The ablation study of p p can be found in Appendix F.1 .

- (1) Prompting Without Memory. For each task u u , at each timestep t t , the policy Ï€ Î¸ \pi_{\theta} generates actions conditioned only on the current state s t s_{t} and the task u u : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ s t , u ) . a_{t+1}\sim\pi_{\theta}(\cdot\mid s_{t},u).
- (2) Memory-Augmented Prompting. For each task u u , at each timestep t t , a retrieval operator Retr â€‹ ( o t ; â„³ ) âŠ† â„³ \mathrm{Retr}(o_{t};\mathcal{M})\subseteq\mathcal{M} selects tips most relevant to the current state s t s_{t} , e.g., via similarity search in the embedding space. We denote the retrieved set as tips t \text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t} . In memory-augmented prompting, the policy conditions its action on both s t s_{t} and tips t \text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t} : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… | s t , u , tips t ) . a_{t+1}\sim\pi_{\theta}\!\left(\cdot\,\middle|\,s_{t},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t}\right). We limit the number of retrieved tips at 10.

Prompting Without Memory. For each task u u , at each timestep t t , the policy Ï€ Î¸ \pi_{\theta} generates actions conditioned only on the current state s t s_{t} and the task u u : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ s t , u ) . a_{t+1}\sim\pi_{\theta}(\cdot\mid s_{t},u).

Memory-Augmented Prompting. For each task u u , at each timestep t t , a retrieval operator Retr â€‹ ( o t ; â„³ ) âŠ† â„³ \mathrm{Retr}(o_{t};\mathcal{M})\subseteq\mathcal{M} selects tips most relevant to the current state s t s_{t} , e.g., via similarity search in the embedding space. We denote the retrieved set as tips t \text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t} . In memory-augmented prompting, the policy conditions its action on both s t s_{t} and tips t \text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t} : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… | s t , u , tips t ) . a_{t+1}\sim\pi_{\theta}\!\left(\cdot\,\middle|\,s_{t},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t}\right). We limit the number of retrieved tips at 10.

Update Modes. Trajectories generated under rollout mode (1) are directly used for updates, whereas those generated under rollout mode (2)â€”memory-augmented promptingâ€”follow one of two update modes chosen at random during the update phase. Mode (b) is selected with off-policy update probability q q , and mode (a) with probability 1 âˆ’ q 1-q . The ablation study of q q can be found in Appendix F.1 .

- (a) On-Policy Updates. On-policy update uses the same prompt as in the rollout, and Ï Î¸ â€‹ ( a t ( i ) ) \rho_{\theta}(a_{t}^{(i)}) in eq. 1 becomes Ï Î¸ â€‹ ( a t ( i ) ) = Ï€ Î¸ â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) Ï€ Î¸ old â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) . \rho_{\theta}(a_{t}^{(i)})=\frac{\pi_{\theta}(a_{t}^{(i)}\mid s_{t}^{(i)},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t})}{\pi_{\theta_{\text{old}}}(a_{t}^{(i)}\mid s_{t}^{(i)},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t})}.
- (b) Off-Policy Updates. In this mode, the stored log-probabilities â„“ t tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u , tips t ) \ell^{\text{tips}}_{t}=\log\pi_{\theta}(a_{t}\mid s_{t},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t}) are replaced with the log-probabilities assigned by the same policy Ï€ Î¸ \pi_{\theta} when conditioned only on ( s t , u ) (s_{t},u) , namely â„“ t no-tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u ) \ell^{\text{no-tips}}_{t}=\log\pi_{\theta}(a_{t}\mid s_{t},u) . In this formulation, the advantage update is performed based on how natural the action appears under the distribution without tips. This construction can be interpreted as a form of reward-guided knowledge distillation . Trajectories sampled under the tips-conditioned policy act as teacher demonstrations, while the student policy Ï€ Î¸ ( â‹… âˆ£ s , u ) \pi_{\theta}(\cdot\mid s,u) is updated to reproduce those trajectories in proportion to their advantage. High-reward trajectories ( A ^ t > 0 \hat{A}_{t}>0 ) are reinforced, while low-reward trajectories ( A ^ t < 0 \hat{A}_{t}<0 ) are suppressed, resulting in selective distillation that emphasizes beneficial behaviors. In this way, tips serve as an intermediate scaffolding mechanism that improves exploration and trajectory quality, while the reward signal ensures that only advantageous behaviors are ultimately retained. Consequently, the final policy learns to internalize the benefits of tip conditioning without requiring tips at inference time. Appendix C provides an illustrative breakdown and a summary table for the calculation of the importance sampling ratio.

On-Policy Updates. On-policy update uses the same prompt as in the rollout, and Ï Î¸ â€‹ ( a t ( i ) ) \rho_{\theta}(a_{t}^{(i)}) in eq. 1 becomes Ï Î¸ â€‹ ( a t ( i ) ) = Ï€ Î¸ â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) Ï€ Î¸ old â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) . \rho_{\theta}(a_{t}^{(i)})=\frac{\pi_{\theta}(a_{t}^{(i)}\mid s_{t}^{(i)},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t})}{\pi_{\theta_{\text{old}}}(a_{t}^{(i)}\mid s_{t}^{(i)},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t})}.

Off-Policy Updates. In this mode, the stored log-probabilities â„“ t tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u , tips t ) \ell^{\text{tips}}_{t}=\log\pi_{\theta}(a_{t}\mid s_{t},u,\text{\fcolorbox{empo2}{white}{\textcolor{empo2}{tips}}}_{t}) are replaced with the log-probabilities assigned by the same policy Ï€ Î¸ \pi_{\theta} when conditioned only on ( s t , u ) (s_{t},u) , namely â„“ t no-tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u ) \ell^{\text{no-tips}}_{t}=\log\pi_{\theta}(a_{t}\mid s_{t},u) . In this formulation, the advantage update is performed based on how natural the action appears under the distribution without tips.

This construction can be interpreted as a form of reward-guided knowledge distillation . Trajectories sampled under the tips-conditioned policy act as teacher demonstrations, while the student policy Ï€ Î¸ ( â‹… âˆ£ s , u ) \pi_{\theta}(\cdot\mid s,u) is updated to reproduce those trajectories in proportion to their advantage. High-reward trajectories ( A ^ t > 0 \hat{A}_{t}>0 ) are reinforced, while low-reward trajectories ( A ^ t < 0 \hat{A}_{t}<0 ) are suppressed, resulting in selective distillation that emphasizes beneficial behaviors. In this way, tips serve as an intermediate scaffolding mechanism that improves exploration and trajectory quality, while the reward signal ensures that only advantageous behaviors are ultimately retained. Consequently, the final policy learns to internalize the benefits of tip conditioning without requiring tips at inference time. Appendix C provides an illustrative breakdown and a summary table for the calculation of the importance sampling ratio.

[FIGURE:low_filtering.png] Figure 6: Masking tokens stabilizes training.

Stabilizing Off-Policy Training. Off-policy training is prone to instability and may collapse (see Figure 6 ). In such cases, gradient normalization, entropy loss, KL loss, and policy gradient loss can all diverge to NaN. Prior work, Yang et al. ( 2025 ) shows that low-probability tokens destabilize training by amplifying gradient magnitudes through unbounded likelihood ratios. Motivated by this, we introduce a masking mechanism that suppresses the advantage term for tokens whose probability under Ï€ Î¸ \pi_{\theta} falls below a threshold Î´ \delta . Finally, the loss in Eq. 1 is modified as

|  | ð”¼ u âˆ¼ p â€‹ ( ð’° ) { Ï„ ( i ) } âˆ¼ Ï€ Î¸ old [ 1 N â€‹ T âˆ‘ i = 1 N âˆ‘ t = 1 T \displaystyle\mathbb{E}_{\begin{subarray}{c}u\sim p(\mathcal{U})\\
\{\tau^{(i)}\}\sim\pi_{\theta_{\text{old}}}\end{subarray}}\Bigg[\frac{1}{NT}\sum_{i=1}^{N}\sum_{t=1}^{T} | min ( Ï Î¸ ( i , t ) A ( a t ( i ) ) , clip ( Ï Î¸ ( i , t ) , 1 âˆ’ Ïµ , 1 + Ïµ ) A ( a t ( i ) ) ) â‹… ðŸ Ï€ Î¸ â€‹ ( a t ( i ) | s t ( i ) , u ) â‰¥ Î´ ] \displaystyle\min\Big(\rho_{\theta}^{(i,t)}A(a_{t}^{(i)}),\;\text{clip}\big(\rho_{\theta}^{(i,t)},1-\epsilon,1+\epsilon\big)A(a_{t}^{(i)})\Big)\cdot\mathbf{1}_{\pi_{\theta}(a_{t}^{(i)}|s_{t}^{(i)},u)\geq\delta}\Bigg] |  |
| --- | --- | --- | --- |
|  |  | âˆ’ Î² D KL ( Ï€ Î¸ ( â‹… | u ) âˆ¥ Ï€ ref ( â‹… | u ) ) . \displaystyle\quad-\beta D_{\text{KL}}\!\big(\pi_{\theta}(\cdot|u)\,\|\;\pi_{\text{ref}}(\cdot|u)\big). |  | (2) |

[FIGURE:entropy.png] Figure 7: Policy entropy comparison with vs. without intrinsic rewards.

Intrinsic Rewards for Exploration. To further encourage exploration, and inspired by prior work on exploration-targeted online RL (Burda et al. , 2018b ; Bellemare et al. , 2016 ; Ecoffet et al. , 2019 ) , we introduce an intrinsic reward based on the novelty of the current state. A memory list stores distinct states, and for each new state we compute its cosine similarity with existing entries. If the similarity falls below a threshold, the state is added to memory and assigned a reward. The intrinsic reward is defined as r intrinsic = 1 n r_{\text{intrinsic}}=\frac{1}{n} , where n n denotes the number of similar past states. This mechanism encourages the agent to explore novel states even when no extrinsic reward is provided by the environment and maintains policy entropy, as shown in Figure 7 .

## 5 ç›¸é—œå·¥ä½œ

**LLM Agents åœ¨å¤šæ­¥å…·èº«ä»»å‹™ä¸­çš„æ‡‰ç”¨**ã€‚LLM agents åœ¨å¤šæ­¥å…·èº«ä»»å‹™ä¸­å·²åœ¨ä¸åŒçš„ç¯„å¼ä¸‹é€²è¡ŒéŽç ”ç©¶ã€‚è³‡æ–™é©…å‹•çš„æ–¹æ³•ï¼ˆSong et al.ï¼Œ2024ï¼›Xiong et al.ï¼Œ2024ï¼›Qiao et al.ï¼Œ2025ï¼›2024ï¼›Tajwar et al.ï¼Œ2025ï¼‰é€éŽæœ‰æ•ˆçš„è³‡æ–™æ”¶é›†æ–¹æ³•å’Œæ¨¡ä»¿å­¸ç¿’ä¾†å¢žå¼·æ±ºç­–èƒ½åŠ›ã€‚åŸºæ–¼æ¨¡åž‹çš„ agentsï¼ˆTang et al.ï¼Œ2024ï¼›Zhou et al.ï¼Œ2024ï¼‰å»ºç«‹ä¸–ç•Œæ¨¡åž‹ï¼Œé€šå¸¸é€éŽä½¿ç”¨ GPT-4 ç­‰å¤§åž‹é–‰æºç³»çµ±ç”Ÿæˆç¨‹å¼ç¢¼ã€‚å…¶ä»–æ–¹æ³•ï¼ˆLin et al.ï¼Œ2023ï¼›Choudhury and Sodhiï¼Œ2025ï¼‰é€éŽæ¨¡åž‹è½‰ç§»æˆ–åˆ©ç”¨æ¨¡æ“¬ç’°å¢ƒæä¾›çš„ç‰¹æ®Šè³‡è¨Šä¾†åŠ å¼·æŽ¨ç†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘å€‘çš„æ–¹æ³•æ¸›å°‘äº†å°æ­¤é¡žå¤–éƒ¨è³‡æºçš„ä¾è³´ï¼Œä¸¦å¼·èª¿é€éŽ agent è‡ªèº«çš„æŽ¢ç´¢å’Œè‡ªæˆ‘æ”¹é€²ä¾†å¯¦ç¾è‡ªä¸»æˆé•·ã€‚

**LLM Agents çš„è¨˜æ†¶æ©Ÿåˆ¶**ã€‚ç‚ºäº†å¯¦ç¾å¾žéŽå¾€ç¶“é©—çš„æ¼¸é€²å¼æ”¹é€²ï¼ŒReflexionï¼ˆShinn et al.ï¼Œ2023ï¼‰å’Œ REMEMBERERï¼ˆZhang et al.ï¼Œ2023ï¼‰åˆ©ç”¨å¤–éƒ¨è¨˜æ†¶ã€‚Reflexion å„²å­˜å£é ­åæ€ä¾›å¾ŒçºŒæç¤ºä½¿ç”¨ï¼Œè€Œ REMEMBERER è¨˜éŒ„è§€å¯Ÿã€è¡Œå‹•ã€çŽå‹µå’Œ Q å€¼ï¼Œæª¢ç´¢é¡žä¼¼æ¡ˆä¾‹ä½œç‚ºå°‘æ¨£æœ¬ç¯„ä¾‹ã€‚é€™äº›æ–¹æ³•è¡¨æ˜Ž LLMs å¯ä»¥åœ¨ä¸é€²è¡Œåƒæ•¸æ›´æ–°çš„æƒ…æ³ä¸‹é€²è¡Œæ”¹é€²ã€‚ç„¶è€Œï¼Œç”±æ–¼åƒæ•¸å›ºå®šï¼Œå®ƒå€‘ç„¡æ³•æ“´å±•å…§åœ¨çŸ¥è­˜ï¼Œæ‰€ä»¥é©æ‡‰åªæ˜¯çŸ­æœŸçš„ï¼ˆZhang et al.ï¼Œ2023ï¼‰ï¼Œä¾è³´å¤–éƒ¨è¨˜æ†¶è€Œä¸æ˜¯å¯¦ç¾é•·æœŸæ¼”é€²å’Œæ³›åŒ–ã€‚

**é€éŽçŸ¥è­˜è’¸é¤¾é€²è¡Œå­¸ç¿’**ã€‚æˆ‘å€‘çš„æ··åˆéžç­–ç•¥æ›´æ–°å‡½æ•¸åœ¨ç·šä¸Šè¨“ç·´æœŸé–“å……ç•¶çŽå‹µå¼•å°Žçš„çŸ¥è­˜è’¸é¤¾ã€‚Snell et al.ï¼ˆ2022ï¼‰å¼•å…¥äº†ä¸Šä¸‹æ–‡è’¸é¤¾ï¼Œå…¶ä¸­æ¨¡åž‹é¦–å…ˆä½¿ç”¨ Teacher æç¤ºï¼ˆåŒ…å«æŒ‡ç¤ºã€ç¯„ä¾‹ã€è§£é‡‹å’Œæš«å­˜æ¿æŽ¨ç†ï¼‰è§£æ±ºä»»å‹™ï¼Œç„¶å¾Œå­¸ç¿’é€éŽé›¢ç·šçš„ã€åŸºæ–¼ SFT çš„è’¸é¤¾å¾žæœ€å° Student æç¤ºç”¢ç”Ÿæœ€çµ‚ç­”æ¡ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘å€‘å°‡çŸ¥è­˜è’¸é¤¾æ•´åˆåˆ°ç·šä¸Š RL ä¸­ï¼Œåˆ©ç”¨ç·šä¸Šé©æ‡‰æ€§åŒæ™‚å¢žå¼·æŽ¢ç´¢ï¼Œä»¥å¯¦ç¾æ›´é«˜æ•ˆçš„è¨“ç·´ã€‚

**LLM Agents çš„ RL**ã€‚RL ç‚ºé€éŽç’°å¢ƒäº¤äº’çš„è§€å¯Ÿå’ŒçŽå‹µä¿¡è™Ÿä¾†æœ€ä½³åŒ– LLM åƒæ•¸æä¾›äº†å¥å…¨çš„æ¡†æž¶ã€‚å…ˆå‰çš„å·¥ä½œ Retrospexï¼ˆXiang et al.ï¼Œ2024ï¼‰è¡¨æ˜Žé›¢ç·š RLï¼ˆå¾žå¤§åž‹è¨˜éŒ„è³‡æ–™é›†å­¸ç¿’æœ€å„ªç­–ç•¥ï¼‰å¯ä»¥æ”¹é€² LLM agent çš„æ€§èƒ½ã€‚æœ€è¿‘çš„ç ”ç©¶å°ˆæ³¨æ–¼ç·šä¸Š RLï¼ˆShao et al.ï¼Œ2024ï¼›Feng et al.ï¼Œ2025bï¼›Wang et al.ï¼Œ2025ï¼‰ï¼Œå…¶ä¸­ agents é€²è¡Œå¯¦æ™‚å­¸ç¿’ã€‚GiGPOï¼ˆFeng et al.ï¼Œ2025bï¼‰é€éŽå°å…·æœ‰ç›¸ä¼¼è§€å¯Ÿçš„ rollouts é€²è¡Œåˆ†çµ„ä¾†æŽ¨é€² GRPOï¼Œå¯¦ç¾æ›´ç´°ç²’åº¦çš„ä¿¡ç”¨åˆ†é…å’Œæ›´å¼·çš„æ€§èƒ½ã€‚æˆ‘å€‘çš„å·¥ä½œé€éŽå°‡éžåƒæ•¸è¨˜æ†¶æ›´æ–°æ•´åˆåˆ°ç­–ç•¥å…§å’Œç­–ç•¥å¤–å­¸ç¿’ä¸­ä¾†æŽ¨é€²é€™ä¸€ç·šä¸Š RL æ–¹å‘ï¼Œå¾žè€Œç”¢ç”Ÿé¡¯è‘—æ›´é«˜çš„æ¨£æœ¬æ•ˆçŽ‡ã€‚

**å¢žå¼·ç·šä¸Š RL çš„æŽ¢ç´¢**ã€‚ç·šä¸Š RL ä¸­çš„æ ¸å¿ƒæŒ‘æˆ°æ˜¯æœ‰æ•ˆçš„æŽ¢ç´¢ã€‚å¤å…¸æ–¹æ³•ï¼Œå¦‚è¨ˆæ•¸åž‹æŽ¢ç´¢ï¼ˆBellemare et al.ï¼Œ2016ï¼‰å’Œéš¨æ©Ÿç¶²è·¯è’¸é¤¾ Random Network Distillationï¼ˆBurda et al.ï¼Œ2018bï¼‰ä½¿ç”¨å…§åœ¨çŽå‹µä¾†é¼“å‹µæ–°ç©Žæ€§ã€‚Go-Exploreï¼ˆEcoffet et al.ï¼Œ2019ï¼‰å„²å­˜é—œéµç‹€æ…‹ä¸¦å¾žä¸­é‡æ–°æŽ¢ç´¢ï¼Œè§£æ±ºäº† Atari éŠæˆ²ç­‰é›£ä»¥æŽ¢ç´¢çš„ä»»å‹™ã€‚å…¶ LLM æ“´å±• Intelligent Go-Exploreï¼ˆLu et al.ï¼Œ2025aï¼‰åœ¨ TextWorldï¼ˆCÃ´tÃ© et al.ï¼Œ2018ï¼‰ç­‰ç’°å¢ƒä¸­å–å¾—äº†å¼·å‹çš„æˆæžœï¼Œä½†ä¾è³´æ–¼å¤§åž‹é–‰æºæ¨¡åž‹ä¸”ä¸åŸ·è¡Œåƒæ•¸æ›´æ–°ã€‚åœ¨æˆ‘å€‘çš„ä¸¦è¡Œå·¥ä½œä¸­ï¼ŒRLVMRï¼ˆZhang et al.ï¼Œ2025ï¼‰æŽ¡ç”¨æš–å•Ÿå‹• SFT ä¾†å¼•ç™¼å¤šæ¨£åŒ–çš„æŽ¨ç†é¡žåž‹ï¼ˆè¦åŠƒã€æŽ¢ç´¢å’Œåæ€ï¼‰ï¼Œä¸¦åœ¨ç·šä¸Š RL æœŸé–“ç‚ºæ¯ç¨®æŽ¨ç†é¡žåž‹æä¾›å¯†é›†çš„éŽç¨‹ç´šçŽå‹µï¼Œå¢žå¼·æŽ¢ç´¢å’Œä¿¡ç”¨åˆ†é…ã€‚é€™äº›ç ”ç©¶å…±åŒå¼·èª¿äº†çµæ§‹åŒ–æŽ¢ç´¢å°æ–¼å°‡ RL æ“´å±•åˆ°è¤‡é›œç’°å¢ƒçš„é‡è¦æ€§ã€‚

## 6 å¯¦é©—

ç‚ºäº†æª¢é©— EMPO 2 çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘å€‘åœ¨å…©å€‹å»£æ³›ä½¿ç”¨çš„ LLM agent åŸºæº–ä¸Šé€²è¡Œäº†å¤§é‡å¯¦é©—ï¼šScienceWorldï¼ˆWang et al., 2022ï¼‰å’Œ WebShopï¼ˆYao et al., 2022ï¼‰ï¼Œä½¿ç”¨ Qwen2.5-7B-Instructï¼ˆQwen et al., 2025ï¼‰ä½œç‚ºåŸºç¤Žæ¨¡åž‹ã€‚æˆ‘å€‘è©•ä¼°çš„ EMPO 2 æ•ˆèƒ½æ˜¯è¨“ç·´æ¨¡åž‹åœ¨æ¸¬è©¦æ™‚ä¸å¸¶è¨˜æ†¶çš„æ•ˆèƒ½ã€‚

### 6.1 ScienceWorld

ScienceWorldï¼ˆWang et al., 2022ï¼‰æ˜¯ä¸€å€‹äº’å‹•å¼æ–‡å­—åŸºæº–ï¼Œå…¶ä¸­ agent åœ¨å°å­¸æ°´æº–åŸ·è¡Œç§‘å­¸å¯¦é©—ã€‚æˆåŠŸå®Œæˆé€™äº›å¯¦é©—éœ€è¦é•·æœŸçš„å¤šæ­¥é©Ÿè¦åŠƒã€å‡è¨­æª¢é©—å’Œçµæžœè§£é‡‹ï¼Œä»¥åŠå……åˆ†çš„æŽ¢ç´¢ä»¥ç¢ºå®šå¿…è¦å·¥å…·çš„ä½ç½®å’Œæ‡‰æŽ¡å–çš„é©ç•¶è¡Œå‹•ã€‚ScienceWorld åŒ…å«ä¾†è‡ªä¸åŒä¸»é¡Œçš„ä»»å‹™ï¼Œåœ¨æˆ‘å€‘çš„å¯¦é©—ä¸­ï¼Œæˆ‘å€‘æ¶µè“‹äº† 19 é …ä»»å‹™ï¼Œè·¨è¶ŠåŒ–å­¸ã€åˆ†é¡žã€ç”Ÿç‰©å­¸ã€é›»å­¸å’Œæ¸¬é‡ã€‚

åŸºç·šã€‚æˆ‘å€‘å°‡ EMPO 2 èˆ‡å¹¾ç¨® RL æ–¹æ³•é€²è¡Œæ¯”è¼ƒã€‚å°æ–¼éžåƒæ•¸ RLï¼ŒReflexionï¼ˆShinn et al., 2023ï¼‰ä»¥éžåƒæ•¸æ–¹å¼æ›´æ–°è¨˜æ†¶ï¼Œé€éŽç´å…¥ä¾†è‡ªå‰åºè»Œè·¡çš„ LLM åæ€ï¼Œä¸¦åœ¨å¾ŒçºŒè©¦é©—çš„æç¤ºä¸­ä½¿ç”¨å®ƒå€‘ã€‚å°æ–¼é›¢ç·š RLï¼ŒRetrospexï¼ˆXiang et al., 2024ï¼‰åˆ©ç”¨ SFT è¨“ç·´çš„æ¨¡åž‹ï¼Œä¸¦ä½¿ç”¨é€éŽéš±å¼ Q å­¸ç¿’ï¼ˆImplicit Q-learningï¼‰ï¼ˆKostrikov et al., 2022ï¼‰å­¸ç¿’çš„ Q å‡½æ•¸å‹•æ…‹é‡æ–°è©•åˆ†è¡Œå‹•ã€‚å®˜æ–¹ Retrospex è«–æ–‡ä½¿ç”¨äº†è¼ƒå°çš„ Flan-T5-Largeï¼ˆChung et al., 2024ï¼‰ï¼ˆ770Mï¼‰ï¼Œä¸¦ç´å…¥äº†äººå·¥è¨­è¨ˆçš„å•Ÿç™¼å¼æ–¹æ³•ä¾†åœ¨è©•ä¼°æœŸé–“å”åŠ© agentã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç‚ºäº†ç¢ºä¿æˆ‘å€‘å¯¦é©—è¨­ç½®çš„ä¸€è‡´æ€§ï¼Œæˆ‘å€‘å°‡ Retrospex çš„åŸºç¤Žæ¨¡åž‹æ¨™æº–åŒ–ç‚º Qwen2.5-7B-Instructï¼Œä¸¦æŽ’é™¤é€™äº›å•Ÿç™¼å¼æ–¹æ³•ã€‚æœ€å¾Œï¼Œå°æ–¼ç·šä¸Š RLï¼Œæˆ‘å€‘å°‡ GRPOï¼ˆShao et al., 2024ï¼‰ç´å…¥ä½œç‚ºä»£è¡¨æ€§åŸºç·šã€‚æ›´å¤šç´°ç¯€åœ¨é™„éŒ„ D ä¸­æä¾›ã€‚

P[1]Â¿ \arraybackslash p#1

[FIGURE:13.png] è¡¨ 1ï¼šScienceWorld çš„æ¯”è¼ƒçµæžœã€‚ScienceWorld ä¸­çš„æ¯é …ä»»å‹™åŒ…å«å¤šå€‹è®Šé«”ã€‚æˆ‘å€‘ä½¿ç”¨å‰äº”å€‹è®Šé«”é€²è¡Œè¨“ç·´ï¼Œä¸¦åœ¨ 20 å€‹æœªè¦‹éŽçš„æ¸¬è©¦è®Šé«”ä¸Šé€²è¡Œè©•ä¼°ã€‚ç²—é«”è¡¨ç¤ºæ¯é …ä»»å‹™çš„æœ€ä½³æ•ˆèƒ½ï¼Œè€Œç´…è‰²é™°å½±æ¨™è¨˜äº†åƒæ•¸æ›´æ–°å¾—åˆ†ä½Žæ–¼éžåƒæ•¸æ›´æ–°çš„æƒ…æ³ã€‚æˆ‘å€‘è©•ä¼°çš„ EMPO 2 æ•ˆèƒ½æ˜¯è¨“ç·´æ¨¡åž‹åœ¨æ¸¬è©¦æ™‚ä¸å¸¶è¨˜æ†¶çš„æ•ˆèƒ½ã€‚