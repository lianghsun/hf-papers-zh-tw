{
  "source": "html",
  "markdown": "# Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization\n\nExploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO 2 ), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO 2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO 2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO 2 as a promising framework for building more exploratory and generalizable LLM-based agents.\n\n## 1 Introduction\n\n[FIGURE:25_graph.png] (a)\n\nLarge Language Models (LLMs) have recently emerged as powerful agents capable of reasoning, planning, and interacting with external environments (Achiam et al. , 2023 ; Park et al. , 2023 ; Yao et al. , 2023 ; Kim et al. , 2025 ) . When combined with reinforcement learning (RL), such agents can adapt their behavior based on experience and feedback, enabling them to go beyond static prompting or supervised fine-tuning (Guo et al. , 2025 ; Tan et al. , 2024 ) . This paradigm has driven recent progress in areas such as interactive decision-making, tool use, and embodied AI (Feng et al. , 2025b ; Lu et al. , 2025b ; Feng et al. , 2025a ; Dong et al. , 2025 ; Luo et al. , 2025 ) .\n\nHowever, a key limitation of current LLM-based agents lies in their reliance on exploiting prior knowledge rather than engaging in systematic exploration. While RL frameworks emphasize balancing exploration and exploitation, many LLM-agent systems primarily leverage pretrained knowledge and conduct only limited search within familiar distributions. As a result, these agents often struggle in environments where progress depends on discovering novel states or actively acquiring new information, rather than reusing what is already known.\n\nTo address this challenge, recent research has incorporated external memory modules into LLMs as a form of long-term memory. This enables models to leverage past experiences to correct failed attempts, thereby improving decision-making in subsequent trials without requiring parameter updates (Shinn et al. , 2023 ; Zhang et al. , 2023 ) . However, as noted in Zhang et al. ( 2023 ) , the performance of such methods tends to saturate quickly, since collecting experiences with static parameters cannot fully capture the diversity needed for continuous improvement.\n\n[FIGURE:concept.png] Figure 2: Non-parametric updates can encourage exploration, bootstrapping parametric updates.\n\nIn this work, we present a unified framework that enables LLM agents to learn more effectively through broader exploration by jointly updating their parametric policy parameters with RL and their non-parametric memory module through interaction. Crucially, the non-parametric updates not only complement but also enhance the efficiency of parametric learning, thereby enabling more effective exploration and adaptation. This dual-update paradigm serves as a bridge between parameter-level optimization and memory-augmented reasoning. While memory is utilized during learning, moving toward more generalizable intelligence requires reducing dependence on external memory and instead embedding its benefits directly into the modelâ€™s parameters. To this end, we propose E xploratory M emory-Augmented On- and Off-P olicy O ptimization (EMPO 2 ), a new hybrid RL algorithm that incorporates two modes in the rollout phaseâ€”depending on whether memory is usedâ€”and two modes in the update phaseâ€”on-policy and off-policy learningâ€”thereby enabling agents to leverage memory when available while remaining robust in its absence.\n\nIn our experiments, we evaluate EMPO 2 on two widely used multi-step embodied reasoning environments that require exploration to solve complex tasks: ScienceWorld (Wang et al. , 2022 ) and WebShop (Yao et al. , 2022 ) . We compare its performance against a range of non-parametric and parametric (offline and online) RL approaches. As summarized in Figure 1 , EMPO 2 substantially outperforms prior algorithms, achieving a 128.6% improvement on ScienceWorld and an 11.3% improvement on WebShop over the strong online RL baseline GRPO. The training curve in Figure 1 (a) further shows that, unlike GRPO, which converges prematurely to a suboptimal solution, EMPOÂ² leverages continuous exploration and successfully solves the task. Moreover, for the OOD experiments (Figure 1 , rightmost), the model also achieves good scores with only a few trials and no weight updates, indicating that the updated model has acquired the ability to use memory to explore unseen or unfamiliar environments. These results highlight EMPO 2 as a promising direction for building more adaptive and generalizable embodied agents.\n\n## 2 Preliminaries\n\nOnline RL consists of alternating between a rollout phase, in which trajectories are generated using the current policy Ï€ \\pi parameterized by Î¸ \\theta , and an update phase, in which the policy is optimized based on those rollouts.\n\nPolicy Rollout. We consider a setting where, given a sampled task u âˆ¼ p â€‹ ( ð’° ) u\\sim p(\\mathcal{U}) , an LLM agent solves the task through multi-step interactions with the environment. Starting from task u u , the LLM Ï€ Î¸ \\pi_{\\theta} generates the first natural-language action a 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ u ) âˆˆ ð’œ a_{1}\\sim\\pi_{\\theta}(\\cdot\\mid u)\\in\\mathcal{A} . Executing this action, the environment returns a reward r 1 r_{1} and the next state s 1 s_{1} . At a general timestep t t , conditioned on the current state s t s_{t} and the task u u , the policy produces the next action a t + 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ s t , u ) a_{t+1}\\sim\\pi_{\\theta}(\\cdot\\mid s_{t},u) . This interaction loop continues until the task is completed or a maximum number of steps is reached. A rollout trajectory is thus defined as the sequence of states, actions, and rewards, Ï„ = ( u , a 1 , r 1 , s 1 , a 2 , r 2 , â€¦ , s T ) . \\tau=\\big(u,a_{1},r_{1},s_{1},a_{2},r_{2},\\ldots,s_{T}\\big).\n\nGroup Relative Policy Optimization. Group Relative Policy Optimization (GRPO) (Shao et al. , 2024 ) updates the policy by comparing multiple rollouts of the same task u u , removing the need for the value function in PPO (Schulman et al. , 2017 ) . Given a task u u , the policy Ï€ Î¸ \\pi_{\\theta} generates N N rollout trajectories { Ï„ ( 1 ) , â€¦ , Ï„ ( N ) } \\{\\tau^{(1)},\\ldots,\\tau^{(N)}\\} . Each trajectory receives a return { R ( 1 ) , â€¦ , R ( N ) } \\{R^{(1)},\\ldots,R^{(N)}\\} , defined as the sum of rewards along the trajectory: R ( i ) = âˆ‘ t = 1 T r t ( i ) . R^{(i)}=\\sum_{t=1}^{T}r_{t}^{(i)}. . For each action a t ( i ) a_{t}^{(i)} taken in trajectory Ï„ ( i ) \\tau^{(i)} , we define its relative advantage as: A â€‹ ( a t ( i ) ) = R ( i ) âˆ’ 1 N â€‹ âˆ‘ j = 1 N R ( j ) Ïƒ â€‹ ( R ) , A(a_{t}^{(i)})=\\frac{R^{(i)}-\\frac{1}{N}\\sum_{j=1}^{N}R^{(j)}}{\\sigma(R)}, where actions from trajectories with higher-than-average reward obtain positive advantage, while those from lower-performing ones obtain negative advantage. The GRPO loss is then:\n\n|  | ð”¼ u âˆ¼ p â€‹ ( ð’° ) { Ï„ ( i ) } i = 1 N âˆ¼ Ï€ Î¸ old \\displaystyle\\mathbb{E}_{\\begin{subarray}{c}u\\sim p(\\mathcal{U})\\\\\n\\{\\tau^{(i)}\\}_{i=1}^{N}\\sim\\pi_{\\theta_{\\text{old}}}\\end{subarray}} | [ 1 N â€‹ T â€‹ âˆ‘ i = 1 N âˆ‘ t = 1 T min â¡ ( Ï Î¸ â€‹ ( a t ( i ) ) â€‹ A â€‹ ( a t ( i ) ) , clip â€‹ ( Ï Î¸ â€‹ ( a t ( i ) ) , 1 âˆ’ Ïµ , 1 + Ïµ ) â€‹ A â€‹ ( a t ( i ) ) ) ] \\displaystyle\\Bigg[\\frac{1}{NT}\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\min\\Big(\\rho_{\\theta}(a_{t}^{(i)})A(a_{t}^{(i)}),\\text{clip}\\big(\\rho_{\\theta}(a_{t}^{(i)}),1-\\epsilon,1+\\epsilon\\big)A(a_{t}^{(i)})\\Big)\\Bigg] |  |\n| --- | --- | --- | --- |\n|  |  | âˆ’ Î² D KL ( Ï€ Î¸ ( â‹… | u ) âˆ¥ Ï€ ref ( â‹… | u ) ) , \\displaystyle\\quad-\\beta\\,D_{\\text{KL}}\\!\\big(\\pi_{\\theta}(\\cdot|u)\\,\\|\\;\\pi_{\\text{ref}}(\\cdot|u)\\big), |  | (1) |\n\nwhere Ï Î¸ â€‹ ( a t ( i ) ) = Ï€ Î¸ â€‹ ( a t ( i ) | s t ( i ) , u ) Ï€ Î¸ old â€‹ ( a t ( i ) | s t ( i ) , u ) , \\rho_{\\theta}(a_{t}^{(i)})=\\frac{\\pi_{\\theta}(a_{t}^{(i)}|s_{t}^{(i)},u)}{\\pi_{\\theta_{\\text{old}}}(a_{t}^{(i)}|s_{t}^{(i)},u)}, with Î² â‰¥ 0 \\beta\\geq 0 controlling the regularization strength toward a reference policy Ï€ ref \\pi_{\\text{ref}} .\n\n## 3 The Exploration Problem of LLM Agents\n\nLLMs encode rich prior knowledge, but such priors often fail to reflect the actual rules or dynamics of a given environment. Blind reliance on these priors can lead to erroneous behaviors, making it necessary for agents to adapt through direct interaction and trial-and-error. A key requirement for such adaptation is exploration , which involves seeking information beyond pre-training, sometimes by taking atypical or counterintuitive actions. However, current LLM-based agents struggle with this (Qiao et al. , 2024 ; Zhou et al. , 2024 ) , as it demands stepping outside the distribution of behaviors where the model feels most confident.\n\nConsequently, many prior studies have sought to align agents with new environments through warm-start supervised fine-tuning (SFT) using numerous golden trajectories (Song et al. , 2024 ; Qiao et al. , 2024 ; Xiang et al. , 2024 ) , leveraging large-scale models such as GPT-4 (Tang et al. , 2024 ; Lin et al. , 2023 ) , or employing human engineering or well-established simulation information (Choudhury and Sodhi, 2025 ) . While these methods achieve strong results in constrained settings, their effectiveness is limited to cases where such external support is available, and they generalize poorly to unseen scenarios without it.\n\n[FIGURE:exploration_problem.png] Figure 3: When training LLM with GRPO in ScienceWorld, the agent struggles because of insufficient exploration. For instance, in the task â€œturn on the red light bulb,â€ the agent must first find the red light bulb before activating it. However, the agent fails to locate it and, as a result, cannot complete the task. Rather than analyzing the cause of failure and exploring alternative actions, the agent proceeds unchanged, so its score stagnates even as additional training steps are taken.\n\nTherefore, we focus on how to efficiently train agents in online RL through trial and error, without any prior embedding of the environmentâ€™s rules. The key challenge is that, without intrinsic exploration capability, online RL struggles to optimize effectively. As illustrated in Figure 3 , in ScienceWorld (Wang et al. , 2022 ) environment the agent is given the mission â€œturn on the red light bulb.â€ The instructions specify that the agent should first focus on the light bulb and then build a circuit to activate it, based on the current room observation. However, since no red light bulb is present in the observation, the agent must search the environment to locate it. Instead, the agent follows the instruction literally, attempts to focus on the red light bulb, and fails because it does not exist in the room. Ideally, when an agent fails to reach its goal, it should analyze the reasons for failure and broaden its action space to discover successful strategies. Yet in representative online RL algorithms GRPO (Shao et al. , 2024 ) , prior trajectory rollouts provide no continuity beyond a scalar reward signal, thereby restricting exploration and ultimately limiting learning.\n\n## 4 Method\n\nIn this section, we present Exploratory Memory-augmented On- and Off-Policy Optimization (EMPO 2 ), a novel algorithm aimed at tackling the exploration challenges in online RL. EMPO 2 operates in two modes for both rollout phase and update phase. During rollout, actions can be generated either through (1) prompting without memory , where no retrieved information is used, or (2) memory-augmented prompting , conditioned on tips retrieved from memory. In the update phase, rollouts with memory-augmented prompting are used in two ways: (a) on-policy , where tips are retained and the update is performed with the original prompt, and (b) off-policy , where tips are removed during update. Notably, tips are generated not by a separate model but by the policy Ï€ Î¸ \\pi_{\\theta} itself, which is continually updated during training. The full algorithm is provided in Appendix A .\n\n### 4.1 Advancing Exploration with Self-Generated Memory\n\nA key component of EMPO 2 is its use of memory to maintain continuity across rollouts. Information obtained from an agentâ€™s interactions can be encoded into parameters through policy optimization, but it can also be recorded in an external memory that the agent continuously consults. Since our policy is initialized from a pretrained LLM with inherent summarization and reflection abilities, these abilities can be leveraged as auxiliary signals in addition to scalar rewards, thereby guiding exploration more effectively. To realize this, EMPO 2 integrates both parametric (parameter updates within the LLM) and non-parametric (external memory) updates, strengthening the linkage between rollouts and promoting exploration, with all data and guidance generated autonomously by the agent.\n\n[FIGURE:motivation3.png] Figure 4: In EMPO 2 , the current policy parameters Ï€ Î¸ \\pi_{\\theta} are used to review past rollouts, with the resulting insights added to memory. This updated memory conditions subsequent rollouts and promotes exploration.\n\nIn the non-parametric updates, similar to Reflexion (Shinn et al. , 2023 ) , the agent reviews past rollouts, generates self-guidance tips , and stores them in memory. These tips help the agent avoid repeated mistakes and explore new strategies. Unlike Reflexion, focuses on iterative verbal guidance to achieve higher rewards in the next trial, our approach aims for these tips to lead to enhanced exploration that is ultimately consolidated through parametric updates.\n\nSelf-Generated Memory and Tips. We define a memory buffer â„³ = { tip 1 , tip 2 , â€¦ } \\mathcal{M}=\\{\\text{tip}_{1},\\text{tip}_{2},\\ldots\\} , which stores reflective tips generated by the policy Ï€ Î¸ \\pi_{\\theta} during trajectory reflection. Formally, when an episode i i of task u u terminates at timestep t t , the policy takes the final state s t s_{t} together with a tip-generation prompt as input and produces a tip, where tip i âˆ¼ Ï€ Î¸ â€‹ ( s t , u , tip-generation prompt ) . \\text{tip}_{i}\\sim\\pi_{\\theta}(s_{t},u,\\text{tip-generation prompt}). A set of illustrative examples is provided below, while the tip-generation prompt is presented in Appendix B , and additional examples are included in Appendix E.1 .\n\n### 4.2 Parameterize non-parametric updates via hybrid policy optimization\n\nAgents can use memory to improve exploration and learning efficiency, but the acquired knowledge needs be internalized into model parameters to enhance inherent capabilities. To this end, we propose two modes for the rollout and update phases, whose combinations yield three hybrid learning modes (Figure 5 ).\n\n[FIGURE:EMPO.png] Figure 5: EMPO 2 mode combinations. By combining the two rollout modes and update modes, three EMPO mode configurations are possible: on-policy learning without memory, on-policy learning with memory and off-policy learning.\n\nRollout Modes. During rollouts, the agent samples between the two modes, selecting one mode at each step: mode (2) with memory rollout probability p p and mode (1) with probability 1 âˆ’ p 1-p . The ablation study of p p can be found in Appendix F.1 .\n\n- (1) Prompting Without Memory. For each task u u , at each timestep t t , the policy Ï€ Î¸ \\pi_{\\theta} generates actions conditioned only on the current state s t s_{t} and the task u u : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ s t , u ) . a_{t+1}\\sim\\pi_{\\theta}(\\cdot\\mid s_{t},u).\n- (2) Memory-Augmented Prompting. For each task u u , at each timestep t t , a retrieval operator Retr â€‹ ( o t ; â„³ ) âŠ† â„³ \\mathrm{Retr}(o_{t};\\mathcal{M})\\subseteq\\mathcal{M} selects tips most relevant to the current state s t s_{t} , e.g., via similarity search in the embedding space. We denote the retrieved set as tips t \\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t} . In memory-augmented prompting, the policy conditions its action on both s t s_{t} and tips t \\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t} : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… | s t , u , tips t ) . a_{t+1}\\sim\\pi_{\\theta}\\!\\left(\\cdot\\,\\middle|\\,s_{t},u,\\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t}\\right). We limit the number of retrieved tips at 10.\n\nPrompting Without Memory. For each task u u , at each timestep t t , the policy Ï€ Î¸ \\pi_{\\theta} generates actions conditioned only on the current state s t s_{t} and the task u u : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… âˆ£ s t , u ) . a_{t+1}\\sim\\pi_{\\theta}(\\cdot\\mid s_{t},u).\n\nMemory-Augmented Prompting. For each task u u , at each timestep t t , a retrieval operator Retr â€‹ ( o t ; â„³ ) âŠ† â„³ \\mathrm{Retr}(o_{t};\\mathcal{M})\\subseteq\\mathcal{M} selects tips most relevant to the current state s t s_{t} , e.g., via similarity search in the embedding space. We denote the retrieved set as tips t \\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t} . In memory-augmented prompting, the policy conditions its action on both s t s_{t} and tips t \\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t} : a t + 1 âˆ¼ Ï€ Î¸ ( â‹… | s t , u , tips t ) . a_{t+1}\\sim\\pi_{\\theta}\\!\\left(\\cdot\\,\\middle|\\,s_{t},u,\\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t}\\right). We limit the number of retrieved tips at 10.\n\nUpdate Modes. Trajectories generated under rollout mode (1) are directly used for updates, whereas those generated under rollout mode (2)â€”memory-augmented promptingâ€”follow one of two update modes chosen at random during the update phase. Mode (b) is selected with off-policy update probability q q , and mode (a) with probability 1 âˆ’ q 1-q . The ablation study of q q can be found in Appendix F.1 .\n\n- (a) On-Policy Updates. On-policy update uses the same prompt as in the rollout, and Ï Î¸ â€‹ ( a t ( i ) ) \\rho_{\\theta}(a_{t}^{(i)}) in eq. 1 becomes Ï Î¸ â€‹ ( a t ( i ) ) = Ï€ Î¸ â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) Ï€ Î¸ old â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) . \\rho_{\\theta}(a_{t}^{(i)})=\\frac{\\pi_{\\theta}(a_{t}^{(i)}\\mid s_{t}^{(i)},u,\\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}^{(i)}\\mid s_{t}^{(i)},u,\\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t})}.\n- (b) Off-Policy Updates. In this mode, the stored log-probabilities â„“ t tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u , tips t ) \\ell^{\\text{tips}}_{t}=\\log\\pi_{\\theta}(a_{t}\\mid s_{t},u,\\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t}) are replaced with the log-probabilities assigned by the same policy Ï€ Î¸ \\pi_{\\theta} when conditioned only on ( s t , u ) (s_{t},u) , namely â„“ t no-tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u ) \\ell^{\\text{no-tips}}_{t}=\\log\\pi_{\\theta}(a_{t}\\mid s_{t},u) . In this formulation, the advantage update is performed based on how natural the action appears under the distribution without tips. This construction can be interpreted as a form of reward-guided knowledge distillation . Trajectories sampled under the tips-conditioned policy act as teacher demonstrations, while the student policy Ï€ Î¸ ( â‹… âˆ£ s , u ) \\pi_{\\theta}(\\cdot\\mid s,u) is updated to reproduce those trajectories in proportion to their advantage. High-reward trajectories ( A ^ t > 0 \\hat{A}_{t}>0 ) are reinforced, while low-reward trajectories ( A ^ t < 0 \\hat{A}_{t}<0 ) are suppressed, resulting in selective distillation that emphasizes beneficial behaviors. In this way, tips serve as an intermediate scaffolding mechanism that improves exploration and trajectory quality, while the reward signal ensures that only advantageous behaviors are ultimately retained. Consequently, the final policy learns to internalize the benefits of tip conditioning without requiring tips at inference time. Appendix C provides an illustrative breakdown and a summary table for the calculation of the importance sampling ratio.\n\nOn-Policy Updates. On-policy update uses the same prompt as in the rollout, and Ï Î¸ â€‹ ( a t ( i ) ) \\rho_{\\theta}(a_{t}^{(i)}) in eq. 1 becomes Ï Î¸ â€‹ ( a t ( i ) ) = Ï€ Î¸ â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) Ï€ Î¸ old â€‹ ( a t ( i ) âˆ£ s t ( i ) , u , tips t ) . \\rho_{\\theta}(a_{t}^{(i)})=\\frac{\\pi_{\\theta}(a_{t}^{(i)}\\mid s_{t}^{(i)},u,\\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}^{(i)}\\mid s_{t}^{(i)},u,\\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t})}.\n\nOff-Policy Updates. In this mode, the stored log-probabilities â„“ t tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u , tips t ) \\ell^{\\text{tips}}_{t}=\\log\\pi_{\\theta}(a_{t}\\mid s_{t},u,\\text{\\fcolorbox{empo2}{white}{\\textcolor{empo2}{tips}}}_{t}) are replaced with the log-probabilities assigned by the same policy Ï€ Î¸ \\pi_{\\theta} when conditioned only on ( s t , u ) (s_{t},u) , namely â„“ t no-tips = log â¡ Ï€ Î¸ â€‹ ( a t âˆ£ s t , u ) \\ell^{\\text{no-tips}}_{t}=\\log\\pi_{\\theta}(a_{t}\\mid s_{t},u) . In this formulation, the advantage update is performed based on how natural the action appears under the distribution without tips.\n\nThis construction can be interpreted as a form of reward-guided knowledge distillation . Trajectories sampled under the tips-conditioned policy act as teacher demonstrations, while the student policy Ï€ Î¸ ( â‹… âˆ£ s , u ) \\pi_{\\theta}(\\cdot\\mid s,u) is updated to reproduce those trajectories in proportion to their advantage. High-reward trajectories ( A ^ t > 0 \\hat{A}_{t}>0 ) are reinforced, while low-reward trajectories ( A ^ t < 0 \\hat{A}_{t}<0 ) are suppressed, resulting in selective distillation that emphasizes beneficial behaviors. In this way, tips serve as an intermediate scaffolding mechanism that improves exploration and trajectory quality, while the reward signal ensures that only advantageous behaviors are ultimately retained. Consequently, the final policy learns to internalize the benefits of tip conditioning without requiring tips at inference time. Appendix C provides an illustrative breakdown and a summary table for the calculation of the importance sampling ratio.\n\n[FIGURE:low_filtering.png] Figure 6: Masking tokens stabilizes training.\n\nStabilizing Off-Policy Training. Off-policy training is prone to instability and may collapse (see Figure 6 ). In such cases, gradient normalization, entropy loss, KL loss, and policy gradient loss can all diverge to NaN. Prior work, Yang et al. ( 2025 ) shows that low-probability tokens destabilize training by amplifying gradient magnitudes through unbounded likelihood ratios. Motivated by this, we introduce a masking mechanism that suppresses the advantage term for tokens whose probability under Ï€ Î¸ \\pi_{\\theta} falls below a threshold Î´ \\delta . Finally, the loss in Eq. 1 is modified as\n\n|  | ð”¼ u âˆ¼ p â€‹ ( ð’° ) { Ï„ ( i ) } âˆ¼ Ï€ Î¸ old [ 1 N â€‹ T âˆ‘ i = 1 N âˆ‘ t = 1 T \\displaystyle\\mathbb{E}_{\\begin{subarray}{c}u\\sim p(\\mathcal{U})\\\\\n\\{\\tau^{(i)}\\}\\sim\\pi_{\\theta_{\\text{old}}}\\end{subarray}}\\Bigg[\\frac{1}{NT}\\sum_{i=1}^{N}\\sum_{t=1}^{T} | min ( Ï Î¸ ( i , t ) A ( a t ( i ) ) , clip ( Ï Î¸ ( i , t ) , 1 âˆ’ Ïµ , 1 + Ïµ ) A ( a t ( i ) ) ) â‹… ðŸ Ï€ Î¸ â€‹ ( a t ( i ) | s t ( i ) , u ) â‰¥ Î´ ] \\displaystyle\\min\\Big(\\rho_{\\theta}^{(i,t)}A(a_{t}^{(i)}),\\;\\text{clip}\\big(\\rho_{\\theta}^{(i,t)},1-\\epsilon,1+\\epsilon\\big)A(a_{t}^{(i)})\\Big)\\cdot\\mathbf{1}_{\\pi_{\\theta}(a_{t}^{(i)}|s_{t}^{(i)},u)\\geq\\delta}\\Bigg] |  |\n| --- | --- | --- | --- |\n|  |  | âˆ’ Î² D KL ( Ï€ Î¸ ( â‹… | u ) âˆ¥ Ï€ ref ( â‹… | u ) ) . \\displaystyle\\quad-\\beta D_{\\text{KL}}\\!\\big(\\pi_{\\theta}(\\cdot|u)\\,\\|\\;\\pi_{\\text{ref}}(\\cdot|u)\\big). |  | (2) |\n\n[FIGURE:entropy.png] Figure 7: Policy entropy comparison with vs. without intrinsic rewards.\n\nIntrinsic Rewards for Exploration. To further encourage exploration, and inspired by prior work on exploration-targeted online RL (Burda et al. , 2018b ; Bellemare et al. , 2016 ; Ecoffet et al. , 2019 ) , we introduce an intrinsic reward based on the novelty of the current state. A memory list stores distinct states, and for each new state we compute its cosine similarity with existing entries. If the similarity falls below a threshold, the state is added to memory and assigned a reward. The intrinsic reward is defined as r intrinsic = 1 n r_{\\text{intrinsic}}=\\frac{1}{n} , where n n denotes the number of similar past states. This mechanism encourages the agent to explore novel states even when no extrinsic reward is provided by the environment and maintains policy entropy, as shown in Figure 7 .\n\n## 5 Related Work\n\nLLM Agents in Multi-Step Embodied Tasks. LLM agents for multi-step embodied tasks have been studied under different paradigms. Data-driven approaches (Song et al. , 2024 ; Xiong et al. , 2024 ; Qiao et al. , 2025 ; 2024 ; Tajwar et al. , 2025 ) enhance decision-making through effective data collection methods and imitation learning. Model-based agents (Tang et al. , 2024 ; Zhou et al. , 2024 ) build world models, often by generating code with large closed-source systems such as GPT-4. Other methods (Lin et al. , 2023 ; Choudhury and Sodhi, 2025 ) strengthen reasoning through model transitions or by leveraging privileged information provided by the simulation environment. In contrast, our approach reduces reliance on such external resources and emphasizes autonomous growth through the agentâ€™s own exploration and self-improvement.\n\nMemory for LLM Agents. To enable progressive improvement from past experiences, Reflexion (Shinn et al. , 2023 ) and REMEMBERER (Zhang et al. , 2023 ) leverage external memory. Reflexion stores verbal reflections for later prompting, while REMEMBERER records observations, actions, rewards, and Q-values, retrieving similar cases as few-shot exemplars. These methods show that LLMs can improve without parameter updates. However, with fixed parameters, they cannot expand intrinsic knowledge, so adaptation remains short-term (Zhang et al. , 2023 ) , relying on external memory rather than achieving long-term evolution and generalization.\n\nLearning by Knowledge Distillation Our hybrid off-policy update functions as reward-guided knowledge distillation during online training. Snell et al. ( 2022 ) introduced context distillation, where the model first solves tasks using a Teacher prompt (with instructions, examples, explanations, and scratch-pad reasoning) and then learns to produce the final answer from a minimal Student prompt via offline, SFT-based distillation. In contrast, we integrate knowledge distillation into online RL, leveraging online adaptability while enhancing exploration for more efficient training.\n\nRL for LLM Agents. RL provides a robust framework for optimizing LLM parameters through observations and reward signals from environment interactions. Prior work, Retrospex (Xiang et al. , 2024 ) , showed that offline RL, which learns optimal policies from large logged datasets, can improve LLM agent performance. Recent studies focus on online RL (Shao et al. , 2024 ; Feng et al. , 2025b ; Wang et al. , 2025 ) , where agents learn in real time. GiGPO (Feng et al. , 2025b ) advanced GRPO by grouping rollouts with similar observations, enabling finer credit assignment and stronger performance. Our work advances this online RL direction by integrating non-parametric memory updates into both on- and off-policy learning, yielding substantially higher sample efficiency.\n\nEnhancing Exploration for Online RL. A central challenge in online RL is effective exploration. Classical methods such as count-based exploration (Bellemare et al. , 2016 ) and Random Network Distillation (Burda et al. , 2018b ) use intrinsic rewards to encourage novelty. Go-Explore (Ecoffet et al. , 2019 ) stores key states and re-explores from them, solving hard-exploration tasks like Atari games. Its LLM extension, Intelligent Go-Explore (Lu et al. , 2025a ) , achieves strong results in environments such as TextWorld (CÃ´tÃ© et al. , 2018 ) , but relies on large closed-source models and does not perform parameter updates. In our concurrent work, RLVMR (Zhang et al. , 2025 ) employs warm-start SFT to elicit diverse reasoning types (planning, exploration, and reflection) and provides dense, process-level rewards for each reasoning type during online RL, enhancing exploration and credit assignment. Together, these studies underscore the importance of structured exploration for scaling RL to complex environments.\n\n## 6 Experiments\n\nTo examine the effectiveness of EMPO 2 , we conduct extensive experiments on two widely used LLM agent benchmarks: ScienceWorld (Wang et al. , 2022 ) and WebShop (Yao et al. , 2022 ) using Qwen2.5-7B-Instruct (Qwen et al. , 2025 ) as the base model. The EMPO 2 performance we evaluate is the performance of the trained model without memory at test time.\n\n### 6.1 ScienceWorld\n\nScienceWorld (Wang et al. , 2022 ) is an interactive text-based benchmark in which an agent performs science experiments at the elementary school level. Successfully completing these experiments requires long-term multi-step planning, hypothesis testing, and interpretation of outcomes, as well as sufficient exploration to determine where the necessary tools are and what appropriate actions should be taken. ScienceWorld includes tasks from diverse topics and in our experiments, we cover 19 tasks spanning chemistry, classification, biology, electricity, and measurement.\n\nBaselines. We compare EMPO 2 with several RL approaches. For non-parametric RL, Reflexion (Shinn et al. , 2023 ) updates memory in a non-parametric manner by incorporating LLM reflections from previous trajectories and using them in the prompt for the subsequent trial. For offline RL, Retrospex (Xiang et al. , 2024 ) leverages an SFT-trained model and uses a Q-function learned via Implicit Q-learning (Kostrikov et al. , 2022 ) to dynamically rescore actions. The official Retrospex paper used the smaller Flan-T5-Large (Chung et al. , 2024 ) (770M) and incorporated human-designed heuristics to assist the agent during evaluation. In contrast, to ensure consistency in our experimental setup, we standardize the base model of Retrospex to Qwen2.5-7B-Instruct and exclude these heuristics. Finally, for online RL, we include GRPO (Shao et al. , 2024 ) as a representative baseline. Further details are provided in Appendix D .\n\nP[1]Â¿ \\arraybackslash p#1\n\n[FIGURE:13.png] Table 1: Comparison results of ScienceWorld. Each task in ScienceWorld contains multiple variants. We use the first five variants for training and evaluate on the 20 unseen test variants. Bold shows the best performance per task, while red shading marks cases where parametric updates score lower than non-parametric updates. The EMPO 2 performance we evaluate is the performance of the trained model without memory at test time.",
  "figures": []
}