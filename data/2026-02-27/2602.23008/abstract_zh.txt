# 論文摘要翻譯

探索仍然是使用強化學習訓練的大型語言模型代理的關鍵瓶頸。雖然先前的方法利用預訓練知識，但它們在需要發現新穎狀態的環境中表現不佳。我們提出 Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO²)，一個混合強化學習框架，利用記憶來進行探索，並結合同策略和異策略更新，使大型語言模型在具有記憶的情況下表現良好，同時也確保在沒有記憶時的穩健性。在 ScienceWorld 和 WebShop 上，EMPO² 分別相對於 GRPO 達到 128.6% 和 11.3% 的改進。此外，在分布外測試中，EMPO² 展示了對新任務卓越的適應性，僅需少數幾次帶有記憶的試驗且無需參數更新。這些結果突出了 EMPO² 作為構建更具探索性和可泛化之大型語言模型代理的有前景框架。