{
  "source": "html",
  "markdown": "# Imagination Helps Visual Reasoning, But Not Yet in Latent Space\n\nLatent visual reasoning aims to mimic human‚Äôs imagination process by meditating through hidden states of Multimodal Large Language Models.\nWhile recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear.\nMotivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis.\nWe model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome.\nOur findings uncover two critical disconnections:\n(a) Input-Latent Disconnect : dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence.\n(b) Latent-Answer Disconnect : perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome.\nFurthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity.\nConsequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine , which teaches the model to explicitly imagine using text.\nExperiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.\n\n[FIGURE:x1.png] Figure 1 : Comparison between visual reasoning with tools and through imagination.\n(a) Reasoing with tools perceive visual content through function calling such as zoom-in or drawing.\n(b) Latent-space imagination exploits the hidden states of MLLMs to conduct visual reasoning.\n(c) We show that imagination can be more effective in text-space.\n\n## 1 Introduction\n\nThe field of Visual Reasoning within Multimodal Large Language Models (MLLMs) has witnessed a surge in interest, driven by steady progress in complex mathematical reasoning, spatial understanding, long-horizon planning, and fine-grained visual perception (Yang et al. , 2025b ; Bai et al. , 2025c ; Li et al. , 2025d ; Hong et al. , 2025 ; Yang et al. , 2025a ; Luo et al. , 2026 ; Lou et al. , 2025 ; Ma et al. , 2025 ) .\nThe increasing complexity of visual reasoning tasks requires MLLM to employ more active percpetion of visual content.\nTo meet this demand, visual reasoning with tools (Zhang et al. , 2025b ; Hong et al. , 2025 ; Zhao et al. , 2025 ) generates interleaved multimodal reasoning trajectories, effectively incorporating visual semantics during the reasoning process.\nHowever, the obtained auxiliary image during reasoning process mostly comes from limited set of rigid tools, posing huge gap with human-like native imagination.\n\nTo address this gap, Latent Visual Reasoning¬†(LVR) (Yang et al. , 2025c ) emerges as a novel paradigm that reasons through the hidden state in MLLMs, which we refers as latent token.\nSince input embeddings from different modalities have been aligned within the model, LVR trains MLLMs to output latent tokens that are compatible with visual embeddings and encode rich visual semantics.\nBy deliberating in this high-dimensional latent space, LVR enables a broader and less constrained form of Visual Imagination (Atwood, 1971 ; Pylyshyn, 2002 ) .\nBuilding on these properties, a series of latent-space visual reasoning methods have been proposed (Li et al. , 2025b ; Wang et al. , 2025b ; Tong et al. , 2025 ; Li et al. , 2025b ; Liu et al. , 2025 ; Wang et al. , 2026 ; Zhang et al. , 2025a , c ) . These approches typically supervise latent tokens using visual features or hidden representations from the teacher model. Empirically, they exhibit strong performance across various vision-centric tasks.\n\nDespite these promising results, the internal mechanism of Latent Visual Reasoning and the behavior of latent tokens are still poorly understood.\nIn particular, it is unclear whether and how MLLM actually performs deliberative reasoning within the latent space.\nTo address this gap, we adopt Causal Mediation Analysis framework and conceptualize latent reasoning as a causal process from input X X to intermediate latent tokens Z Z , and finally to output Y Y , i.e., X ‚Üí Z ‚Üí Y X\\rightarrow Z\\rightarrow Y .\nOur analysis focuses on systematic perturbations on both X X and Z Z to examine the full causality.\n\nWe begin by conducting instance-level perturbations on the input X X , where the entire input sequence is altered.\nSurprisingly, the resulting latent tokens Z Z exhibit a high degree of homogeneity measured by cosine similarity, even across diverse inputs and tasks.\nThis similarity indicates a disconnect in the X ‚Üí Z X\\rightarrow Z causality.\nTaken a step further, we implement systemtic intervention analysis and probing analysis on Z Z .\nAcross multiple latent reasoning methods and diverse benchmarks, we find that drastic perturbations to Z Z lead to negligible changes in final answer.\nMoreover, probing analysis reveals that latent tokens encode only minimal task-relevant visual semantics and are insufficient to support downstream reasoning on their own.\nThe intervention and probing analysis demonstrates a disconnect in the Z ‚Üí Y Z\\rightarrow Y causality\nCollectively, these results show that latent tokens neither vary meaningfully according to the inputs, nor do they actually affect the final answer.\n\nThese observations naturally leads to a second question: how can MLLMs perform visual reasoning?\nTo explore this, we propose a simple yet effective method under a strictly controlled setting.\nSpecifically, we convert the original Monet-SFT-125K (Wang et al. , 2025b ) training data into a text-space imagination format.\nFor each interleaved reasoning image, we generate textual descriptions of visual manipulations, such as highlighting or zooming into regions of interest, and train the MLLM to internalize these operations purely through text.\nUsing the same data source as Monet, this simple data reformulation strategy yields substantially stronger results than latent-space approaches.\nExtensive evaluations on V* (Wu and Xie, 2024 ) , HR-Bench (Wang et al. , 2025c ) , MME-RealWorld-Lite (Zhang et al. , 2024 ) , and other vision-centric benchmarks show consistent improvements, surpassing Monet by 4.0% on HR-Bench-8K and 4.9% on MME-RealWorld-Lite.\nWe believe our work sheds light on how to build more faithful, interpretable, and causally effective visual reasoning methods.\n\nIn conclusion, our contribution can be concluded as follows:\n\n- ‚Ä¢ We conduct a systematic study on latent tokens in visual reasoning through Causal Mediation Analysis, revealing that latent tokens contributes little to the causal reasoning process.\n- ‚Ä¢ We propose a simple yet effective text-space imagination method CapImagine , showing better causality than latent-space methods.\n- ‚Ä¢ Our method substantially outperforms latent-space approaches across multiple vision-centric benchmarks, demonstrating strong effectiveness and generality.\n\nWe conduct a systematic study on latent tokens in visual reasoning through Causal Mediation Analysis, revealing that latent tokens contributes little to the causal reasoning process.\n\nWe propose a simple yet effective text-space imagination method CapImagine , showing better causality than latent-space methods.\n\nOur method substantially outperforms latent-space approaches across multiple vision-centric benchmarks, demonstrating strong effectiveness and generality.\n\n## 2 Related Work\n\n### 2.1 Visual Reasoning with Tools\n\nTool-augmented visual reasoning approaches actively engage with the visual modality, adaptively perceiving visual content through explicit manipulation to lead to the final answer.\nThese methods can be further distinguished by how intermediate visual observations are produced. Some works rely on fixed tool set such as zoom-in or image-drawing operations (Zheng et al. , 2025 ; Qi et al. , 2024 ; Lai et al. , 2025 ; Jiang et al. , 2025 ; Cao et al. , 2025 ; Fu et al. , 2025 ; Chen et al. , 2025 ) to actively perceive the visual elements, dramatically expanding perceptual bandwidth compared with static perception. From the cognition and knowledge perspective, another stream of work (Wu et al. , 2025a ; Yu et al. , 2026 ; Narayan et al. , 2025 ) seeks to utilize retrieval or web-search tools for factual verification and external multimodal knowledge injection. Expanding the scope of predefined tool set, other approaches leverage self-rendered code to enable more flexible and free-form visual manipulations (Zhao et al. , 2025 ; Geng et al. , 2025 ; Hong et al. , 2025 ) , faciliating agentic MLLM in visual reasoning.\n\n### 2.2 Visual Reasoning through Imagination\n\nVisual Imagination could be achieved through self-generation or latent-space reasoning. Unified multimodal models attempt to visually imagine through its inner generation ability, explicitly instantiating internal reasoning states (Deng et al. , 2025 ; Li et al. , 2025c ; Shi et al. , 2025 ) . Latent visual reasoning proposes to conduct imagination through the hidden states in MLLMs, without decoding it into specific text token. Latent visual reasoning was first introduced by Mirage (Yang et al. , 2025c ) , which addresses the challenge of latent supervision design by compressing visual features extracted from intermediate reasoning images. Subsequent works (Li et al. , 2025b ; Tong et al. , 2025 ; Dong et al. , 2025 ; Zhang et al. , 2025a ) largely follow adopting vision encoder features as supervision signals, and further extend latent reasoning to broader perception scenarios, more flexible latent formats, and improved strategies for selecting supervisory visual features.\n\nHowever, visual features are inherently continuous and semantically sparse. The compression strategy in Mirage tends to dilute discriminative semantics, and directly supervising latents with entire visual token sequences (Li et al. , 2025b ) can lead to latent mode collapse during inference. Monet (Wang et al. , 2025b ) introduces a distillation-based framework (Shen et al. , 2025 ) that restricts gradient propagation exclusively to latent tokens, thereby preserving informative semantics from both intermediate images and key textual cues. Despite these advances, the LVR field still lacks rigorous investigation of many core design choices and mechanisms, an issue this paper aims to address.\n\n## 3 Analysis: Latent Tokens Hardly Helps\n\n### 3.1 Formulation\n\nLatent Visual Reasoning refers to a reasoning paradigm in which the last hidden states of the final transformer layer are treated as latent tokens for solving visual question answering tasks. Given a set of input images I i i = 0 N {I_{i}}_{i=0}^{N} and a question q q , the model is required to produce an answer conditioned on the joint input X = ( { I i } i = 0 N , q ) X=(\\{I_{i}\\}_{i=0}^{N},q) . During inference, the model ‚Ñ≥ \\mathcal{M} can adaptively switch between decoding normal text tokens and latent tokens. The inference process is formally defined as:\n\n|  | h i = ‚Ñ≥ ‚Äã ( E ‚Äã ( x ) ; y < i ) , y 0 = ‚àÖ h_{i}=\\mathcal{M}\\left(E(x);y_{<i}\\right),\\quad y_{0}=\\emptyset |  | (1) |\n| --- | --- | --- | --- |\n\n|  | y i = ùïÄ ‚Äã ( i ‚àà ‚Ñê L ) ‚ãÖ œï ‚Äã ( h i ) + ùïÄ ‚Äã ( i ‚àâ ‚Ñê L ) ‚ãÖ E ‚Äã ( Decode ‚Äã ( h i ) ) y_{i}=\\mathbb{I}(i\\in\\mathcal{I}_{L})\\cdot\\phi(h_{i})+\\mathbb{I}(i\\notin\\mathcal{I}_{L})\\cdot E\\left(\\text{Decode}(h_{i})\\right) |  | (2) |\n| --- | --- | --- | --- |\n\nwhere ‚Ñê L \\mathcal{I}_{L} denotes the index set of latent tokens, œï ‚Äã ( h i ) \\phi(h_{i}) is an optional projection layer applied to hidden states, and E ‚Äã ( ‚ãÖ ) E(\\cdot) represents the embedding process. The indicator function ùïÄ ‚Äã ( ‚ãÖ ) \\mathbb{I}(\\cdot) determines whether the current decoding step operates in latent mode or normal text mode.\n\n[FIGURE:x2.png] Figure 2 : Our systematic latent analysis framework for investigating the internal mechanisms and behavioral patterns of latent tokens. (a) Model Inference illustrates the latent inference process. (b) and (c) respectively illustrate two causal analysis approaches. In diagram Intervention on Z Z , œÑ \\tau denotes a fixed tensor, œµ \\epsilon represents random Gaussian noise with œµ ‚àº ùí© ‚Äã ( 0 , œÉ 2 ) \\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}) , and Œº \\mu is a small value close to zero.\n\nIn practice, the number of latent tokens is usually predefined. The latent mode starts immediately after the model outputs <|latent_start|> . During latent mode, the model takes the last hidden state as the input for the next step. The model exits latent mode when the current hidden state is decoded as <|latent_end|> , after which normal text decoding resumes.\n\nWhen text and latent tokens are interleaved together, our primary research focus are the latent tokens, denoted as Z Z . Given the original input X X and the whole reasoning process, the model ultimately produces the final answer Y Y . Explicitly tracing the role of latent tokens in visual reasoning, we abstract the overall reasoning process as:\n\n|  | X ‚Üí Z ‚Üí Y X\\rightarrow Z\\rightarrow Y |  | (3) |\n| --- | --- | --- | --- |\n\nIn the following content, we will conduct targeted interventions P ‚Äã ( Z ‚à£ d ‚Äã o ‚Äã ( X ) ) P(Z\\mid do(X)) and P ‚Äã ( Y ‚à£ d ‚Äã o ‚Äã ( Z ) ) P(Y\\mid do(Z)) to elucidate the role of latent tokens in visual reasoning.\n\n### 3.2 Causal Analysis of X ‚Üí Y X\\rightarrow Y\n\nWe begin with a causal mediation analysis (Pearl, 2009 ) conducting instance-level perturbations on the input X X and measure how the latent reasoning tokens change accodingly with the entire input sequence altering, i.e., P ‚Äã ( Z ‚à£ d ‚Äã o ‚Äã ( x ) ) P(Z\\mid do(x)) .\n\nExperiment Setting We evaluate three representative baselines: (1) Monet (Wang et al. , 2025b ) , a distillation-based model focused on general scenarios; (2) LVR (Li et al. , 2025b ) , which leverages image features as supervision in general scenarios; and (3) Mirage (Yang et al. , 2025c ) , which also uses image features but is fine-tuned for task-specific settings. For general VQA scenarios, we uniformly sample instances from V* (Wu and Xie, 2024 ) , MME (Yin et al. , 2024 ) , OCRBench-v2 (Fu et al. , 2024a ) , MME-Realworld-Lite (Zhang et al. , 2024 ) , and TableVQA (Kim et al. , 2024 ) , resulting in a total of 100 testing instances. These instances have been sorted and grouped in results visualization. For the task-specific Mirage model, we adopt its released Visual Spatial Planning dataset, which requires the model to reach the destination circumventing obstacles such as frozen lakes.\n\nDuring inference, all three models are prompted to perform latent reasoning and only instances with valid latent reasoning process are reserved.\nAs illustrated in Figure 2 , we examine latent tokens from two perspectives: inter-instance , by sampling latent tokens at fixed positions across different instances; intra-instance , by sampling all latent tokens within a single instance. The intra-instance results are then averaged across all instances.\nAdditionally, we have considered the pattern of text tokens, image tokens and the inner representation of MLLM after the input sequence. For intra-instance pattern of textual reasoning, we analyze the hidden states of first 16 tokens during generation.\n\nInter-instance Analysis. As shown in Figure 2 , latent tokens at the same position across different instances exhibit consistently high cosine similarity.\nThis indicates that these latent tokens encode little information from the input images or questions.\nAdditionaly, latent tokens from different tasks also remain highly similar, suggesting that the they also fail to capture coarse task-level distinctions.\nFurthermore, the degree of similarity intensifies as the reasoning continues, with all latent tokens increasingly degenerating as more latent tokens are generated.\nIn contrast, text/image tokens and inner representation of MLLM all carry informative and distinctive semantics, exhibiting low similarity cross instances and tasks.\n\nIntra-instance Analysis. When examining the average latent token behaviour within individual instances under input X X alteration, all three models display a progressive degeneration phenomenon: latent tokens collapse into clusters of highly similar representations as reasoning continues. With more autoregressive steps, the LLM backbone applies increasingly smaller modification to the latent states, causing tokens to converge toward uniform representations. Specifically, the LVR model degenerates the fastest, with token collapse occurring as early as at the second step. Monet initially produces semantically rich latent tokens, but they gradually lose distinctiveness by the fifth step. In comparison, the hidden states similarity of text reasoning are dramatically lower. This reveals the clear and steady states transition in text generation and obscure reasoning trajectory in latent reasoning.\n\nLatent Pattern across Various Models. Across different approaches, distinct latent patterns emerge.\nAs shown in the Inter-instance Analysis in Figure 2 (b) and Appendix B , Monet exhibits slower degeneration speed at different latent index, but ultimately converges into a highly uniform latent space.\nAs for LVR, although it collapses rapidly, some latent tokens retain partial distinctiveness even after lengthy reasoning.\nBy contrast, Mirage, which compresses the original lengthy visual tokens into a few latent tokens, demonstrates minimal distinctiveness throughout the entire reasoning process.\n\n### 3.3 Causal Analysis of Z ‚Üí Y Z\\rightarrow Y\n\nDespite the degenerate nature of the latent tokens, they may still helpful to get the final answers.\nTo investigate, we first directly intervating on the latent tokens Z Z to explicitly diagnose its causal effect on the final answer Y Y , then conduct a probing analysis to test whether Z Z sufficiently lead to Y Y .\n\n#### 3.3.1 Intervation on Latent Tokens Z Z\n\nExperiment Setting. We conduct interventions d ‚Äã o ‚Äã ( Z ) do(Z) on both Monet and Mirage , representing general-purpose and task-specific scenarios. For Monet, we apply a strong intervention by forcing all latent tokens across different positions and instances as an shared identical tensor. For Mirage, we further explore more diverse intervention strategies. Besides the intervation strategy on Monet, we also consider (1) injecting Gaussian noise into the latent tokens, (2) replacing latent tokens entirely with Gaussian noise, and (3) setting all latent tokens to a small value close to zero.\n\nResults Analysis. The experimental results are summarized in Figure 2 (c) Intervention on Z Z and detailed in Appendix. Surprisingly, across V*, HR-Bench, and MME-Realworld-Lite, these drastic alteration applied to the latent tokens Z Z result in only marginal answer variations. On V*, overall performance even exhibits a slight improvement by 0.5%. Only minor degradation is observed on the HR-Bench-4K and on MME-RealWorld-Lite, by 1.0% and 0.7% respectively. Overall, even fundamental alterations to the latent tokens lead to negligible performance fluctuations, suggesting that these latent tokens Z Z exerts limited influence on the final output.\n\nWe further evaluate Mirage on the VSP dataset (Yang et al. , 2025c ; Wu et al. , 2025b ) , considering both its stage-1 and stage-2 variants. Dramatic decline only happens when setting latent tokens as small value on stage-2 variant, where the intervention is so strong and results in repetition. In other cases, even fundamental change of latent tokens such as directly replacing the latent tokens with Gaussian noise result in negligible changes. These findings consistently indicate that the model neither meaningfully attends to the latent tokens nor encodes critical information within them.\n\n#### 3.3.2 Probing Analysis on Latent tokens Z Z\n\nExperiement Setting. Here, we further diagnose the causal effect Z Z imposing on answer Y Y by a probing analysis on latent tokens Z Z . In this analysis, we focus on Monet , whose latent supervision is obtained by jointly optimizing over visual signals and textual semantics in the original interleaved multimodal reasoning data. Through multi-stage training, these latent tokens are encouraged to encode informative visual semantics that facilitate solving the question.\n\nSpecifically, we sample question‚Äìimage pairs { ( I i , q i ) } i = 0 N \\{(I_{i},q_{i})\\}_{i=0}^{N} from V* and collect the corresponding latent embeddings { Z i } i = 0 N \\{Z_{i}\\}_{i=0}^{N} generated during inference. These embeddings are expected to encode key visual evidence supporting not only the initial task but also other related queries grounded in the same visual content. To examine the semantics captured by the latent tokens, we further construct 30 multiple-choice VQA questions { ( Z i , q ~ i ) } i = 0 N \\{(Z_{i},\\tilde{q}_{i})\\}_{i=0}^{N} that focus on the same image regions but probe different attributes of the referenced objects. If the latent tokens effectively capture essential visual semantics, they should support solving these derived questions. Example are provided in the Appendix A .\n\n[FIGURE:x3.png] Figure 3 : Illustration of Our Method and Data Construction Pipeline , through which we conduct a strictly controlled training setting with Monet for fair and convincing comparisons. The upper section presents the interleaved format of original data. The middle section clarifies the key methodological differences between the two approaches. The lower section shows the data construction procedures.\n\nResult Analysis. The experimental results are summarized in Figure¬†2¬†(c). The results reveal that directly using latent tokens as the sole input leads to notably weak performance, falling behind even text-only guessing baselines. In contrast, when the original image is provided, both Monet and Qwen3-VL-32B (Bai et al. , 2025a ) achieve strong performance, reaching 76.67% accuracy, which also validates the quality and consistency of our manually curated questions. Taken together, these findings cast huge doubt on the extent to which the latent tokens alone effectively capture and preserve actionable visual semantics within the model‚Äôs reasoning process.\n\nIn summary, these highly homogeneous latent tokens ( Findings-1 ) contribute marginally to the final prediction. The model potentially adopts an implicit shortcut circumventing the latent visual reasoning pathway ( Findings-2 ). Moreover, the encoded semantics of latent tokens are also minimal ( Findings-3 ). So far, the full potential of latent tokens in current methods has not yet been fully discovered, and the latent tokens are behaving similarly with soft prompt or placeholders instead of active carrier of visual imagination or reasoning.\n\n[FIGURE_CAPTION] Table 1 : Performance comparison across perception-centric and visual reasoning benchmarks , where our method consistently outperforms competing baselines.\nThe best results are highlighted in bold .\nResults marked with ‚Äò*‚Äô are reported from prior work.\n\n## 4 CapImagine\n\n### 4.1 Method Design\n\nThe essence of visual imagination primarily lies in interleaved multimodal reasoning , where internal visual thought could be explicitly outlined and evolve alongside the textual reasoning chain. Existing Latent Visual Reasoning methods attempt to internalize such visual thoughts into latent tokens. However, as shown in the previous sections, these latent representations fail to preserve meaningful visual semantics and contribute little to downstream reasoning.\n\nMotivated by this limitation, we explore whether text-space reasoning can more effectively retain the essential information embedded in interleaved data and support visual imagination. Instead of relying on latent variables, we convert the semantic changes introduced by intermediate images into textual captions. This forces the model to imagine visual transformations over the original image through an explicit text-space reasoning chain.\nUnlike prior text-space reasoning approaches, our method is grounded in concrete intermediate visual evidence. By verbalizing visual transitions that would otherwise occur in hidden space, the model performs imagination as if intermediate images were present.\n\n### 4.2 Dataset Construction\n\nData Rewriting. Specifically, our data construction is based on the Monet-SFT-125K (Wang et al. , 2025b ) dataset and adopts two forms of image rewriting. For the Visual-CoT and Zebra-CoT visual search (Li et al. , 2025a ) subsets, which primarily focus on zooming into key image regions, we provide the original question together with the highlighted image region to Qwen3-VL-4B (Bai et al. , 2025a ) , prompting it to generate concise and accurate captions that refocus the highlighted visual semantics. For other subsets such as Refocus (Fu et al. , 2025 ) and CogCoM (Qi et al. , 2024 ) , which involve direct image manipulations such as marking or drawing auxiliary lines, we present both the original image and its manipulated counterpart to Qwen3-VL-4B. The model is instructed to describe the visual differences and explicitly verbalize the key information revealed by the manipulation, such as marked numerical values or highlighted textual entities. Through this process, language fully carries the semantics of auxiliary images, effectively bypassing latent representations.\n\nHowever, directly inserting the rewritten text into the original reasoning trajectories often results in rigid transitions and disrupts logical coherence. To address this issue, we further employ the MLLM to globally refine the reasoning chains. This step corrects potential inconsistencies and improves fluency, allowing the newly generated textual descriptions to integrate smoothly into the original reasoning process. The above data construction pipeline is in Figure 3 .\n\nData Filtering. Although the Monet-SFT-125K dataset has already undergone rigorous filtering procedure, the inherently low quality of the Visual-CoT (Shao et al. , 2024 ) data, which accounts for 94.88% of Monet-SFT-125K, significantly undermines the effectiveness of our data rewriting strategy. We identify two primary issues. First, the final answer of the original question often conflicts with the newly generated visual observations, resulting in misalignment between the reasoning process and the answer. Second, a large portion of questions in the Visual-CoT subset are overly ambiguous or fundamentally unanswerable, lacking a clear reference to the target object. As a result, early experiments using the raw rewritten data yield only limited improvements on downstream tasks.\n\nTo mitigate these issues, we perform a comprehensive quality assessment of each training instance through MLLM. The evaluation focuses on both the correctness of the reasoning process and the degree of question ambiguity, and instances with evident flaws are filtered out. Manual inspection confirms the effectiveness of this automated filtering procedure. After filtering, we retain 17k high-quality training instances. To eliminate the effect of data quantity difference with Monet-SFT-125K, we conduct strict ablation study in Section 5.3 , ensuring a fair comparison with Monet.\n\n## 5 Experiments\n\n### 5.1 Experiment Setup\n\nBenchmarks. To comprehensively assess the effectiveness of text-driven visual imagination, we adopt a diverse set of high-resolution¬†(HR) visual perception benchmarks following the experimental protocol of Monet: V*, HR-Bench-4K, HR-Bench-8K, and MME-RealWorld-Lite, which emphasize fine-grained perception under high-resolution settings. Beyond the zoom-in ability, we further incorporate the jigsaw and multi-view reasoning tasks from the BLINK (Fu et al. , 2024b ) benchmark to assess compositional and multi-perspective reasoning abilities. Finally, we evaluate on TableVQA to examine the generalization of our approach in diagram and table images.\n\nBaselines. We compare against three categories of baselines. (1) Open-source models: we evaluate InternVL3-8B (Zhu et al. , 2025 ) and Qwen2.5-VL-7B (Bai et al. , 2025b ) , the latter serving as the backbone model for all following baselines. (2) Reasoning with Tool methods, including DeepEyes and PixelReasoner (Wang et al. , 2025a ) , which leverage reinforcement learning to enhance perception via zoom-in operations. (3) Reasoning through Imagination Methods, namely LVR and Monet, which perform visual imagination in latent space across general scenarios. Additionally, we also report results from the proprietary GPT-4o (Hurst et al. , 2024 ) model. For Monet, we adopt an LLM-as-a-judge protocol to extract final answers.\n\nTraining Details. Our model is built upon Qwen2.5-VL-7B and trained on reconstructed data from Monet-SFT-125K. We perform CoT-SFT fine-tuning using the Monet codebase on 8 A800-80G GPUs, with a batch size of 1 and gradient accumulation of 16. To mitigate training instability and incentivate the full potential of the data, we select the best-performing checkpoint during training (Nishida et al. , 2025 ) .\n\n### 5.2 Main Results\n\nAcross evalution on various perception-centric benchmarks, our method consistently outperforms the strong baseline Monet, achieving average improvements of 3.44% on HR-Bench and 2.6% on V*. On MME-RealWorld-Lite, our reproduced Monet shows only marginal gains over its base model, whereas our approach effectively handles diverse real-world queries. These results highlight the effectiveness of zoom-in-based visual imagination for fine-grained visual perception. Compared with reasoning with tools approaches, our method substantially outperforms PixelReasoner, while remaining slightly behind DeepEyes, suggesting that direct image replay does provide complementary benefits.\n\nBeyond high-resolution perception, we further evaluate more abstract visual reasoning tasks. Jigsaw and multi-view reasoning require reconstructing global structure and performing spatial reasoning across views. Our method generalizes well to these settings, surpassing both LVR and Monet by over 10 points. On TableVQA, which emphasizes identifying and comparing key values, our approach achieves a 6.1% improvement over Monet.\n\n[FIGURE_CAPTION] Table 2 : Results on TableVQA benchmark. Through imagination in text-space, our method consistently outperforms baselines.\n\nOverall, these results demonstrate that text-driven visual imagination offers an effective mechanism for both fine-grained perception and abstract visual reasoning. Imagination does help visual reasoning, but not yet in latent space.\n\n### 5.3 Ablation Study\n\nWe conduct controlled ablation studies to disentangle the effects of data rewriting and data filtering.\nTo assess the role of data rewriting, we replace the text-space imagination descriptions in our training data with a single <think_image> token and fine-tune the model under identical settings. As shown in Table 1 , this modification leads to consistent performance degradation across all benchmarks, including a 3.13% drop on V*, confirming the effectiveness of text-driven visual imagination.\nWe further examine the impact of data filtering by fine-tuning directly on the original Monet-SFT-125K dataset. To eliminate the training-inference misalignment in Monet, where auxiliary images are present during training but unavailable at inference, we replace intermediate images with the <think_image> token during supervised fine-tuning. We find that training without data filtering results in another continual performance decline, demonstrating the necessity of quality control. Notably, after removing the training‚Äìinference mismatch, direct supervised fine-tuning on Monet-SFT-125K achieves performance comparable to Monet, which additionally undergoes a Policy Optimization stage. This observation further questions the role of latent in visual imagination.\n\n### 5.4 Dependency Analysis\n\nExperiment Setup. In this subsection, we conduct a causal mediation analysis on the text-form imagination variable Z Z , following perturbation protocols analogous to those in the previous section. For interventions on the input X X , we perform instance-level modifications to the input sequence and analyze the resulting changes in the hidden states of text imagination tokens, considering both inter-instance and intra-instance similarities. For interventions on Z Z , we follow the protocol of (Zhang et al. , 2025c ) and explicitly manipulate the reasoning process. Specifically, CapImagine is first prompted to answer a question. The generated answer is then removed, and Qwen3-32B is used to deliberately alter the imagination content so that it leads to an incorrect conclusion. This corrupted reasoning trace is finally fed back to CapImagine , which is asked to complete the generation and produce the final answer.\n\n[FIGURE:x4.png] Figure 4 : Inter-instance and Intra-instance Analysis of the inner hidden states of CapImagine during reasoning process.\n\n[FIGURE_CAPTION] Table 3 : Performance change under the intervetion of the intermediate reasoning process of CapImagine .\n\nResults analysis. As shown in Figure 4 , inter-instance analysis yields consistently low cosine similarity, indicating a strong causal dependency between X X and Z Z . Intra-instance analysis further shows substantial diversity among consecutive hidden states, suggesting that each imagination token encodes distinct semantic content. Intervening on Z Z produces a pronounced impact on the final prediction Y Y . When key information in the imagination content is modified, performance drops sharply below random-guess levels. Overall, from a causal mediation perspective, the text-form imagination process in CapImagine exhibits a substantially stronger and more direct causal influence than latent tokens, and plays a central role in the end-to-end reasoning process.\n\n### 5.5 Efficiency Analysis\n\n[FIGURE:image5.png] Figure 5 : Inference Speed Comparison of Monet, CapImagine and DeepEyes on V*. (Unit: Seconds)\n\nAlthough CapImagine employs relatively long text-form imagination sequences, we compare its inference efficiency against the latent-space method Monet and the tool-augmented reasoning method DeepEyes. We measure only decoding time and ensure that all models generate complete answers. The results are summarized in Figure 5 . CapImagine achieves inference speed comparable to Monet, despite operating entirely in text space. At the same time, it is nearly twice as fast as the reasoning with tools method DeepEyes while delivering competitive performance. These results demonstrate that CapImagine offers a favorable trade-off between effectiveness and efficiency, combining strong reasoning ability with practical inference cost.\n\n## 6 Conclusion\n\nIn this work, we systematically investigate the internal mechanisms of latent-space visual reasoning methods through causal mediation analysis.\nOur results reveal that latent tokens are highly homogeneous, minimally sensitive to input, weakly result-oriented and semantically limited, failing to serve as effective carrier of visual imagination and genuine reasoning. To address this limitation, we propose a text-space imagination method exhibiting better causal effect and higher performance.\nWe believe our study provides a rigorous investigation of current latent visual reasoning methods and offers guidance toward developing more faithful, interpretable, and effective latent reasoning approaches.\n\n## Impact Statement\n\nThis paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\n\n## References\n\n- G. Atwood (1971) An experimental study of visual imagination and memory . Cognitive Psychology 2 ( 3 ), pp.¬†290‚Äì299 . Cited by: ¬ß1 .\n- S. Bai, Y. Cai, R. Chen, K. Chen, X. Chen, Z. Cheng, L. Deng, W. Ding, C. Gao, C. Ge, W. Ge, Z. Guo, Q. Huang, J. Huang, F. Huang, B. Hui, S. Jiang, Z. Li, M. Li, M. Li, K. Li, Z. Lin, J. Lin, X. Liu, J. Liu, C. Liu, Y. Liu, D. Liu, S. Liu, D. Lu, R. Luo, C. Lv, R. Men, L. Meng, X. Ren, X. Ren, S. Song, Y. Sun, J. Tang, J. Tu, J. Wan, P. Wang, P. Wang, Q. Wang, Y. Wang, T. Xie, Y. Xu, H. Xu, J. Xu, Z. Yang, M. Yang, J. Yang, A. Yang, B. Yu, F. Zhang, H. Zhang, X. Zhang, B. Zheng, H. Zhong, J. Zhou, F. Zhou, J. Zhou, Y. Zhu, and K. Zhu (2025a) Qwen3-vl technical report . arXiv preprint arXiv:2511.21631 . Cited by: ¬ß3.3.2 , ¬ß4.2 .\n- S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin (2025b) Qwen2.5-vl technical report . arXiv preprint arXiv:2502.13923 . Cited by: ¬ß5.1 .\n- S. Bai, M. Li, Y. Liu, J. Tang, H. Zhang, L. Sun, X. Chu, and Y. Tang (2025c) Univg-r1: reasoning guided universal visual grounding with reinforcement learning . arXiv preprint arXiv:2505.14231 . Cited by: ¬ß1 .\n- M. Cao, H. Zhao, C. Zhang, X. Chang, I. Reid, and X. Liang (2025) Ground-r1: incentivizing grounded visual reasoning via reinforcement learning . arXiv preprint arXiv:2505.20272 . Cited by: ¬ß2.1 .\n- Y. Chen, Y. Shen, W. Huang, S. Zhou, Q. Lin, X. Cai, Z. Yu, J. Bu, B. Shi, and Y. Qiao (2025) Learning only with images: visual reinforcement learning with reasoning, rendering, and visual feedback . arXiv preprint arXiv:2507.20766 . Cited by: ¬ß2.1 .\n- C. Deng, D. Zhu, K. Li, C. Gou, F. Li, Z. Wang, S. Zhong, W. Yu, X. Nie, Z. Song, et al. (2025) Emerging properties in unified multimodal pretraining . arXiv preprint arXiv:2505.14683 . Cited by: ¬ß2.2 .\n- S. Dong, S. Wang, X. Liu, and Z. Wei (2025) Interleaved latent visual reasoning with selective perceptual modeling . arXiv preprint arXiv:2512.05665 . Cited by: ¬ß2.2 .\n- L. Fu, Z. Kuang, J. Song, M. Huang, B. Yang, Y. Li, L. Zhu, Q. Luo, X. Wang, H. Lu, et al. (2024a) Ocrbench v2: an improved benchmark for evaluating large multimodal models on visual text localization and reasoning . arXiv preprint arXiv:2501.00321 . Cited by: ¬ß3.2 .\n- X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W. Ma, and R. Krishna (2024b) Blink: multimodal large language models can see but not perceive . In European Conference on Computer Vision , pp.¬†148‚Äì166 . Cited by: ¬ß5.1 .\n- X. Fu, M. Liu, Z. Yang, J. Corring, Y. Lu, J. Yang, D. Roth, D. Florencio, and C. Zhang (2025) Refocus: visual editing as a chain of thought for structured image understanding . arXiv preprint arXiv:2501.05452 . Cited by: ¬ß2.1 , ¬ß4.2 .\n- X. Geng, P. Xia, Z. Zhang, X. Wang, Q. Wang, R. Ding, C. Wang, J. Wu, Y. Zhao, K. Li, et al. (2025) Webwatcher: breaking new frontier of vision-language deep research agent . arXiv preprint arXiv:2508.05748 . Cited by: ¬ß2.1 .\n- J. Hong, C. Zhao, C. Zhu, W. Lu, G. Xu, and X. Yu (2025) DeepEyesV2: toward agentic multimodal model . arXiv preprint arXiv:2511.05271 . Cited by: ¬ß1 , ¬ß2.1 .\n- A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. (2024) Gpt-4o system card . arXiv preprint arXiv:2410.21276 . Cited by: ¬ß5.1 .\n- C. Jiang, Y. Heng, W. Ye, H. Yang, H. Xu, M. Yan, J. Zhang, F. Huang, and S. Zhang (2025) VLM-r 3 : region recognition, reasoning, and refinement for enhanced multimodal chain-of-thought . arXiv preprint arXiv:2505.16192 . Cited by: ¬ß2.1 .\n- Y. Kim, M. Yim, and K. Y. Song (2024) Tablevqa-bench: a visual question answering benchmark on multiple table domains . arXiv preprint arXiv:2404.19205 . Cited by: ¬ß3.2 .\n- X. Lai, J. Li, W. Li, T. Liu, T. Li, and H. Zhao (2025) Mini-o3: scaling up reasoning patterns and interaction turns for visual search . arXiv preprint arXiv:2509.07969 . Cited by: ¬ß2.1 .\n- A. Li, C. Wang, D. Fu, K. Yue, Z. Cai, W. B. Zhu, O. Liu, P. Guo, W. Neiswanger, F. Huang, et al. (2025a) Zebra-cot: a dataset for interleaved vision language reasoning . arXiv preprint arXiv:2507.16746 . Cited by: ¬ß4.2 .\n- B. Li, X. Sun, J. Liu, Z. Wang, J. Wu, X. Yu, H. Chen, E. Barsoum, M. Chen, and Z. Liu (2025b) Latent visual reasoning . arXiv preprint arXiv:2509.24251 . Cited by: ¬ß1 , ¬ß2.2 , ¬ß2.2 , ¬ß3.2 .\n- C. Li, W. Wu, H. Zhang, Y. Xia, S. Mao, L. Dong, I. Vuliƒá, and F. Wei (2025c) Imagine while reasoning in space: multimodal visualization-of-thought . arXiv preprint arXiv:2501.07542 . Cited by: ¬ß2.2 .\n- Y. Li, H. Huang, C. Chen, K. Huang, C. Huang, Z. Guo, Z. Liu, J. Xu, Y. Li, R. Li, et al. (2025d) Migician: revealing the magic of free-form multi-image grounding in multimodal large language models . arXiv preprint arXiv:2501.05767 . Cited by: ¬ß1 .\n- C. Liu, Y. Yang, Y. Fan, Q. Wei, S. Liu, and X. E. Wang (2025) Reasoning within the mind: dynamic multimodal interleaving in latent space . arXiv preprint arXiv:2512.12623 . Cited by: ¬ß1 .\n- X. Lou, Y. Li, J. Xu, X. Shi, C. Chen, and K. Huang (2025) Think in safety: unveiling and mitigating safety alignment collapse in multimodal large reasoning model . arXiv preprint arXiv:2505.06538 . Cited by: ¬ß1 .\n- H. Luo, Y. Wang, W. Zhang, S. Zheng, Z. Xi, C. Xu, H. Xu, H. Yuan, C. Zhang, Y. Wang, et al. (2026) Being-h0. 5: scaling human-centric robot learning for cross-embodiment generalization . arXiv preprint arXiv:2601.12993 . Cited by: ¬ß1 .\n- X. Ma, Z. Ding, Z. Luo, C. Chen, Z. Guo, D. F. Wong, X. Feng, and M. Sun (2025) Deepperception: advancing r1-like cognitive visual perception in mllms for knowledge-intensive visual grounding . arXiv preprint arXiv:2503.12797 . Cited by: ¬ß1 .\n- K. Narayan, Y. Xu, T. Cao, K. Nerella, V. M. Patel, N. Shiee, P. Grasch, C. Jia, Y. Yang, and Z. Gan (2025) Deepmmsearch-r1: empowering multimodal llms in multimodal web search . arXiv preprint arXiv:2510.12801 . Cited by: ¬ß2.1 .\n- Y. Nishida, M. Isonuma, and Y. Oda (2025) Instability in downstream task performance during llm pretraining . In Findings of the Association for Computational Linguistics: EMNLP 2025 , pp.¬†22883‚Äì22895 . Cited by: ¬ß5.1 .\n- J. Pearl (2009) Causal inference in statistics: an overview . Cited by: ¬ß3.2 .\n- Z. W. Pylyshyn (2002) Mental imagery: in search of a theory . Behavioral and brain sciences 25 ( 2 ), pp.¬†157‚Äì182 . Cited by: ¬ß1 .\n- J. Qi, M. Ding, W. Wang, Y. Bai, Q. Lv, W. Hong, B. Xu, L. Hou, J. Li, Y. Dong, et al. (2024) Cogcom: train large vision-language models diving into details through chain of manipulations . Cited by: ¬ß2.1 , ¬ß4.2 .\n- H. Shao, S. Qian, H. Xiao, G. Song, Z. Zong, L. Wang, Y. Liu, and H. Li (2024) Visual cot: advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning . Advances in Neural Information Processing Systems 37 , pp.¬†8612‚Äì8642 . Cited by: ¬ß4.2 .\n- Z. Shen, H. Yan, L. Zhang, Z. Hu, Y. Du, and Y. He (2025) Codi: compressing chain-of-thought into continuous space via self-distillation . arXiv preprint arXiv:2502.21074 . Cited by: ¬ß2.2 .\n- Y. Shi, Y. Dong, Y. Ding, Y. Wang, X. Zhu, S. Zhou, W. Liu, H. Tian, R. Wang, H. Wang, et al. (2025) Realunify: do unified models truly benefit from unification? a comprehensive benchmark . arXiv preprint arXiv:2509.24897 . Cited by: ¬ß2.2 .\n- J. Tong, J. Gu, Y. Lou, L. Fan, Y. Zou, Y. Wu, J. Ye, and R. Li (2025) Sketch-in-latents: eliciting unified reasoning in mllms . arXiv preprint arXiv:2512.16584 . Cited by: ¬ß1 , ¬ß2.2 .\n- H. Wang, A. Su, W. Ren, F. Lin, and W. Chen (2025a) Pixel reasoner: incentivizing pixel-space reasoning with curiosity-driven reinforcement learning . arXiv preprint arXiv:2505.15966 . Cited by: ¬ß5.1 .\n- Q. Wang, Y. Shi, Y. Wang, Y. Zhang, P. Wan, K. Gai, X. Ying, and Y. Wang (2025b) Monet: reasoning in latent visual space beyond images and language . arXiv preprint arXiv:2511.21395 . Cited by: ¬ß1 , ¬ß1 , ¬ß2.2 , ¬ß3.2 , ¬ß4.2 .\n- W. Wang, L. Ding, M. Zeng, X. Zhou, L. Shen, Y. Luo, W. Yu, and D. Tao (2025c) Divide, conquer and combine: a training-free framework for high-resolution image perception in multimodal large language models . In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 39 , pp.¬†7907‚Äì7915 . Cited by: ¬ß1 .\n- Y. Wang, S. Li, P. Li, X. Yang, Y. Tang, and Z. Wei (2026) Render-of-thought: rendering textual chain-of-thought as images for visual latent reasoning . arXiv preprint arXiv:2601.14750 . Cited by: ¬ß1 .\n- J. Wu, Z. Deng, W. Li, Y. Liu, B. You, B. Li, Z. Ma, and Z. Liu (2025a) MMSearch-r1: incentivizing lmms to search . arXiv preprint arXiv:2506.20670 . Cited by: ¬ß2.1 .\n- J. Wu, J. Guan, K. Feng, Q. Liu, S. Wu, L. Wang, W. Wu, and T. Tan (2025b) Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing . arXiv preprint arXiv:2506.09965 . Cited by: ¬ß3.3.1 .\n- P. Wu and S. Xie (2024) V?: guided visual search as a core mechanism in multimodal llms . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.¬†13084‚Äì13094 . Cited by: ¬ß1 , ¬ß3.2 .\n- S. Yang, J. Yang, P. Huang, E. Brown, Z. Yang, Y. Yu, S. Tong, Z. Zheng, Y. Xu, M. Wang, et al. (2025a) Cambrian-s: towards spatial supersensing in video . arXiv preprint arXiv:2511.04670 . Cited by: ¬ß1 .\n- Y. Yang, X. He, H. Pan, X. Jiang, Y. Deng, X. Yang, H. Lu, D. Yin, F. Rao, M. Zhu, et al. (2025b) R1-onevision: advancing generalized multimodal reasoning through cross-modal formalization . arXiv preprint arXiv:2503.10615 . Cited by: ¬ß1 .\n- Z. Yang, X. Yu, D. Chen, M. Shen, and C. Gan (2025c) Machine mental imagery: empower multimodal reasoning with latent visual tokens . arXiv preprint arXiv:2506.17218 . Cited by: ¬ß1 , ¬ß2.2 , ¬ß3.2 , ¬ß3.3.1 .\n- S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen (2024) A survey on multimodal large language models . National Science Review 11 ( 12 ), pp.¬†nwae403 . Cited by: ¬ß3.2 .\n- X. Yu, C. Feng, L. Mei, and C. Chen (2026) M 3 searcher: modular multimodal information seeking agency with retrieval-oriented reasoning . arXiv preprint arXiv:2601.09278 . Cited by: ¬ß2.1 .\n- C. Zhang, H. Qiu, Q. Zhang, Z. Zeng, L. Ma, and J. Zhang (2025a) Deepsketcher: internalizing visual manipulation for multimodal reasoning . arXiv preprint arXiv:2509.25866 . Cited by: ¬ß1 , ¬ß2.2 .\n- Y. Zhang, X. Lu, S. Yin, C. Fu, W. Chen, X. Hu, B. Wen, K. Jiang, C. Liu, T. Zhang, et al. (2025b) Thyme: think beyond images . arXiv preprint arXiv:2508.11630 . Cited by: ¬ß1 .\n- Y. Zhang, H. Zhang, H. Tian, C. Fu, S. Zhang, J. Wu, F. Li, K. Wang, Q. Wen, Z. Zhang, et al. (2024) Mme-realworld: could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? . arXiv preprint arXiv:2408.13257 . Cited by: ¬ß1 , ¬ß3.2 .\n- Y. Zhang, B. Tang, T. Ju, S. Duan, and G. Liu (2025c) Do latent tokens think? a causal and adversarial analysis of chain-of-continuous-thought . arXiv preprint arXiv:2512.21711 . Cited by: ¬ß1 , ¬ß5.4 .\n- S. Zhao, H. Zhang, S. Lin, M. Li, Q. Wu, K. Zhang, and C. Wei (2025) Pyvision: agentic vision with dynamic tooling . arXiv preprint arXiv:2507.07998 . Cited by: ¬ß1 , ¬ß2.1 .\n- Z. Zheng, M. Yang, J. Hong, C. Zhao, G. Xu, L. Yang, C. Shen, and X. Yu (2025) DeepEyes: incentivizing\" thinking with images\" via reinforcement learning . arXiv preprint arXiv:2505.14362 . Cited by: ¬ß2.1 .\n- J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, H. Tian, Y. Duan, W. Su, J. Shao, et al. (2025) Internvl3: exploring advanced training and test-time recipes for open-source multimodal models . arXiv preprint arXiv:2504.10479 . Cited by: ¬ß5.1 .\n\n## Appendix A Examples for Derived Questions in Probing Analysis.\n\nWe present qualitative examples of derived questions used in our probing analysis.\nThese questions focus on the same visual regions as the original queries while varying the queried attributes to examine the semantic consistency of latent representations. During probing analysis, the obtained latent embeddings and these derived questions are presented to the model for final answer generation.\n\n[FIGURE:image6.png] Figure 6 : The derived questions focus on the same region or object as the original question, while differing in the queried attribute aspects.\n\n## Appendix B Detailed Results for Intervention on Z Z .\n\n[FIGURE_CAPTION] Table 4 : Performance variation under different latent interventions. The upper table reports the results of Monet when all latent tokens are set to the same tensor, denoted by d ‚Äã o ‚Äã ( Z ) do(Z) .\nThe lower table focuses on Monet under various latent intervention strategies. œÑ \\tau denotes a fixed tensor, œµ \\epsilon represents random Gaussian noise with œµ ‚àº ùí© ‚Äã ( 0 , œÉ 2 ) \\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}) , and Œº \\mu is a small constant close to zero. Z i Z_{i} denotes the latent token at the i t ‚Äã h i_{th} position.\n\n## Appendix C Limitations and Future Work\n\nThe limitations of this work are threefold. First, our proposed text-form approach introduces higher inference latency compared to latent-based methods due to the autoregressive decoding of longer sequences. Second, CapImagine serves primarily as a verification probe to demonstrate the causality gap in current latent paradigms rather than an optimal solution. We acknowledge that natural language is inherently limited in granularity compared to the theoretical information capacity of high-dimensional latent spaces. Consequently, how to rigorously construct a high-quality, causal reasoning chain within the latent space remains an unsolved and challenging objective for future exploration.",
  "figures": []
}