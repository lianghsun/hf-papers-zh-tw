{
  "source": "pdf",
  "markdown": "The Trinity of Consistency as a Defining Principle for General World Models\n\narXiv:2602.23152v1 [cs.AI] 26 Feb 2026\n\n---\n\n# Contents\n\n<table>\n   <thead>\n    <tr>\n     <td>1</td>\n     <td>Introduction</td>\n     <td>4</td>\n    </tr>\n   </thead>\n   <tbody>\n    <tr>\n     <td>2</td>\n     <td>Foundational Exploration of Consistencies</td>\n     <td>5</td>\n    </tr>\n    <tr>\n     <td>2.1</td>\n     <td>The Anatomy of General World Models</td>\n     <td>5</td>\n    </tr>\n    <tr>\n     <td>2.2</td>\n     <td>Modal Consistency</td>\n     <td>6</td>\n    </tr>\n    <tr>\n     <td>2.2.1</td>\n     <td>Theoretical Foundations</td>\n     <td>7</td>\n    </tr>\n    <tr>\n     <td>2.2.2</td>\n     <td>Discrete Sequences vs. Continuous Manifolds</td>\n     <td>8</td>\n    </tr>\n    <tr>\n     <td>2.2.3</td>\n     <td>Architectural Evolution</td>\n     <td>10</td>\n    </tr>\n    <tr>\n     <td>2.2.4</td>\n     <td>Intent Alignment via RL</td>\n     <td>12</td>\n    </tr>\n    <tr>\n     <td>2.2.5</td>\n     <td>Cognitive Loop via Test-time Compute</td>\n     <td>13</td>\n    </tr>\n    <tr>\n     <td>2.3</td>\n     <td>Spatial Consistency</td>\n     <td>14</td>\n    </tr>\n    <tr>\n     <td>2.3.1</td>\n     <td>Geometric Decomposition of Consistency</td>\n     <td>14</td>\n    </tr>\n    <tr>\n     <td>2.3.2</td>\n     <td>Theoretical Formulation.</td>\n     <td>15</td>\n    </tr>\n    <tr>\n     <td>2.3.3</td>\n     <td>2D Proxy Manifold &amp; Domain Mismatch</td>\n     <td>16</td>\n    </tr>\n    <tr>\n     <td>2.3.4</td>\n     <td>Implicit Continuous Fields</td>\n     <td>18</td>\n    </tr>\n    <tr>\n     <td>2.3.5</td>\n     <td>Explicit Lagrangian Primitives</td>\n     <td>19</td>\n    </tr>\n    <tr>\n     <td>2.3.6</td>\n     <td>Generative Statistical Priors</td>\n     <td>20</td>\n    </tr>\n    <tr>\n     <td>2.4</td>\n     <td>Temporal Consistency</td>\n     <td>22</td>\n    </tr>\n    <tr>\n     <td>2.4.1</td>\n     <td>From Frequency Stability to Physical Compliance</td>\n     <td>23</td>\n    </tr>\n    <tr>\n     <td>2.4.2</td>\n     <td>Latent Temporal Inflation</td>\n     <td>24</td>\n    </tr>\n    <tr>\n     <td>2.4.3</td>\n     <td>Discrete Autoregressive Modeling</td>\n     <td>25</td>\n    </tr>\n    <tr>\n     <td>2.4.4</td>\n     <td>Unified Spatiotemporal Modeling via DiT</td>\n     <td>26</td>\n    </tr>\n    <tr>\n     <td>2.4.5</td>\n     <td>Logical Consistency and Causal Reasoning</td>\n     <td>27</td>\n    </tr>\n    <tr>\n     <td>2.5</td>\n     <td>Outlook of the Consistencies.</td>\n     <td>28</td>\n    </tr>\n    <tr>\n     <td>3</td>\n     <td>Initial Integration of Multiple Consistencies</td>\n     <td>28</td>\n    </tr>\n    <tr>\n     <td>3.1</td>\n     <td>The Rise of Large Multimodal Models</td>\n     <td>28</td>\n    </tr>\n    <tr>\n     <td>3.1.1</td>\n     <td>LLM as a Core Cognitive Base</td>\n     <td>28</td>\n    </tr>\n    <tr>\n     <td>3.1.2</td>\n     <td>Cognitive Evolution as a Multimodal</td>\n     <td>29</td>\n    </tr>\n    <tr>\n     <td>3.2</td>\n     <td>Integration of Modal and Spatial Consistency</td>\n     <td>30</td>\n    </tr>\n    <tr>\n     <td>3.2.1</td>\n     <td>Pixel Space Manipulation</td>\n     <td>31</td>\n    </tr>\n    <tr>\n     <td>3.2.2</td>\n     <td>View Space Mapping.</td>\n     <td>34</td>\n    </tr>\n    <tr>\n     <td>3.2.3</td>\n     <td>Volume Space Representation</td>\n     <td>35</td>\n    </tr>\n    <tr>\n     <td>3.2.4</td>\n     <td>Reinforcement Learning for Modal-Spatial Alignment</td>\n     <td>37</td>\n    </tr>\n    <tr>\n     <td>3.3</td>\n     <td>Integration of Modal and Temporal Consistency</td>\n     <td>38</td>\n    </tr>\n    <tr>\n     <td>3.3.1</td>\n     <td>End-to-End Scalable Modeling</td>\n     <td>39</td>\n    </tr>\n    <tr>\n     <td>3.3.2</td>\n     <td>Explicit Structured Control</td>\n     <td>43</td>\n    </tr>\n    <tr>\n     <td>3.3.3</td>\n     <td>Unified Comprehension and Generation Symbiosis Architecture</td>\n     <td>46</td>\n    </tr>\n    <tr>\n     <td>3.3.4</td>\n     <td>Reinforcement Learning for Modal-Temporal Alignment</td>\n     <td>47</td>\n    </tr>\n    <tr>\n     <td>3.4</td>\n     <td>Integration of Spatial and Temporal Consistency</td>\n     <td>49</td>\n    </tr>\n    <tr>\n     <td>3.4.1</td>\n     <td>Implicit Spatiotemporal Learning</td>\n     <td>50</td>\n    </tr>\n    <tr>\n     <td>3.4.2</td>\n     <td>Explicit Geometric Anchoring.</td>\n     <td>52</td>\n    </tr>\n   </tbody>\n  </table>\n\n---\n\n<table>\n    <tbody>\n        <tr>\n            <td>3.4.3</td>\n            <td>Unified Spatiotemporal Representation</td>\n            <td>54</td>\n        </tr>\n        <tr>\n            <td>3.4.4</td>\n            <td>Reinforcement Learning for Spatial-Temporal Alignment</td>\n            <td>57</td>\n        </tr>\n        <tr>\n            <td>3.5</td>\n            <td>Preliminary Emergence of World Models</td>\n            <td>58</td>\n        </tr>\n        <tr>\n            <td>3.5.1</td>\n            <td>From Benchmark Establishment to Diverse Evolution</td>\n            <td>58</td>\n        </tr>\n        <tr>\n            <td>3.5.2</td>\n            <td>Combat Loop of Three Consistencies.</td>\n            <td>60</td>\n        </tr>\n        <tr>\n            <td colspan=\"2\">4 Challenges, Benchmarks, and Outlook</td>\n            <td>61</td>\n        </tr>\n        <tr>\n            <td>4.1</td>\n            <td>Core Challenges from Preliminary Fusion to True Unification</td>\n            <td>61</td>\n        </tr>\n        <tr>\n            <td>4.2</td>\n            <td>Constructing Comprehensive Evaluation Benchmarks</td>\n            <td>62</td>\n        </tr>\n        <tr>\n            <td>4.2.1</td>\n            <td>Modal Consistency: From Symbol Mapping to Knowledge Synergy.</td>\n            <td>62</td>\n        </tr>\n        <tr>\n            <td>4.2.2</td>\n            <td>Spatial Consistency: From Visual Similarity to Topological & Physical Verification</td>\n            <td>63</td>\n        </tr>\n        <tr>\n            <td>4.2.3</td>\n            <td>Temporal Consistency: From Inter-frame Smoothness to Logical Causal Evolution</td>\n            <td>63</td>\n        </tr>\n        <tr>\n            <td>4.2.4</td>\n            <td>Limitations of Existing Benchmarks & Design Rationale of Our Benchmark</td>\n            <td>64</td>\n        </tr>\n        <tr>\n            <td>4.3</td>\n            <td>Ultimate Outlook: General World Simulator.</td>\n            <td>65</td>\n        </tr>\n        <tr>\n            <td colspan=\"2\">5 CoW-Bench</td>\n            <td>66</td>\n        </tr>\n        <tr>\n            <td>5.1</td>\n            <td>Dataset</td>\n            <td>66</td>\n        </tr>\n        <tr>\n            <td>5.1.1</td>\n            <td>Dataset Construction.</td>\n            <td>66</td>\n        </tr>\n        <tr>\n            <td>5.1.2</td>\n            <td>Dataset Analysis</td>\n            <td>66</td>\n        </tr>\n        <tr>\n            <td>5.2</td>\n            <td>Evaluation metrics</td>\n            <td>68</td>\n        </tr>\n        <tr>\n            <td>5.3</td>\n            <td>Comparison with Existing Benchmarks</td>\n            <td>71</td>\n        </tr>\n        <tr>\n            <td>5.4</td>\n            <td>Main Results</td>\n            <td>72</td>\n        </tr>\n        <tr>\n            <td>5.5</td>\n            <td>Single-Axis Consistency</td>\n            <td>73</td>\n        </tr>\n        <tr>\n            <td>5.5.1</td>\n            <td>Modal Consistency Results</td>\n            <td>73</td>\n        </tr>\n        <tr>\n            <td>5.5.2</td>\n            <td>Temporal Consistency Results</td>\n            <td>75</td>\n        </tr>\n        <tr>\n            <td>5.5.3</td>\n            <td>Spatial Consistency Results</td>\n            <td>76</td>\n        </tr>\n        <tr>\n            <td>5.6</td>\n            <td>Cross-Axis Consistency.</td>\n            <td>78</td>\n        </tr>\n        <tr>\n            <td>5.6.1</td>\n            <td>Modal-Space Consistency Results: Semantic-to-Geometry Binding</td>\n            <td>78</td>\n        </tr>\n        <tr>\n            <td>5.6.2</td>\n            <td>Modal-Time Consistency Results: Executing a Temporal Program.</td>\n            <td>79</td>\n        </tr>\n        <tr>\n            <td>5.6.3</td>\n            <td>Time-Space Consistency Results: Navigation Exposes the Missing World State</td>\n            <td>80</td>\n        </tr>\n        <tr>\n            <td>5.7</td>\n            <td>Sample Analysis.</td>\n            <td>81</td>\n        </tr>\n        <tr>\n            <td>5.7.1</td>\n            <td>Single Consistency Tasks.</td>\n            <td>81</td>\n        </tr>\n        <tr>\n            <td>5.7.2</td>\n            <td>Compound Consistency Tasks</td>\n            <td>83</td>\n        </tr>\n        <tr>\n            <td colspan=\"2\">6 Conclusion</td>\n            <td>84</td>\n        </tr>\n        <tr>\n            <td colspan=\"2\">7 Contributions</td>\n            <td>97</td>\n        </tr>\n    </tbody>\n</table>\n\n---\n\nThe Trinity of Consistency as a Defining Principle for General World Models\n\n# 1 Introduction\n\nThe pursuit of Artificial General Intelligence (AGI) is fundamentally anchored in the aspiration to endow machines with a profound understanding of the physical reality. A truly intelligent agent must evolve from a passive observer [1] into a proactive simulator [2, 3], possessing an internal world model capable of learning objective physical laws, reasoning about counterfactual scenarios [4], and predicting future states from current actions [5].\n\nRecent years have witnessed an explosion in generative capability, driven by the data-driven Scaling Laws. Video generation models, represented by Sora [2] and Gen-3 [6], have demonstrated an astonishing ability to approximate complex dynamics, creating high-fidelity visual sequences that often are indistinguishable from reality. Simultaneously, the rise of Unified Multimodal Models (UMMs) [7, 8] has offered a promising architectural paradigm for integrating diverse sensory inputs into a shared semantic manifold [9]. However, a critical gap remains: existing models, despite their visual plausibility, often behave as naive physicists. They frequently suffer from structural hallucinations, temporal inconsistencies, and violations of causality—symptoms of a system that mimics pixel statistics rather than internalizing physical principles. The field lacks a principled theoretical framework to define the essential properties requisite for a General World Model.\n\nTo bridge the chasm between visual generation and physical simulation, we propose that a robust World Model must be grounded in the *Trinity of Consistency*. We argue that a valid internal simulator must satisfy three orthogonal yet synergistic constraints:\n\n• **Modal Consistency (The Semantic Interface):** The ability to align heterogeneous information (text, image, tactile) into a unified semantic space, serving as the cognitive interface for instruction and feedback.\n\n• **Spatial Consistency (The Geometric Basis):** The capacity to construct a 3D-aware representation that respects geometry, occlusion, and object permanence, ensuring the static plausibility of the simulated world.\n\n• **Temporal Consistency (The Causal Engine):** The adherence to physical laws and causal logic over time, ensuring that dynamic evolution follows a predictable and logically sound trajectory.\n\nThrough this tripartite lens, we systematically review the evolution of generative models from specialized modules to unified world simulators. We trace the trajectory from loosely coupled specialized modules toward end-to-end unified architectures. We argue that dissolving the barriers between these dimensions is the necessary substrate for the emergence of world simulation capabilities, ensuring that modality, space, and time do not operate in isolation but synergize to model a coherent reality.\n\nThis paper is organized to mirror the evolutionary path from specialized modules to unified world simulators. First (§2), we deconstruct the independent development of Modal, Spatial, and Temporalconsistencies, analyzing their respective theoretical foundations. Second (§3), we investigate the paradigm shift enabled by UMMs, detailing how the deep integration of these dimensions facilitates the emergence of physical simulation capabilities. Third (§4), we identify the remaining gaps between current probabilistic generators and true physical simulators, setting the stage for rigorous evaluation. The notation used is summarized in Table 1.\n\nFinally, theoretical frameworks require rigorous verification. We introduce **CoW-Bench (Consistency of World-models Benchmark)**, a unified evaluation suite centered on multi-frame reasoning and constraint satisfaction. Unlike previous benchmarks, CoW-Bench rigorously tests the model's ability to maintain the *Trinity of Consistency* under complex, open-ended scenarios, forcing it to prove it understands the world, not just how to paint it.\n\n---\n\nFigure 2: Performance Comparison of Mainstream Models across Different Tasks. The score has been linearly rescaled from the original range of [0, 10] to a percentage scale of [0, 100].\n\n# 2 Foundational Exploration of Consistencies\n\n## 2.1 The Anatomy of General World Models\n\nAs discussed in Section 1 (§1), the construction of world models relies on the organic integration of modal consistency (serving as the information interface), spatial consistency (serving as the geometric cornerstone), and temporal consistency (serving as the dynamic engine). In the evolution of specialized models, these consistencies have not developed in isolation but have rather interpenetrated one another: the unified representation space derived from modality alignment provides semantic priors for the reconstruction of spatial geometry, while the 3D manifold of spatial consistency establishes physical constraints for temporal evolution.\n\nThis section deconstructs that evolutionary history. We trace how specialized models first conquered these challenges in isolation: modality alignment matured through high-dimensional manifold mapping, spatial consistency was solved via the transition from 2D proxies to explicit 3D primitives, and temporal consistency evolved from simple frame interpolation to causal dynamics modeling. Here, we systematically analyze the theoretical foundations and mechanism shifts of each dimension, establishing the necessary prerequisites that eventually enabled the emergence of the unified world simulators discussed in later sections.\n\n---\n\nTable 1: Notation and Descriptions\n\n<table><thead><tr><th>Symbol</th><th>Description</th><th>Symbol</th><th>Description</th></tr></thead><tbody><tr><td><i>W</i></td><td>World Model</td><td><b>p</b></td><td>3D Position</td></tr><tr><td><i>S</i>, <i>A</i></td><td>State &amp; Action Space</td><td><i>P</i><sub><i>t</i></sub></td><td>Camera Pose at <i>t</i></td></tr><tr><td><b>s</b><sub><i>t</i></sub>, <b>a</b><sub><i>t</i></sub></td><td>State &amp; Action Instance</td><td><i>K</i></td><td>Intrinsic Matrix</td></tr><tr><td><i>&pi;</i></td><td>Policy</td><td><i>&Pi;</i></td><td>Projection Operator</td></tr><tr><td><i>&tau;</i></td><td>Trajectory</td><td><i>K</i></td><td>Keyframe Set</td></tr><tr><td><i>T</i></td><td>Dynamics Function</td><td><i>M</i><sub>geo</sub></td><td>Geometric Manifold</td></tr><tr><td><i>Z</i></td><td>Latent World State</td><td><i>G</i><sub>k</sub></td><td>3D Gaussian Primitive</td></tr><tr><td><b>x</b><sub>obs</sub></td><td>Multimodal Observation</td><td><i>&sigma;</i></td><td>Volume Density</td></tr><tr><td><b>z</b></td><td>Latent Vector</td><td><i>c</i></td><td>View-dependent Radiance</td></tr><tr><td><i>E</i>, <i>D</i></td><td>Encoder / Decoder</td><td><i>F</i><sub>fund</sub></td><td>Fundamental Matrix</td></tr><tr><td><i>C</i></td><td>VQ Codebook</td><td><i>O</i><sub>flow</sub></td><td>Optical Flow</td></tr><tr><td><i>S</i></td><td>Token Sequence</td><td><i>M</i><sub>epi</sub></td><td>Epipolar Mask</td></tr><tr><td><i>W</i><sub>proj</sub></td><td>Projection Weight</td><td><i>T</i>(<i>t</i>)</td><td>Continuous Trajectory</td></tr><tr><td><i>I</i>(<i>X</i>; <i>Z</i>)</td><td>Mutual Information</td><td><i>&Phi;</i></td><td>Spatiotemporal Field</td></tr><tr><td><i>&epsilon;</i><sub><i>&theta;</i></sub></td><td>Noise Predictor</td><td><i>&Psi;</i></td><td>Physical Property Field</td></tr><tr><td><b>v</b><sub><i>t</i></sub></td><td>Velocity Field</td><td><i>D&Phi;&n Labor from t</i></td><td>Material Derivative</td></tr><tr><td><i>g</i>(<i>t</i>)</td><td>Diffusion Coefficient</td><td>&#x20D7; &middot; <i>v</i></td><td>Divergence</td></tr><tr><td><i>&alpha;</i><sub><i>t</i></sub>, <i>&sigma;</i><sub><i>t</i></sub></td><td>SNR Parameters</td><td><b>&# manifold evolutionary flow</b></td><td>Force Vector</td></tr><tr><td><i>w</i></td><td>Wiener Process</td><td>&#x20D7;<i>f</i></td><td>Implicit Gradient</td></tr><tr><td><i>F</i><sub><i>t</i></sub></td><td>STFT (Fourier Transform)</td><td><i>M</i><sub>dyn</sub></td><td>Dynamic Manifold</td></tr><tr><td><i>L</i></td><td>Loss Function</td><td><i>Phys</i></td><td>Physics Score</td></tr><tr><td><i>G</i><sub>graph</sub></td><td>Causal Graph</td><td><i>&Delta;</i><sub>const</sub></td><td>Constraint Deviation</td></tr><tr><td><i>D</i><sub>KL</sub></td><td>KL Divergence</td><td><i>w</i></td><td>Guidance Scale</td></tr></tbody></table>\n\n2.2 Modal Consistency\n\nThe core challenge in constructing general world models lies in the semantic alignment of hetero-\ngeneous modalities. Unlike the homogeneity of unimodal generation, multimodal consistency is\nessentially a problem of solving high-dimensional heterogeneous manifold alignment, as illustrated\nin Figure 3. The model must transcend entropy disparity and topological mismatch to construct a\nunified representation space that is physically complete and logically self-consistent. To this end, we\nintroduce two fundamental theoretical assumptions, the Platonic Representation Hypothesis and the\nHypersphere Geometry Hypothesis, and use these as a basis to expound on the cognitive architectural\nevolution from direct feed-forward mapping to iterative reasoning and planning.\n\nTo systematically deconstruct this alignment process, this section will first elucidate the origins of\nthe modality gap from the perspective of geometric topology (§2.2.1); subsequently, it will analyze\ntwo mainstream generative manifold mechanisms—namely, discrete autoregression and continuous\nflow matching (§2.2.2); it will then explore the orthogonal decoupled architecture evolved to minimize\ngradient conflicts (§2.2.3); and finally, it will introduce feedback-based intent alignment and the\ncognitive inference loop moving towards test-time compute (§2.2.5).\n\n---\n\n**Figure 3: Unified Representation Goal.** Modal consistency aims to project heterogeneous inputs (Text, Image, Video, Audio) into a unified, physically-aligned latent space.\n\n2.2.1 Theoretical Foundations\n\n**Platonic Cave & Projected Manifolds** The theoretical foundation of multimodal learning can be traced back to the platonic representation hypothesis [9]. This hypothesis formally defines the existence of an objective latent physical state space, Z<sub>world</sub>, in the real world, where images and text are projections of this high-dimensional entity onto different low-dimensional subspaces. The essence of modal consistency is solving a joint inverse projection problem: reconstructing the shared latent variable z via observed shadows {x<sub>img</sub>, x<sub>txt</sub>}.\n\nHowever, this is a typical ill-posed problem—the visual projection P<sub>img</sub> retains a vast amount of high-frequency physical entropy, whereas the textual projection P<sub>txt</sub> highly abstracts discrete symbolic logic. This Entropy Asymmetry constitutes the primary obstacle to direct alignment.\n\n**Hypersphere Hypothesis & Modal Gap** To mathematically align these two heterogeneous spaces, mainstream paradigms (such as CLIP) introduce the Hypersphere Hypothesis [43], which forces feature vectors to be uniformly distributed on a unit hypersphere S<sup>d−1</sup>. However, this strong assumption ignores the pervasive modal gap in multimodal representations [44]. On one hand, empirical studies by Liang et al. pointed out the cone effect, as shown in Figure 5: joint optimization causes visual and textual embeddings to collapse into two narrow and separated conical regions, destroying the isotropy of the feature space. On the other hand, from the perspective of manifold learning, this gap reveals a deeper topological mismatch: visual data is typically distributed on a continuous, dense low-dimensional manifold, while linguistic data presents a sparse, discrete clustering structure. This fundamental difference in intrinsic dimensionality and data density leads to manifold non-isomorphism, rendering the achievement of perfect isometric alignment between the two spaces, while maintaining their respective semantic structures, an ill-posed problem.\n\n**Evolution of Computational Paradigms: From Amortized Inference to Test-time Compute** Facing the inherent representation errors caused by the aforementioned geometric topological mismatch, simple parameter internalization strategies face theoretical bottlenecks, prompting the modeling of modal consistency to undergo a transition between two major computational paradigms. This\n\n---\n\nFigure 4: Evolution of Modal Consistency: From Geometric Isolation to Cognitive Alignment\n\nprofoundly reflects the trade-off between train-time compute and test-time compute [45].\n\nEarly direct feed-forward mapping corresponds to Dual-Tower architectures [10] and single-step generative models, the core of which is identifying physical rules into neural network weights through large-scale training, i.e., Amortized Inference [46]. This paradigm requires only one forward pass during inference (NFE = 1). Although highly efficient, it is limited by in-distribution statistical correlations and essentially can only interpolate within established conical regions, making it difficult to handle unseen counterfactual combinations [47].\n\nIn contrast, the current trend is shifting towards iterative reasoning & planning, corresponding to\niterative reasoning architectures. This paradigm acknowledges the limitations of single-pass mapping\nin bridging the modality gap and instead introduces explicit state space search during the inference\nphase. By constructing a Tree of Thoughts [48] in the latent space or executing gradient-guided\ndynamic planning, the model utilizes additional reasoning compute to instantly correct physical drift.\nThis marks a shift in consistency modeling from static pattern matching to dynamic manifold planning.\n\n2.2.2 Discrete Sequences vs. Continuous Manifolds\n\nTo computationally realize the Joint Inverse Projection process in the above theory, academia has explored two distinct mathematical paths to model the target conditional probability density P(ximg|xtxt). This choice determines the physical nature of the latent space manifold: *Is it treated as a Discrete Symbolic Sequence or a Continuous Euclidean Vector Field?* We compare the mathematical forms and dynamic characteristics of these two paradigms in Table 2.\n\nTable 2: Mechanism Comparison: Discrete AR vs. Continuous Flow Matching. The formulations highlight the trade-off between optimization objectives and error propagation dynamics.\n\n<table>\n   <thead>\n    <tr>\n     <td>\n      Paradigm\n     </td>\n     <td>\n      Objective (The Soul)\n     </td>\n     <td>\n      Error\n     </td>\n     <td>\n      Topology\n     </td>\n    </tr>\n   </thead>\n   <tbody>\n    <tr>\n     <td>\n      Discrete AR\n     </td>\n     <td>\n      LAR = -E [∑ log P(s\n      <sub>\n       t\n      </sub>\n      |s&lt;_\n      <sub>\n       t\n      </sub>\n      )]\n     </td>\n     <td>\n      Exp.\n     </td>\n     <td>\n      Discrete\n     </td>\n    </tr>\n    <tr>\n     <td>\n      Flow Matching\n     </td>\n     <td>\n      LFM = E(|v\n      <sub>\n       q\n      </sub>\n      (x\n      <sub>\n       t\n      </sub>\n      ) - (x\n      <sub>\n       1\n      </sub>\n      - x\n      <sub>\n       0\n      </sub>\n      )|^2]\n     </td>\n     <td>\n      Linear\n     </td>\n     <td>\n      Euclidean\n     </td>\n    </tr>\n   </tbody>\n  </table>\n\n---\n\nFigure 5: The Modal Gap Challenge. (Left) Ideal hypersphere alignment assumes uniform distribution. (Right) In reality, entropy disparity causes visual embeddings to collapse into a narrow \"cone,\" leading to topological mismatch with discrete text tokens.\n\n**Discrete Autoregressive (AR)** The core of this paradigm lies in the Token-centric philosophy, attempting to transform visual generation into a sequence prediction problem through a unified discrete symbol interface [49, 50]. Its generation process involves strictly coupled stages: first quantizing continuous images into discrete symbols via VQ-GAN, followed by maximizing the sequence log-likelihood using the causal attention mask of a Transformer.\n\n*Exponential Drift & Codebook Collapse.* Although the AR paradigm achieves interface unification, it suffers from two endogenous defects when viewed from a dynamic perspective [51]. First is the curse of dimensionality. The discretization process is governed by the Dirichlet process; as the codebook dimension increases, the effective utilization rate decays exponentially, leading to the loss of high-frequency textures [52, 53]. Second is error accumulation dynamics. The essence of autoregressive generation is the recursive application of operators. Assuming the local Lipschitz constant of the operator is $L > 1$, the cumulative drift of the initial quantization error $\\epsilon_0$ after $T$ steps is $\\|\\delta_T\\| \\approx L^T \\|\\epsilon_0|$. This exponential error amplification explains why AR models often exhibit structural collapse at the tail end when generating long sequences [54].\n\n**Continuous Flow Matching (FM)**\nTo circumvent quantization errors, the new generation of paradigms (such as Stable Diffusion 3 [27], Emu3 [28]) returns to the continuous latent space. Unlike traditional diffusion models based on the SDE denoising perspective, Flow Matching (FM) [55] adopts an ODE perspective, constructing a deterministic transport path connecting noise and data.\n\n*Velocity Field Regression & Rectified Path.* The core idea of continuous FM is to directly fit the velocity field of the probability flow. During training, the intermediate state $x_t$ is defined as a linear interpolation between data and noise, corresponding to an ideal straight trajectory with a target velocity field constantly being $v_t = x_1 - x_0$. The neural network directly regresses this velocity vector via Mean Squared Error loss. Rectified Flow [56] demonstrates that this Reflow operation rectifies the transport\n\n---\n\ntrajectory, corresponding to a Lipschitz constant $L \\approx 1$. This implies that error accumulation transforms into linear growth $\\|\\delta_T\\| \\approx T \\cdot \\epsilon_{step}$, allowing FM to generate high-fidelity samples in very few steps while perfectly preserving the continuous semantic manifold of the latent space.\n\n### 2.2.3 Architectural Evolution\n\nEstablishing the generation mechanism only solves the mathematical expression of the target manifold. How to inject heterogeneous modal information into this manifold depends on the conditioning mechanism of the model. The evolution of multimodal architectures exhibits non-linear characteristics, essentially seeking the optimal parameter space topology to minimize gradient conflict and information loss between modalities. This process has undergone a three-stage evolution from geometric isolation to early fusion, and finally converging to orthogonal decoupling, as shown in Figure 6.\n\n**Figure 6:** Evolution of Multimodal Fusion Paradigms. Transitioning from geometric isolation (Dual-Tower) to unstable Early Fusion (Adapter), and finally to the orthogonally decoupled Native unified multimodal model (MM-DiT) in large-scale unified architectures.\n\n**(1) Early Evolution: Establishment of Dual-Tower Architectures and Connector Paradigms.** Early exploration of multimodal alignment presented two clear technological evolution paths. First was the *Dual-Tower Architecture*, represented by CLIP [10] and ALIGN [57]. This paradigm utilized contrastive learning to project heterogeneous modalities onto a shared hypersphere. Although excellent in retrieval tasks, the separate processing of images and text by independent encoders resulted in a natural asymmetry in geometric topology, lacking deep, fine-grained interaction.\n\nTo address this limitation, the *Connector-based Paradigm*, represented by Flamingo [14] and BLIP/BLIP-2 [15, 16], emerged. These methods froze the pre-trained visual encoder and innovatively introduced learnable bridge modules (such as Perceiver Resampler or Q-Former) to align visual features with the semantic space of LLMs. This design of Frozen Visual Backbone & Lightweight Connector not only reduced training costs but also established a standard architectural template for subsequent LMMs.\n\n**(2) Early Fusion and the Challenge of Unified Optimization.** To further break the geometric isolation between modalities, academia began exploring more radical *Early Fusion* strategies. Representative\n\n---\n\nworks such as Unified-IO [58] attempted to handle various heterogeneous tasks within a unified sequence-to-sequence framework, promoting the development of general interfaces.\nHowever, this fully unified paradigm exposes deep Optimization Instability. Particularly when introducing discretization strategies (such as Chameleon [59]), despite achieving interface unification, different modalities exhibited significant differences in training dynamics. Empirical evidence shows that the gradient variance of visual tokens is significantly higher than that of text, making it difficult for the model to converge to an optimal solution during joint training.\nFurthermore, continuous asymmetric paradigms, such as LLaVA [23], interface with large language models through a projection layer. However, the linear projection layer Wproj essentially acts as a low-rank compressor (as shown in Figure 7). During optimization, the model is encouraged to preserve semantic information that is relevant for textual reasoning, while suppressing high-frequency components that are essential for image synthesis. As a result, the mutual information between the input image and the projected representation is substantially reduced. This explains why LLaVA excels in understanding tasks but fails to restore texture details in generation tasks.\n\nFigure 7: Information Asymmetry in LLaVA. The linear projection layer $W_{proj}$ acts as a low-rank compressor, prioritizing semantic alignment with the LLM while discarding high-frequency visual textures needed for controllable visual generation.\n\n(3) The **Mainstream Paradigm of Orthogonal Decoupling.** Addressing the aforementioned gradient conflict, works represented by Stable Diffusion 3.5 [27] and Emu3 [28] established the current MM-DiT architecture. The core lies in the *weight decoupling* strategy—maintaining independent weight sets $W_{txt}$, $W_{img}$ for text and images, exchanging data only during attention operations, as shown in Figure 8.\n\nFrom the perspective of optimization dynamics, this design forces the Hessian matrix of the joint loss function to exhibit an approximate block-diagonal structure:\n\n$$H_{\\text{total}} \\approx \\begin{bmatrix} H_{\\text{txt}} & 0 \\\\ 0 & H_{\\text{img}} \\end{bmatrix}, \\quad \\text{s.t. } \\frac{\\partial^2 \\mathcal{L}}{\\partial W_{\\text{txt}} \\partial W_{\\text{img}}} \\rightarrow 0, \\tag{1}$$\n\nwhere $H_{total}$ denotes the joint Hessian matrix, and $W_{txt}/{img}$ represents the modality-specific parameters. This structure effectively isolates modality-specific curvature, causing gradient updates for different modalities to tend towards orthogonality in the parameter space.\nEmpirical data indicates that this\n\n---\n\nFigure 8: MM-DiT Architecture. By maintaining independent weight sets for both text and image modalities and interacting only via joint Attention, MM-DiT achieves orthogonal gradient updates, effectively resolving the modality conflict.\n\nmechanism significantly reduces the gradient conflict rate from over 50% in AR paradigms to approximately 30% [60]. This was validated in Stable Diffusion 3.5 Large: thanks to modality decoupling, the model demonstrates instruction following capabilities and physical fidelity significantly superior to asymmetric architectures such as LLaVA on tasks requiring complex typography rendering and long-text comprehension.\n\n#### 2.2.4 Intent Alignment via RL\n\nAfter achieving orthogonal decoupling with the MM-DiT architecture, the focus of consistency modeling shifts from physical representation fitting to high-level semantic alignment. Although traditional maximum likelihood estimation (MLE) captures pixel statistical correlations, it often falls into semantic drift due to a lack of explicit supervision when dealing with ill-posed joint inverse projection problems [9]. To this end, academia has introduced reinforcement learning with human feedback (RLHF) [61], reframing alignment as a reward-guided search on the hypersphere manifold [43].\n\n**Process Supervision & Physical Constraints** The architectural evolution based on preference fine-tuning began with efficient DiT baselines, exemplified by PixArt-α [29]. Owing to their relatively low training cost, these architectures enable practical end-to-end alignment under preference supervision. Addressing the sparsity of trajectory feedback in traditional DPO (Direct Preference Optimization), SPO [39] and VisualPRM [40] introduced stepwise evaluation mechanisms, performing fine-grained supervision on every inference step in the denoising path. Meanwhile, to address non-physical phenomena such as gravity violation, PhyGDPO [41] introduced physics-aware VLM feedback, where the core loss function is implemented by penalizing a physical violation term ΛPhysScore:\n\n$$ \\mathcal{L}_{\\text{Phy-DPO}} = -\\mathbb{E} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_{\\theta}(v_w)}{\\pi_{\\text{ref}}(v_w)} - \\beta \\log \\frac{\\pi_{\\theta}(v_l)}{\\pi_{\\text{ref}}(v_l)} + \\alpha \\Delta \\text{PhysScore} \\right) \\right], \\quad (2) $$\n\n---\n\nwhere $\\beta$ is the KL divergence penalty coefficient that controls the deviation from the reference policy $\\pi_{ref}$, $v_w$ and $v_l$ denote the winning and losing video samples respectively, and $\\Delta$PhysScore measures the difference in physical compliance scores.\n\n**Perception-Generation Synergistic Loop** To further break through the upper limits of static datasets, academia has established an interactive optimization paradigm centered on *VLM-as-a-Judge*. This paradigm utilizes the strong semantic perception capabilities of Multimodal Large Models as a Critic to construct a Generate-Evaluate-Refine closed-loop system. Representative works such as Meta-Morph [35] achieved unified alignment of understanding and generation through instruction tuning; while SRUM [36] further proposed a unified multimodal self-correction mechanism. SRUM guides the iterative fine-tuning of the diffusion model by backpropagating discriminant gradients to the generator or by utilizing fine-grained deReact captions generated by the VLM. This reciprocal improvement between perception and generation not only resolves attribute omission issues under complex prompts but also enables T2I models to continuously approach the semantic understanding upper bound of VLMs through bootstrapping in the absence of external human annotation.\n\n**Factorized Optimization for AR Models** Unlike the denoising optimization of Diffusion models, AR models face the dual challenges of discrete space non-differentiability and temporal error accumulation. Addressing this, AR-GRPO [42] and ReasonGen-R1 [62] in 2025 proposed a factorized optimization strategy for sequence generation:\n\n$$ \\mathcal{L}_{AR-RL} = \\underbrace{E_\\pi[R(x)]}_{\\text{Alignment Gain}} - \\beta \\underbrace{D_{KL}(\\pi || \\pi_{ref})}_{\\text{Temporal Smoothing}}, \\quad (3) $$\n\nwhere $R(x)$ is the reward function derived from CLIP or VQA feedback, and $\\beta$ serves as the regularization coefficient for the KL divergence term $D_{KL}$. This paradigm explicitly decomposes the loss function into alignment gain and a temporal smoothing term. The alignment term utilizes CLIP/VQA rewards to guide token selection to conform to semantic intent, while the KL divergence constraint forces the policy to remain within the pre-trained language manifold, preventing the model from suffering Language Collapse due to over-optimization of rewards. Empirical evidence shows that this strategy effectively suppresses token repetition and garbled text in long sequence generation.\n\n**2.2.5 Cognitive Loop via Test-time Compute**\n\nAlthough reinforcement learning has achieved preliminary alignment of human intent, modal consistency remains limited by the platonic statistical boundary [9]. Existing generative models are essentially pattern-matching interpolators that fit the training distribution solely through amortized inference [47]. When faced with counterfactual tasks that require multi-step chain deduction, this one-pass mapping mechanism lacks real-time verification and is prone to logical hallucinations [63].\n\nTo correct logical drift in long-range generation, consistency modeling is shifting towards the test-time compute [45] paradigm. This paradigm acknowledges the limitations of single-shot inverse projection and instead introduces explicit state space search during the inference phase. In this closed loop, the generation process is redefined as an optimal path search problem on the spatiotemporal manifold $\\mathcal{M}$.\n\nRecent paradigms like UniGen [64] and EvoSearch [65] have introduced multi-step reasoning architectures, combining monte carlo tree search (MCTS) [66] with verifier mechanisms [67], to achieve inference-time scaling during generation. Addressing the high-dimensional nature of visual tasks, VisualPRM [40] utilizes a process reward model to perform fine-grained verification on logical nodes of the denoising trajectory, thereby mathematically enhancing the logical consistency of generated results. Furthermore, by integrating an explicit causal planning layer [68], the model is enabled to utilize additional reasoning compute to detect and correct deviations in physical trajectories.\n\n---\n\n## 2.3 Spatial Consistency\n\nFigure 9: Spatial Consistency via Multi-View Constraints. The model ensures that the generated subject (Doge) maintains geometric coherence across Front, Side, and Top-down views, preventing structural distortion and the Janus problem.\n\nThe modal consistency discussed in the previous section successfully constructed a unified semantic mapping for heterogeneous data. However, for constructing an executable Internal Simulator, having only semantic alignment is incomplete. As developmental psychology research points out, cognition of the world is built upon the foundations of Object Permanence [69] and 3D Exclusivity [70]. Such semantic representations, lacking geometric entities, cannot support an agent's navigation and interaction within a three-dimensional space [71]. The core mission of spatial consistency is to ground these semantic latent variables onto a three-dimensional geometric manifold $M_{geo}$ that conforms to physical laws. This is essentially solving a typical Ill-posed Inverse Problem [72], as shown in Figure 9: specifically, how to recover a high-dimensional state space satisfying multi-view geometric constraints (such as epipolar equivariance) from dimensionality-reduced, sparse 2D observations, while avoiding structural artifacts like the Janus Problem.\n\nTo construct a unified theoretical framework, we formalize this process as solving a set of coupled differential equation inverse problems on a spatiotemporal manifold. This section will elucidate how models establish the static geometric basis of the world model by introducing physical priors and generative diffusion priors, following an evolutionary path from 2D proxy manifolds to 3D implicit fields, and finally converging to Explicit Lagrangian Primitives.\n\n### 2.3.1 Geometric Decomposition of Consistency\n\nTo mathematically characterize spatial consistency, we decompose this abstract concept into two complementary and hierarchically progressive topological constraints: the former governs the microscopic continuity of the physical surface, while the latter guarantees the macroscopic uniqueness and coherence of the object structure.\n\n**Micro-level: Local Neighborhood Topological Consistency.** This constraint focuses on the **Intrinsic Continuity** of the manifold $M$, which corresponds mathematically to the Lipschitz Condition. That is, for any two adjacent points on the manifold, the difference in their physical attributes (such as color, density) should be strictly constrained linearly by their Euclidean distance. In 3D reconstruction and generation tasks, this constraint is typically implemented explicitly through geometric regularization\n\n---\n\nFigure 10: Evolution of Spatial Consistency Paradigms: From 2D Proxy to Generative Primitives.\n\nterms. For example, IGR (Implicit Geometric Regularization) [87] utilizes the Eikonal equation to\nconstrain the norm of gradients, while RegNeRF [105] introduces a smoothness loss to suppress non-\nphysical high-frequency noise generated under sparse views, ensuring the generated object possesses\na smooth and physically reasonable surface.\n\n**Macro-level: Global Geometric Consistency.** Local smoothness alone is insufficient; the model must also satisfy **Epipolar Equivariance** in multi-view geometry [72]. That is, when observing the same object from different viewpoints $v_a, v_b$, its projected coordinates should satisfy strict algebraic constraints $x_b^{\\top} F_a x_a = 0$. In generative models, violating this constraint is the root cause of the Janus Problem [97], where different viewpoints produce incompatible object geometries. To address this, SyncDreamer [106] constructs an explicit 3D cost volume to enforce alignment, while MVDream [99] utilizes a multi-view self-attention mechanism to internalize hard geometric constraints into attention weights, directly locking the global topological uniqueness of the generated object.\n\nThe above decomposition clarifies the geometric objectives of spatial consistency. However, how to systematically solve these topological constraints within the parameter space of a neural network requires establishing a unified differential equation perspective.\n\n2.3.2 Theoretical Formulation\n\nTo construct a theoretical framework, we formalize the spatial consistency in 3D visual generation as\nsolving a set of coupled Inverse Differential Problems on the spatiotemporal manifold $\\mathcal{M} \\subseteq \\mathbb{R}^3 \\times \\mathbb{R}^+$.\nFrom this perspective, the construction of the full state field $\\Phi(\\boldsymbol{x}, t)$ follows three core physical laws,\nwhich respectively define the world's presentation mode, generation rules, and motion laws.\n\n**Physical Rendering: The RTE.** Both explicit and implicit 3D representations can be physically viewed as discretized solutions to the Radiative Transfer Equation (RTE) [107]. For a ray $r(s) = o + sd$, the variation of its radiance $L$ along the path follows:\n\n$$ \\underbrace{d \\cdot \\nabla L(\\boldsymbol{x}, \\boldsymbol{d})}_{\\text{Transport}} = \\underbrace{-\\sigma(\\boldsymbol{x}) L(\\boldsymbol{x}, \\boldsymbol{d})}_{\\text{Absorption}} + \\underbrace{\\sigma(\\boldsymbol{x}) c(\\boldsymbol{x}, \\boldsymbol{d})}_{\\text{Emission}}, \\quad (4) $$\n\n---\n\nwhere $\\sigma(x)$ represents the Volume Density at position $x$, and $c(x, d)$ denotes the view-dependent Color Emission. The difference in discretization constitutes the divergence in technical routes: NeRF (Implicit Fields) employs volume rendering integration, approximating the solution by dense Riemann summation of Eq. (4) along the ray; while 3DGS (Explicit Primitives) discretizes the continuous field into a set of Lagrangian Gaussian basis functions, transforming the integral into efficient analytical rasterization. The former ensures continuity, while the latter achieves real-time performance.\n\n**Generative Evolution: The SDE.** In the generative prior paradigm, spatial consistency originates from the probability distribution of the pre-trained model. We model the process of recovering from Gaussian white noise $z_T$ to the data manifold $z_0$ as a Stochastic Differential Equation (SDE) [108]:\n\n$$d\\Phi_t = f(\\Phi_t, t)dt + g(t)dw, \\qquad (5)$$\n\nwhere $f(\\cdot)$ is the deterministic drift term governing semantic evolution, $g(t)$ denotes the diffusion coefficient, and $w$ represents the standard Wiener process. Modern generative models aim to learn the reverse process of the above SDE (score matching). When the diffusion term $g(t) = 0$, the SDE degenerates into a deterministic Ordinary Differential Equation (ODE), i.e., Flow Matching. This provides a theoretical basis for understanding how generative models recover “smooth and topologically consistent” geometric structures from disordered noise.\n\n**Motion Law: Lagrangian Transport.** To ensure topological consistency of the spatial structure along the time axis, the motion of material points $x$ must follow Lagrangian Flow:\n\n$$\\frac{dx}{dt} = v(x, t), \\quad \\text{s.t.} \\quad \\frac{D\\Phi}{Dt} = 0 \\quad (\\text{Material Derivative}), \\qquad (6)$$\n\nwhere $v$ represents the velocity field driving the particle motion, and $\\frac{D\\Phi}{Dt}$ denotes the material derivative. This constraint implies that feature $\\Phi$ remains conserved as it moves with the fluid (the material derivative is 0). This directly corresponds to the particle tracking mechanism in the explicit primitive paradigm and serves as the mathematical bridge connecting static geometry and dynamic video.\n\nThe history of spatial consistency evolution is essentially a process where academia shifted from solving the static RTE (NeRF) to inversely solving the generative SDE (Diffusion), and finally integrating Lagrangian dynamic constraints. This iterative process of moving from attempting to fit dynamics on 2D projected manifolds to implicit continuous field integration, and then returning to explicit Lagrangian primitives, is illustrated in Figure 11.\n\n#### 2.3.3 2D Proxy Manifold & Domain Mismatch\n\nBefore explicit 3D representations established their mainstream status, the primary path to addressing spatiotemporal consistency was video prediction based on the Manifold Hypothesis. This paradigm avoided expensive SE(3) spatial modeling and instead attempted to reduce the high-dimensional physical state field $\\Phi$'s evolutionary dynamics operator $\\mathcal{F}_{3D}: SE(3) \\times \\mathbb{R}^3 \\to \\mathbb{R}^3$ into a parameterized mapping $\\mathcal{F}_{\\theta}: \\mathbb{R}^{H \\times W} \\to \\mathbb{R}^{H \\times W}$ on the 2D image manifold $M_{img}$. Although this proxy manifold strategy offered computational complexity advantages, it introduced a fundamental Domain Mismatch.\n\n**Dynamics Fitting Lacking SE(3) Equivariance.** Early works like ConvLSTM [73] and PredRNN [74, 109], while mitigating long-sequence gradient decay through improved recurrent units (e.g., Gradient Highway Unit, GHU), relied on convolution operations $W * I$ that only possess Translation Equivariance and lack the ability to perceive the 3D rotation group SO(3). As stated in [110, 111, 112, 113], attempting to simulate 3D rigid body rotation through non-linear transformations of a 2D pixel grid is essentially approximating high-dimensional topology on a low-dimensional manifold. This misalignment of inductive bias leads to the model's inability to decouple extrinsic camera motion from\n\n---\n\n**Figure 11: Evolution of Spatial Consistency Paradigms.** We trace the trajectory from early 2D Proxy Manifolds, to Implicit Continuous Fields like NeRF, moving towards Explicit Lagrangian Primitives like 3DGS, and finally integrating Generative Diffusion Priors.\n\nintrinsic object deformation, inevitably causing non-physical Non-rigid Distortion or texture stretching in generated videos during large viewpoint transformations.\n\n**Early Attempts and Limitations of Physics-aware Modeling.** To alleviate the blurriness caused by pure statistical fitting and enhance the robustness of temporal extrapolation, academia attempted to endow black-box models with physical interpretability, the core idea being to inject physical conservation laws into the neural network's parameter space. A pioneer in this direction is *Physics-Informed Neural Networks (PINN)* [77], which adds the residuals of Partial Differential Equations (PDEs) as regularization terms to the loss function, forcing the network output to conform to physical constraints like fluid mechanics or wave equations. Subsequently, Deep Lagrangian Networks (DeLaN) [78] and Hamiltonian Neural Networks (HNN) [79] further introduced energy conservation priors, explicitly modeling the system's total energy (Hamiltonian) using Euler-Lagrange equations, thereby achieving precise trajectory prediction for complex dynamic systems in continuous time.\n\nIn the field of video prediction, PhyDNet [75] drew on these ideas by explicitly disentangling the hidden state into a physical dynamics branch $\\mathcal{H}_{phy}$ and a residual texture branch $\\mathcal{H}_{res}$. Unlike the soft constraints of PINNs, PhyDNet directly restricts convolution kernel weights via Moment Matching, making them approximate PDE finite difference operators on a discrete grid:\n\n$$ \\frac{\\partial \\mathcal{H}}{\\partial t} \\approx \\sum_k c_k \\frac{\\partial^k \\mathcal{H}}{\\partial \\mathbf{x}^k} \\Rightarrow \\text{Filter Weights} \\xrightarrow{\\text{Moment}} \\text{Finite Difference Stencils}, \\quad (7) $$\n\nwhere $\\mathcal{H}$ denotes the disentangled hidden state, $\\mathbf{x}$ is the spatial coordinate, and $c_k$ represents the partial differential coefficients.\n\nFurthermore, addressing the limitations of discrete time sampling, Latent ODEs [80] proposed by Rubanova et al. utilize a continuous time ODE Solver to model hidden state evolution, effectively\n\n---\n\nhandling temporal consistency issues under non-uniform sampling.\n\nAlthough these methods and variational inference models like SVG [76] made progress in short-term\nprediction, modeling based on 2D manifolds implies a spatial continuity assumption. Once depth\nmutations caused by Occlusion occur, the optical flow field becomes non-differentiable, and PDE\nconstraints immediately fail. This defect of being unable to model object permanence indicates a\ntheoretical limitation in solving strict 3D consistency on a 2D proxy manifold.\n\n2.3.4 Implicit Continuous Fields\n\nAddressing the theoretical limitations of 2D proxy manifolds in 3D consistency, academia turned to\ndefining state fields directly in 3D Euclidean space. The establishment of this paradigm was built\nupon Mesh-based differentiable rendering works like SoftRas [114] and DIB-R [115], which verified\nthe feasibility of calculating gradients $\\partial I / \\partial \\mathcal{V}$ through a smooth rasterization process. NeRF [81]\nfurther discarded discrete geometry, using MLPs to parameterize the scene as a continuous coordinate\nmapping function $F_{\\Theta}: (x, \\mathbf{d}) \\rightarrow (c, \\sigma)$, and connecting the 3D field with 2D observations through\ndifferentiable Volume Rendering Integral.\n\n(1) **Representation Efficiency & Frequency Fidelity.** The evolution of Neural Radiance Fields is essentially a process of seeking balance between *parameter efficiency* and *signal fidelity*. The challenges in this field have deepened from initial inference acceleration (introducing discrete representations) to maintaining frequency domain anti-aliasing characteristics in discrete space.\n\n(i) *The Shift to Hybrid Representations.* To break the efficiency bottleneck of pure MLP architectures, NVIDIA's Instant-NGP [84] introduced **Multiresolution Hash Grids**, using spatial hashing to map continuous coordinates to a learnable feature table; while in the generative domain, EG3D [116] proposed **Tri-plane** representation, establishing the mainstream paradigm for 3D GANs. These methods (including TensoRF [117]) significantly improved training efficiency and geometric generation capabilities by introducing explicit spatial inductive biases.\n\n(ii) *Aliasing & Signal Processing Correction.* However, the aforementioned discretized representations (as well as point-wise sampling in original NeRF) introduced severe aliasing in high-frequency regions. Mip-NeRF [82] corrected this defect from a signal processing perspective, pointing out that discrete sampling ignoring the sampling volume violates the Nyquist sampling theorem. By introducing Cone Tracing and Integrated Positional Encoding (IPE), Mip-NeRF calculated the feature expectation within a Gaussian volume, revealing the essence of anti-aliasing in its mathematical form:\n\n$$\n\\gamma(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\mathbb{E}_{x \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})}[\\gamma(\\boldsymbol{x})] \\approx \\sin(\\boldsymbol{\\mu}) \\circ \\exp\\left(-\\frac{1}{2} \\text{diag}(\\boldsymbol{\\Sigma})\\right), \\qquad (8)\n$$\n\nwhere $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ denote the mean vector and covariance matrix of the conical frustum, and $\\circ$ represents the element-wise product. This formula reveals a profound physical mechanism: the exponential decay term $\\exp(-\\boldsymbol{\\Sigma})$ essentially acts as an **Adaptive Low-pass Filter**. When the sampling cone radius increases (i.e., variance $\\boldsymbol{\\Sigma}$ increases, corresponding to distant views or low-resolution regions), high-frequency features are exponentially suppressed.\n\nTo transfer this excellent anti-aliasing property to efficient grid representations, Zip-NeRF [118] further\ncombined Multisampling with feature smoothing techniques, resolving the scale uncertainty inherent\nin hash grids. This series of evolutions is mathematically equivalent to the **Uncertainty Principle**\nin Fourier transforms: the wider the spatial localization ($\\boldsymbol{\\Sigma}$ is large), the narrower the frequency\nbandwidth, thereby mechanistically eliminating moiré patterns and high-frequency artifacts, achieving\na unification of efficiency and fidelity.\n\n---\n\n**NeRF's** density field $σ$ suffers from physical ambiguity. When extracting surfaces, the artificially set threshold $τ$ leads to Level Set Ambiguity. To obtain precise geometric surfaces, NeuS [85] and VolSDF [86] converted the representation from a density field to a Signed Distance Field (SDF). By introducing an unbiased Logistic transformation $φ_s(f(x))$ and imposing an Eikonal regularization term:\n\n$$ \\mathcal{L}_{\\text{geo}} = \\mathbb{E}_x[(\\|\\nabla f(\\mathbf{x})\\|_2 - 1)^2], \\quad (9) $$\n\nwhere $f(x)$ is the signed distance function, and the gradient norm constraint $\\|\\nabla f\\|_2 = 1$ ensures physical validity. This constraint forces the gradient norm of the implicit field to be constant at 1, ensuring the zero-level set $S = \\{x|f(x) = 0\\}$ converges to a smooth, closed manifold surface that satisfies physical constraints.\n\nViewing from the perspective of manifold optimization, implicit continuous fields essentially trade Inference Latency for Geometric Completeness [85]. Due to the continuous differentiability of SDF, this paradigm constitutes an ideal basis for high-fidelity inverse rendering. It is not only suitable for reconstructing closed Watertight Manifolds to realize static asset digitization [86, 119], but also effectively avoids geometric holes common in explicit methods through Eikonal regularization-induced smoothing priors under sparse views [87]. However, its mathematical properties also define a theoretical upper bound: the high sampling cost of volume integration $O(N_{\\text{samples}})$ makes it difficult to support high-frame-rate real-time interaction [81], and the smoothing assumption of continuous fields faces expressive bottlenecks when modeling dynamic scenes with drastic topological fractures [120, 89].\n\n### 2.3.5 Explicit Lagrangian Primitives\n\nAlthough implicit continuous fields established theoretical completeness for multi-view consistency, their sampling mechanism relying on volume integration constitutes a computational bottleneck for real-time simulation. The 3D Gaussian Splatting (3DGS) proposal [89] CC2 marks the return of the representation form of the state field $\\Phi$Q3 from an implicit field to explicit particles (as shown in Figure 12CR1(c)). This paradigm discretizes the scene into a set of anisotropic Gaussian primitives $\\Phi = \\{G_i(\\mu, \\Sigma, \\alpha, SH)\\}_{i=1}^M$ and reconstructs the projection operator $\\mathcal{P}$ as Rasterization.\n\n**Mechanisms of Static Representation.** Unlike the ray marching of NeRF [81], 3DGS [89] utilizes the GPU sorting pipeline for acceleration, containing three key characteristics:\n\n*   **Rasterization Pipeline.** The algorithm involves two key steps: first is Frustum Culling and Projection, projecting 3D Gaussian into a 2D screen space covariance matrix $\\Sigma^{2D} = J_W \\Sigma^{3D} W^T j_T^T$; second is tiled radix sort, which is the computational bottleneck with complexity $O(N \\cdot k)$. By leveraging a tile-based parallel rendering strategy, the method restricts computation to overlapping Gaussians and requires only $\\alpha$-blending on during rasterization, avoiding invalid sampling of empty space.\n\n*   **Integral Duality.** NeRF adopts a Backward Pull, prone to gradient masking ($\\partial C / \\partial \\sigma_{far} \\approx 0$). In contrast, 3DGS adopts a Forward Push; explicit sparsity allows error gradients $\\frac{\\partial L}{\\partial \\mu}$ to bypass the MLP and backpropagate directly and sparsely to geometric parameters. This explicit gradient flow is the mathematical foundation for the efficient convergence of 3DGS.\n\n*   **Adaptive Density Control.** This approach can be viewed as a variant of AMR (Adaptive Mesh Refinement). The core idea is: if the gradient is too large and variance is small ($\\|\\nabla \\mathcal{L}\\| > \\tau, \\|\\Sigma\\| < \\epsilon$), it is judged as underfitting and the Gaussian is cloned; if the gradient is large and variance is large, it is judged as overfitting and the Gaussian is split. Through this mechanism, the method dynamically adjusts the density of Lagrangian particles in response to the underlying optimization landscape.\n\n**Evolution towards 4D Dynamics.** Addressing 4D spatiotemporal modeling, the explicit primitive paradigm has developed three main evolutionary paths based on how the time dimension $t$ is handled:\n\n---\n\nFigure 12: Key Mechanisms for Advanced Spacetime Modeling. A taxonomy of core techniques underpinning modern models: (a) Full Spacetime Attention enables dense long-range dependencies; (b) Causal Masking ensures temporal causality; (c) 3D Gaussian Splatting offers explicit, differentiable 3D structure; (d) Object-Centric Slots decompose complex scenes into distinct entities.\n\n(i) *Lagrangian Particle Tracking.* As in PhysGaussian [93], it assumes Gaussian primitives possess material point properties, solving the equation of motion $\\mu(t) = \\mu_0 + \\int v(\\tau)d\\tau$ by introducing continuum mechanics equations ($\\rho\\ddot{x} = \\nabla\\cdot\\sigma + g$). By embedding physical constraints into the optimization process, the method enables joint learning of visual appearance and physical behavior.\n\n(ii) *Eulerian Tensor Decomposition.* As in 4D-GS [121], the 4D spatiotemporal field is modeled as a high-dimensional tensor $\\mathcal{T}$, using CP or Tucker decomposition to reduce dimensionality:\n\n$$\\mathcal{T}(x, y, z, t) \\approx \\sum_{r=1}^{R} \\mathbf{u}_r(x) \\circ \\mathbf{v}_r(y) \\circ \\mathbf{w}_r(z) \\circ \\mathbf{h}_r(t), \\quad (10)$$\n\nwhere $\\circ$ denotes the outer product, $R$ is the tensor rank, and $\\mathbf{u}_r, \\mathbf{v}_r, \\mathbf{w}_r, \\mathbf{h}_r$ represent the factor vectors along each dimension. This form optimizes storage complexity from $O(N^4)$ to $O(N^2)$, effectively supporting dynamic changes in topological structure.\n\n(iii) *Canonical Deformation.* As in Deformable-GS [95], it adopts a static base with transient offsets for- mulation, predicting coordinate offsets $\\Delta\\mu$ via MLP, leveraging the spectral bias of MLPs to effectively capture high-frequency motion fields.\n\nThe explicit primitive paradigm shows significant advantages in balancing high frame rate rendering and high-resolution reconstruction. However, its discrete nature introduces topological adaptability limitations, making it difficult to naturally handle fractures and fusions in fluid dynamics like implicit fields [122], indicating the need to introduce higher-order generative dynamics models.\n\n### 2.3.6 Generative Statistical Priors\n\nIn open-world generation tasks, observation conditions are extremely sparse, causing the problem to degenerate into an ill-posed one. In this phase, works utilize video diffusion models as implicit world\n\n---\n\nmodel priors, establishing an algorithm-data synergistic framework.\n\n(1) **Algorithmic & Geometric Constraints.** To elevate 2D priors to 3D consistency, academia has reconstructed optimization objectives and architectural designs:\n\n(i) *Score Distillation Sampling (SDS) & Variational Correction.* Unlike photometric loss, SDS [97] obtains gradients by calculating the score function of a pre-trained diffusion model. Addressing the over-smoothing problem of SDS, VSD (Variational Score Distillation) [123] introduces a variational distribution, minimizing the KL divergence between the generated distribution and the prior distribution, thereby recovering high-frequency texture details.\n\n(ii) *Multi-View Geometric Attention.* Pure 2D priors are difficult to guarantee multi-head consistency. Works like MVDream [99] modify the U-Net architecture, upgrading spatial self-attention to 3D correspondence attention. This design forces the model to perform feature alignment via camera parameters $(R, T)$ when generating different views, achieving soft geometric consistency.\n\n(2) **Scaled Data Foundation.** To break the 3D data bottleneck, academia has adopted a Synthetic-Real-Generative hybrid construction strategy for large-scale dataset construction:\n\n(i) *Aggregation.* Objaverse-XL [101] integrated tens of millions of 3D assets collected from the internet, fundamentally alleviating the scarcity of large-scale 3D data. G-Objaverse [124] provided high-quality RGB-D-Normal triplets through a physical rendering pipeline, becoming the standard source for training Large General Reconstruction Models (LGM).\n\n(ii) *Real-world Perception.* MVImgNet [125] and Co3D-v2 [126] provide millions of object-centric video sequences captured in real-world environments. While dense geometric ground truth is largely unavailable, these datasets play a crucial role in reducing the domain discrepancy between synthetic and real data, particularly in appearance and texture distributions.\n\n(iii) *Inverse Generative Engine.* See3D [104] advances an automated data generation paradigm by coupling generative video models with geometric reconstruction. Specifically, large-scale pseudo-3D videos are synthesized using video diffusion models such as SV3D [127], followed by geometric inference via Dust3R [103] and rapid reconstruction through LGM, constructing a closed-loop data production engine to achieve exponential asset expansion.\n\nThe development of spatial consistency modeling exhibits a clear iterative trajectory. Early methods relied on 2D proxy fitting, which gradually evolved into 3D implicit representations to improve geometric coherence. Subsequently, explicit formulations such as 3D Gaussian Splatting reintroduced computational efficiency and rendering scalability. Current trends indicate a convergence toward hybrid architectures that combine explicit geometric primitives with implicit diffusion-based priors, leveraging the complementary strengths of both representations [124, 127].\n\nLooking ahead, the research focus in this field is shifting from pure visual reconstruction to deep physical interaction modeling. On one hand, Neuro-symbolic Grounding will become the key to connecting semantic space and geometric space. Future models aim to establish differentiable mappings between LLM symbolic logic and numerical parameters, as shown in works like Eureka [128], to realize an endogenous understanding of object materials and force mechanisms, thus transcending pixel statistics-based imitation. On the other hand, the scope of spatial consistency is expanding to Action-Consistency. As World Models evolve towards interactive environments [129], Reinforcement Learning (RL) will be introduced into the generative loop, ensuring that the scene follows physical causality when responding to actions $\\pi(a_t|s_t)$. To support this capability, the architectural level is expected to break the Cascaded Generation pipeline and shift towards End-to-End Native 4D Streaming, i.e., performing real-time streaming inference directly with compressed 4D Tokens [130].\n\n---\n\nFigure 13: Evolution of Temporal Consistency Paradigms: From Latent Inflation and Discrete Sequence Modeling to Native Spatiotemporal DiT and Causal World Simulators.\n\n2.4 Temporal Consistency\n\nThrough the modeling of spatial consistency (§2.3), we have successfully constructed a geometrically\ncomplete static world. However, the core value of a World Model lies not in archiving the state of a\nmoment, but in rehearsing future trajectories. If spatial consistency is regarded as the Static Geometric\nBasis of the world model [163], temporal consistency constitutes the key element establishing its\nphysical evolutionary Temporal Dynamics [1]. Mathematically, this process is equivalent to solving a\nMulti-objective Optimization Problem constrained by both physical constraints L_phy and causal logic\nL_causal within a high-dimensional manifold space [164], as illustrated in Figure 14.\n\nFigure 14: Temporal Consistency and Identity Preservation. An illustration of the temporal attention mechanism ensuring identical subject features across consecutive frames ($t_0 \\to t_n$). The generation process is governed by two key constraints: *Physical Constraints* (*L<sub>phy</subollen*) enforce smoothness in motion trajectories to prevent flickering artifacts, while *Causal Constraints* (*L<sub>causal</sub>*) ensure logical progression of events (e.g., object permanence) throughout the timeline.\n\n---\n\n### 2.4.1 From Frequency Stability to Physical Compliance\n\nTo objectively measure the evolutionary trajectory of temporal consistency technologies, evaluation metrics must transcend traditional perceptual dimensions. For a long time, academia relied on FVD (Fréchet Video Distance) [165] to assess video quality, but empirical studies indicate that FVD primarily characterizes the similarity of spatial feature distributions and has limitations in detecting temporal high-frequency Flickering and non-physical deformations.\n\nIt must be pointed out that frequency stability in temporal consistency does not exist in isolation; it must be built upon the semantic foundation of modality alignment (§2.2) and the topological constraints of spatial geometry (§2.3). For instance, frontier models like Veo 3 [152] effectively suppress high-frequency artifacts and achieve physically compliant causal reasoning precisely by integrating MM-DiT (modal consistency) and 3DGS (spatial consistency).\n\nTo fill this gap, Video Consistency Distance (VCD) [166] was designed as a Reward-based Fine-tuning Objective. As shown in Figure 15, VCD measures the feature difference between the generated video $\\hat{V}$ and natural video in the temporal frequency spectrum:\n\n$$\\mathcal{L}_{\\text{VCD}}(\\hat{V}) = \\mathbb{E}_t \\left[ \\|\\mathcal{F}_t(\\phi(\\hat{\\theta}_t)) - \\mathcal{F}_t(\\phi(\\hat{\\theta}_{t-1}))\\|^2_{\\text{High-Pass}} \\right], \\quad (11)$$\n\nwhere $\\phi(\\cdot)$ denotes the feature extractor (e.g., CLIP Image Encoder [10]), and $\\mathcal{F}_t$ represents the Short-Time Fourier Transform (STFT) along the time axis. The physical meaning of this formula is that motion features in the real world should possess continuity in the frequency domain, whereas temporal inconsistencies in generative models (such as texture flickering) will manifest as significant energy fluctuations in the high-frequency band.\n\nFigure 15: Video Consistency Analysis: Spatial vs. Frequency View. Traditional distribution-based metrics such as FVD primarily assess spatial perceptual quality and smooth motion in feature space, often overlooking high-frequency temporal flickering. In contrast, VCD explicitly models temporal consistency by analyzing the Fourier spectrum of feature embeddings, enabling the detection of subtle high-frequency noise and flicker artifacts invisible to spatial statistics.\n\n**From Perception to Physical Reasoning** Traditional evaluations focus on visual quality, while new standards have expanded to physical causal dimensions. As shown in Table 3, firstly, addressing temporal jitter, the Generative Prior Paradigm (World Model Priors) significantly reduces high-frequency potentially delicate visual factors [32].\n\n---\n\n**Table 3:** Empirical Evolution of Cross-Generational Models. Data synthesized from VBench (Temporal), Physics-IQ (Physics), and Veo 3 Technical Report (Reasoning) benchmarks.\n\n<table>\n  <thead>\n    <tr>\n      <th rowspan=\"2\">Generation Paradigm</th>\n      <th rowspan=\"2\">Rep. Model</th>\n      <th>Temporal Consistency ↑</th>\n      <th>Physics Compliance ↑</th>\n      <th>Causal Reasoning ↑</th>\n      <th>Freq. Fidelity (VCD) ↓</th>\n    </tr>\n    <tr>\n      <th>(VBench Norm.)</th>\n      <th>(Physics-IQ)</th>\n      <th>(Task Success Rate)</th>\n      <th>(Reward Penalty)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Temporal Inflation</td>\n      <td>Move your diff goes here</td>\n      <td>0.68</td>\n      <td>0.42</td>\n      <td>N/A (&lt; 10%)</td>\n      <td>High (&gt; 1.2)</td>\n    </tr>\n    <tr>\n      <td>Discrete AR</td>\n      <td>Video Poetry</td>\n      <td>0.79</td>\n      <td>0.55</td>\n      <td>Low (~ 25%)</td>\n      <td>Medium (~ 0.9)</td>\n    </tr>\n    <tr>\n      <td>Native DiT</td>\n      <td>Hunyuan Video</td>\n      <td>0.88</td>\n      <td>0.78</td>\n      <td>Medium (~ 45%)</td>\n      <td>Low (~ 0.6)</td>\n    </tr>\n    <tr>\n      <td>World Model Priors</td>\n      <td>Google Veo 3</td>\n      <td><strong>0.95*</strong></td>\n      <td><strong>0.86*</strong></td>\n      <td><strong>High (&gt; 70%)<sup>†</sup></strong></td>\n      <td><strong>Minimal (&lt; 0.3)</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n\\*Note: World Model scores are extrapolated based on relative improvements reported in [152] compared to Native DiT baselines.\n†Refers to success rates on complex physical interaction tasks (e.g., object manipulation) as demonstrated in [167].\n\nartifacts (VCD < 0.3) by introducing frequency domain reward fine-tuning. Secondly, to assess adherence to physical laws, Physics-IQ [161] is used to quantify model compliance in rigid body dynamics and fluid simulation. Finally, causal reasoning has become a core evaluation dimension for models like Veo 3 [152]. Veo 3 demonstrates emergent capabilities in zero-shot physical interaction tasks (such as predicting domino toppling), with a task success rate exceeding 70%, marking the evolution of video generation technology from pure visual simulation to dynamic systems capable of logical deduction.\n\n### 2.4.2 Latent Temporal Inflation\n\nIn the early stages when large-scale 4D data was not yet widespread, academia dedicated efforts to lowering the training threshold for video generation. Works represented by Tune-A-Video [134] and AnimateDiff [135] established the Temporal Inflation paradigm of Spatial Freeze, Temporal Insertion.\n\n**Independence Assumption & ELBO Relaxation.** The core strategy of this paradigm is to extend pre-trained 2D Text-to-Image (T2I) models into video generators, specifically by freezing the spatial convolution layers of the 2D U-Net and inserting learnable 1D temporal attention modules only between layers. Viewing from a probabilistic graph perspective, this is essentially simplifying the joint distribution of video generation $p(x_{1:T})$ into a first-order Markov chain. Theoretical derivation shows that this relaxation of the Evidence Lower Bound (ELBO) ignores high-order dependencies of $p(x_t|x_{<t-1})$, leading to a significant increase in the KL divergence term over long sequences. In practical applications (such as VideoCrafter1 [136]), this mathematical relaxation manifests as significant Semantic Drift: as the number of generated frames increases ($T > 16$), the identity features of the initial frame are gradually diluted by independent noise injection.\n\n**Spatial Anchoring & Zero-shot Injection.** To suppress semantic drift, early works explored training-free consistency enhancement paths. Text2Video-Zero [131] and FateZero [132] adopted a Zero-shot Attention Injection mechanism, forcing subsequent frames to reuse the Key/Value feature matrices of the first frame. Meanwhile, inspired by ControlNet [133], some works introduced explicit geometric conditions (such as Depth/Pose) as spatial anchors. Empirical data shows that although these methods perform well in static backgrounds, when object motion amplitude exceeds 20% of the screen width, forced feature injection leads to obvious Smearing Artifacts, revealing the limitations of the inflation paradigm in handling complex dynamics.\n\n**Frequency Filtering & Dynamic Correction.** Besides temporal drift, existing temporal inflation models typically face the problem of Frequency Blindness. Since the temporal attention mechanism operates independently in the $(B \\cdot HW)$ dimension, it often exhibits a lack of inductive bias when capturing high-frequency texture changes. Fourier spectral analysis reveals that generated videos exhibit significant energy loss in the high-frequency band ($> 15Hz$), visually manifesting as non-physical texture flickering. Addressing the capture of long-range dependencies and high-frequency information, frequency domain learning offers a novel perspective. Global Filter Networks (GFN) [139]\n\n---\n\nproposed using 2D Discrete Fourier Transform (2D DFT) instead of self-attention mechanisms, achiev-\ning long-range spatiotemporal interaction capture with O(N log N) complexity by performing global\nfiltering operations in the frequency domain. Building on this, Adaptive Fourier Neural Operators\n(AFNO) [140] further optimized inter-channel information aggregation, proving that frequency domain\nToken Mixers can effectively overcome spatial blindness and precisely retain high-frequency details.\nFurthermore, addressing noise interference in sequence modeling, BERT4Rec [168] and Denoising\nSASRec [169] introduced uncertainty quantification mechanisms, achieving dynamic suppression\nof irrelevant perturbations by zeroing out gradients of high-noise samples during backpropagation\n(gradient pruning). In the video generation domain, FastInit [141] drew on these denoising ideas,\nproposing a learning-based noise initialization strategy. This method discards traditional independent\nGaussian sampling and instead trains a lightweight inversion network to directly predict the optimal\ninitial noise for the current frame based on spatiotemporal features of preceding frames, significantly\nenhancing generation coherence while suppressing latent space temporal high-frequency jitter.\n\n**_The Theoretical Boundary of Inflation._** Although methods like FastInit [141] alleviate frequency\ndomain flickering, the temporal inflation paradigm is perpetually limited by its 2D topological anchor.\nSince the core spatial convolution layers are frozen, the model is essentially performing minute elastic\ndeformations on static images rather than generating true temporal dynamics. Empirical research [170]\nindicates that when facing large viewpoint transformations (such as an object rotating 180 degrees)\nor the emergence of new content, this class of models often produces severe texture stretching. This\nover-reliance on pre-trained 2D priors condemns it to the role of a transitional solution. To capture true\nphysical world dynamics, academia has turned to exploring native video architectures trained from\nscratch, which is the driving force behind the development of the discrete autoregressive paradigm.\n\n**2.4.3 Discrete Autoregressive Modeling**\n\nTo break the theoretical bottleneck of long-sequence modeling, VideoPoet [143], CogVideo [171], and W.A.L.T [144] drew on the scaling law of LLMs, establishing the two-stage autoregressive generation paradigm. By expanding the context window, this paradigm reconstructs video generation as long-range causal prediction of discrete Tokens.\n\n**Causal 3D Tokenizer & Data Compression.** The cornerstone of the discrete autoregressive paradigm\nis an efficient 3D VQ-VAE. Unlike image Tokenizers, video compression must strictly adhere to\ntemporal causality. MagViT-v2 [145] innovatively introduced asymmetric Temporal Padding and\nCausal 3D Convolution, strictly limiting the receptive field of convolution kernels to the current frame\nt and preceding moments, ensuring that future information does not leak during the compression\nprocess. Addressing reconstruction blurriness in low-motion scenes, VToice-Plus [172] further\nintroduced Object-Centric representation, significantly improving texture fidelity of static backgrounds\nby separating foreground and background codebooks.\n\n**Memory Decay in Long Sequences.** With the release of models like NVIDIA Cosmos [146], the AR\nparadigm has regained attention due to its superior data scaling capabilities. However, Error Accum-\nulation remains the core challenge of this paradigm. According to sequence modeling theory [54],\nthe distribution shift between Teacher Forcing during training and autoregressive generation during\ninference (Exposure Bias) causes minute inter-frame prediction errors to amplify exponentially with\ntime step t. To suppress this sequence variance, VAPR [173] proposed the Next-Scale Prediction mecha-\nnism, reconstructing the autoregressive process from pixel scanning to coarse-to-fine scale recursion,\nmathematically reducing inference steps from linear O(N) to logarithmic O(log N). Furthermore,\nFramePack [148] introduced a frame context packing mechanism and bidirectional anti-drift sampling,\ncombined with the PFP (Pretraining Frame Preservation) [174] objective, significantly improving\nreconstruction fidelity under long time sequences.\n\n---\n\n**Return to Continuous Latent Space.** Despite continuous architectural optimization, the non-differentiability of the discretization operation $z_q = \\operatorname{arg min} \\|z_e - e_k\\|$ constitutes an inherent optimization difficulty for this paradigm. Training typically relies on the Straight-Through Estimator (STE) [175] for approximation, but in high-dimensional video space ($D > 4096$), the gradient variance caused by STE ($\\sigma^2 > 10^3$) easily triggers codebook collapse [49]. This discretization gap limits the precision of AR models in generating minute textures and sub-pixel motion. Precisely this limitation has driven the technical focus to shift towards Continuous Latent Space, utilizing Diffusion Transformers to directly model continuous probability density on the manifold.\n\n**Hybrid Transition: Fusing AR and Diffusion.** Between pure AR and DiT, academia has explored fusion paths of the two, aiming to combine the long-range causality of AR with the high-fidelity decoding capability of Diffusion. First, at the inference level, Diffusion Forcing [149] proposed a non-rigid sequence modeling scheme, modeling each time step as an independent diffusion process, supporting rollback and branch exploration during inference, breaking the traditional AR restriction of no return. Second, at the architectural level, Show-o [176] proposed the Unified Omni-Model paradigm. This method is not a simple stacking of modules, but achieves isomorphic modeling of discrete tokens (for semantic understanding) and continuous tokens (for visual generation) within a single set of weights. Through a mixed masking mechanism, Show-o achieves bidirectional interoperability of understanding and generation in physical weights.\n\n### 2.4.4 Unified Spatiotemporal Modeling via DiT\n\nCompared to the spatiotemporal fragmentation caused by the temporal inflation paradigm and the quantization loss brought by the discrete AR paradigm, the new generation of paradigms represented by Sora [2] and HunyuanVideo [150] established the current benchmark for temporal consistency in video generation by thoroughly returning to continuous latent space and adopting the Diffusion Transformer (DiT) architecture. This evolutionary path from Spatiotemporal Decoupling to Full Spatiotemporal Isomorphism is shown in Figure 16.\n\n**Native Spatiotemporal Architecture.** Native 3D DiT treats video as a sequence of 3D Patches $N = T \\times H \\times W$, its core advantage being the capture of non-local physical interactions through a global receptive field.\n\n**(i) Full Sequence Joint Attention.** By introducing 3D-RoPE to calculate joint attention Attn = $Softmax(QK^T/\\sqrt{d} + M)V$, the model can calculate joint attention across the full sequence. Empirical studies (such as Physics-IQ [161]) indicate that decomposition architectures which sever spatiotemporal connections are mathematically difficult to approximate the convective terms and long-range correlations in Navier-Stokes equations. Only the global spatiotemporal receptive field provided by full attention mechanisms can capture such non-local physical interactions.\n\n**(ii) Manifold Diffeomorphism.** On this basis, the generation process based on Flow Matching [55] corresponds mathematically to a diffeomorphism on the manifold, enabling the model to smoothly recover minute texture details from Gaussian noise, eliminating the edge flickering caused by discretization.\n\n**Computational Evolution: Linearization & Inference Acceleration.** Although DiT established the image quality benchmark, the quadratic complexity of Transformers ($O(N^2)$) causes VRAM usage to become a physical obstacle in moving from short clips to long videos.\n\n**(i) Linearization & Caching.** On the architecture side, Video-TTT [130] introduced the Test-Time Training paradigm, compressing historical context into neural network weights, achieving memory retention for long videos while maintaining $O(N)$ linear complexity. Complementary to this, Pyramid Flow [153] utilized the spatiotemporal redundancy of video, proposing a pyramid flow matching mechanism, reducing the computational cost of high quality video generation by 5-10 times through a hierarchical\n\n---\n\nFigure 16: Evolution of Video Generation Paradigms. The technical path advances from Temporal\nInflation (prone to drift) and Discrete AR (quantization loss) to the current Native DiT. This paradigm\nachieves full spatiotemporal isomorphism, serving as the foundation for world models.\n\ndecoupling strategy. On the inference side, TeaCache [154] exploited the extremely high similarity\nof feature outputs in adjacent time steps in diffusion models (Pearson correlation coefficient > 0.98),\nintroducing a training-free dynamic caching mechanism to achieve 2-3 times end-to-end acceleration\nwith zero image quality loss.\n\n**Convergence & Divergence in Industry.** The industry has not simply piled up parameters but has\ndemonstrated three distinct evolutionary routes:\n\n*(i) *Standardization vs. Heterogeneity.** Works represented by Meta Movie Gen [177] established the standardized paradigm of DiT + Flow Matching, where the proposed temporally causal 3D VAE solved the temporal slice flickering problem in long videos. In contrast, Google DeepMind persisted with the Space-Time U-Net architecture in Lumiere and Veo [151, 152], avoiding the temporal inconsistency caused by cascaded super-resolution through the full spatiotemporal attention mechanism, defining the upper limit of high-fidelity simulation quality.\n\n*(ii) *Ecosystem & Controllability.** Application-layer models like Runway Gen-3 [6] and ByteDance PixelDance [178] focus on fine-grained interaction, achieving complex instruction following through multimodal director modes and trajectory-level control. Meanwhile, open-source foundations like CogVideoX [179] and HunyuanVideo [150] lowered the fine-tuning threshold, directly promoting the development of the video fine-tuning ecosystem in the HuggingFace community.\n\n## 2.4.5 Logical Consistency and Causal Reasoning\n\nAlthough DiT-based generative models have solved visual continuity, they still face challenges when\ndealing with long-range physical logic (such as causal irreversibility). To bridge this gap, academia\nis shifting from a pure fitting paradigm to a cognitive reasoning paradigm, mainly manifested in\nthe exploration of two complementary directions: image-text interleaving reasoning in multimodal\nperception models and temporal chain reasoning in generative video models.\n\n---\n\n**Think-with-Image in Multimodal Perception.** As the cognitive front-end of world models, LMMs\nare attempting to enhance logical capabilities by introducing the visual modality as Intermediate\nReasoning Steps, rather than relying solely on text CoT. Works represented by Mini-O3 [158] and\nVisCoT [156] assist logical jumps by generating or retrieving images during the inference process. RE-\nCAP [180] further formalized this flow, proposing a recursive Retrieve-Generate-Verify loop, utilizing\nvisual information to compensate for text's deficiencies in spatial relation reasoning. UV-CoT [157]\nexplored image-text thought alignment under unsupervised conditions. Although these works mainly\nfocus on the perception and understanding side, their image-assisted thinking mechanism provides\nvaluable architectural insights for generative models tasked with complex spatiotemporal logic.\n\n**Chain-of-Frame & Temporal Causality.** On the generation side, the core of temporal consistency\nhas ascended from visual fluency to event causality. The model must understand the sequence of\noccurrence of physical events, not just pixel interpolation. Video-CoT [159] and Video Espresso [142]\nintroduce the Chain-of-Frame paradigm, which decomposes video generation into keyframe planning\nand intermediate frame synthesis. In contrast to pixel-level autoregressive approaches, this framework\nexplicitly deduces future key states in the latent space, forcing the model to determine *causal nodes*\nfirst, then generate the *visual process*. Think Sound [160] further extended this causality to the auditory\nmodality, constraining the physical evolution of video via audio cues. By aligning the underlying\ncausal graph structures across modalities, this approach enforces logical self-consistency throughout\nthe full spatiotemporal span, mitigating the logical degradation that commonly emerges in long videos.\n\n2.5 Outlook of the Consistencies\n\nThrough the evolution of specialized models, three distinct computational engines have effectively\nemerged. *Modal Consistency* has addressed semantic translation across modalities; *Spatial Consistency*\nhas progressed from coarse 2D approximations to explicit 3D primitives; and *Temporal Consistency* has\nadvanced from simple frame interpolation toward causal world simulation.\n\nYet treating these capabilities as independent optimization objectives introduces a fundamental bot-\ntleneck. A collection of highly specialized modules, regardless of individual sophistication, cannot\nconstitute a coherent world simulator in the absence of a shared cognitive substrate. The central\nchallenge therefore shifts from refining isolated components to achieving architectural unification.\nThe future of world models lie in reaching a equilibrium in which semantic understanding, geometric\nstructure, and causal reasoning co-emerge within a single parameter space. This requirement motivates\nthe paradigm shift examined next: the emergence of the UMMs.\n\n# 3 Initial Integration of Multiple Consistencies\n\n3.1 The Rise of Large Multimodal Models\n\nIn previous chapters, *Modal, Spatial, and Temporal Consistency* were treated as independent technical\ndimensions. However, the construction of a general world model ultimately hinges not on the isolated\nadvancement of these capabilities, but on their coherent integration into a unified cognitive system.\nAddressing this challenge requires moving beyond modular solutions toward architectures that can\njointly reason across modalities, space, and time. The rise of Large Multimodal Models (LMMs),\nrepresented by LLaVA [23] and GPT-4V [181], marks a decisive paradigm shift from single-task\nspecialists toward general cognitive entities.\n\n3.1.1 LLM as a Core Cognitive Base\n\nThe core design philosophy of modern LMMs [181, 7, 184] is to treat the pre-trained LLM [181, 185] as\na universal reasoning engine [186, 187]. Its essence lies in mapping heterogeneous modality data into\n\n---\n\nthe LLM's Word Embedding Space [16, 188]. This process is not a simple dimension transformation but is achieved through specific translator mechanisms (e.g., visual connectors or adapters) [14, 23, 189] to realize semantic alignment and conversion across modalities [10].\n\n**nings** & Representation Bridging. In the specific implementation path, the model first utilizes a Visual Encoder (such as CLIP-ViT [10] or SigLIP [190]) to extract high-dimensional feature maps $\\mathcal{F} \\in \\mathbb{R}^{H \\times W \\times C}$. To enable the LLM to process these non-text signals, LLaVA [23] and its subsequent improvements [18, 188] employ an MLP or Linear Projection Layer $W_{\\phi}$ to directly project image patch features into a set of Visual Tokens $\\mathcal{V} = \\{v_1, v_2, \\dots, v_n\\}$ (where $v_i \\in \\mathbb{R}^d$) that are\ndimensionally aligned with the text tokens. These tokens are then concatenated with text embeddings as soft prompts to form a hybrid input sequence:\n\n$$ \\boldsymbol{X}_{\\text{input}} = \\left[ e_{\\text{text},1}^{(1)}, \\dots, e_{\\text{text},1}^{(m)}, v_1, \\dots, v_n \\right], \\quad (12) $$\n\nwhere $\\boldsymbol{X}_{\\text{input}}$ represents the aligned multimodal sequence, $e_{\\text{text}}$ denotes the text embeddings, and $\\boldsymbol{v} \\in \\mathbb{R}^d$ is the visual token. From this perspective, the physical significance of alignment is to enable the LLM's self-attention mechanism to compute the association entropy between visual tokens in the same manner as it processes text tokens.\n\n**(2) From Rigid Projection to Perceiver Bottleneck.** To address the issue of sequence length redundancy potentially caused by direct projection, BLIP-2 [16] and Flamingo [14]—as representative architectures of Q-Former and Perceiver Resampler methods—utilize a fixed number of Learned Queries as intermediaries to filter out redundant information from massive Pixel Features.\n\nThis mechanism is mathematically equivalent to a form of semantic pooling: it forces the model to compress thousands of Spatial Patches into dozens of tokens with highly abstract semantics. This not only resolves the problem of computational overhead but also theoretically satisfies the Information Bottleneck hypothesis [191]; by constraining the capacity of $I(Z; X_{vis})$, the model is forced to retain only those features conducive to Language Reasoning during the alignment process. Furthermore, experiments from DeepSeek-VL [4] and InternVL [25] demonstrate that this alignment process can induce the formation of a cross-modal physical manifold within the LLM during the pre-alignment stage, allowing the model to maintain fundamental logical consistency even in unseen scenarios.\n\n3.1.2 Cognitive Evolution as a Multimodal\n\nThe emergence of LMMs transcends the traditional end-to-end mapping paradigm [192, 193], en-\ndowing systems with resource scheduling and logic coordination capabilities akin to a multimodal\noperating system. Within this architecture, the LLM no longer functions merely as a feature processor\nbut serves as the Kernel [194, 195], responsible for managing complex instruction flows and invoking\nheterogeneous Specialized Modules on demand [196, 59, 197].\n\n(1) **Hierarchical Task Planning & Programmatic Instruction.** To address semantic drift in long-horizon tasks, LMMs demonstrate a capability for recursive decomposition, breaking down high-level ambiguous instructions into atomic sub-tasks. Distinct from earlier static mapping, VisProg [198] and ViperGPT [197] proposed the visual programmatic reasoning paradigm, which parses visual queries into executable python code flows, achieving logical self-consistency by combining low-level visual operators. The essence of this mechanism—transforming physical instructions into logical programs—is the utilization of the LLM's in-context learning to project open-domain problems onto a constrained operator space. Furthermore, PaLM-E [199] and Voyager [200] have demonstrated that by incorporating real-time feedback from multimodal perception, LMMs can perform hierarchical search within a latent action space, maintaining long-term consistency in dynamic environments.\n\n---\n\n> Figure 17: Tool-use & Closed-loop Verification. Diagram illustrating the ReAct paradigm, where an AI agent cyclically calls external detection and correction tools to refine generation outputs.\n\n**(2) Tool-use & Closed-loop Verification.** To rectify physical hallucination during the generation\nprocess, LMMs have evolved a closed-loop refinement mechanism based on test-time compute. Frame-\nworks represented by Visual ChatGPT [195] and HuggingGPT [201] utilize the ReAct (Reasoning and\nActing) paradigm [202], as illustrated in **Figure 17**. This allows the model to actively suspend the\ngeneration path to invoke external expert models (e.g., calling a detector to verify spatial relations or a\ndiffusion model to redraw irrational textures). Architectures like Chameleon [59] and Auto-GPT [203]\nfurther introduce a feedback evaluation stage: by calculating the mutual information or geometric\nconstraint deviation $\\Delta_\\phi$ between the generated intermediate state and the original instruction, the\nmodel can execute gradient-guided iterative refinement.\n\n## 3.2 Integration of Modal and Spatial Consistency\n\nThe fusion of modality and spatial consistency constitutes a core bridge toward physical world sim-\nulators [204, 205]. This profound cross-domain synergy aims to resolve the persistent issue of rich\nsemantics but collapsed geometry in traditional generative models, with its core utility manifesting in\ntwo dimensions. In terms of semantic-spatial alignment, it empowers models with the capability for\nprecise responses to complex spatial instructions (such as occlusion, surrounding, and perspective\nstacking) [206], achieving a qualitative leap in controllability from text describes texture to language\ndefines layout [207], as shown in Figure 18. In terms of geometric-physical grounding, it forces gener-\nated content to adhere to the geometric laws of the objective world, effectively eliminating structural\nnon-rigid deformation and spatial misalignment hallucinations under multi-view conditions [208].\nThis integration ensures that AI is no longer confined to the statistical fitting of 2D pixels but possesses\nthe capacity to infer spatiotemporal dynamics within a 3D manifold [209, 151].\n\nIn current research, the deep integration of modality and spatial consistency presents four parallel\ntechnical paths, as illustrated in Figure 19, exploring unique paradigms of implicit emergence, explicit\nsynergy, structured isomorphism, and reinforcement learning. Pixel space manipulation focuses on\nleveraging the scale effects of large-scale multimodal corpora to internalize geometric transformations\n\n---\n\nFigure 18: Modal + Spatial Consistency: Language Controls Precise Spatial Relations. The model combines modality and spatial consistency, using language instructions to precisely control the spatial relations between the subject (Doge) and objects (e.g., Sit ON the box, Hide BEHIND the cushion).\n\nas implicit semantic mappings, achieving intuitive instruction as space control within universal generation [210, 211, 212]. In parallel, view space mapping introduces camera poses and depth maps as explicit geometric conditions, allowing semantic and geometric flows to co-exist and synergize on a 2D plane through cross-attention mechanisms, effectively balancing generative flexibility with perspective accuracy [208, 213, 214]. Meanwhile, volume space representation adopts the world coordinate system as its foundation, anchoring semantic features directly to neural volume fields or 3D Gaussian primitives, making spatial consistency an intrinsic physical attribute of the representation [119, 230]. Finally, reinforcement learning addresses the bag-of-words deficiencies in compositional instructions by introducing the System-2 Reasoning paradigm. Through region-temporal decoupling and inference-side scaling mechanisms, it elevates the generation process from pure statistical sampling to a planned solution equipped with logical verification [209, 227, 223, 250]. These four paradigms are not linear replacements but are complementary and symbiotic; they collectively expand the boundaries of semantics-space fusion in world models from the four dimensions of data-driven generalization, conditional control flexibility, physical modeling precision, and logical reasoning robustness.\n\n### 3.2.1 Pixel Space Manipulation\n\nThe core philosophy of this paradigm lies in anchoring on data distribution, explicitly trading geometric priors for scale [97, 220]. Unlike traditional graphics that rely on expensive, hard-coded geometric priors [81, 84], pixel space manipulation advocates for constructing a joint distribution $p_{\\theta}(x_{\\mathit{img}}, c)$ of image-text and spatiotemporal data [221] based on pre-trained 2D generative bases (such as Latent Diffusion or Autoregressive Transformers) [222, 223].\n\nMathematically, this is equivalent to assuming that the massive volume of 2D projection data $\\{x_{\\mathit{img},i}\\}_{i=1}^N$ is sufficient to cover the topological structure of the high-dimensional 3D manifold $\\mathcal{M}_{\\mathit{world}}$ [208]. In this context, *Modal Consistency* is manifested as the semantic alignment of conditional probability $p(x_{\\mathit{img}}|c_{\\mathit{sem}})$ [10, 133], while *Spatial Consistency* spontaneously emerges as an outcome of optimizing the joint distribution when reconstruction error is minimized [224, 135].\n\n** (1) Instruction-Driven Image Editing. ** To address the common issue of geometric collapse in text-based editing (e.g. non-physical distortion of the background when instructing a dog to sit), instruction-\n\n---\n\nFigure 19: Evolution of Modal Consistency and Spatial Consistency.\n\ndriven image editing has established a hybrid paradigm integrating gradient decoupling & update and attention injection as gating. This paradigm aims to resolve the intrinsic contradiction between semantic reconstruction and structural preservation by constructing orthogonal control paths, achieving a heavy semantic bridge [249], light weight structural constraint architecture as illustrated in Figure 20.\n\n*Gradient Decoupling & Update.* To effectively decouple and protect the original spatial layout $S_{\\text{orig}}$ while injecting new semantics $\\Delta c$, mainstream paradigms (such as ControlNet [133] or IP-Adapter [250]) employ a structured decoupling architecture. By freezing the pre-trained base (e.g., SDXL) and only fine-tuning the side-network or decoupled cross-attention, the model constructs a gradient update path on the parameter manifold that is orthogonal to the base:\n\n$$ \\nabla_{\\theta\\mathcal{L}} = \\underbrace{\\nabla_{\\theta_{\\text{base}}}\\mathcal{L}_{\\text{prior}}}_{\\approx 0 \\ (\\text{Frozen})} + \\underbrace{\\nabla_{\\theta_{\\text{adapter}}}\\mathcal{L}_{\\text{edit}}}_{\\text{Semantics}} \\quad (13) $$\n\nwhere $\\theta_{\\text{base}}$ represents the frozen parameters of the base model, and $\\theta_{\\text{adapter}}$ denotes the trainable parameters of the side-network. This ensures that the physical common sense (e.g., lighting, occlusion) internalized within the base remains undisturbed.\n\n*Attention Injection as Gating.* During the inference phase, Prompt-to-Prompt [249] and MasaCtrl [251] reveal a strong correlation between cross-attention maps and spatial layouts. To maintain spatial consistency, the model injects the attention map $M_{\\text{attn}}^{\\text{src}}$ of the original image into the editing steps as a geometric hard-gating mechanism:\n\n$$ \\text{Attn}_{\\text{edit}}(Q, K, V) \\leftarrow \\alpha \\cdot \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) + (1-\\alpha) \\cdot M_{\\text{attn}}^{\\text{src}}, \\quad (14) $$\n\nwhere $M_{\\text{attn}}^{\\text{src}}$ denotes the attention map preserved from the source image to guide spatial layout, and $\\alpha$ is the injection strength coefficient. Combined with the MLLM Semantic Hub mechanism proposed by Step-1X Edit [225], this method successfully achieves semantic change with topological conservation. Subsequent work such as EditWorld [210] further introduced a post-edit closed-loop, utilizing SAM masks for second-order geometric verification to resolve pixel artifacts at object edges.\n\n## (2) General Image Generation.\nGeneral image generation is undergoing a paradigm reconstruction from external plug-in alignment to native full-duplex modeling, aiming to directly capture the\n\n---\n\nFigure 20: Instruction-Driven Image Editing. The diagram illustrating structure-preserving image editing, where a Doge is repositioned via text instruction. It depicts the mechanisms of gradient decoupling (frozen base) and attention injection (geometric gating) to maintain spatial consistency.\n\nspatiotemporal dynamics distribution of the physical world through end-to-end joint training. The paradigm shift in this field is characterized as: From external alignment (CLIP-based) to End-to-End Interleaved Modeling. This transition no longer relies on frozen feature extractors but instead constructs a generative foundation where modality and space are tightly coupled through joint modeling [226], video stream supervision [252], and lightweight connections [228].\n\n(i) Joint Modeling Breaking Information Bottleneck. Traditional two-stage models (such as DALL-E 2) are limited by the modality isolation of the CLIP encoder, which results in the loss of spatial relations during feature compression. A new generation of models, such as DreamLLM [226] and Emu [253], abandons this design in favor of directly performing joint modeling on raw image-text sequences using autoregressive or diffusion approaches:\n\n$$ \\mathcal{L}_{\\text{joint}} = - \\sum_{t} \\log p(\\mathbf{x}_{\\text{img},t} | \\mathbf{x}_{\\text{img},<t}, \\mathbf{x}^{\\text{txt}}) - \\sum_{j} \\log p(\\mathbf{x}_{j}^{\\text{txt}} | \\mathbf{x}_{\\text{img}}, \\mathbf{x}_{<j}^{\\text{txt}}), \\quad (15) $$\n\nwhere $L_{joint}$ denotes the unified training objective, $\\mathbf{x}_{\\text{img},t}$ represents the image tokens at step $t$, and $\\mathbf{x}^{\\text{txt}}$ corresponds to the text tokens. This full-duplex information flow enables the model to capture pixel-level spatial constraints implicit in descriptions such as “a cat on a table.”\n\n(ii) Video as World Simulator. Transcending simple geometric perspective transformations, empirical research on Sora [2] reveals the profound value of video data: it provides endogenous supervision signals regarding physical plausibility.\n\nUnlike static images, temporal dependencies in video streams force the model to learn object permanence [252, 254]—for instance, inferring that an occluded object has not disappeared but continues to move along its trajectory. This self-supervision compels the model to construct a dynamics model within the latent space that conforms to physical conservation laws (e.g., gravity, collision, fluid dynamics) [163], thereby elevating the generative model from mere pixel statistical fitting to a predictive simulation of physical world evolution [204].\n\n(iii) Lightweight Connection Layer. To balance computational efficiency with multimodal alignment, the perceiver resampler in Flamingo [14] and the MLP connection layer design in Mentor [228] demonstrate how visual features can be projected onto the LLM’s semantic manifold using minimal parameters. This proves that as long as the base is sufficiently powerful, simple linear mappings can maintain complex space-semantics correspondence.\n\n---\n\n### 3.2.2 View Space Mapping\n\nFigure 21: Pose-Aligned View Synthesis. Diagram illustrating pose-conditioned generation using a decoupled backbone and epipolar attention. The target view features a split display of RGB texture and a purple-blue Normal Map, representing cross-domain mutual supervision for geometric accuracy.\n\n**Pose-Aligned Coupled Training.** The core philosophy of this paradigm lies in abandoning purely data-driven black-box assumptions and injecting 3D geometric information as structured condition variables $τ = \\{P_t, \\mathcal{D}\\}$ (where $P_t \\in SE(3)$ is the camera pose and $\\mathcal{D}$ is the depth prior) into a pre-trained diffusion model [133, 208], as illustrated in Figure 21. Its mathematical essence is the construction of a conditional denoising distribution constrained by geometry:\n\n$$ \\mathcal{L}_{\\text{view}} = \\mathbb{E}_{z_t, t, c, \\tau, \\epsilon} \\left[ \\|\\epsilon - \\epsilon_{\\theta}(z_t, t, c, \\tau)\\|_2^2 \\right] + \\lambda \\mathcal{R}_{\\text{consist}}, \\qquad (16) $$\n\nwhere $z_t$ represents the noisy latent at timestep $t$, $\\epsilon_{θ}$ is the noise prediction network conditioned on geometry $τ$, and $\\mathcal{R}_{consist}$ denotes the regularization term for multi-view consistency. The successful implementation of this paradigm relies on the following three synergistic mechanisms:\n\n(i) *Backbone Decoupling & Injection.* To circumvent catastrophic forgetting while preserving the semantic generation capability of the pre-trained model, the academic community has established a design of frozen backbone and bypass control. Represented by Zero-1-to-3 [208] and ControlNet [133], this approach achieves selective gradient flow by locking the backbone network $\\mathcal{F}_{locked}$ and introducing a trainable copy $\\mathcal{F}_{copy}: \\mathbf{h}_{out} = \\mathcal{F}_{locked}(\\mathbf{h}_{in}) + \\mathcal{Z}(\\mathcal{F}_{copy}(\\mathbf{h}_{in}, \\tau))$. This zero convolution strategy ensures that the model generates photo-realistic textures while precisely executing geometric instructions.\n\n(ii) *Structured Sparse Attention.* To address the janus problem in multi-view generation, models introduce structured sparse attention. MVDream [99] and SyncDreamer [106] innovatively transform the epipolar geometry constraints in 3D space into an attention mask:\n\n$$ \\operatorname{Attn}(\\mathbf{Q}_i, \\mathbf{K}_j, V_j) \\propto \\exp \\left( \\frac{\\mathbf{Q}_i \\mathbf{K}_j^T}{\\sqrt{d}} + \\mathcal{M}_{\\text{epi}}(i, j) \\right), \\qquad (17) $$\n\nwhere $\\mathbf{Q}_i$ and $\\mathbf{K}_j$ denote features from view $i$ and view $j$, respectively, and $\\mathcal{M}_{\\text{epi}}$ represents the geometric bias derived from epipolar constraints. This mechanism forces tokens from different views\n\n---\n\nto interact only with their geometrically corresponding epipolar line regions, thereby converting geometric hard constraints into a soft inductive bias within the attention mechanism.\n\n(iii) *Cross-Domain Attention Regularization*. To further enhance geometric accuracy, *Wonder3D* [214] and *MoAI* [213] achieve mutual supervision between texture semantics and geometric structure by generating RGB and normal maps in parallel and introducing cross-domain attention injection $f_{\\text{rgb}} \\leftrightarrow f_{\\text{geo}}$. Coupled with a 3D consistent noise initialization strategy (initializing noise based on the camera projection matrix), this paradigm successfully breaks the i.i.d. assumption from the initial state, achieving a transition from simple image generation to geometrically controllable generation.\n\n### 3.2.3 Volume Space Representation\n\nUnlike the previous two paradigms that simulate 3D on a 2D plane, volume space representation chooses to directly confront the three-dimensional essence of objects [97, 230]. The core philosophy of this direction is to utilize 3D Native Representations (NeRF, 3D Gaussian Splatting) as the primary layer of architectural abstraction. This makes spatial consistency an intrinsic property of the representation, while modal consistency is transformed into a synergistic optimization problem between cross-modal queries and differentiable rendering.\n\n(1) **Conditional 3D Generation: From 2D Distillation to Video Manifold Constraints.** Conditional 3D generation aims to overcome the bottleneck of 3D data scarcity by restructuring pre-trained generative models as frozen cognitive engines, establishing a technical trajectory that evolves from 2D semantic distillation toward video manifold constraints. Due to the extreme scarcity of high-quality 3D-text data pairs (which are 2-3 orders of magnitude fewer than 2D data), this direction no longer seeks to train 3D generators from scratch. Instead, it focuses on discovering and transferring the spatial intelligence inherent in pre-trained 2D or video models [97, 104].\n\n(i) *Gradient Flow from 2D Priors*. DreamFusion [97] and RealFusion [230] established the foundational formula for this field: Score Distillation Sampling (SDS). Its core principle is not to optimize pixel error, but to optimize a parameterized 3D field $\\theta$ (such as NeRF or 3DGS) such that the image rendered from any viewpoint, $x_{\\text{img}} = g(\\theta, \\mathbf{P}_t)$, resides in the low-energy regions of a 2D diffusion model:\n\n$$\\nabla_{\\theta} \\mathcal{L}_{\\text{SDS}} = \\mathbb{E}_{t,\\epsilon} \\left[ w_{\\text{guidance}} (\\epsilon_{\\theta}(z_t, t) - \\epsilon) \\frac{\\partial x_{\\text{img}}}{\\partial \\theta} \\right], \\qquad (18)$$\n\nwhere $w_{\\text{guidance}}$ is the weighting factor, $\\epsilon_{\\theta}$ is the predicted noise from the frozen diffusion model, and $\\frac{\\partial x_{\\text{img}}}{\\partial \\theta}$ represents the Jacobian of the differentiableunic勒。This formula indicates that the semantic residual computed by the 2D model is backpropagated through the Jacobian matrix $\\frac{\\partial x_{\\text{img}}}{\\partial \\theta}$ of the differentiable renderer $g$ to directly sculpt the 3D geometry.\n\n(ii) *Video Manifold as Dynamic 3D Prior*. To address the janus problem caused by 2D priors, recent research has shifted toward leveraging the physical consistency inherent in Video Diffusion Models (VDMs). The core hypothesis is that Temporal Correlation $\\cong$ Spatial Consistency. *See3D* [104] and *V3D* [255] propose utilizing video generative models as multi-view generators. By fine-tuning the VDM, the time axis *T* is implicitly reconstructed as a camera trajectory $\\mathbf{P}_t$ (e.g., an orbital viewpoint):\n\n$$p(\\mathbf{x}_{\\text{img, novel}} | \\mathbf{x}_{\\text{img, ref}}) \\approx p_{\\text{video}}(\\mathbf{x}_{\\text{img, t+1}} | \\mathbf{x}_{\\text{img, t}}, \\text{motion\\_cond}), \\qquad (19)$$\n\nwhere $p_{\\text{video}}$ denotes the transition probability learned by the video model, and $\\text{motion\\_cond}$ represents the camera trajectory condition. Under this paradigm, *SV3D* [127] utilizes the temporal attention layer of the video model as a soft epipolar constraint, forcing the generation of a multi-view sequence with geometric continuity. Subsequently, SDS is used to distill this dynamic video prior into static 3D assets, fundamentally resolving viewpoint conflicts.\n\n---\n\n(iii) Prior- constraint Two-Stage Loop. Given the ill-posedness of single-view generation, Magic123 [256] and One-2-3-45 [257] established the paradigm of coarse generation → fine optimization. Current trends involve using video models [231] to rapidly generate multi-views as an initial guess, followed by geometry refinement using SDS in combination with a lightweight solver [258]. This strategy of video initialization and physics fine-tuning preserves semantic richness while utilizing video priors to rectify the topological plausibility of the 3D structure.\n\nFigure 22: Multimodal Alignment. The diagram illustrating the convergence of Text, Image, and 3D Point Cloud into a central Unified Embedding. It depicts contrastive alignment for inputs and generative tokenization for 3D data integration.\n\n(2) **Multimodal Alignment.** Multimodal alignment aims to construct universal representations that span geometry and semantics. By establishing a dual-track mechanism of discriminative metric alignment and generative interaction fusion, it breaks the long-standing representation silo dilemma of 3D data. To process 3D data as CLIP processes images, this direction focuses on building a Unified Embedding Space $Z_{uni}$. The technical philosophy is to transform spatial consistency into structured constraints during network forward propagation, as shown in Figure 22.\n\n*Contrastive Metric Learning.* ULIP-2 [235] and OpenShape [215] employ large-scale triplet contrastive learning. By mining hard negatives and utilizing the InfoNCE loss, the feature distribution of the 3D encoder (PointNet++ or Transformer) is forced to align with CLIP's text/image space:\n\n$$\\mathcal{L}_{\\text{align}} = -\\log \\frac{\\exp (z_{3D} \\cdot z_{\\text{txt}} / \\tau)}{\\sum_{j} \\exp (z_{3D} \\cdot z_{\\text{txt}}^{j} / \\tau)}, \\quad (20)$$\n\nwhere $z_{3D}$ and $z_{txt}$ represent the feature embeddings of the 3D shape and text, respectively, and $\\tau$ is the temperature parameter. Genesis [237] further extends this to 4D spatiotemporal alignment\n\n---\n\nby introducing cross-view attention in voxel space to fuse video and LiDAR modalities, achieving\nalignment across space-time dimensions.\n\n**Generative Integration.** Unlike the holistic alignment of contrastive learning, ShapeLLM-Omni [236] and ViewSetDiffusion [238] introduce 3D VQ-VAE to discretize continuous geometry into token sequences. This enables the LLM to directly read and generate 3D geometry, achieving generative interaction between modalities rather than simple retrieval matching.\n\n**(3) 3D Understanding & Editing: Semantic Lifting.** 3D understanding and editing aim to endow 3D\ngeometric entities with the dual capabilities of semantic perception and linguistic manipulation. The\ncore paradigm involves injecting the cognitive priors of 2D vision foundation models into 3D space\nvia Semantic Lifting, constructing a mapping $F(x,y,z) \\mapsto \\mathbb{R}^{D_{clip}}$.\n\n**Semantic Field Construction.** LERF [239] and Lang3D-XL [216] propose training a semantic head in parallel with the color head of a NeRF. This module learns CLIP feature fields through multi-scale supervision, enabling every coordinate point $p(x, y, z)$ in space to respond to natural language queries (e.g., Find the crack on the chair). SKED [244] and CoRe-3D [241] introduce hierarchical semantic fields, embedding instance-part-material hierarchies into the representation to solve fine-grained semantic localization problems.\n\n**Language-Driven Topology Editing.** For editing tasks, CLIP-NeRF [242] utilizes decoupled latent mapping to achieve near-instant modifications of shape and appearance. InstructNeRF2NeRF [259] employs an iterative dataset update strategy: it first modifies the rendering view images using InstructPix2Pix and then uses the modified images as Pseudo-GT to back-update the NeRF. Lift3D [243] and ICE-G [245] introduce canonical space constraints to ensure that topological structures do not collapse even during significant geometric deformations, such as instructing a cat to stand up.\n\n### 3.2.4 Reinforcement Learning for Modal-Spatial Alignment\n\nDespite the explicit geometric conditions provided by architectures such as ControlNet [133], LMMs still frequently exhibit severe modal-spatial misalignment when processing compositional instructions (e.g., attribute binding: Red cat on blue car) [206]. To address this bag-of-words model deficiency, the academic community is undergoing a paradigm shift from black-box optimization toward System-2 Reasoning [207].\n\nThis paradigm evolution can be summarized into three stages:\n\n**Discriminator-Guided Explicit Anchoring.** Early efforts focused on utilizing off-the-shelf visual discriminators as an external reward function $R$ to forcibly establish the correspondence between text prompts and bounding boxes.\n\n**Black-box Discrete Optimization.** DDPO [217] models diffusion denoising as a Markov Decision Process (MDP). For spatial instructions, it introduces an open-vocabulary detector (such as GroundingDINO [260]) to compute an IoU reward. This represents a loosely-coupled fusion; while it enhances object recall, the sparsity of the reward signal makes it difficult to resolve complex attribute binding.\n\n**White-box Gradient Backpropagation.** AlignProp [246] proposes fine-tuning the discriminator into a differentiable reward model. This establishes an end-to-end gradient path $\\nabla_{pixels} L_{align}$, allowing spatial errors to back-propagate directly to the denoising network, thereby achieving pixel-level precision in refinement.\n\n**Region-Temporal Decoupling.** To prevent global rewards from confusing semantics with spatial information, subsequent work shifted toward fine-grained control.\n\n---\n\nR-DPO [247] proposed sub-manifold optimization under spatial masks. Unlike traditional DPO, it decomposes the image **x**<sub>img</sub> and text **c** into several local pairs (*x*<sup>k</sup><sub>crop</sub>, *c*<sup>k</sup><sub>sub</sub>), ensuring that specific modal attributes (e.g., Red) only back-propagate to specific spatial regions (e.g., within the coordinates of the Cat):\n\n$$ \begin{aligned} (\\mathcal{L}_{\\text{R-DPO}} &= -\\sum_k \\mathbb{E}_{(\\mathbf{x}_w, \\mathbf{x}_l) \\sim \\mathcal{B}_k} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_{\\theta}(\\mathbf{x}_{\\text{img, w}}^k | \\mathbf{c}_k)}{\\pi_{\\text{ref}}(\\mathbf{x}_{\\text{img, w}}^k | \\mathbf{c}_k)} - \\beta \\log \\frac{\\pi_{\\theta}(\\mathbf{x}_{\\text{img, l}}^k | \\mathbf{c}_k)}{\\pi_{\\text{ref}}(\\mathbf{x}_{\\text{img, l}}^k | \\mathbf{c}_k)} \\right) \\right], \\end{aligned} \\quad (21) $$\n\nwhere B_k represents the local preference dataset for region k, x_w and x_l denote the winning and losing image crops respectively, and σ is the sigmoid function.\n\nConcurrently, SPO [39] leverages the frequency characteristics of diffusion models by adopting a time-division multiplexing strategy: focusing on spatial IoU optimization during the early stages of denoising (t ∈ [T, T/2]) and switching to semantic optimization in the later stages (t ∈ [T/2, 0]) to avoid gradient conflicts. Furthermore, DRaFT [261] utilizes VLMs to generate natural language critiques regarding spatial errors and maps them to a dense reward map $\\mathcal{R} \\in \\mathbb{R}^{H \\times W}$. This marks the transition of RL alignment from discrete boxes to continuous pixel fields, enabling generative models to comprehend extremely subtle spatial-modal instructions such as the left leg is distorted.\n\n**TTT & Visual CoT.** Following the success of DeepSeek-R1 and OpenAI o1 in demonstrating the efficacy of inference-side scaling, recent research has begun introducing RL into the inference-time stage of generation, achieving a strong logical fusion between modality and space.\n\n*Test-Time Preference Optimization (TTPO).* To address the insufficient perceptual quality of pre-trained\nmodels under specific distributions, TTPO [218] proposes an on-the-fly optimization mechanism. This\nmethod avoids heavy re-training by using a lightweight reward model (such as an image quality\nscore) to iteratively update the latent variable *z* during the inference stage. While this work primarily\nvalidates its effectiveness in image restoration tasks, this test-time fine-tuning paradigm provides a\ngeneral compute-for-quality path for resolving highly counter-intuitive generative tasks.\n\n*Visual Chain-of-Thought (Visual CoT)*. To resolve the logic breaks inherent in one-step generation, Layout-CoT [219] borrow the reasoning paradigm of LLMs. This approach decomposes the generation process into an explicit chain: *Planning* → *Alignment* → *Generation*. The model first generates a discrete layout plan in a low-dimensional space and employs RL to perform logical verification on this plan. Only chains-of-thought that pass verification are decoded into pixels. This mechanism essentially moves System-2 logic verification to the front end, fundamentally eliminating hallucinations such as interpenetration or spatial misalignment.\n\n3.3 Integration of Modal and Temporal Consistency\n\nThe deep integration of modality and temporal consistency marks the formal transition of Generative AI from the Frozen Moment of static images toward the Continuous Deduction of the dynamic world as illustrated in Figure 23 [2]. The core utility of this dimension lies in constructing a Probabilistic Simulation of Spatiotemporal Causality: at the Semantic Level, it ensures that video content strictly adheres to the definitions of text or image instructions (e.g., Blooming, Running), thereby eliminating cross-modal semantic drift [262, 151]; at the Dynamics Level, it endows the model with an endogenous understanding of Object Permanence and Physical Conservation Laws, ensuring that the generated frame sequence is no longer a random stacking of discrete pixels, but rather a Continuous Manifold consistent with logical evolution [221, 263]. This fusion fundamentally resolves chronic issues in traditional video generation, such as motion flickering, temporal logic chaos, and long video collapse.\n\nBased on this objective, current exploration paths present Four Progressive Technical Paradigms\nas shown in Figure 24: End-to-End Scalable Modeling follows the data philosophy of “Brute Force\n\n---\n\nwith Data ethnicity, relying on Diffusion Models and Autoregressive Architectures to validate the Scaling Law, aiming to learn a general physical simulator directly from massive data [209, 264, 265]; Explicit Structured Control targets the controllability requirements of industrial applications by introducing motion vectors, trajectory heatmaps, and orthogonal decoupling mechanisms to explicitly inject human intent into the generation process, addressing the ambiguity issues of end-to-end models [266, 178, 267]; meanwhile, the Unified Comprehension and Generation Symbiosis Architecture attempts to break the barriers between perception and generation through shared representation and bi-directional adaptation, constructing a general agent with a closed loop of perceiving and acting [268, 269]; finally, Reinforcement Learning Driven Alignment addresses the non-convexity of SFT (Supervised Fine-Tuning) in optimizing modal semantic and “temporal dynamics” by constructing a Multi-dimensional Reward Manifold. By integrating DPO and Self-Refinement mechanisms, this paradigm achieves joint optimization of alignment targets, driving the model to surpass binary games and converge to the Pareto Frontier of spatiotemporal trade-offs [270, 271]. These four paradigms collectively build a complete architecture for modality and temporal intelligence from the dimensions of General Foundation, Controllable Interface, Cognitive Top-level, and Value Optimization.\n\nFigure 23: Modal + Temporal Consistency: Language Controls Time Evolution. The model integrates modality and temporal consistency, using language instructions to control the time evolution process (e.g., the cherry blossom tree behind the dog blooms and scatters across T1-Winter, T2-Spring, T3-Late Spring), ensuring coherent changes over time.\n\n**3.3.1 End-to-End Scalable Modeling**\n\nEnd-to-End Scalable Modeling represents a paradigm shift in the field of video generation from “Divide and Conquer” toward a “Unified Field.” Its core objective is to validate the efficacy of the **Scaling Law** on high-dimensional spatiotemporal manifolds—specifically, by synergistically expanding both model and data scales to directly fit the joint distribution *p(x<sub>img</sub>|c)* from multi-modal inputs to video outputs. Unlike earlier cascaded pipelines that relied heavily on hand-crafted interpolation and super-resolution modules, this paradigm is dedicated to constructing a general physical simulator, driving the industrialization of models from Sora [2] to Wan 2.1 [273].\n\n**(1) Diffusion Model.** As the core engine of end-to-end video generation, the Diffusion Model has completely restructured the technical path from “Image Animation” to “Native World Simulation” by\n\n---\n\n**Figure 24: Evolution of Modal Consistency and Temporal Consistency.**\n\nvalidating the Scaling Law within the Latent Space $\\mathcal{Z}$. To support this complex goal of physical consistency, the evolution of modern architecture is no longer confined to simple denoising iterations; instead, it has systematically reshaped four core pillars: from the ODE unification of generation theory [31], the causal decoupling of compressed representations [179], and the native three-dimensionalization of attention modeling [135], to the progressive cascading of generation strategies [280]. Together, these form the underlying foundation for spatiotemporal intelligence.\n\n(i) Theoretical Unification via Flow Matching. Although early works followed the DDPM paradigm based on SDEs, state-of-the-art (SOTA) models such as Sora [2] and Wan 2.1 [273] have generally shifted toward the **Flow Matching (FM)** framework to enhance sampling efficiency and temporal coherence. Rather than predicting Gaussian noise $\\epsilon$, FM formalizes the generation process as constructing a deterministic Ordinary Differential Equation (ODE) trajectory between the noise distribution $\\pi_0$ and the data distribution $\\pi_1$. The core optimization objective transforms into regressing the velocity field $v_t$ on the optimal transport path:\n\n$$ \\mathcal{L}_{\\text{FM}}(\\theta) = \\mathbb{E}_{t,z_0,z_1} [|| v_{\\theta}(t,(1-t)z_0 + tz_1) - (z_1 - z_0) ||^2], \\qquad (22) $$\n\nwhere $v_\\theta$ denotes the velocity field predicted by the network parameters $\\theta$, and $z_0, z_1$ represent samples from the prior noise and data distributions respectively. As demonstrated by Rectified Flow [31], this paradigm forces the latent variable $z$ to evolve along a linear trajectory, significantly reducing transport curvature. This allows the model to generate dynamic textures with physical conservation in very few steps, resolving the structural collapse issues inherent in DDPMs during long-term sampling.\n\n(ii) Causal Spatiotemporal Compression. To circumvent the computational bottlenecks of high-dimensional video data, the primary challenge in architecture design lies in constructing a compact latent space $\\mathcal{Z}$ that satisfies causality. MagViT-v2 [315] and CogVideoX [179] identified the risk of “future information leakage” in traditional 3D convolutions. Consequently, modern encoders generally introduce Causal 3D VAEs [263, 151], utilizing asymmetric temporal padding and causal convolution kernels to ensure that the generation of latent code $z_t$ depends only on historical frames $x_{img, \\le t}$. This design not only\n\n---\n\nmathematically guarantees the unidirectionality of temporal logic but also provides the architec-\ntural foundation for streaming inference. Furthermore, by employing heterogeneous downsampling-\nstrategies (e.g., $t \\times 4, h \\times 8, w \\times 8$), models achieve decoupled compression of high-frequency motion\ninformation and low-frequency semantic features [223, 316].\n\n**(iii) Native 3D Attention Modeling.** Regarding dynamics modeling in latent space, the academic commu-\nnity has undergone a profound correction of inductive bias. Early works like AnimateDiff [135]\nutilized \"spatial-temporal factorized\" attention, which reduced computational costs but severed spa-\ntiotemporal coupling, making it difficult to simulate complex fluid dynamics. HunyuanVideo [278]\nand OpenSora [317] have since established the dominance of the **Native 3D DiT**, calculating joint\nself-attention across the entire spatiotemporal sequence using 3D-RoPE. Although this introduces\na quadratic complexity of $O((THW)^2)$, the integration of sequence parallelism techniques such as\nRing Attention [318] enables the model to capture long-range spatiotemporal dependencies, thereby\nallowing the emergence of coherent motion consistent with physical laws.\n\n**(iv) Progressive Alignment & Cascading.** To address error accumulation in long video generation, models employ a “coarse-to-fine” condition control strategy. Visual Dialect, proposed by Tar [280], achieves native alignment of semantics by mapping text to visually compatible tokens. For generating long-range videos (> 10s), Kling [279] and Vidu [274] utilize spatiotemporal cascade strategies. The model first generates a semantic skeleton at a low frame rate, which then serves as a condition $c_{ctx}$ for a Temporal Super-Resolution Model. This cascade architecture essentially decomposes the high-dimensional joint distribution $p(x_{img})$ [319] into a product of multiple conditional probabilities, effectively mitigating VRAM pressure and logic drift in single models during long sequence generation [320].\n\n**(2) Autoregressive Model (AR).** The Autoregressive Model draws on the Scaling Law of LLMs [321, 322], with its core philosophy being \"Everything is a Token.\" This paradigm discards the denoising prior of diffusion models and reformulates video generation as a causal sequence prediction problem within a discrete latent space [222, 323]. Its mathematical essence is the maximization of the log-likelihood of the joint probability distribution, forcing the model to learn the temporal causality of the physical world through the unidirectional chain rule. To support this vision of unified sequence modeling, the technical evolution of this paradigm is unfolding across four key dimensions: the fidelity of discrete encoding, the topology of multi-modal interaction, the temporal robustness of hybrid generation, and the generalization boundaries of multi-task reasoning.\n\n**(*i*) The Discretization Bottleneck & Causal 3D Codebook.** The upper bound of an AR model depends on the compression quality of the tokenizer. Early VQGANs suffered from severe codebook collapse and high-frequency flickering. VideoPoet [143] and MagViT-v2 [315] achieved breakthroughs by introducing Lookup-Free Quantization (LFQ) and Causal 3D Convolution. The former reduces quantization variance through direct projection, while the latter ensures that the compression process does not violate physical causality via asymmetric padding. VILA-U [283] further proposed a Unified Vision Tower, which forces the alignment of visual tokens and text embeddings during the pre-training phase, fundamentally resolving the semantic gap between heterogeneous modalities in discrete space.\n\n**(ii) Omni-Modal Interaction Topology.** During the sequence modeling phase, the core of architectural\ndesign lies in handling the interaction granularity of multi-modal tokens. *Sequence Concatenation:\nUniForm* [282] adopts an aggressive early fusion strategy, concatenating video, audio, and text to-\nkens into a single long sequence. While using shared-weights Transformers to capture cross-modal\ndependencies maximizes knowledge transfer between modalities, it faces an $O(N^2)$ explosion in\nattention computation. *Dual-Stream Gated Modulation:* To reduce computational overhead, RFLAV [281]\nand Ovi [264] employ late fusion. Ovi designs a symmetric dual-backbone architecture, aligning the\nsampling rates of different modalities through RoPE frequency scaling; RFLAV introduces temporal\naveraging modulation in the AdaLN layer of the Transformer, achieving soft alignment of audio-video\nfeatures without a significant increase in parameter count.\n\n---\n\n(iii) Long-Horizon Dynamics & Hybrid Paradigms. Pure discrete AR models often face collapse due to error accumulation when generating long videos. To correct this deficiency, researchers have begun exploring hybrid paths of “discrete planning + continuous correction”. Non-Quantized AR: NoVA [284] challenges the assumption that data “must be discretized,” proposing continuous AR prediction in a continuous space. It decomposes video into “frame-wise temporal steps” and “set-wise spatial steps,” predicting continuous features via a diffusion decoder to circumvent information loss from quantization. Rolling Flow Matching: RFLAV [281] innovatively introduces a sliding window mechanism. After the AR predicts coarse tokens, flow matching is used for local refinement. Through a rolling strategy of “removing the first frame and adding the noised last frame,” it theoretically achieves physically consistent generation of infinite duration, solving the inherent malady of AR models being “logical but lacking details.”\n\n(iv) **Unified Multi-Task Reasoning.** The ultimate advantage of the AR architecture lies in its **zero-shot generalization**. As demonstrated by VideoPoet, by introducing specific task tokens (e.g., ‘optical-flow¿’, ‘depth¿’), a single model can perform video generation, style transfer, and even audio-visual QA tasks without fine-tuning. This “omnipresent” characteristic proves the unique potential of the autoregressive paradigm in building a universal world simulator.\n\nFigure 25: Autoregressive-Diffusion Model. It depicts an AR Planner constructing a causal temporal skeleton (blue wireframes), which then passes through a Diffusion Refiner (warm mist) for detail injection, resulting in high-quality long-duration video output.\n\n(3) Autoregressive-Diffusion Hybrid Model. The core synergy mechanism of the Autoregressive- Diffusion Hybrid Model, as essentially illustrated in Figure 25, is the injection of the temporal causal constraints of AR into the iterative denoising manifold of the Diffusion Model. The universal mathematical representation of this hybrid generation is no longer a simple probability superposition, but rather the construction of a joint probability density of “causal logic and high-quality generation”:\n\n$$ p_{\\theta}(\\mathbf{x}_{\\text{img},1:T}, \\mathbf{z}_{1:T} | \\mathbf{c}) = \\prod_{t=1}^{T} \\underbrace{p_{\\text{AR}}(\\mathbf{z}_t \\mid \\mathbf{z}_{<t}, \\mathbf{x}_{\\text{img},<t}, \\mathbf{c})}_{\\text{Causal Temporal Dynamics}} \\cdot \\underbrace{p_{\\text{Diff}}(\\mathbf{x}_{\\text{img},t} \\mid \\mathbf{z}_t, \\mathbf{x}_{\\text{img},<t}, \\mathbf{c})}_{\\text{Conditionally Denoised Rendering}} \\quad (23) $$\n\nwhere $x_{\\text{img},1:T}$ denotes the generated multi-modal sequence (Video/Audio), $z_t$ represents the noisy latent or intermediate features, and $p_{\\text{AR}}$ and $p_{\\text{Diff}}$ correspond to the low-dimensional causal modeling and the high-fidelity conditional denoising distribution, respectively. This mechanism aims to combine the long-range planning advantage of AR with the detail generation capability of diffusion, overcoming the inherent defects of single models. Based on this joint modeling approach, the technical evolution of this paradigm is unfolding along two orthogonal paths: temporal fusion optimization and cross-paradigm modal synergy, aiming to simultaneously solve the bottleneck of dynamical consistency in long-sequence generation and the challenge of alignment between heterogeneous modalities.\n\n(i) *Temporal Fusion Optimization.* The core lies in balancing strict causal dependency with generation flexibility, breaking the efficiency bottleneck of long video generation through differentiated\n\n---\n\narchitecture design. *Real-time Streaming Dynamics.* To address generation speed and VRAM limits, several works have restructured the inference paradigm. AR-Diffusion [289] proposed a training-inference unified diffusion corruption mechanism, establishing a temporal baseline by enforcing a non-decreasing frame time step constraint ($t_1 \\le t_2 \\le \\dots \\le t_F$), which, combined with a dynamic scheduler, enables error-free variable-length generation. CausVid [290] converts bidirectional diffusion into an AR architecture through distribution matching distillation; combined with KV cache and sliding window mechanisms, it balances the real-time performance of streaming generation with infinite length extension capabilities. Furthermore, NFD [285] utilizes block-wise causal attention and speculative sampling to achieve real-time generation at 30+ FPS for the first time at a scale of 300M+ parameters. RFLAV [281] innovatively introduces rolling flow matching and a lightweight temporal modulation module, achieving precise alignment generation of infinite-length audio-video while significantly reducing computational overhead. *Long-term Coherence Guidance.* To address logic drift in long-term sequences, synergistic guidance strategies have become key. ARLON [288] employs a “coarse-grained anchoring - fine-grained refinement” strategy, using an AR model to generate coarse features containing long-range semantics to guide a Diffusion Transformer (DiT) in detail refinement, while utilizing a VQ-VAE unified representation space to resist noise interference. ACDC [286] proposes a zero-shot synergy framework that, without modifying the architecture, allows the AR model to act as a global context “planner” and the diffusion model as a local “corrector,” utilizing the external memory module of an LLM to effectively alleviate error accumulation in long sequence prediction.\n\n(ii) *Cross-Paradigm Modal Synergy.* This focuses on the precision of modal alignment and the tightness of integrated architecture, aiming for deep coupling of heterogeneous signals. *Diffusion-Augmented Representation*. DiCoDe [287] challenges traditional discretization methods with a diffusion cascaded tokenization scheme. It first encodes video into continuous latent features and then uses a diffusion process to compress them into high-fidelity discrete tokens. This approach achieves thousand-fold compression while preserving visual details and strengthens text-video semantic alignment through cross-attention mechanisms, providing a high-quality “vocabulary” for long video generation. *End-to-End Architectural Fusion*. HybridVLA [265] demonstrates the potential of paradigm fusion in the field of Embodied AI. It seamlessly integrates diffusion generation and AR prediction within a single LLM framework, projecting continuous action vectors generated by diffusion into the LLM’s word embedding space. By introducing special tokens to separate the two paradigms and adaptively fusing prediction results based on AR confidence, the model achieves an end-to-end logical loop across visual, language, and action modalities, significantly strengthening the coherence of the agent’s “perception-reasoning-execution” link.\n\n### 3.3.2 Explicit Structured Control\n\nAlthough end-to-end models have achieved breakthroughs in image quality, their “text-as-all” interaction mode exhibits significant ambiguity in industrial applications. Explicit structured control aims to resolve the challenge of **controllability**. Its core concept involves projecting the high-dimensional dynamics manifold $M_{dyn}$ onto a low-dimensional interpretable control manifold (e.g., depth, optical flow, skeleton). This paradigm reformulates video generation as a constrained optimization problem:\n\n$$ \\max_{\\theta} \\mathbb{E}_{x_{\\text{img}},c_{\\text{struct}},c_{\\text{mot}}} \\left[ \\log p_{\\theta}(x_{\\text{img}} | \\mathcal{E}_{\\text{txt}}(c_{\\text{txt}}), \\mathcal{E}_{\\text{str}}(c_{\\text{struct}}), \\mathcal{E}_{\\text{mot}}(c_{\\text{mot}我院}) ) \\right], \\quad (24) $$\n\nwhere $\\mathcal{E}_{\\text{str}}$ and $\\mathcal{E}_{\\text{mot}}$ denote the encoders processing explicit conditions for spatial structure and temporal motion, respectively.\n\n**(1) Motion-Geometry Explicit Encoding.** This school of thought primarily inherits and extends the principles of 2D ControlNet, aiming to eliminate generated geometric hallucinations by injecting explicit physical priors. Facing spatiotemporal degrees of freedom that far exceed those of static images, this paradigm is dedicated to constructing a set of “hard-constrained” physical interfaces. It\n\n---\n\nhas achieved breakthrough progress in two key dimensions—residual-based spatiotemporal feature injection and multimodal narrative structure orchestration.\n\n(i) **Residual-based Feature Injection.** The core challenge lies in injecting strong geometric constraints without compromising pre-trained generation priors. *Spatiotemporal ControlNet Adaptation*. Video-Composer [262] proposed an explicit encoding strategy using Motion Vectors (MVsw), utilizing MV signals in the compressed domain as a low-rank approximation of temporal conditions. This addresses control difficulties in complex motion scenarios, such as the coupling of camera translation and object deformation. ControlVideo [293] explored a training-free path by introducing cross-frame geometric masks in the self-attention layer to force multiple frames to share the same ControlNet features, thereby achieving temporal consistency in structure. *Trajectory-aware Latent Navigation*. For fine-grained control of object movement paths, DragNUWA [324] and MotionCtrl [325] introduced the joint encoding of trajectory heatmaps and camera poses $P_t$. Unlike simple optical flow $O_{flow}$ injection, they explicitly map user-drawn 2D trajectories to manifold evolution directions in 3D latent space by a flow $\\mathcal{F}: z_t \\rightarrow z_{t+1}$.\n\n(ii) **Multimodal Storyboarding.** To handle long-range narratives, VAST [266] introduced a storyboard mechanism, decoupling text descriptions into dual-stream constraints of “Layout + Pose.” Its innovation lies in constructing a bi-directional autoencoder that maps discrete control signals to continuous sequence latent vectors, providing a rigid skeleton for cross-frame generation and effectively suppressing object identity drift in long sequences.\n\n### (2) Start-End Frame Anchoring and Interpolation.\nThis paradigm transforms video generation from extrapolation into a mathematically more stable interpolation problem, specifically solving for a Brownian Bridge with $x_{img,start}$ and $x_{img,end}$ as boundary conditions. Under this mathematical framework, technical evolution focuses on constructing smooth, high-fidelity spatiotemporal transition manifolds and exploring multimodal interactive control within constrained spaces, forming two core pillars: boundary-condition-driven path planning and dynamic instruction injection.\n\n(i) **Boundary-Conditioned Path Planning.** *Temporal Generative Inpdfaming.* SEINE [326] and MorphStudiod [327] treat two input images as masks $\\boldsymbol{m} \\in \\{0,1\\}^{T \\times H \\times W}$, performing denoising only on the intermediate frames during the diffusion process. To ensure transition smoothness, they introduced interpolated attention, allowing the query vectors of intermediate frames to simultaneously query the keys/values of the start and end frames, thus achieving a smooth blending of physical states in the feature space. *Cascaded Super-Resolution Architecture.* To address the blurriness caused by interpolation, Show-1 [297] proposed a cascaded strategy of coarse-to-fine anchoring. Stage 1 utilizes a pixel-level model to generate a low-frequency motion skeleton, while Stage 2 employs latent diffusion for high-frequency texture inpainting. This design skillfully leverages the structural sensitivity of pixel space and the texture generation capability of latent space.\n\n(ii) **Dynamic Instruction Injection.** For complex interactive generation, InteractiveVideo [298] refines control signals into a quadruple (Image, Content, Action, Trajectory) and injects them at specific time steps via gated cross-attention. KeyVID [299] focuses on audio-driven scenarios, utilizing ImageBind to extract audio peaks as implicit keyframes, achieving automated anchoring of “audio-visual sync.”\n\n### (3) Multi-Condition Decoupling Architecture.\nTo resolve feature entanglement between multimodal signals (e.g., changing a character’s action causing background texture changes), recent architectures favor the **orthogonal decoupling** design illustrated in Figure 26. Within this framework, technical evolution is proceeding along two key axes: the separation of appearance-motion features and the complementary interaction of spatiotemporal dimensions, aiming to simultaneously solve identity drift under high dynamics and the imbalance between spatial structure and temporal manifolds in long-sequence generation\n\n(i) *Appearance-Motion Two-Stream.* This is currently the mainstream paradigm for digital human animation [328, 329]. Facing the inherent conflict between maintaining identity (appearance) and driving\n\n---\n\nFigure 26: Multi-Condition Decoupling Architecture. The diagram illustrating the Two-Stream Archi- tecture for digital human animation. It depicts the orthogonal decoupling of Reference Appearance (static portrait) and Motion Skeleton (stick figure), which are fused via a Spatial Attention ”Zipper” to generate a spatiotemporally consistent video loop.\n\ncomplex actions (motion), this paradigm advocates for abandoning single-stream processing in favor\nof a two-stream decoupling mechanism at the architectural level. This involves extracting static\ntexture features and dynamic pose features separately, then fusing them through specific modules\northogonally. This includes: *Explicit Spatial Decoupling*. Targeting the issue where single-stream net-\nworks lose appearance features over time, Animate Anyone [330] and MagicConcat [331] introduced\nan independent ReferenceNet as the “appearance stream.” This branch does not participate in the\ndenoising process but specifically extracts high-fidelity features from the reference image, which are\nthen injected layer-by-layer via spatial attention into the Main UNet (motion stream) responsible for\naction generation. Formally, this achieves an explicit decomposition of the generated features z<sub>gen</sub>\n\n$$\nz_{\\text{gen}} = \\underbrace{\\mathcal{F}_{\\text{motion}}(z_t, \\mathbf{c}_{\\text{pose}})}_{\\text{Motion Stream}} + \\lambda \\cdot \\underbrace{\\mathcal{F}_{\\text{app}}(I_{\\text{ref}})}_{\\text{Appearance Stream}}, \\quad (25)\n$$\n\nwhere $z_{gen}$ denotes the synthesized feature map, $\\mathbf{c}_{pose}$ represents the pose control signal, $I_{ref}$ is the reference source image processed by the appearance encoder $\\mathcal{F}_{app}$, and $\\lambda$ is the fusion coefficient. This dual-tower design forces the separation of texture encoding and motion inference, ensuring consistency of details during large-scale dynamic movements [332, 333]. *Implicit Attention Disentanglement*. Moving beyond physical dual-network structures, Moonshot [302] and CCEdit [303] explore “logical dual-streams” within a single network. They argue that traditional cross-attention tends to confuse structural signals (pose/shape) with content signals (texture/identity). Consequently, these works propose a decoupled attention mechanism that splits key/value mappings into independent structure branches and appearance branches. Through orthogonal gradient backpropagation, the model is forced to ensure that changes in the motion stream do not interfere with the feature distribution of the appearance stream. This mechanism achieves zero-interference between appearance and motion at the micro-level, resolving the chronic issue of identity drift caused by action changes [334].\n\n(ii) Spatiotemporal Complementary Loop. TATS [304] and Swap Attention [301] explore the decoupling of spatiotemporal dimensions. Swap Attention utilizes a role-swapping mechanism within 3D windows to construct a loop where “space guides time, and time feeds back to space.” This design mathematically forces the model to maintain texture consistency along the spatial axis and optical flow $\\mathcal{O}_{flow}$ coherence along the temporal axis, effectively solving the “infinite loop” or “motion freezing” phenomena common in autoregressive generation.\n\n---\n\n### 3.3.3 Unified Comprehension and Generation Symbiosis Architecture\n\nTraditional computer vision research treats “Understanding (Discriminative)” and “Generation (Generative)” as opposing binary tasks: the former models the conditional probability $p(y|x_{img})$, while the latter models $p(x_{img}|y)$ [223, 335]. However, the Unified Comprehension and Generation Symbiosis Architecture seeks to construct a unified probability model $p(x_{img}, y)$, aiming to dismantle the barriers between perception and simulation [59, 307, 336]. The core assumption of this paradigm, as illustrated in Figure 27, is that: A perfect generator should implicitly contain a perfect discriminator.\n\nFigure 27: Unified Comprehension and Generation Symbiosis Architecture. It features a central LLM converting multimodal inputs into a unified cloud of discrete tokens, enabling seamless \"Any-to-Any\" transformation (e.g., video to text and vice versa).\n\n(1) **Shared Representation Bidirectional Synergy.** This direction aims to map heterogeneous signals into the same manifold space by constructing an Omni-modal Isomorphic Representation, thereby achieving “Any-to-Any” conversion within a single set of model parameters. Specifically, to break the chasm between perception and generation, this paradigm establishes the dominance of discrete tokens as universal interaction primitives and explores the unique value of geometric representations as physical anchors in embodied scenarios. This has resulted in two primary technical tracks based on symbolic unification and geometric symbiosis.\n\n(i) *Token-based World Modeling.* Inspired by the success of LLMs, discretized tokens have become the \"general currency\" for unifying understanding and generation. *Fully Discretized Autoregression.* Gaia-1 [205] and Phenaki [268] proposed video encoding schemes based on C-ViViT, which unify the encoding of driving videos, control signals, and text descriptions into a discrete token sequence $z_{1:L}$. The training objective of the model is unified into standard Next-Augmentation Prediction:\n\n$$ \\mathcal{L}_{\\text{uni}} = - \\sum_{i=1}^{L} \\log p_{\\theta}(z_i | z_{<i}, \\text{TaskToken}), \\tag{26} $$\n\nwhere $z_{1:L}$ represents the unified discrete token sequence combining visual and textual information,\n\n---\n\nand TaskToken serves as the prompt indicator to switch between understanding and generation modes.\nThis paradigm allows the model to switch functions via simple “Task Prompting”: inputting video\ntokens to predict text tokens constitutes “understanding,” while the reverse constitutes “generation.“\n*Unified Discrete Diffusion*. Unified Discrete Diffusion [307] and Show-O [176] challenge the notion that\n“autoregression is the only solution.” They designed a Unified Transition Matrix that allows image\nand text tokens to undergo bidirectional denoising within the same diffusion process. Show-O further\nutilizes a Hybrid Attention Mechanism that applies a Causal Mask to the text portion and a Full Mask\nto the visual portion, achieving the seamless coexistence of understanding and generation within\nsingle Transformer weights.\n\n(ii) *Domain-Specific Geometric Symbiosis*. In the field of Embodied AI, HERMES [310] proposed using BEV (Bird's-Eye-View) features as a shared hub. It utilizes a World Queries mechanism to compress 2D images from multi-view cameras into 3D BEV features. This not only supports downstream path planning (understanding) but also enables the generation of future prediction videos (generation) via a decoder for BEV features, proving that 3D geometric constraints serve as a strong bridge connecting perception and simulation.\n\n(2) **Pre-training Driven Synergistic Adaptation.** Unlike training a unified multimodal model from scratch [337, 59], this paradigm advocates a “Shoulders of Giants” strategy: using a frozen MLLM (such as GPT-4V or LLaVA) as the cognitive hub (Brain), connected via lightweight adapters to a visual generation decoder (Eyes/Hands). The goal is to transfer the general reasoning capability of LLMs to video generation tasks at a low cost [338, 339]. Under this architecture, the core technical challenge shifts to constructing a high-bandwidth interface connecting the cognitive space and the generative space, aiming to precisely map high-level reasoning to low-level generative conditions through an LLM-centric projection mechanism.\n\n(i) *LLM-Centric Projection.* The core challenge lies in achieving a *zero-loss* interface between the semantic space of the LLM and the pixel space of video generation. *Input-Output Bidirectional Adaptation.* Omni-Video [269] and NExT-GPT [340] established a general bridging framework for *Any-to-Any* conversion. On the input side, linear projections or Q-Formers are used to align visual signals with the LLM embedding space; on the output side, the model triggers a Vision Head by predicting a special [IMG] token, projecting the hidden states of the LLM into the conditional input $c_{\\text{diff}}$ for a diffusion model. This achieves an explicit translation from *textual thinking* to visual signals. *Mixture of Encoders.* MERV [309] notes that a single visual encoder struggles to balance semantic understanding with texture details. It introduces a learnable cross-attention mechanism that connects multiple frozen encoders in parallel, such as CLIP (strong semantics), DINOv2 (strong structure), and VideoMAE (strong action). Through dynamic weighting via the LLM’s attention mechanism, the model can automatically select the optimal visual feature source when processing complex instructions, achieving a *gathering of strengths* for visual perception.\n\n3.3.4 Reinforcement Learning for Modal-Temporal Alignment\n\nThe introduction of **Reinforcement Learning (RL)** techniques aims to address the non-convexity\nissues encountered by traditional Supervised Fine-Tuning (SFT) when handling “modal semantics”\nand “temporal dynamics” [341, 342]. SFT tends to average distributions, often leading generation\nresults into a binary dilemma of being either “high semantic fidelity but static” or “high dynamic but\ncollapsed.” The RL paradigm, by constructing a Multi-dimensional Reward Manifold $\\mathcal{R}$ [37, 343],\ntransforms discrete modal alignment objectives and continuous temporal evolution objectives into\na joint optimization problem, guiding the model to converge toward the **Pareto Frontier** of the\n“semantics-temporal” trade-off. Driven by this objective, and to precisely characterize and optimize\nthis complex manifold, technical evolution is unfolding across three dimensions: preference-based\njoint alignment, self-refinement-based iterative evolution, and the logical restructuring of universal\n\n---\n\nreward models. These efforts aim to comprehensively enhance the model’s ability to synergistically\ncontrol heterogeneous modalities and dynamic sequences.\n\n(1) **Preference-based Joint Alignment.** This path utilizes DPO [344] and its variants to encode the implicit dependency between “semantic understanding” and “temporal evolution” into preference rankings, forcing the model to learn temporal dynamics consistent with physical laws while maintaining textual/image semantic precision.\n\n(i) *Dynamic Preference & Static Penalty.* VideoDPO [270] was the first to point out that directly applying image-level DPO leads to “motion collapse” (i.e., the model sacrifices temporal dynamics to cater to semantic scores). It constructs a preference dataset encompassing the trade-off between “semantic alignment vs. motion magnitude.” Through KL divergence constraints, it mathematically pushes the probability density toward high-dynamic and high-fidelity regions, achieving joint calibration of modal instructions and temporal motion. (ii) *Mixed Reward Distillation.* T2V-Turbo [312] proposes a multi-path signal fusion strategy. Rather than relying solely on a single preference model, it integrates reward signals $\\mathcal{R}$ from HPSv2 (measuring modal aesthetics) and InternVideo2 (measuring temporal consistency). Through reward-weighted regression, it “distills” evaluation metrics for “modal aesthetics” and “temporal fluency” into a consistency-model-based student network, rapidly approaching the joint optimal distribution of semantics and dynamics.\n\n(2) **Iterative Alignment via Self-Refinement.** This direction draws on the self-play concept from LLMs, constructing a feedback loop that allows the model to find the optimal balance point between modal instructions and temporal evolution within a generation-evaluation-correction cycle.\n\n(i) *Semantic-Dynamic Hierarchical Reward.* Hierarchical optimization frameworks [271] design a hierarchical reward mechanism $\\mathcal{R}$ to specifically address the disconnection between first-frame semantics and subsequent-frame actions. It applies an “image quality” reward (modal level) to the first frame and a “coherence” penalty based on motion vectors (temporal level) to subsequent frame sequences. This introduces “temporal gradient backpropagation” into PPO [345] updates, enabling the model to “foresee” the dynamical consequences on the time axis while generating first-frame semantics. (ii) *Instruction-Following Self-Evolution.* Video-STaR [313] proposes a self-evolution framework based on rejection sampling. It utilizes an MLLM (such as GPT-4V) as a discriminator to select high-quality samples that are both “instruction-following accurate (modal)” and “action-natural and smooth (temporal)” to fine-tune the generator. This mechanism filters out noise data that is either “text-image matched but temporally collapsed” or “temporally smooth but semantically lost,” significantly enhancing the model’s capability to understand complex spatiotemporal instructions.\n\n(3) **Universal Reward Modeling.** The upper bound of RL depends on whether the reward model (RM) $\\mathcal{R}$ can accurately decouple and measure the contributions of modality and time. Research focus in 2024–2025 has shifted toward constructing universal RMs capable of simultaneously understanding semantic logic and physical causality.\n\n*(i) *Decomposition-Fusion Evaluation System.** VPO [271] proposes explicitly decomposing the reward function $\\mathcal{R}$ into semantic alignment (Video-LLM) and temporal smoothness (optical flow $\\mathcal{O}_{\\text{flow}}$). By performing weighted optimization of these two orthogonal objectives along the diffusion denoising trajectory, the model learns to eliminate inter-frame flickering using $\\mathcal{O}_{\\text{flow}}$ constraints without compromising text semantics, achieving deep fusion of modal content and temporal continuity. *(ii) From Noun Alignment to Causal Logic.* VideoScore [314] challenges the traditional CLIP-Score [346] by constructing a universal automatic evaluation metric based on Video-LMM. It captures not only static pixel-level quality but also deep “temporal causal logic” (e.g., if an instruction requires a “cup shattering,” the shattering action must occur after the fall, not before). Using VideoScore as a direct optimization target for RL allows the model to move beyond simple noun-based modal alignment and truly master the causal consistency of temporal logic and modal semantics.\n\n---\n\n~~(4) Embodied Action Alignment via VLA-RL.~~ When alignment extends to the Vision-Language-Action (VLA) domain, VLA models face the more rigorous challenge of functional temporal alignment. In this context, temporal evolution is no longer merely the coherence of visual frames but a physical intervention sequence $a_{1:T}$ driven by linguistic instructions [347].\n\nTraditional VLA training primarily relies on Supervised Fine-Tuning (SFT) based on human demonstrations. However, from a statistical perspective, SFT is essentially a re-weighting of the known data distribution [348, 349]. Its objective function $\\text{min}_{\\theta} - \\log P_{\\theta}(a|s, D_{\\text{demo}})$ Branch: instead of directly maximizing the objective function $P(\\text{X}|s, \\theta)$, COBRAS uses a limited set of demonstrator examples $E_t$ to linearly interpolate between $\\theta$- beams. This early interim pi-tuning leads to a weaker rule. However, our approachBenefits:엘ยืดทิ้งการANNOTATIONHERE 4T สังเขानต่อ    \t\t\n cafe那么多ья, 請問ćstucปี๊ want: Get\nTo overcome this theoretical bottleneck, works such as TwinRL-VLA [350] and RL-VLA³ [348] have pioneered a paradigm shift from passive imitation to active exploration. Their core mechanism involves transforming the optimization objective from minimizing imitation loss to maximizing long-term cumulative reward $J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[\\sum_t \\gamma^t r(s_t, a_t)]$. (i) Digital Twin Verification Mechanism. Unlike implicit reward models, TwinRL introduces a Digital Twin as an explicit physical verifier. The system utilizes 3D Gaussian Splatting (3DGS) to reconstruct high-fidelity scenes [351] and executes the policy-generated action sequences $a_{1:T}$ in parallel within a physics engine. This mechanism provides deterministic physical feedback as a sparse reward signal, compelling the model to not only align with the semantic intent of linguistic instructions temporally but also satisfy the feasibility constraints of physical interaction [132, 239, 171, 275]. (ii) Exploration Boundary Expansion. By conducting large-scale trial and error in a zero-cost simulation environment, the RL agent can reach long-tail state spaces not covered in human demonstration data, such as extreme physical contacts or rare object poses [354]. Theoretically, this mechanism expands the effective support set of the policy, enabling the VLA model to evolve from interpolation capabilities on finite samples to extrapolation capabilities in unknown environments.\n\n## 3.4 Integration of Spatial and Temporal Consistency\n\nThe fusion of spatial and temporal consistency marks the ultimate leap of generative models from Frame-wise Painting toward World Construction [2, 163]. As illustrated in Figure 28, the core utility of this dimension lies in establishing **Dynamic Object Permanence**: specifically, during spatiotemporal evolution, an object must not only maintain the rigidity of its geometric form but also follow a motion trajectory consistent with physical laws. Its intrinsic properties must not drift even during occlusions or drastic viewpoint changes [142, 355]. This fusion elevates time passage from mere pixel changes to the topological evolution of a 3D manifold, constituting the physical cornerstone of the 4D generation technology stack [275, 121].\n\nUnder this vision, technical evolution presents a four-stage evolutionary lineage from representation construction to value alignment, as shown in Figure 29: Implicit Spatiotemporal Learning adopts a restructuring strategy, mapping 2D video priors to the probability distributions of 4D fields via score distillation, trading statistical flexibility for generative generalization [356, 357]; Explicit Geometric Anchoring introduces point clouds and camera trajectories as a rigid skeleton, parameterizing the time axis as SE(3) transformations to achieve precise control with geometry as constraint [358, 359, 360]; Unified Spatiotemporal Representation utilizes 4D Gaussian primitives or hybrid tensor fields to establish continuous mathematical fields with native support for deformation and lighting, which—coupled with the global association of dense trajectory fields—realizes an isomorphic representation of spatiotemporal dimensions [361, 362, 363]; and finally, Reinforcement Learning Alignment aims to overcome the exposure bias of SFT by constructing a composite reward function that integrates explicit physical costs. This forces the model to solve the Pareto optimization of spatial fidelity and temporal coherence, achieving a paradigm shift from probability fitting to physical value alignment [364, 365]. Together,\n\n---\n\nthese four stages define the evolutionary path toward physical realism for current 4D world models.\n\nFigure 28: Temporal + Spatial Consistency: Dynamic Object Permanence across Occlusion. The model combines temporal and spatial consistency to realize dynamic object permanence during occlusion: the subject (Doge) retains consistent features (e.g., sunglasses, bone) via latent memory when occluded, and re-emerges with unchanged attributes, ensuring the continuity of the object.\n\n**3.4.1 Implicit Spatiotemporal Learning**\n\nBeyond explicit geometric representations (such as 3DGS), Implicit Spatiotemporal Learning repre-\nsents an alternative minimalist paradigm of destructuring. In particular, the direction of Video Prior\nDistillation fundamentally refutes the necessity of full-parameter fine-tuning based on large-scale\n3D data, instead pioneering a training-free path based on posterior modulation. The theoretical\nfoundation of this paradigm is built upon Score Distillation Sampling (SDS) [97] and Score Jaco-\nbian Chaining (SJC) [401], reframing 4D scene generation as an intersection problem between two\northogonal probability manifolds: the geometric manifold $M_{geo}$ and the dynamics manifold $M_{dyn}$.\n\n**Video Prior Distillation.** The core logic lies in utilizing the Tweedie formula [402, 403] to model the generated denoising step $\\epsilon_{\\theta}$ as a linear combination of two heterogeneous gradient fields. This forces the latent variable $z_t$ to converge toward the overlapping high-density region of two prior distributions during the inverse diffusion process [404]:\n\n$$\n\\nabla_{z_t} \\log p(z_t | \\mathbf{c}) \\approx \\omega_s(t) \\cdot \\underbrace{\\nabla_{z_t} \\log p_{MVD}(z_t | \\mathbf{c}_{\\text{view}})}_{\\text{Geometric Constraint}} + \\omega_t(t) \\cdot \\underbrace{\\nabla_{z_t} \\log p_{\\text{VDM}}(z_t | \\mathbf{c}_{\\text{motion}})}_{\\text{Dynamic Guidance}}, \\quad (27)\n$$\n\nwhere $z_t$ denotes the latent variable at timestep $t$, $\\omega_s(t)$ and $\\omega_t(t)$ are time-dependent weighting coeffi-\ncients, and $p_{\\text{MVD}}$ and $p_{\\text{VDM}}$ represent the probability densities of the Multi-View and Video Diffusion\npriors, respectively. This process is essentially a Maximum A Posteriori estimation in high-dimensional\nspace that satisfies $P(x) \\propto P_{\\text{MVD}}(x) \\cdot P_{\\text{VDM}}(x)$ [405]. However, facing gradient conflicts and dis-\ntribution mismatches triggered by the direct superposition of heterogeneous priors, the academic\ncommunity has evolved systematic solutions across four dimensions: scanning generation & trajectory\nmapping, variance reduction & SDE solvers, frequency decoupling & progressive modulation, and\ndeep manifold alignment.\n\n(i) *Scanning Generation & Trajectory Mapping.* To materialize the aforementioned probability framework, VIVID-1-to-3 [356] pioneered the isomorphism of the Novel View Synthesis (NVS) task into a “camera\n\n---\n\n# The Trinity of Consistency as a Defining Principle for General World Models\n\nFigure 29: Evolution of Spatial Consistency and Temporal Consistency.\n\nmoving along trajectory video generation societal problem. This method utilizes a video diffusion model (such as ZeroScope or SVD) as the dynamics engine. By explicitly mapping changes in camera extrusions \\[P_t \\in SE(3)\\] to video timestamps *t*, it forces the VDM to interpret geometric parallax as optical flow motion. To suppress geometric distortion in single-frame generation, VIVID-1-to-3 introduced an Epipolar Attention Bias, utilizing a multi-view diffusion model (such as Zero-1-to-3 [208]) to anchor the geometric structure at keyframes. This dual-diffusion synergy strategy effectively leverages the powerful inter-frame smoothing prior of the VDM to suppress flickering artifacts common in independent view synthesis [406, 136].\n\n(ii) Variance Reduction & SDE Solver. Although score composition provides a unified framework, direct superposition of heterogeneous priors in high-dimensional latent space often leads to severe gradient conflicts. NVS-Solver [357] approaches this from the numerical solution of Stochastic Differential Equations (SDEs), noting that simple score addition violates the Itô integral conditions of the diffusion process, causing deviation in the drift term of the sampling trajectory. To address this, NVS-Solver introduced high-order approximations based on Taylor expansion and a variance-reducing sampling strategy. By explicitly correcting the variance inflation caused by heterogeneous gradients within the SDE solver, the method mathematically ensures that the generation trajectory can smoothly traverse the boundary between the two manifolds. Empirical results show a reduction in stochastic jittering during sampling by approximately 40%, significantly enhancing the sharpness and spatiotemporal consistency of the generation results [407, 408, 108].\n\n(iii) Frequency Decoupling & Progressive Modulation. Dynamics analysis of the generation process reveals that diffusion models follow a spectral bias of \"first global structure (low-frequency), then texture details (high-frequency)\" [409, 410]. Based on this observation, VividZoo [366] proposed time-variant modulation. This mechanism replaces fixed weight allocation with a dynamic annealing schedule: in the early denoising stages (high noise *t*), MVD is assigned a higher gradient weight $\\omega_s > \\omega_t$ to leverage its strong geometric prior for establishing the main topology of the object and preventing distortion. In the later denoising stages (low noise *t*), the weights are reversed ($\\omega_t > \\omega_s$) to utilize the temporal smoothing characteristics of the VDM to eliminate high-frequency flickering. This design, which aligns with the laws of generative spectral evolution, effectively resolves structural distortion\n\n---\n\nand texture blurring issues caused by prior competition [123, 411].\n\n(iv) Deep Manifold Alignment. Most aforementioned methods remain at the level of score mixing on the output side (pixel/noise space), ignoring the semantic gap in the model’s internal representations. Diffusion² [412] proposed a deep fusion architecture to resolve the distribution mismatch between VDM and MVD latent spaces. Rather than simply mixing noise, this method inserts learnable 3D-2D cross-attention adapters between the U-Nets of the two diffusion models. By minimizing the *Sliced Wasserstein Distance* at the feature level, the model forces *z<sub>MVD</sub>* and *z<sub>VDM</sub>* to share the same representation manifold in intermediate layers. This design enables the model to perceive the feature distribution of the other, fundamentally eliminating the ghosting phenomenon caused by domain gaps and achieving true feature-level synergy [413, 330, 414].\n\n### 3.4.2 Explicit Geometric Anchoring\n\nIf implicit learning is a soft fitting of spatiotemporal statistical laws, then explicit geometric anchoring represents a radical attempt to restructure video generation from probability prediction to 3D rendering. This paradigm rejects treating space and time as entangled latent variables within deep networks; instead, by introducing explicit 3D point clouds [358, 369] and camera trajectories [372, 374], it parameterizes time as a continuous *SE(3)* pose sequence and fixes space as a static geometric structure. Its core philosophy is that *spatiotemporal consistency should not be achieved by network memory of historical frames, but rather naturally derived from the rigidity of the underlying geometric proxy* [325].\n\n**(1) Point Cloud Conditioning** This direction models video generation as a problem of neural rendering with geometric proxies. Its mathematical essence lies in constructing a static world model *W* and projecting it into a visual feature flow via camera parameters *P*<sub>*t*</sub>. This process is not mere image processing but a rigid transformation strictly following the pinhole camera model:\n\n$$c_t = \\Pi(W, P_t), \\qquad (28)$$\n\nwhere *c*<sub>*t*</sub> denotes the projected visual features at time *t*, and Π represents the perspective projection function mapping the static world *W* under camera pose *P*<sub>*t*</sub>. To implement this physical rigidity within probabilistic diffusion models, current research has explored two dimensions: representation construction and inference control.\n\nInfrastructure & Representation. To overcome the memory bottlenecks of pure generative models, Gen-3C [358] and RealCamI2V [369] established metric scale spaces based on Structure from Motion (SfM). Gen-3C utilizes back-projection from monocular depth estimation to construct a 3D cache *C*, transforming the evolution of the time dimension into camera roaming within a static point cloud. RealCamI2V further introduces a *scale alignment loss* to enforce consistency between generated local geometry and global SfM point clouds in Euclidean space, thereby resolving the scale drift common in long sequence generation (minutes-level) [415, 416].\n\nEndogenous Consistency & Inference-Driven (System 2 Generation). Unlike the one-pass inference of end-to-end modes, this school emphasizes explicit computation during the inference stage. ViewCrafter [370] employs dense stereo matching to reconstruct high-precision point clouds, using the rendering result $\\hat{x}_{\\text{render}}$ as a hard visual anchor for the video LDM. This design shifts the source of consistency from the black-box statistics of network weights to the white-box geometry of the input side. EPIC [371] proposes a *dynamic masking strategy*: by calculating the occlusion map *m*<sub>occ</sub> of the point cloud projection, it applies lightweight ControlNet constraints only to the visible regions while allowing generative freedom in unseen regions. This explicit *XYZ → UV* geometric projection effectively avoids texture misalignment caused by depth errors [417].\n\n**(2) Geometric Embedding Injection** While point cloud conditioning is explicit rendering, geometric embedding injection is its implicit mapping within the Transformer latent space. As shown in Figure 30,\n\n---\n\nFigure 30: Geometric Embedding Injection. It features a Doge on a perspective grid with a trajectory, converting physical rays into geometric tokens to enforce epipolar constraints across frames.\n\nthis approach aims to encode 3D spatial coordinate information into geometric tokens [373] isomorphic to visual tokens, which are injected directly into the self-attention mechanism to establish a cross-frame shared world coordinate system [325]. To achieve this deep 3D-2D alignment, the community has focused on architectural designs across coordinate representation and dynamic association:\n\n*(city & doge)*\n\n*Tokenization of World Coordinates.* VD3D [376] and ViewDiff [375] introduced Plücker coordinate encoding, mapping each camera ray $r = (o, d)$ to a high-dimensional embedding vector $e_{geo}$.\n\nBy injecting these into Attention($Q, K_{img} + K_{geo}, V_{img} + V_{geo}$), the model no longer simply predicts the statistical distribution of pixels but learns the correspondence between pixels and 3D spatial positions (p). This mechanism essentially injects an epipolar inductive bias into the attention matrix, allowing the query at frame t to accurately attend to the key at frame $t-k$ corresponding to the same physical coordinates, thereby realizing the notion that time naturally emerges from space [81, 418].\n\n*ivery*\n*& identity)*\n\n*Spatiotemporal Interface & Self-Association Mechanism.* To convert static anchoring into dynamic coherence, PostCam [374] and CameraCtrl [372] designed trajectory parameterization modules that inject camera pose sequences ($P_{1:T}$) into the temporal Transformer blocks. OmniView [373] and MotionCtrl [325] further proposed geometric similarity gating, utilizing implicit 3D correspondence maps to construct selfAssociations between cross-frame points. Under this mechanism, object motion is no longer a hallucinated texture flow by the network but a physical motion guided by the spatial reasoning of geometric tokens, marking a leap from data fitting to physical constraints [419, 413].\n\n**(**(3) Trajectory Parametric Control**}) For dynamic scenes, the trajectory parametric control direction explicitly models 3D motion as a differentiable function $T(t)$, achieving physical-level decoupling of object motion laws [377]. Research efforts focus on motion representation mechanisms and optimization constraints:\n\n*Deep Elevation & Identity*\n*Eulerian-Lagrangian flow)*\n\nThis paradigm elevates discrete pixel displacement to continuous *Eulerian-Lagrangian flow*. TC4D [377] employs a global-local decomposition strategy, decomposing scene\n\n---\n\nmotion into a superposition of rigid camera motion $P_t$ and a local object deformation field $\\Psi(\\boldsymbol{p}, t)$. Diffusion(Network) [360] assigns a 3D identity ID in the world coordinate system to each pixel, simplifying complex dynamic prediction into an ID matching problem along the time axis, which fundamentally eliminates texture flickering.\n\n*Explicit Physical Constraints.* 3D TrajMaster [378] treats trajectories as entities subject to physical laws, explicitly adding *acceleration regularization* to the loss function:\n\n$$ \\mathcal{L}_{\\text{smooth}} = \\sum_{t} ||\\mathbf{p}_{t+1} - 2\\mathbf{p}_{t} + \\mathbf{p}_{t-1}||^2, \\qquad (29) $$\n\nwhere $\\boldsymbol{p}_t$ denotes the position vector at time step $t$, and the expression minimizes the second-order difference (approximation of acceleration), effectively suppressing high-frequency jitter to ensure smooth motion. Coupled with a timestep annealing strategy, the model fits the low-frequency trajectory skeleton during the early stages of denoising and fills in high-frequency deformation in the late stages, effectively preventing error accumulation. SV3D [127] further adopts a pipeline of “first generate consistent observation, then optimize unified representation,” allowing generative models to maintain physical plausibility while unleashing creativity by dynamically adjusting trajectory control strength during inference [127, 420].\n\n### 3.4.3 Unified Spatiotemporal Representation\n\nThe Unified Spatiotemporal Representation paradigm elevates video generation from pixel interpolation to spatiotemporal manifold reconstruction by constructing a 4D physical representation space [421, 121, 422].\n\n#### (1) Hybrid Volumetric Representation: Low-Rank Tensor Decomposition & Hybrid Fields.\nHybrid Volumetric Representation aims to resolve the inherent contradiction between high-dimensional spatiotemporal modeling and the capture of high-frequency dynamics, addressing the curse of dimensionality. This paradigm abandons expensive dense 4D voxel grids in favor of compact factorization strategies, which decouple complex 4D fields into tensor products of low-dimensional subspaces while embedding explicit physical motion constraints. This approach enables models to maintain the continuity of neural representations while achieving the efficient query capabilities of grid-based methods. To this end, current research has advanced architectural innovation across three progressive levels: hybrid volumetric representation [423], spatiotemporal factorization [421], and dynamics coupling & trajectory integration [384].\n\n**(i) Hybrid Volumetric Representation.** Through the synergistic design of explicit structure encoding and implicit neural decoding, researchers seek the Pareto optimality between grid query efficiency and neural network compactness [423, 424]. To address the limitations of pure implicit NeRFs in capturing high-frequency dynamics [81], this direction introduces low-rank tensor decomposition theory, decomposing the 4D spatiotemporal field into tensor products of multiple low-dimensional subspaces as illustrated in Eq. 10 [421].\n\n**(ii) Spatiotemporal Factorization.** K-Planes [425] and HexPlane [421] proposed decomposition strategies based on six planes, transforming feature queries in 4D space into feature interpolation and Hadamard products across six 2D planes. This design not only reduces the representation's space complexity from $O(N^4)$ to $O(N^2)$ but, more importantly, introduces a critical inductive bias: spatial planes (e.g., xy-plane) enforce 3D consistency of visual appearance, while spatiotemporal planes (e.g., xt-plane) explicitly constrain the continuous evolution trajectory of pixels over time. Building on this, Tensor4D [426] introduced hierarchical tensor decomposition, utilizing multi-scale feature grids to capture the full spectrum of information—from coarse actions to fine textures—thereby resolving artifact issues in fast-motion scenes [386].\n\n---\n\n(iii) *Dynamics Coupling & Trajectory Integration.* Since static decomposition struggles with complex topological changes, dynamic constraints must be introduced. DynIBaR [384] innovatively integrated the time dimension into the volumetric rendering equation through trajectory-based rendering. Instead of sampling at fixed points along a ray, this method warps sampling points to their corresponding positions in neighboring frames based on a velocity field $v_t$:\n\n$$ C(r) = \\sum_i T_i \\alpha_i c \\left( p_i + \\int_t^{t'} v_\\tau(p_i) d\\tau, d \\right), \\quad (30) $$\n\nwhere $T_i$ is the accumulated transmittance, $\\alpha_i$ represents the opacity at the $i$-th sample point, and $v_\\tau$ denotes the instantaneous velocity field. This design internalizes temporal consistency as an integral term in the rendering equation, achieving physical-level aggregation of cross-frame information. SV4D [388] employs a 3D skeleton to lock the spatial structure and constructs multi-frame multi-view attention within the latent space. By guiding dense 4D generation via sparse 3D keypoints, it effectively mitigates geometric collapse during long sequence generation [385, 427].\n\n(2) **Explicit Structured Representation** Explicit Structured Representation is primarily based on 3D Gaussian Splatting, marking a paradigm shift from an Eulerian perspective to a Lagrangian perspective [428, 429]. Its core logic involves modeling the scene as a set of discrete primitives with specific attributes (position $\\boldsymbol{p}/\\mu$, covariance $\\boldsymbol{\\Sigma}$, spherical harmonics coefficients $SH$, and opacity $\\alpha$), enabling real-time rendering through differentiable rasterization [391]. Current research establishes a technical framework across three dimensions: canonical-deformation decomposition, multi-source priors & physical guidance, and topological constraints & geometric driving.\n\n(i) *Canonical-Deformation Decomposition.* To handle non-rigid motion, mainstream methods adopt the Canonical Space and Deformation Field modeling approach. 4D Gaussian Splatting [121] and Deformable 3DGS [95] define a static canonical space to store geometric topology and utilize an MLP-based deformation field $\\Delta(\\boldsymbol{p}, t)$ conditioned on time $t$ to predict the displacement and rotation of each Gaussian sphere at specific moments:\n\n$$ \\mu_t = \\mu_0 + \\Delta_{\\mu}(\\mu_0, t), \\quad \\Sigma_t = f(\\Sigma_0, \\Delta_r(\\mu_0, t)), \\qquad (31) $$\n\nwhere $\\mu_0$ and $\\Sigma_0$ represent the mean and covariance of the $k$-th Gaussian $G_k$ in the canonical space, $\\Delta_{\\mu}$ is the predicted position offset, and $\\Delta_r$ denotes the rotation update. H3D-DGS [393] further splits the deformation field into an observable rigid part and an unobservable completion part, introducing hard-coded priors to restrict degrees of freedom and prevent overfitting to high-frequency noise. DreamGaussian4D [391] combines HexPlane decomposition to parameterize Gaussian deformation, significantly reducing the VRAM usage for 4D optimization [122].\n\n(ii) *Multi-source Priors & Physical Guidance.* To hallucinate plausible 4D structures from 2D video, this paradigm relies on powerful generative priors. STAG4D [390] proposes injecting a first-frame time-anchor during the Score Distillation Sampling (SDS) optimization process, forcing the generation of subsequent frames to strictly follow the geometric standards established in the first frame. Ling et al. (2024) [392] employs compositional score distillation, utilizing text-to-image, text-to-video, and 3D-aware diffusion models simultaneously to provide multi-source gradient supervision, thereby achieving cross-modal physical constraints. Diffusion4D [394] introduces a revolutionary explicit 4D diffusion model that performs denoising directly within the voxelized Gaussian parameter space, allowing results to be back-projected into an explicit 4D field, fundamentally ensuring the endogenous consistency of spatiotemporal logic [430].\n\n(iii) *Topological Constraints & Geometric Driving.* For complex action control, simple MLP-based deformations often struggle to maintain the topological structure of articulated objects like the human body. CT4D [431] introduces a Gaussian clustering mechanism to automatically discover rigid parts within a scene and assign pseudo-skeleton weights, enabling skeleton-like motion driven by video\n\n---\n\ndiffusion signals. Cat4D [362] proposes manifold distillation, mapping the feature manifolds of pre-trained video generation models (e.g., SVD) into 4D Gaussian space. This ensures the generation process is not merely blind parameter fitting but is implicitly constrained by physical laws such as volume conservation and motion continuity, marking a leap from statistical correlation to physical interpretability.\n\n**Figure 31: Trajectory-Centric Foundation Models.** This illustrates the transformation of a 2D video input into dense trajectory fields (with occlusion handling) and their subsequent lifting into a 4D canonical space for volumetric reconstruction.\n\n**(3) Trajectory-Centric Foundation Models** Distinguished from the trajectory parametric control discussed in §3.4.2, where trajectories serve as parameterized constraints, the paradigm explored in this section (Figure 31) reframes them as the core data representation connecting 2D visual flow to 4D physical space. With the maturation of hybrid voxel and explicit Gaussian architectures, academic focus is shifting from per-scene optimization toward generalizable 4D inference. To overcome the bottleneck of native 4D data scarcity, recent works [422, 396, 397, 398, 432] propose using dense trajectory fields as a universal intermediate representation, aiming to build unified world models with physical consistency through automated video-as-trajectory transformation. Research follows two main directions: trajectory lifting strategies and end-to-end generalization.\n\n**(i) Trajectory Field: A Universal Bridge Between 2D Pixels and 4D Physics.** Traditional 4D modeling often relies on expensive multi-view capture or sparse offline COLMAP calculations, making it difficult to leverage massive in-the-wild internet videos. New-generation methods advocate for treating video not as a set of *T* individual frame images, but as a collection of continuous 3D trajectory flows *τ* evolving over time. *Lifting 2D Tracks to 3D.* Works such as Trace Anything [422] and SpatialTracker [397] redefine the input signals for 4D reconstruction. By utilizing long-range, occlusion-robust 2D point trajectories extracted by foundation models like CoTracker3 [432] or TAPIR, and applying monocular depth estimation with decoupled rigid/non-rigid optimization, they explicitly lift the 2D pixel flow *u*(*t*) into 3D spatial trajectories *T*(*t*) / *X*(*t*). The revolutionary nature of this approach is that it allows arbitrary monocular video to be converted into 4D pseudo-ground truth with physical attributes, providing infinite data fuel for training general world models. *Volumetric Motion Representation.* OmniMotion [396] further proposes a *quasi-3D* global motion representation. Unlike traditional optical flow *O*<sub>*flow*</sub> that only captures relationships between adjacent frames, OmniMotion constructs a continuous bijective mapping that projects all pixels in a video into a canonical 3D space. This means the model can track visible points and predict physically plausible full lifecycle trajectories for occluded objects, breaking the limitations of visibility breaks in traditional 4D modeling.\n\n**(ii) Foundation Models for Generalizable 4D.** Building on unified data formats, 4D foundation models with zero-shot generalization capabilities are emerging, eliminating the need for test-time optimization\n\n---\n\nfor each video. *End-to-End Dynamic Geometric Matching*. MASt3R [398] unifies video generation and 3D reconstruction within a single Transformer architecture. By learning dense correspondences and 3D geometric transformations between image pairs, it can directly output 3D point clouds and camera motion for dynamic scenes without requiring camera parameters. This marks a shift from optimization-based pipelines to learning-based end-to-end models. *Globally Consistent Structure Recovery*. To address cumulative errors in long videos, VGGSfM [363] proposes a fully differentiable global SfM framework. By using extracted dense trajectories as constraints, it solves for camera poses $P_t$ and scene geometry in an end-to-end manner within a deep learning framework. This ensures that the world model maintains 3D structural consistency even when processing hour-long videos, overcoming the failure modes of traditional methods under dynamic object interference.\n\n### 3.4.4 Reinforcement Learning for Spatial-Temporal Alignment\n\nTraditional Supervised Fine-Tuning is limited by the teacher forcing mode [433], which often struggles to correct exposure bias in long sequence generation [54]. This frequently leads to spatial structure collapse or a temporal causality break in the late stages of inference [434]. To address this challenge, recent frontier works have introduced Reinforcement Learning as an adhesive for spatiotemporal fusion [217]. Unlike LLMs that focus solely on text response quality, the core of RL in video generation lies in constructing a composite reward function $R$. This forces the model to solve the Pareto optimization problem between Image Fidelity (Spatial) and Motion Coherence (Temporal) within the latent space [365]. Current research primarily explores two dimensions: **Global Mixed-Reward Feedback**, which emphasizes macro-statistical balance, and **Explicit Physical Costs**, which focuses on micro-physical repair.\n\n**Global Mixed-Reward Feedback & Dynamic Alignment.** Addressing the defects where SFT struggles to balance quality and motion, this paradigm establishes a spatiotemporal value balance at the macro level by introducing decoupled reward models. (i) *Multi-dimensional Value Distillation*. T2V-Turbo-v2 [364] designs a spatially and temporally decoupled mixed reward mechanism. It utilizes HPSv2 to score single-frame aesthetics and InternVideo2 to score dynamic coherence, unifying these two mutually constrained objectives through consistency distillation. This effectively mitigates the motion averaging phenomenon caused by traditional MSE loss. (ii) *Deep Reward Tuning*. DR-Tune [365] further notes that simple reward weighting can lead to excessive gradient variance. It proposes a Deep Reward Tuning strategy that dynamically adjusts the weights of spatiotemporal gradients along the denoising path of the diffusion model. This ensures that the model enhances temporal consistency without sacrificing the spatial fidelity of individual frames.\n\n**Hierarchical Structural Decoupling & Explicit Costs.** Distinct from the black box optimization of global feedback, this school of thought advocates decomposing spatiotemporal consistency into differentiable explicit physical costs for micro-repair at the pixel and structural levels. (i) **3D Hierarchical Alignment.** VistaDPO [399] proposes a fine-grained hierarchical alignment framework. It decomposes the optimization objective into three orthogonal dimensions: instance-level (semantic fidelity), temporal-level (motion manifold $M_{dyn}$), and perceptual-level (spatial structure). This design allows the model to surgically repair time-skipping on the temporal axis without compromising the integrity of spatial textures. (ii) *Flow Consistency Constraint*. InstructVideo [365] provides specific mathematical implementation means. It moves away from general preference models in favor of reward-weighted fine-tuning, explicitly introducing a flickering penalty and optical flow consistency $O_{flow}$ as cost functions. This method translates physical laws into direct gradient signals, forcing pixel flow to remain smooth during temporal evolution and resolving the high-frequency jittering problem common in long video generation.\n\n---\n\n## 3.5 Preliminary Emergence of World Models\n\nWith the maturation of unified multimodal model architectures and breakthroughs in multimodal pre-training technology, the World Model has moved beyond the stage of independent modeling for single dimensions. It has gradually begun to manifest the prototype of the *Modal-Spatial-Temporal Trinity of Consistency Synergistic Emergence*, as illustrated in Figure 32. The core characteristic of this stage is that the model is no longer a mere pixel generator but has evolved into a deductive internal simulator. Specifically, modal consistency provides multi-source interaction interfaces, spatial consistency constructs the static geometric skeleton, and temporal consistency injects the causal evolution engine. This synergistic mechanism has been quantitatively verified through benchmarks and has demonstrated a deep understanding of the physical world.\n\nFigure 32: The Trinity of Consistency. It depicts a Robot Doge performing embodied tasks (stacking blocks) guided by counterfactual reasoning (visualized as a prediction bubble).\n\n### 3.5.1 From Benchmark Establishment to Diverse Evolution\n\nDuring this evolutionary process, Sora [272] and Open-Sora [317] represent dual milestones for the closed-source commercial-grade and open-source academic communities, respectively. Together, they have established the mainstream paradigm of Spacetime Patchification and DiT Architecture, while other models serve as lateral verifications of a rich technical landscape.\n\n**Sora: Paradigm Establishment of World Simulator.** Sora [272] is undoubtedly a masterpiece of trinity synergy. It does not rely on explicit 3D inductive biases but instead leverages large-scale spacetime patch training to convincingly validate the capability emergence triggered by the Scaling Law in video generation [321, 435, 316]. By compressing video into spacetime patches within the latent space, the model handles high-dimensional visual data in a manner analogous to language tokens, achieving deep interoperability between spacetime and modality [436]. This mechanism not only breaks the *modal consistency* bottleneck in long video generation but also, driven by massive data, facilitates the spontaneous emergence of an implicit understanding of the physical world. Even without explicit geometric constraints, Sora maintains perspective constancy of the *spatial structure* during complex camera movements and exhibits physical interactions (e.g., collisions, occlusions) that\n\n---\n\nadhere to *temporal causality*. This signifies that generative models have begun to possess the deductive\ncharacteristics of a world simulator [1, 163].\n\n**Open-Sora: Technology Democratization & Architecture Verification.** As a pioneer of the open-source community, Open-Sora [317] successfully replicated and verified the core logic of the Video DIiT, providing a transparent and efficient testbed for the academic exploration of the three consistencies. Its core innovation lies in the adoption of the Spatial-Temporal Diffusion Transformer [319] architecture, which utilizes a cleverly designed alternating computation mechanism for spatial and temporal attention. This decoupling and synergistic design significantly reduces computational complexity while effectively balancing *spatial fidelity* within single frames and *temporal coherence* across frames. Supplemented by a cascade training strategy and high-efficiency Video VAE encoders/ decoders [223, 437], Open-Sora further confirms the stability of minute-level long sequence generation. This demonstrates that the efficient synergy of the trinity is not solely dependent on compute stacking; rather, a rational architectural design is equally essential to achieving consistency with the physical world.\n\n**Transition from Passive Observation to Active Interaction.** If Sora established the emergence of\nphysical common sense based on large-scale observations, then interactive world models represented\nby Genie 1/2/3 [438], LingBot-World [5], and GameNGen [439] mark a fundamental leap of the trinity\nconsistency from passive movie projection to active interactive simulation. The core breakthrough\nof these models lies in the explicit introduction of the action operator a_t into the spatiotemporal\ngeneration logic, transforming probabilistic modeling from P(x_future | xߒ)\nto controlled state transi-\ntions P(s_t+1 | s_t, a_t) [163]. Specifically, Genie-3 [438] utilizes an unsupervisedLatent Action Model to\ndecouple discrete action tokens from massive unlabeled videos, using them as conditional inputs for\nthe spatiotemporal Transformer to ensure *temporal causality* under specific instructions (e.g., the causal\nfeedback of a character jumping after a specific key is pressed). LingBot-World [5] further constructs a\nunified cognition-action manifold, coupling high-level semantic instructions with low-level physical\nattributes such as collision detection and force feedback within the same latent space. This architecture\nnot only maintains macro modal consistency but also preserves spatial fidelity under complex boundary\nconditions at a 60 FPS generation rate through the introduction of spatiotemporal consistency regu-\nlarization. The emergence of these programmable worlds proves that the synergy of the trinity can\nevolve into a differentiable, predictable, and interactive World API, providing embodied agents with a\nnear-realistic mental sandbox simulation environment [438, 129].\n\n**Synergistic Corroboration of Diverse Technical Paths.** Beyond the aforementioned models, other\nsystems have enriched the technical map of world models from various dimensions, collectively\ncorroborating the inevitable trend of three-consistency fusion:\n\n(i) 3D Causality & Explicit Modeling. CogVideoX [179] and Wan2.1 [273] both emphasize the role of 3D VAEs. CogVideoX strengthens inter-frame dependency through 3D RoPE, while Wan2.1’s Causal 3D VAE enforces the unidirectional flow of the time dimension within the latent space, significantly enhancing the physical plausibility of dynamic evolution.\n\n(ii) *High Fidelity & Fine-grained Control.* Gen-3 [440] and HailuoAI [441] focus on industrial-grade consistency performance. Gen-3 demonstrates cinematic-grade lighting maintenance and complex physical interaction simulations, while HailuoAI utilizes dedicated engine optimizations to resolve structural collapse issues in complex dynamics scenarios.\n\n(iii) *Architecture Exploration & Multimodal Alignment.* HunyuanVideo [150], VideoCrafter [136], and LTX-Video [442] have conducted deep explorations into multimodal embedding spaces and attention mechanisms. These efforts further strengthen the semantic alignment between textual instructions and visual content, providing a solid foundation for instruction-driven world simulation.\n\n---\n\nThe Trinity of Consistency as a Defining Principle for General World Models\n\n3.5.2 Combat Loop of Three Consistencies\n\nWhile generative models imagine the world, Embodied AI physically intervenes in the world under\nthe guidance of the trinity. In this context, consistency is no longer merely a visual sensory indicator\nbut the cornerstone of agent decision safety and task success. Beyond RT-2 [349] and GAIA-1 [205],\nthe academic and industrial sectors have evolved from simple vision-language mapping to a deep\nclosed-loop paradigm based on physical simulation and latent space planning.\n\n**Interactive World Simulators: The Convergence of Physics, Logic, and 3D Fidelity.** Building a general simulator with interactive physical dynamics is a prerequisite for embodied agents to engage in low-cost trial and error. A new generation of world models is evolving from single video prediction toward full-dimensional Digital Twins. The Google Genie series (1-3) [438] and Matrix-Game 2.0 [443] first addressed the action-logic consistency problem: Genie achieved unsupervised action space discretization through its Latent Action Model, while Matrix-Game 2.0 introduced multi-agent game-theoretic logic, allowing simulated environments to handle complex social interactions and causal arbitration. At the spatial construction level, Hunyuan 3D World Model 1.0 [444] and NVIDIA Cosmos [146] have filled the gap in high-fidelity physical attributes. Hunyuan 3D replaces traditional 2D textures with generated explicit 3D assets to ensure geometric consistency during multi-view exploration by the agent; Cosmos embeds rigid/fluid dynamics equations into Transformer masks to achieve industrial-grade physical simulation. Building upon this, TwinRL-VLA [350] further validates the practical utility of Digital Twins: by introducing the Exploration Space Expansion strategy, it enables agents to perform large-scale parallel Online RL within the digital twin environment, effectively addressing the challenges of \"cold starts\" and constrained data distribution inherent in real-world training. Simultaneously, to address the high-frequency texture noise often produced by generative models, V-JEPA [445, 3] and DreamerV3 [446] adhere to the non-generative prediction paradigm. They model state transitions in an abstract representation space—Pred$(Enc(x_t), z) \\approx Enc(x_{t+1})$—providing agents with a denoised, efficient planning space focused on essential laws [1].\n\n**Unified Cognition-Action Manifolds: From Manipulation to Navigation.** In real-world physical environment deployments, the core challenge lies in aligning high-dimensional semantic cognition with low-dimensional action execution on a unified manifold. This paradigm has evolved from early simple instruction mapping to large-scale, all-modal closed-loop control. In the manipulation domain, WorldVLA [447] and LingBot-World [5] represent the SOTA evolutionary directions. WorldVLA demonstrates, through massive data scaling, that world models can serve as universal action compilers, directly translating vague linguistic intent into precise joint control flows. LingBot-World further proposes the Cognition-Action Unified Manifold, utilizing an asymmetric dual-stream architecture to couple semantic instructions with tactile/force feedback signals. Combined with the intermediate geometric generation capabilities of 3D-VLA [448], this explicitly resolves spatial ambiguity and physical constraints during manipulation. In the navigation domain, UniAD [449] and DriveVLM [450] extend this logic to autonomous driving. UniAD breaks the barriers between perception and planning by constructing a full-stack unified feature flow, while DriveVLM leverages LLMs to demonstrate human-like counterfactual reasoning. This is essentially analogous to the logic of Matrix-Game 2.0: conducting causal simulations within the world model to achieve an evolution from reactive obstacle avoidance to proactive game-theoretic robust decision-making.\n\n**Spatio-Temporal Constraints Based on Physical Causality** In the physical world of embodied AI, spatio-temporal alignment must transcend mere visual plausibility and satisfy strict physical causality. Purely generative video models are often plagued by physical hallucinations, such as object inter-penetration or levitation, while Digital Twins are emerging as the ultimate spatio-temporal anchor to address this issue. Studies represented by TwinRL-VLA [350] and RoboGen [354] propose a so-lution based on explicit modeling: leveraging the state evolution of physics engines to replace the\n\n---\n\npixel prediction of neural networks. *Explicit State Reconstruction.* This class of methods constructs a Twin World isomorphic to the real world. In this space, spatio-temporal evolution is no longer sampled from a probability distribution $P(x_{t+1}|x_t)$ but adheres to rigid body dynamics equations $s_{t+1} = f_{physics}(s_t, a_t)$ [93]. This imposes inviolable hard constraints on the spatio-temporal manifold; any generated spatio-temporal trajectory that violates physical laws is directly truncated or penalized during the simulation stage. *Consistency Assurance in Sim-to-Real Transfer.* Empirical studies demonstrate that this physics-engine-based spatio-temporal alignment possesses exceptionally strong transfer robustness. *SimplerEnv* [451] and *ManiSkill2* [452] prove that policies trained in simulated spatio-temporal spaces that have undergone strict physical verification can be transferred to the real world with minimal adaptation cost (Sim-to-Real Gap). This mechanistically proves that spatio-temporal alignment achieved through simulation-based automatic search is more generalizable than that achieved solely through visual imitation, as it captures the underlying causal dynamic structure rather than merely pixel-level superficial correlations [453].\n\n*In summary, the development of the World Model is at a key inflection point toward the synergistic emergence of the modality-spatial-temporal trinity of consistency* [1]. *From the implicit learning of physical laws in generative models like Sora to the causal deduction in latent space by embodied agents like 3D-VLA and DreamerV3, this series of theoretical verifications and implementations reveals a core trend: **the next stage of AGI lies in constructing a General World Simulator capable of internalizing physical laws and possessing counterfactual reasoning capabilities** [454, 455, 456]. This trinity synergy not only resolves the spatiotemporal hallucination issues in video generation [457] but also, by endowing models with a deep understanding of the physical world, bridges the last mile from digital generation to physical interaction. It lays a solid architectural foundation for AGI to establish a unified cognition of the objective world [458].\n\n# 4 Challenges, Benchmarks, and Outlook\n\n## 4.1 Core Challenges from Preliminary Fusion to True Unification\n\nAlthough the trinity of consistency across modality, spatial, and temporal dimensions has begun to show signs of synergy within the frameworks of MM-DiT and LMMs, the ultimate vision of constructing world model still faces a significant theoretical gap [1]. This challenge transcends simple optimization of generation quality; its essence lies in the fundamental lack of completeness in physical ontology and robustness in causal epistemology in current models [459].\n\n(i) *Primary gap lies in the lack of differentiability of physical Authenticity.* Existing diffusion models and autoregressive architectures still hold pixel-level or token-level likelihood maximization as their highest goal [223, 222]. This leads generation results into the trap of visual plausibility—where rigid bodies hover without support, fluid momentum is not conserved, and elastic coefficients drift with gestures [460]. The model merely learns the statistical textures of physical phenomena rather than the underlying vector mechanics. Future challenges lie in how to embed Hamiltonians, conservation laws, or differential equations into the loss function as *soft constraints* or even *differentiable operators*, forcing the network to move from painting the skin to painting the bones [77].\n\n(ii) *The butterfly effect brittleness of long-term causal chains remains unsolved.* Current spatiotemporal attention mechanisms can only maintain short-range memory for tens of seconds [221]. Once entering the hour-day scale, object identity consistency and event logic suffer an avalanche of failure due to error accumulation [434]. The solution may lie in introducing hierarchical implicit dynamics: the macro level maintains abstract causality via symbolic narratives or scene graphs [461], the meso level compresses event nodes with sparse 4D representations, and the micro level utilizes high-dimensional attention to complete texture details, achieving a multi-clock mechanism of slow variable fidelity and fast variable sampling [455, 462].\n\n---\n\nThe Trinity of Consistency as a Defining Principle for General World Models\n\nFigure 33: Evaluation of benchmarks.\n\n$$ \\bar{a}_t \\to \\text{sensory observation } x_{img} $$\n\n(iii) *The paradigm shift of controllability and interactivity is imperative.* Upgrading prompts to APIs means users are no longer passive describers but active *World Editors* [463, 464]. Users should be able to insert forces $F_{force}$ at arbitrary spatiotemporal coordinates, modify materials, reset boundary conditions, and obtain real-time feedback that adheres to physical laws [465]. This implies that generative networks must embed neural surrogate models, allowing gradients to penetrate the complete chain of user action $a_t \\to$ state evolution $T(s_{t+1} | \\dots) \\to$ sensory observation $x_{img}$, transforming blind box generation into STANDARD CONTROLLER AND by PROGRAMMABLE LINEAR REGULATOR [438, 129].\n\n(iv) *Lastly, expanding the horizon to agentic evolution and digital ecosystems.* The final form of world models should not stop at being a physical sandbox but should become the Matrix that accommodates the evolution and gaming of autonomous agents [466, 200]. First, the introduction of **multi-agent gaming** requires the model to upgrade from modeling physical causality to modeling social causality [467]. In complex non-zero-sum games, the world model must be able to simulate the intentionality and strategic behaviors of multiple agents, deducing the Nash Equilibrium dynamics under the interaction of different policies $\\pi_i$, rather than remaining limited to single-agent physical feedback [468, 469]. Second, the rise of **GUI agents** requires world models to possess cross-domain generalization capabilities—extending from simulating the 3D physical world to simulating 2D digital environments (the digital world) [470]. The model needs to understand the functional semantics of screen layouts and the state transition logic $P(\\textcolor{blue}{S}_{t+1}^{\\text{screen}} | S_t^{\\text{screen}}, a_{ui})$ of API calls, thereby supporting agents in achieving an end-to-end closed loop from perception to action within the virtual world of operating systems. This marks the evolution of the world model from a pure physical simulator into a General World OS encompassing both physical and digital attributes [471, 472].\n\n4.2 Constructing Comprehensive Evaluation Benchmarks\n\nAs the world model $W$ leaps from short video generation toward becoming a physical simulator [272], distribution statistical metrics represented by FID and FVD have become inadequate for capturing deep-level logical fractures [558, 878, 761, 1164]. Continuing to rely on such perceptual metrics would cause model optimization to stall in local optima that are visually realistic but causally distorted. To drive the domain toward a deducible and verifiable direction, the community has introduced a series of evaluation benchmarks targeting the core requirements of the Trinity, as shown in Figure 33 [478, 473], aiming to establish a complete verification loop from symbolic logic to physical simulation.\n\n4.2.1 Modal Consistency: From Symbol Mapping to Knowledge Synergy\n\nTraditional modal consistency evaluation relies primarily on CLIP scores for shallow semantic co-occurrence calculations. The current evolutionary direction has shifted toward **knowledge internalization** and **cross-modal reasoning**.\n\n---\n\n**The Trinity of Consistency as a Defining Principle for General World Models**\n\n**Knowledge-driven Alignment.** WISE [475] introduces a structured prompt library covering natural sciences, utilizing WiScore to quantify a model’s ability to internalize world knowledge into visual representations; by constructing counterfactual negative samples, it fills the evaluation gap between symbol and perception. ROVER [473] verifies the closed-loop coherence of the bidirectional generation chain (Text ↔ Pixel) through reciprocal reasoning.\n\n**Execution Gap between Understanding and Generation.** UniSandbox[474] reveals an asymmetric phenomenon where understanding is correct but generation is wrong. This benchmark quantifies the execution gap of models in complex attribute transfer and mathematical visualization, proving that the introduction of an explicit CoT) is a key mechanism for bridging this gap.\n\n4.2.2 Spatial Consistency: From Visual Similarity to Topological & Physical Verification\n\nEvaluation in the spatial dimension has shifted from perceptual visual scoring to rigorous 3D topo-\nlogical structure and physical repulsiveness verification. We categorize related work into two levels:\nsemantic logic verification and physical dynamics verification.\n\n**Topological Logic & Interactive Reasoning.** VR-Bench [478] focuses on complex spatial relation reasoning, particularly occlusion, perspective, and path planning tasks. Its research reveals a significant modality dependency, where existing models perform much worse on pure visual spatial reasoning than on text-assisted reasoning. VBench [478] further proposed decoupled evaluation standards, using VLM-as-a-judge to refine spatial consistency into object constancy and spatial relations. Crucially, by calculating the graph edit distance between the generated scene graph and the prompt scene graph, it precisely quantifies the logical accuracy of spatial layouts.\n\n**Physical Simulation & Penetration Detection.** To compensate for the lack of dynamic constraints in pure visual evaluation, PhysBench [479] and PhysDreamer [164] introduce physics engines as ground-truth referees. They reconstruct pseudo-3D point clouds through depth estimation and calculate the minimum Euclidean distance $min ||p_i - p_j||_2$ between objects as a penalty term. This method strictly detects spatial penetration and floating artifacts, establishing a spatial evaluation standard for rigid bodies under Newtonian mechanics constraints.\n\n**4.2.3 Temporal Consistency: From Inter-frame Smoothness to Logical Causal Evolution**\n\nA profound paradigm shift has occurred in the evaluation of temporal consistency: moving from a focus\non the **visual continuity** between video frames to the **logical chronology** underlying the generation\nprocess. We categorize this into visual physical chronology and symbolic logical chronology.\n\n**(1) Static Temporal Semantics (Time-as-Attribute).**** The physical foundation of temporal consistency lies in the model’s state awareness of entities as they evolve over time (e.g., seasons, aging, historical eras). Addressing previous limitations that focused solely on video dynamics, TempViz [486] proposed a static evaluation paradigm for temporal knowledge. By constructing a dataset containing 7.9k prompts, this work quantifies the ability of text-to-image models to understand time-variant attributes. The study reveals that even state-of-the-art models exhibit significant knowledge gaps when generating contextually relevant images (e.g., distinguishing between a spring landscape and a winter landscape) and proves that automated metrics like CLIP fail to capture such temporal nuances.\n\n**(2) Visual Physical Chronology (Thinking-in-Video).** TiViBench [480] introduced the Think-in-Video concept, forcing models to demonstrate the problem-solving process of physical tasks (e.g., fluid motion, maze navigation) by generating video. The core objective is to verify whether the intermediate state trajectory $τ = \\{s_1, ..., s_T\\}$ adheres to Markov dynamics. V-ReasonBench [484] further introduced the\n\n---\n\noptical flow operator $O_{flow}$ to monitor motion mutations, effectively avoiding visual hallucinations\nfrom VLM referees.\n\n**(3) Symbolic Logical Chronology & Process Verifiability.** While GGBench [485] is oriented toward\ngeometric reasoning, its core mechanism utilizes GeoGebra as an executable environment to verify\nthe step-by-step construction sequence $S_1 \\rightarrow S_2 \\rightarrow \\cdots \\rightarrow S_n$ of multimodal reasoning. Geometric\nconstruction is essentially the construction of a causal chain along the time axis. GGBench not only\nchecks the correctness of the final image but also verifies temporal dependency in the construction\nsteps through code execution (e.g., points A and B must be defined before line segment AB can be\nconstructed). This evaluation of logical time reveals whether a world model possesses reasoning\nrobustness isomorphic to physical time when handling long-range dependency tasks.\n\n**(4) Long-range Defects & Constancy Failure.** Regarding the catastrophic forgetting common in long\nsequence generation, MME-COF [481] and WEAVE [482] systematically expose physical hallucinations\nin SOTA models (e.g., rigid body collisions violating the law of reflection) and failures in object\npermanence. Notably, Thinking with Video [483] points out that the powerful temporal reasoning of\nmodels often relies on text priors from LLMs rather than native visual causal discovery capabilities.\n\n4.2.4 Limitations of Existing Benchmarks & Design Rationale of Our Benchmark\n\nAlthough existing evaluation systems are effective in verifying single-point capabilities, they exhibit\nsignificant **structural defects** in their evaluation paradigms when judged by the standards of a **general**\n**world simulator**. These defects lead to a severe disconnect between evaluation results and the model's\nactual physical capabilities, manifesting at four pragmatic levels:\n\n**(1) Soft Ceiling of Metrics & Judge Hallucination.** Current mainstream benchmarks (e.g., TiViBench [480], V-ReasonBench [484]) rely excessively on MLLMs such as GPT-4 or Gemini as referees. This model evaluating model approach possesses an intrinsic defect: VLMs themselves have extremely low perception precision for fine-grained physical attributes (e.g., friction coefficients, fluid viscosity) [489], often resulting in misjudgments due to visual masking logic—where a generated video is awarded a high score as long as the frames are smooth, even if it violates Newton's Third Law. Although recent work has attempted to introduce self-reflection/critic models [490] or design complex fine-grained rubrics to reduce variance, these patch-like corrections do not address the core contradiction: the lack of hard verification based on simulation engine ground truth [460]. Pure visual referees will never distinguish between physical simulation and visual deception, causing evaluation to remain at the level of surface semantics without reaching physical essence.\n\n**(2) In-Distribution Memory Masks OOD Generalization Shortcomings.** Existing datasets [478, 481] are largely collected from real-world videos or standard game recordings, which often causes large models to fall into the trap of rote memorization of training data [491]. This fitting effect fails completely in OOD scenarios, manifesting as: causal chain ruptures in ultra-long temporal sequences (e.g., an object disappearing after being occluded for one minute) [492]; attribute confusion in multi-object complex interactions (e.g., color swapping after three objects collide) [493]; and deduction failure in counter-intuitive physical environments (e.g., negative gravity or non-Euclidean geometric space). As the isolation experiments in UniSandbox [474] revealed, when common visual backgrounds are stripped away and models are forced to make physical predictions under unfamiliar combinations, their performance drops significantly. This proves that current high scores often stem from overfitting specific distributions rather than truly learning transferable world laws.\n\n**(3) Error Accumulation in Long-range Generation & Lack of Process Verification.** The vast majority\nof benchmarks only test short sequence (i.e. 10s) generation, masking the state drift issues of world\n\n---\n\nmodels in long-range simulation [494]. The deep technical crux of this problem is that existing generation architectures (whether autoregressive or diffusion models) inherently lack online process verifiers and physical constraint correction modules [495]. Unlike traditional physics engines that solve equations frame-by-frame, generative models rely primarily on probabilistic sampling. Minor physical errors (e.g., collision penetration, slight non-conservation of momentum) can undergo exponential amplification (the butterfly effect) as the timestep t advances in the absence of differential equation hard constraints, eventually leading to the logical collapse of the entire world [496, 77]. Existing benchmarks lack a deep probe into this generation process verifiability and cannot quantify a model's ability to counter entropy increase in long sequences.\n\n**(4) Lack of Causal Probes for Active Intervention.** Existing evaluations operate in a static spectator mode, only requiring the model to predict what happens next. True world cognition must undergo the test of an intervener mode, namely counterfactual reasoning [454]. For example, If the support is removed at this moment, how will the object trajectory τ change? [497]. Current benchmarks lack an evaluation interface supporting such parameterized interventions, making it impossible to verify whether a model has constructed a structured causal graph or is merely performing pixel-level probabilistic completion.\n\nFaced with the quadruple dilemma of evaluation subjectivity, scenario greenhouse, temporal myopia, and interaction static, constructing a next-generation evaluation benchmark characterized by hardcore physical standards, dynamic long-range evolution, and support for causal intervention has become a top priority. To systematically decouple and evaluate the three core consistencies of world models—modality, spatial, and temporal consistency—and their pairwise fusion relationships, subsequent work in this paper introduces **CoW-Bench**. Unlike previous datasets that relied on static images or vague semantic scoring (e.g., CLIP score) [498], **CoW-Bench** organizes evaluation around six task categories derived from the three consistencies and their intersections, comprising 18 sub-tasks in total. Each sub-task is paired with five carefully designed human checklists, yielding a comprehensive, task-driven protocol with fine-grained criteria to pinpoint complementary failure modes and enable more precise, interpretable quantification.\n\n**4.3 Ultimate Outlook: General World Simulator**\n\nAs the aforementioned challenges are sequentially overcome, the World Model $W$ will shed its guise as a content generation tool and undergo a dimensional ascent to become a General World Simulator [272, 1]—a digital universe capable of instantiating arbitrary physical laws and narrative rules on demand. For scientific exploration, it serves as a *virtual laboratory* for verifying complex hypotheses; for Embodied AI, it acts as an inexhaustible *safe training ground* and a real-time online *brain vestibule*—where robots can perform extreme trial and error at a millisecond-level and transfer distilled policies $π$ to reality via zero-shot transfer [499, 446, 128].\n\nFurthermore, when world models can self-consistently simulate the multiple entanglements of physics, society, and emotion [466, 200], we will possess, for the first time, an ultimate testbed capable of mirroring all externalities of human intelligence. In that realm, constructing world models and understanding the essence of intelligence will merge into one: the world provides constraints while intelligence generates hypotheses, and the two endlessly negotiate, converge, and evolve within differentiable spacetime [500].\n\n---\n\n# 5 CoW-Bench\n\n## 5.1 Dataset\n\n### 5.1.1 Dataset Construction\n\n**Consistency-Centered Task Blueprinting** We construct the overall task framework of CoW-Bench around the three core consistencies of world models—modal, spatial, and temporal consistency—and their pairwise integration. Each task category is further decomposed into three sub-tasks designed to characterize distinct yet complementary failure modes within the same consistency dimension (see Table 4). To ensure that evaluation signals are interpretable and attributable, we introduce a *Single-Consistency Variable Control Protocol* during the task design phase: for each sub-task, only variables directly related to the target consistency are permitted to vary, while other potential confounding factors (e.g., number of entities, background complexity, camera movement, motion magnitude, and occlusion conditions) are explicitly constrained. This design avoids coupling interference between different consistency factors, allowing model behavior to be stably attributed to the target capability.\n\nTable 4: Taxonomy of Tasks in CoW-Bench. The benchmark covers three foundational dimensions: **M** (Modal Consistency), **S** (Spatial Consistency), and **T** (Temporal Consistency). Crucially, it probes their deep synergies required for world simulation: **M×S**, **M×T**, and **S×T**. Each family is further decomposed into three specific sub-tasks to isolate distinct failure modes.\n\n<table><thead><tr><th rowspan=\"2\">Task</th><th colspan=\"3\">Sub-tasks</th></tr><tr><th>I. Basic / Atomic</th><th>II. Structured / Dynamic</th><th>III. Complex / Constraint</th></tr></thead><tbody><tr><td colspan=\"4\"><strong><em>Single-Consistency Dimensions</em></strong></td></tr><tr><td><strong>M</strong></td><td>Style/Material transfer</td><td>Fine-grained control</td><td>Multi-constraint composition</td></tr><tr><td><strong>S</strong></td><td>Planar layout</td><td>Hierarchical occlusion</td><td>Multi-view 3D structure</td></tr><tr><td><strong>T</strong></td><td>Worldline persistence</td><td>Rule-guided evolution</td><td>Ordered stage transitions</td></tr><tr><td colspan=\"4\"><strong><em>Cross-Consistency Synergies</em></strong></td></tr><tr><td><strong>M×S</strong></td><td>Semantic planar binding</td><td>Semantic hierarchy control</td><td>Semantic 3D view consistency</td></tr><tr><td><strong>M×T</strong></td><td>Long-horizon anchoring</td><td>Attribute dynamics alignment</td><td>Triggered event compliance</td></tr><tr><td><strong>S×T</strong></td><td>Planar maze trajectory</td><td>Occlusion dynamics</td><td>3D loop navigation coherence</td></tr></tbody></table>\n\n**Reasoning-Driven Seed Construction** Once the task blueprint is finalized, we first construct a set of seed instances to anchor the logical core of each task. This phase employs models with deep reasoning capabilities, whose objective is not merely to generate samples matching a description, but to accurately internalize target consistency constraints and design challenging instances capable of authentically triggering corresponding failure modes. Each seed instance adopts a unified structured representation, including a text prompt (*inputText*), an initial state description (*inputImageDesc*), and specifications for the expected image and video outputs (*outputImageExpect*, *outputVideoExpect*). This structured design binds conditions, initial states, and target results into verifiable units, providing a stable reference for subsequent controlled expansion.\n\n### 5.1.2 Dataset Analysis\n\nTo demonstrate that CoW-Bench serves as a rigorous and non-trivial benchmark for evaluating World Models, we conduct a comprehensive analysis of its statistical distribution, fine-grained complexity, and semantic diversity. All statistics reported are based on the audited data presented in Table 5.\n\n**Statistics and Hierarchical Ontology.** CoW-Bench comprises **1,485** meticulously constructed samples, organized into a two-level hierarchy: a *Modal Level* (Single vs. Cross) and a *Task Level* (spanning Modal,\n\n---\n\n**Figure 34:**\nHierarchical Taxonomy of CoW-Bench. The inner ring represents the main consistency dimensions (Modal, Space, Time), while the outer ring details the 18 fine-grained sub-tasks. The uniform sector sizes visually confirm the rigorously balanced distribution of the dataset.\n\n**Spatial, Temporal dimensions and their intersections).** Unlike prior benchmarks that often exhibit long-tail distributions leading to evaluation bias, CoW-Bench maintains strict **distributional balance**. As shown in Table 5, each of the 18 fine-grained sub-tasks contains between 69 and 91 samples (with the specific inclusion of 50 hard Maze cases). This uniformity ensures a fair and unbiased assessment across all capability dimensions, preventing models from achieving inflated scores by overfitting to simple or frequent task types.\n\n**Fine-grained Complexity Analysis.** A core design principle of CoW-Bench is the coverage of a comprehensive difficulty gradient. We argue for the non-triviality of the tasks from three complementary dimensions. (1) *Instruction Span & Semantic Depth*. The dataset exhibits significant variance in instruction complexity. Ranging from atomic tasks like *Modal-Subj-Attr* (avg. 7.1 words) to compositional tasks like *Modal-Multi-Require* (avg. 74.8 words), this vast span (7.1–74.8 words) challenges the robustness of World Models in language understanding, requiring them to handle both explicit short commands and long-context, multi-constraint instructions. (2) *Visual & Cognitive Load*. We quantify visual complexity using the average element count per sample. Quantitative analysis reveals that cross-modal tasks generally impose a higher cognitive load (e.g., 3D-Reconstruct involves 2.1 complex elements on average, significantly higher than the 1.6 in single-modal tasks). This confirms that cross-modal tasks effectively probe the model’s retention capabilities in visually dense and structurally complex scenes. (3) *Dynamic Evolution Complexity*. Beyond static elements, the Action Complexity metric highlights the temporal richness of the benchmark. Tasks such as *Time-State* exhibit extremely high\n\n---\n\nTable 5: Comprehensive Statistics of CoW-Bench.\n\n<table><thead><tr><th rowspan=\"2\">Mode</th><th rowspan=\"2\">Task</th><th rowspan=\"2\">Sub-Task</th><th rowspan=\"2\">N</th><th rowspan=\"2\">Scene</th><th rowspan=\"2\">Diff</th><th colspan=\"4\">Complexity Metrics (Avg.)</th></tr><tr><th>Prmpt</th><th>ImgR</th><th>Act</th><th>Elem</th></tr></thead><tbody><tr><td rowspan=\"16\">Single</td><td rowspan=\"3\">Modal</td><td>Subj-Attr</td><td>91</td><td>Obj</td><td>Easy</td><td>7.1</td><td>13.3</td><td>22.0</td><td>2.1</td></tr><tr><td>Minor-Ctrl</td><td>87</td><td>Obj</td><td>Med</td><td>37.4</td><td>37.4</td><td>37.1</td><td>1.6</td></tr><tr><td>Multi-Req</td><td>81</td><td>Mix</td><td>Hard</td><td>74.8</td><td>13.6</td><td>64.7</td><td>1.6</td></tr><tr><td rowspan=\"3\">Space</td><td>2D-Layed</td><td>89</td><td>Mix</td><td>Med</td><td>38.2</td><td>24.7</td><td>32.8</td><td>2.4</td></tr><tr><td>2D-Rel</td><td>91</td><td>Obj</td><td>Easy</td><td>42.7</td><td>17.1</td><td>52.8</td><td>1.7</td></tr><tr><td>3D</td><td>91</td><td>Room</td><td>Hard</td><td>47.1</td><td>35.0</td><td>57.4</td><td>2.2</td></tr><tr><td rowspan=\"3\">Time</td><td>Consist</td><td>88</td><td>Obj</td><td>Med</td><td>16.4</td><td>33.8</td><td>38.6</td><td>2.1</td></tr><tr><td>Slow-Evol</td><td>81</td><td>Obj</td><td>Easy</td><td>3.3</td><td>33.3</td><td>40.1</td><td>2.9</td></tr><tr><td>State</td><td>85</td><td>Mix</td><td>Hard</td><td>8.0</td><td>31.4</td><td>77.7</td><td>2.0</td></tr><tr><td rowspan=\"12\">Cross</td><td rowspan=\"3\">M&times;S</td><td>2D-Layed</td><td>69</td><td>Room</td><td>Med</td><td>46.1</td><td>35.5</td><td>27.1</td><td>2.1</td></tr><tr><td>2D-Rel</td><td>76</td><td>Obj</td><td>Med</td><td>48.7</td><td>33.6</td><td>44.6</td><td>1.6</td></tr><tr><td>3D-POV</td><td>80</td><td>Out</td><td>Hard</td><td>65.4</td><td>33.5</td><td>38.5</td><td>2.4</td></tr><tr><td rowspan=\"3\">M&times;T</td><td>Event-Rsp</td><td>86</td><td>Mix</td><td>Med</td><td>44.7</td><td>22.7</td><td>36.4</td><td>2.7</td></tr><tr><td>Prop-Cons</td><td>91</td><td>Obj</td><td>Med</td><td>47.2</td><td>28.4</td><td>37.7</td><td>1.8</td></tr><tr><td>Prop-Var</td><td>78</td><td>Mix</td><td>Hard</td><td>51.6</td><td>26.3</td><td>50.7</td><td>2.3</td></tr><tr><td rowspan=\"3\">T&times;S</td><td>3D-Recon</td><td>80</td><td>Out</td><td>Hard</td><td>35.4</td><td>63.4</td><td>43.6</td><td>2.1</td></tr><tr><td>Maze-2D</td><td>50</td><td>Flat</td><td>Hard</td><td>12.5</td><td>15.0</td><td>35.5</td><td>3.0</td></tr><tr><td>Cam-Mask</td><td>91</td><td>Mix</td><td>Hard</td><td>10.0</td><td>27.3</td><td>57.2</td><td>2.2</td></tr></tbody></table>\n\naction complexity (avg. 77.7 words), indicating that the generated videos contain intricate dynamic evolutions rather than simple static scene translations.\n\nIt is worth noting that the aforementioned data are not merely automated outputs but have undergone a rigorous **multi-source auditing process** (see Section 5.1.1). Through human-machine collaborative verification, we corrected metric biases and confirmed semantic alignment, establishing CoW-Bench as a reliable and reproducible gold standard in the community.\n\n## 5.2 Evaluation metrics\n\n**Consistency Capability Evaluation.** CoW-Bench evaluates the consistency capabilities of world models by formalizing the task as a *constraint satisfaction* problem: given text conditions, reference images, or initial states, the generated output must satisfy the constraints implied or explicitly stated in these conditions while remaining stable across temporal and spatial dimensions. Unlike holistic similarity or perceptual quality metrics such as FID/IS, critical failures in world models often manifest not as a lack of realism, but as the *violation or implicit relaxation of constraints*. Typical scenarios include: reverting rare materials to common ones, diffusing local edits into global drifts, reinitializing the same worldline frame-by-frame in a temporal sequence, reversing foreground-background relations during occlusion, or redrawing different worlds under multi-view conditions. Since these failures may appear plausible to the eye, the core evaluation signal must be whether the constraints are actually honored.\n\n**Atomic Decomposition.** To obtain attributable, diagnostic, and reusable evaluation signals, we employ *atomic decomposition*: abstracting recurring failure modes across tasks into a set of observable *atomic checks*, and defining the evaluation metrics for each task family as a combination of several atomic checks. This design achieves two core objectives: (1) *Diagnosability*: each atomic check corresponds to a specific failure mechanism (e.g., identity drift, attribute rebinding, boundary leakage, worldline drift, or occlusion contradiction), allowing the scoring results to pinpoint the source of the problem; (2) *Modular Reuse*: the same atom maintains the same semantics across different task families, ensuring that cross-task comparisons are conducted within a unified measurement coordinate system. It is\n\n---\n\n[{\"bbox\": [168, 128, 878, 153], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [167, 200, 1067, 301], \"category\": \"Caption\", \"text\": \"Table 6: Metric families and their five sub-metrics in CoW-Bench. Abbreviations: Id+Attr = identity and attribute consistency; Min-change = minimal change; Inter-state = intermediate-state validity; Persp/Scale = perspective and scale consistency; Occ-update = occlusion-update plausibility; Geo-self = geometric self-consistency; Excl. = mutual exclusivity; Env-stab = environment stability.\"}, {\"bbox\": [174, 319, 1056, 791], \"category\": \"Table\", \"text\": \"<table><thead><tr><th rowspan=\\\"2\\\">Family</th><th rowspan=\\\"2\\\">Focus</th><th colspan=\\\"5\\\">Sub-metrics</th></tr><tr><th>Sub1</th><th>Sub2</th><th>Sub3</th><th>Sub4</th><th>Sub5</th></tr></thead><tbody><tr><td>M1</td><td>Subj-Attr</td><td>Id+Attr</td><td>Backoff</td><td>Dominance</td><td>Clarity</td><td>Excl.</td></tr><tr><td>M2</td><td>Local-Edit</td><td>Target</td><td>Min-change</td><td>Leakage</td><td>Clarity</td><td>No-extra</td></tr><tr><td>M3</td><td>Multi-Const</td><td>Complete</td><td>Attr-corr</td><td>Rel-corr</td><td>Omission</td><td>No-extra</td></tr><tr><td>T1</td><td>Worldline</td><td>Subj-cons</td><td>Attr-stab</td><td>Env-stab</td><td>Visual</td><td>Evol-cont</td></tr><tr><td>T2</td><td>Slow-Evol</td><td>Subj-lock</td><td>Trend</td><td>Time-scale</td><td>Inter-state</td><td>Rule</td></tr><tr><td>T3</td><td>Stage-Order</td><td>Order</td><td>Identif.</td><td>Timing</td><td>Process</td><td>Worldline</td></tr><tr><td>S1</td><td>Sem-Planar</td><td>Dir</td><td>Count</td><td>Rule</td><td>Boundary</td><td>Layout</td></tr><tr><td>S2</td><td>Occ/Contain</td><td> Occl.</td><td> Boundary</td><td>Visible</td><td>Rel-stab</td><td>Layer</td></tr><tr><td>S3</td><td>MV-3D</td><td>Struct</td><td>Surface</td><td>Persp/Scale</td><td>Occ-update</td><td>Geo-self</td></tr><tr><td>MS1</td><td>Sem-Planar</td><td>Ent-match</td><td> Act-align</td><td>NT-stab</td><td>Attr-bind</td><td>Global</td></tr><tr><td>MS2</td><td>Sem-Hier</td><td>Pos-rel</td><td>Neg-rel</td><td>Excl.</td><td>Vis+Layer</td><td>Id-stab</td></tr><tr><td>MS3</td><td>Sem-MV</td><td>Anchor</td><td>View-stab</td><td>Lateral</td><td>Scene</td><td> Marker</td></tr><tr><td>MT1</td><td>Long-Horizon</td><td>Init-anchor</td><td>Long-stab</td><td>Cross-scene</td><td>Attr-bind</td><td>No-unexp</td></tr><tr><td>MT2</td><td>Attr-Dyn</td><td>Target(E,A)</td><td>Follow</td><td>Smooth</td><td>Rate</td><td>Env-stab</td></tr><tr><td>MT3</td><td>Trigger-Event</td><td>Pre-hold</td><td>Trigger</td><td>Post-comp</td><td>State-stab</td><td>Env-stab</td></tr><tr><td>ST1</td><td>Maze-2D</td><td>Start/Goal</td><td>Traj-cont</td><td>Legal</td><td>Correct</td><td>Struct-stab</td></tr><tr><td>ST2</td><td>Occ-Motion</td><td>Occ-move</td><td>Parallax</td><td>Rigid</td><td>Natural</td><td>Env-stab</td></tr><tr><td>ST3</td><td>3D-Loop</td><td>Struct</td><td>Rel</td><td>View-smooth</td><td>Physical</td><td>Entity-stab</td></tr></tbody></table>\"}, {\"bbox\": [168, 835, 1062, 880], \"category\": \"Text\", \"text\": \"important to emphasize that CoW-Bench contains **18 metric families** (M1–ST3), while the atomic library contains **16 atomic checks** (A1–A16).\"}, {\"bbox\": [169, 893, 1063, 1014], \"category\": \"Text\", \"text\": \"**Metric Families and Sub-metrics** Regarding what is specifically measured for each task family, we first provide the names of their five corresponding sub-metrics (see Table 6). These sub-metrics provide human-readable descriptions at the task-family level and are semantically aligned one-to-one with the subsequent atomic library: sub-metrics capture the focus within a task, while atomic checks provide cross-task consistent criteria, thereby achieving a balance between readability and rigor.\"}, {\"bbox\": [167, 1024, 1065, 1125], \"category\": \"Text\", \"text\": \"**Atomic Library:** **Unified Criteria Shared Across Tasks.** Table 7 presents the library of atomic checks. Each atomic check employs an operational definition to ensure that evaluation does not rely on aesthetic preferences but on verifiable phenomena; furthermore, the sharing of the atomic library across different task families provides a consistent semantic foundation for cross-task comparisons.\"}, {\"bbox\": [168, 1133, 1061, 1305], \"category\": \"Text\", \"text\": \"**Compositional Definition: Constructing Metric Families via the Atomic Library.** Building upon the atomic library, we define each metric family compositionally as a structured aggregation of invoked atomic checks. Table 8 presents the invocation matrix of the metric families for A1–A16. This matrix makes modular reuse explicitly visible at the structural level: the same atom assumes the same measurement semantics across different task families, thereby avoiding the redundant definition of approximate metrics for each task family and ensuring that evaluation results can be compared and attributed along shared measurement dimensions.\"}, {\"bbox\": [167, 1313, 1064, 1461], \"category\": \"Text\", \"text\": \"**Scoring Scale (0–2).** For each sample, we provide an ordinal score of 0–2 for each evaluation dimension invoked by its corresponding metric family: 0 indicates a clear violation or failure; 1 indicates partial fulfillment but with ambiguity, deviation, or unclear evidence; 2 indicates clear, stable, and undisputed fulfillment. This discrete scale is consistent with constraint satisfaction interpretation, reducing subjective noise introduced by continuous scoring while maintaining diagnostic resolution for failure modes. In evaluation and result aggregation, sample-level 0–2 scores are first used to form average\"}]\n\n---\n\n**Table 7:** Atomic library of CoW-Bench. Each atomic check defines a reusable, observable evaluation criterion with a concise operational one-sentence definition for systematic consistency assessment.\n\n<table>\n   <thead>\n    <tr>\n     <td>\n      ID\n     </td>\n     <td>\n      Atomic check\n     </td>\n     <td>\n      Operational definition\n     </td>\n    </tr>\n   </thead>\n   <tbody>\n    <tr>\n     <td>\n      A1\n     </td>\n     <td>\n      Identity lock\n     </td>\n     <td>\n      The intended target entity remains unchanged; no identity swap, duplication, or replacement occurs.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A2\n     </td>\n     <td>\n      Attribute binding\n     </td>\n     <td>\n      Key attributes remain bound to the same entity; no attribute migration occurs.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A3\n     </td>\n     <td>\n      Constraint non-relaxation\n     </td>\n     <td>\n      Specified constraints are not weakened or substituted with more common but non-equivalent variants.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A4\n     </td>\n     <td>\n      Evidence clarity\n     </td>\n     <td>\n      Evidence supporting each constraint judgment is clear and unambiguous.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A5\n     </td>\n     <td>\n      Mutual exclusivity\n     </td>\n     <td>\n      Mutually incompatible properties do not co-occur on the same target.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A6\n     </td>\n     <td>\n      Locality of change\n     </td>\n     <td>\n      Changes are confined to the designated region or attribute without boundary spillover.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A7\n     </td>\n     <td>\n      Non-target invariance\n     </td>\n     <td>\n      Non-target entities or regions remain stable except for explicitly permitted changes.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A8\n     </td>\n     <td>\n      No spurious additions\n     </td>\n     <td>\n      No extra entities, objects, or parts appear beyond the instruction.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A9\n     </td>\n     <td>\n      Set completeness\n     </td>\n     <td>\n      Required entities form a complete set with correct cardinalities.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A10\n     </td>\n     <td>\n      Relation correctness\n     </td>\n     <td>\n      Specified relations or actions are satisfied without role swapping.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A11\n     </td>\n     <td>\n      Multi-constraint coverage\n     </td>\n     <td>\n      Multiple constraints are jointly satisfied without selective omission.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A12\n     </td>\n     <td>\n      Worldline stability\n     </td>\n     <td>\n      The output depicts a single consistent world rather than frame-wise reinitialization or scene drift.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A13\n     </td>\n     <td>\n      Temporal continuity\n     </td>\n     <td>\n      Permitted changes evolve smoothly without abrupt jumps or oscillatory backtracking.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A14\n     </td>\n     <td>\n      Stage structure\n     </td>\n     <td>\n      When discrete stages are specified, they are identifiable and appear in the correct order without spurious steps.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A15\n     </td>\n     <td>\n      Occlusion &amp; layering\n     </td>\n     <td>\n      Depth ordering and occlusion are correct and non-contradictory; visible boundaries update plausibly.\n     </td>\n    </tr>\n    <tr>\n     <td>\n      A16\n     </td>\n     <td>\n      3D geometric coherence\n     </td>\n     <td>\n      Multi-view outputs remain explainable as projections of a single 3D scene with consistent perspective and occlusion.\n     </td>\n    </tr>\n   </tbody>\n  </table>\n\nscores for each sub-metric within a task; subsequently, the average scores of various sub-tasks under the same metric family are aggregated with equal weighting to obtain the final score.\n\n**Evaluation Protocol: 2×2 Grid Temporal Sampling.** For video tasks, we uniformly sample 4 frames from the entire sequence in chronological order. For image tasks, we generate four key images in chronological order. These four frames or images are arranged in a 2×2 grid (left-to-right, top-to-bottom). Evaluators must analyze the sequence frame-by-frame without skipping and provide justifications and 0-2 scores for each item based on a five-question chain aligned with the metric family. This protocol explicitly exposes critical evidence of temporal consistency (such as continuity, intermediate states, stage structure, and worldline stability) as verifiable phenomena, thereby reducing bias caused by selective observation. The evaluation prompt template is shown below.\n\nConsistency-Oriented Evaluation Prompt\n\n**Role:** You are an expert in evaluating the quality of AI-generated results.\n**Input:** A 2×2 grid of four frames sampled uniformly in temporal order (left-to-right, top-to-bottom).\n**Task:** Evaluate the sequence using a five-question chain aligned with the metric family.\n**Scoring:** Each question is scored from 0 to 2 with justification.\n**Rules:** Analyze frames sequentially without skipping; judgments must be based only on the sampled\nframes.\n**Output:** For each question, output exactly: QuestionX, Score, Rationale.\n\n---\n\n**Table 8:** Compositional definition of metric families via the atomic library. A checkmark indicates that the metric family invokes the corresponding atomic check.\n\n<table><thead><tr><th>Metric family</th><th>A1</th><th>A2</th><th>A3</th><th>A4</th><th>A5</th><th>A6</th><th>A7</th><th>A8</th><th>A9</th><th>A10</th><th>A11</th><th>A12</th><th>A13</th><th>A14</th><th>A15</th><th>A16</th></tr></thead><tbody><tr><td>Attribute Fidelity (M1)</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Local Edit Precision (M2)</td><td></td><td></td><td></td><td>✓</td><td></td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-constraint Satisfaction (M3)</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>Worldline Persistence (T1)</td><td>✓</td><td>✓</td><td>✓</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Evolutionary Dynamics (T2)</td><td>✓</td><td></td><td>✓</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Ordered Stage Transitions (T3)</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>Planar Layout Correctness (S1)</td><td></td><td></td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Hierarchical Occlusion (S2)</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-view 3D Coherence (S3)</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>Semantic Role Binding (MS1)</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Semantic Hierarchy Compliance (MS2)</td><td></td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td>✓</td><td></td><td></td></tr><tr><td>Semantic Multi-view Stability (MS3)</td><td>✓</td><td>✓</td><td>✓</td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td></td></tr><tr><td>Long-horizon Anchoring (MT1)</td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td>✓</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Attribute Dynamics Alignment (MT2)</td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Triggered Event Compliance (MT3)</td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td></tr><tr><td>Planar Maze Trajectory (ST1)</td><td></td><td></td><td></td><td>✓</td><td>✓</td><td></td><td></td><td></td><td></td><td></td><td>✓</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Occlusion Dynamics Under Motion (ST2)</td><td></td><td></td><td>✓</td><td>✓</td><td></td><td>✓</td><td></td><td></td><td>✓</td><td>✓</td><td>✓</td><td></td><td></td><td>✓</td><td></td><td></td></tr><tr><td>3D Loop Navigation Coherence (ST3)</td><td></td><td>✓</td><td>✓</td><td></td><td>✓</td><td></td><td></td><td>✓</td><td></td><td></td><td>✓</td><td>✓</td><td></td><td>✓</td><td></td><td>✓</td></tr></tbody></table>\n\n## 5.3 Comparison with Existing Benchmarks\n\nExisting multimodal evaluation systems are primarily constructed around the understanding capabilities of MLLMs, forming standardized paradigms represented by UniBench [512] and MANBench [513]. However, a significant dimensional gap remains in the evaluation of generative world models. We define the essential differences between CoW-Bench and existing work across three key dimensions:\n\n**Discriminative Perception vs. Generative Simulation.** UniBench addresses the fragmentation of multimodal evaluation by integrating over 50 existing datasets to comprehensively assess the discriminative capabilities of models in visual perception, attribute reasoning, and spatial relationship understanding [512]. This evaluation paradigm implicitly assumes that the model functions as a passive observer, tasked with deconstructing static images or videos provided as input. In contrast, CoW-Bench targets the generative simulation capabilities of world models, treating them as active simulators. Rather than emphasizing whether a model demonstrates exceptional question-answering proficiency, as in MANBench [513], our focus lies in assessing whether a model can actively preserve physical constraints and causal coherence throughout dynamic world evolution. In this sense, CoW-Bench fills a critical evaluation gap between assessing a model’s ability to perceive and reason about the world and its capacity to consistently construct and simulate it over time.\n\n**Evaluation Signals: QA Accuracy vs. Dynamic Constraint Satisfaction.** The core contribution of MANBench lies in establishing a human performance reference frame, where evaluation signals are derived from the VQA (Video Question Answering) accuracy measured against static ground truth [513]. However, this discrete binary judgment (correct versus incorrect) is insufficient for capturing continuous, non-binary physical failures that frequently arise in generative settings. CoW-Bench instead formulates evaluation as a multi-factor constraint satisfaction problem. As UniBench identifies hallucination as a major bottleneck for MLLMs [512], in generative scenarios such hallucinations typically manifest as breakdowns in spatio-temporal consistency, such as objects disappearing after\n\n---\n\n# Table 9: Main results on CoW-Bench over 18 sub-tasks (higher is better); MEAN averages all sub-tasks.\nAbbrev: SUAT=Subj-Attr, LCED=Local-Edit, MCON=Multi-Const; WLIN=Worldline, SLEV=Slow-Evol, STOR=Stage-Order; SEPL=Sem-Planar, OCCO=Occ/Contain; MV3D=MV-3D; TREV=Trigger-Event; LOHO=Long-Horizon, ATDY=Attr-Dyn; SEMV=Sem-MV; 3DLO=3D-Loop; OCMO=Occ-Motion; MAZE=Maze-2D. AVG is rescaled from the original [0, 10] to a percentage scale of [0, 100].\n\n<table><thead><tr><th rowspan=\"2\">Model</th><th colspan=\"4\">Modal</th><th colspan=\"4\">Temporal</th><th colspan=\"4\">Spatial</th><th colspan=\"4\">Modal-Temporal</th><th colspan=\"4\">Modal-Spatial</th><th colspan=\"4\">Temporal-Spatial</th><th rowspan=\"2\">AVG</th></tr><tr><th>SUAT</th><th>LCED</th><th>MCON</th><th>WLIN</th><th>SLEV</th><th>STOR</th><th>SEPL</th><th>OCCO</th><th>MV3D</th><th>TREV</th><th>LOHO</th><th>ATDY</th><th>SEPL</th><th>OCCO</th><th>SEMV</th><th>3DLO</th><th>OCMO</th><th>MAZE</th><th>SEPL</th><th>OCCO</th><th>SEMV</th><th>3DLO</th><th>OCMO</th><th>MAZE</th></tr></thead><tbody><tr><th colspan=\"28\">Closed-Source Video-Generation Models</th></tr><tr><td>Sora [2]</td><td>5.16</td><td>8.35</td><td>8.38</td><td>9.32</td><td>5.80</td><td>6.22</td><td>8.41</td><td>6.19</td><td>8.51</td><td>6.96</td><td>8.12</td><td>5.25</td><td>8.64</td><td>5.97</td><td>9.49</td><td>8.40</td><td>9.25</td><td>4.17</td><td>73.66</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Kling [279]</td><td>4.11</td><td>8.19</td><td>5.63</td><td>9.10</td><td>5.17</td><td>5.53</td><td>8.72</td><td>7.88</td><td>9.32</td><td>7.08</td><td>8.71</td><td>6.58</td><td>8.20</td><td>6.79</td><td>9.44</td><td>8.08</td><td>9.30</td><td>5.30</td><td>73.96</td><td>11.72</td><td>14.92</td><td>10.53</td><td>16.87</td><td>21.09</td><td>12.79</td></tr><tr><th colspan=\"28\">Closed-Source Image-Generation Models</th></tr><tr><td>GPT-image-1 [501]</td><td>7.37</td><td>8.96</td><td>7.96</td><td>9.14</td><td>7.75</td><td>5.68</td><td>8.09</td><td>7.43</td><td>9.26</td><td>6.95</td><td>9.28</td><td>7.22</td><td>9.00</td><td>6.83</td><td>9.79</td><td>8.46</td><td>8.22</td><td>7.24</td><td>80.35</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Seedream-4-0 [502]</td><td>5.73</td><td>7.57</td><td>6.73</td><td>8.63</td><td>6.25</td><td>6.27</td><td>6.09</td><td>6.84</td><td>8.82</td><td>6.70</td><td>8.63</td><td>6.77</td><td>8.12</td><td>7.36</td><td>9.50</td><td>8.13</td><td>6.78</td><td>3.48</td><td>71.33</td><td>7.12</td><td>8.70</td><td>10.14</td><td>4.70</td><td>10.60</td></tr><tr><td>Seedream-4-5 [502]</td><td>6.69</td><td>7.78</td><td>6.91</td><td>8.77</td><td>7.46</td><td>6.76</td><td>6.27</td><td>7.21</td><td>9.06</td><td>7.17</td><td>9.11</td><td>6.77</td><td>8.04</td><td>7.27</td><td>9.59</td><td>8.23</td><td>7.51</td><td>2.28</td><td>73.82</td><td>7.08</td><td>8.78</td><td>10.27</td><td>3.83</td><td>9.82</td></tr><tr><td>Nano Banana [503]</td><td>7.19</td><td>8.47</td><td>5.63</td><td>8.87</td><td>7.86</td><td>6.71</td><td>7.64</td><td>7.84</td><td>9.34</td><td>7.33</td><td>9.66</td><td>6.67</td><td>8.68</td><td>7.95</td><td>9.20</td><td>8.13</td><td>8.76</td><td>5.16</td><td>78.38</td><td>8.10</td><td>7.89</td><td>12.14</td><td>6.62</td><td>11.67</td></tr><tr><td>Nano Banana Pro [503]</td><td>7.39</td><td>8.81</td><td>6.98</td><td>8.88</td><td>8.39</td><td>7.48</td><td>8.10</td><td>8.48</td><td>9.61</td><td>7.84</td><td>9.56</td><td>7.36</td><td>9.51</td><td>9.17</td><td>9.10</td><td>8.86</td><td>8.65</td><td>4.46</td><td>82.57</td><td>10.38</td><td>12.99</td><td>13.94</td><td>5.34</td><td>12.70</td></tr><tr><td>GPT-image-1.5 [504]</td><td>7.75</td><td>8.99</td><td>8.34</td><td>9.23</td><td>8.65</td><td>7.14</td><td>8.32</td><td>8.53</td><td>9.32</td><td>8.13</td><td>9.74</td><td>8.05</td><td>9.45</td><td>8.69</td><td>9.79</td><td>8.54</td><td>8.20</td><td>7.26</td><td>85.62</td><td>10.08</td><td>12.66</td><td>13.71</td><td>7.07</td><td>11.51</td></tr><tr><th colspan=\"28\">Open-Source Video-Generation Models</th></tr><tr><td>Allegro [505]</td><td>1.97</td><td>5.79</td><td>4.41</td><td>7.03</td><td>1.91</td><td>3.33</td><td>6.82</td><td>5.75</td><td>7.89</td><td>4.72</td><td>7.67</td><td>4.80</td><td>4.22</td><td>5.30</td><td>7.19</td><td>6.87</td><td>7.27</td><td>1.86</td><td>52.67</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HunyuanVideo [506]</td><td>2.91</td><td>6.89</td><td>2.41</td><td>8.62</td><td>3.06</td><td>2.94</td><td>6.52</td><td>5.67</td><td>6.04</td><td>4.01</td><td>9.52</td><td>3.66</td><td>5.83</td><td>4.78</td><td>9.64</td><td>6.97</td><td>6.79</td><td>2.08</td><td>54.63</td><td>8.00</td><td>13.12</td><td>17.51</td><td>8.43</td><td>18.42</td></tr><tr><td>LTX-Video [442]</td><td>3.59</td><td>6.78</td><td>4.54</td><td>8.53</td><td>3.67</td><td>3.20</td><td>6.83</td><td>6.17</td><td>6.49</td><td>4.76</td><td>7.34</td><td>4.95</td><td>5.48</td><td>5.13</td><td>8.77</td><td>6.73</td><td>8.67</td><td>1.24</td><td>57.15</td><td>6.91</td><td>12.90</td><td>17.02</td><td>3.66</td><td>17.37</td></tr><tr><td>CogVideoX [179]</td><td>3.75</td><td>5.90</td><td>5.82</td><td>8.29</td><td>4.13</td><td>3.72</td><td>6.61</td><td>5.55</td><td>5.70</td><td>5.15</td><td>8.66</td><td>5.29</td><td>6.44</td><td>5.01</td><td>8.93</td><td>6.37</td><td>8.23</td><td>2.04</td><td>58.66</td><td>7.66</td><td>11.30</td><td>17.53</td><td>7.11</td><td>21.45</td></tr><tr><td>Easy Animate [507]</td><td>3.78</td><td>7.11</td><td>5.10</td><td>8.70</td><td>4.33</td><td>3.59</td><td>7.35</td><td>6.36</td><td>7.81</td><td>4.94</td><td>7.85</td><td>5.29</td><td>6.01</td><td>5.58</td><td>7.95</td><td>6.78</td><td>8.71</td><td>2.98</td><td>61.23</td><td>6.55</td><td>9.72</td><td>15.17</td><td>5.98</td><td>23.03</td></tr><tr><td>Wan2.2-I2V-14B [273]</td><td>3.32</td><td>7.61</td><td>6.57</td><td>8.54</td><td>4.00</td><td>3.80</td><td>7.37</td><td>6.10</td><td>6.27</td><td>5.33</td><td>8.37</td><td>5.24</td><td>6.69</td><td>6.17</td><td>9.51</td><td>7.11</td><td>6.84</td><td>2.46</td><td>61.83</td><td>5.82</td><td>6.42</td><td>12.67</td><td>5.98</td><td>23.83</td></tr><tr><td>SkyReels-V2 [508]</td><td>3.16</td><td>7.45</td><td>5.29</td><td>8.89</td><td>4.03</td><td>3.74</td><td>7.92</td><td>5.80</td><td>7.93</td><td>5.39</td><td>8.87</td><td>5.66</td><td>7.70</td><td>6.75</td><td>9.07</td><td>8.18</td><td>8.18</td><td>3.66</td><td>65.37</td><td>6.60</td><td>6.28</td><td>11.89</td><td>2.47</td><td>5.95</td></tr><tr><th colspan=\"28\">Open-Source Image-Generation Models</th></tr><tr><td>Qwen-Image [509]</td><td>0.72</td><td>2.21</td><td>7.73</td><td>2.10</td><td>0.41</td><td>1.64</td><td>1.89</td><td>1.70</td><td>1.35</td><td>1.72</td><td>0.84</td><td>1.61</td><td>1.69</td><td>0.61</td><td>1.71</td><td>2.96</td><td>0.77</td><td>0.32</td><td>17.77</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BAGEL [8]</td><td>5.01</td><td>5.86</td><td>5.53</td><td>6.27</td><td>5.01</td><td>3.96</td><td>5.33</td><td>6.43</td><td>7.68</td><td>4.45</td><td>8.60</td><td>4.91</td><td>7.08</td><td>5.22</td><td>8.89</td><td>5.51</td><td>6.00</td><td>5.08</td><td>59.34</td><td>5.30</td><td>5.36</td><td>11.86</td><td>7.24</td><td>58.43</td></tr><tr><td>UniVideo [510]</td><td>4.07</td><td>7.27</td><td>4.14</td><td>8.58</td><td>3.87</td><td>3.08</td><td>6.84</td><td>6.15</td><td>6.81</td><td>4.29</td><td>8.72</td><td>5.69</td><td>6.26</td><td>5.09</td><td>9.08</td><td>7.34</td><td>7.49</td><td>3.16</td><td>59.96</td><td>6.06</td><td>7.49</td><td>14.10</td><td>4.60</td><td>58.59</td></tr><tr><td>Emu3.5 [511]</td><td>6.15</td><td>8.76</td><td>8.61</td><td>8.77</td><td>5.31</td><td>4.72</td><td>8.81</td><td>8.62</td><td>8.77</td><td>5.70</td><td>9.42</td><td>6.10</td><td>8.47</td><td>8.61</td><td>9.76</td><td>8.58</td><td>9.22</td><td>5.58</td><td>77.76</td><td>5.58</td><td>6.31</td><td>12.70</td><td>5.56</td><td>77.76</td></tr></tbody></table>\n\nocclusion. To address this limitation, we adopt fine-grained atomic checks that explicitly quantify a model’s robustness with respect to modal, spatial, and temporal constraints in long-horizon generation, rather than relying solely on semantic alignment.\n\n**Complexity Sources: Cognitive Depth vs. Spatiotemporal Entanglement.** MANBench primarily evaluates the high-order cognitive abilities of models, where task difficulty is largely attributed to the depth of logical reasoning and the breadth of knowledge invocation required to exceed human-level performance [513]. In contrast, the difficulty of CoW-Bench arises from the intrinsic entanglement of spatiotemporal dynamics. Empirical results demonstrate that even models with strong cognitive reasoning capabilities, such as GPT-4V, exhibit pronounced failures on cross-consistency tasks, particularly those involving modal-temporal coupling (e.g., $M \\times T$). These findings indicate that the core challenge for world models does not lie in abstract problem-solving capacity, but rather in maintaining coherent dynamic inference under multiple interacting physical constraints.\n\n## **5.4 Main Results**\n\nTable 9 reports the task-level scores of CoW-Bench, covering 18 sub-tasks that span modal, temporal, spatial, and cross-consistency regimes. The overall ranking highlights a clear trend: closed-source image generation models dominate the average score, while open-source video generators remain substantially behind on most consistency-sensitive tasks. In particular, GPT-image-1.5 achieves the best overall performance, followed by Nano Banana Pro and GPT-image-1. This gap suggests that today’s strongest unified multimodal priors already encode rich static world regularities, yet still face systematic failure modes when consistency constraints require long-horizon, multi-factor enforcement.\n\n---\n\n(1) **Temporal control is the bottleneck rather than coherence.** Across multiple families, T-WL (world-line persistence) is consistently high even for several video models (e.g., Sora reaches 9.32), indicating that generating visually continuous footage is no longer the hardest part. However, temporal tasks that demand rule-grounded evolution or structured state progression show a more uneven landscape (e.g., T-Rule and T-Stage-Order vary sharply across models). This separation supports a key CoW-Bench thesis: world models require constraint satisfaction over time, not merely smoothness. A model can look temporally plausible while still violating causal constraints.\n\n(2) Spatial consistency is strong in single-view 3D, but cross-view anchoring still breaks. Most top models score highly on S-3D (single-scene 3D plausibility), with several exceeding 9.0 (e.g., Nano Banana Pro reaches 9.61). Yet cross-consistency tasks reveal a tighter bottleneck: while MS-3D (text-to-3D viewpoint control) remains high for leading models (often ≥ 9), TS-Maze-2D and some time-space settings remain much lower. This pattern suggests that local geometric plausibility is easier than maintaining a globally anchored spatial structure under motion and decision-like trajectories.\n\n(3) Fusion tasks reveal the real world-model gap: persistent semantics under dynamics. The strongest separation between state-of-the-art models and the rest occurs in cross-consistency families (MT, MS, TS). For instance, leading models obtain near-ceiling performance on MT-PropKeep (property persistence under temporal evolution), yet degrade on MT-PropChange (attribute change alignment) and especially on TS-Maze-2D (navigation-style structure preservation). Notably, some high-avg models still exhibit pronounced weaknesses on TS-Maze-2D (e.g., Nano Banana Pro reports 4.46), indicating that global world-state maintenance and trajectory-level constraint enforcement remain unsolved even when per-frame fidelity is excellent. This is precisely the regime where UMMs must evolve from perceptual generators into genuine internal simulators.\n\n(4) Open-source models expose failure modes aligned with CoW-Bench's motivation. Open-source video generators typically underperform on modal grounding (M-Subj-Attr) and cross-consistency tasks, consistent with qualitative observations that they either (i) relax rare constraints into common defaults, or (ii) preserve motion while drifting in identity/attributes. Meanwhile, open-source image models show large variance: Emu3.5 is competitive in many columns but still drops on time-centric and time-space settings, reinforcing that CoW-Bench targets the gap between single-shot plausibility and multi-step consistency.\n\n**Takeaway.** UMMs perform well on static or single-view settings, where local plausibility is sufficient. However, when a task requires maintaining a stable world while changes unfold over time and space, performance drops sharply. These cross-consistency scenarios are the clearest indicator of whether a model truly behaves as a world model rather than a frame generator.\n\n## 5.5 Single-Axis Consistency\n\n### 5.5.1 Modal Consistency Results\n\nTable 10 reports modality-consistency performance across three metric families: subject attribute fidelity (M1), local edit precision (M2), and multi-constraint satisfaction (M3). Overall, the table reinforces the core motivation of CoW-Bench: even when generations look plausible, models frequently weaken, mis-bind, or silently reinterpret the specified conditions, which is exactly the failure mode that a world-model interface cannot afford.\n\n(1) Identity-and-attribute binding is the hardest modal interface primitive. Across nearly all model groups, Id+Attr remains noticeably lower than other M1 dimensions. Even top closed-source image\n\n---\n\n# The Trinity of Consistency as a Defining Principle for General World Models\n\nTable 10: Modal-consistency results on CoW-Bench (0–2 scale; higher is better), grouped into three\nmetric families: **M1** Subject–Attribute Fidelity (IDAT, BKOF, DOMN, CLAR, EXCL), **M2** Local Edit\nPrecision (TARG, MNCH, LEAK, CLAR, NEXA), and **M3** Multi-constraint Satisfaction (CMPL, ATCO,\nRLCO, OMIS, NEXA). Refer to Table 9 for abbreviations. BKOF measures whether rare constraints are\nreplaced by common defaults, while NEXA penalizes spurious additions beyond the instruction.\n\n<table><thead><tr><th rowspan=\"3\">Model</th><th colspan=\"6\">Subj-Attr (SUAT)</th><th colspan=\"3\">Local-Edit (LCED)</th><th colspan=\"4\">Multi-Const (MCON)</th></tr><tr><th>IDAT</th><th>BKOF</th><th>DOMN</th><th>CLAR</th><th>EXCL</th><th>TARG</th><th>MNCH</th><th>LEAK</th><th>CLAR</th><th>NEXA</th><th>CMPL</th><th>ATCO</th><th>RLCO</th><th>OMIS</th><th>NEXA</th></tr></thead><tbody><tr><td colspan=\"16\" style=\"text-align:center; font-weight:bold;\">Closed-Source Video-Generation Models</td></tr><tr><td>Sora [2]</td><td>0.40</td><td>1.48</td><td>0.91</td><td>1.15</td><td>1.22</td><td>1.12</td><td>1.96</td><td>1.82</td><td>1.45</td><td>2.00</td><td>1.96</td><td>1.67</td><td>1.32</td><td>1.59</td><td>1.84</td></tr><tr><td>Kling [279]</td><td>0.33</td><td>1.32</td><td>0.58</td><td>0.95</td><td>0.93</td><td>1.23</td><td>1.82</td><td>1.74</td><td>1.49</td><td>1.91</td><td>1.36</td><td>1.14</td><td>0.74</td><td>0.86</td><td>1.53</td></tr><tr><td colspan=\"16\" style=\"text-align:center; font-weight:bold;\">Closed-Source Image-Generation Models</td></tr><tr><td>GPT-image-1 [501]</td><td>0.96</td><td>1.71</td><td>1.69</td><td>1.26</td><td>1.75</td><td>1.40</td><td>1.98</td><td>1.91</td><td>1.73</td><td>1.95</td><td>1.89</td><td>1.45</td><td>1.34</td><td>1.47</td><td>1.80</td></tr><tr><td>GPT-image-1.5 [504]</td><td>1.19</td><td>1.76</td><td>1.77</td><td>1.35</td><td>1.68</td><td>1.50</td><td>1.94</td><td>1.92</td><td>1.73</td><td>1.90</td><td>1.89</td><td>1.51</td><td>1.49</td><td>1.66</td><td>1.79</td></tr><tr><td>Seedream-4-0 [502]</td><td>0.94</td><td>1.44</td><td>1.35</td><td>0.89</td><td>1.10</td><td>1.25</td><td>1.77</td><td>1.70</td><td>1.54</td><td>1.31</td><td>1.78</td><td>1.44</td><td>1.28</td><td>1.44</td><td>0.78</td></tr><tr><td>Seedream-4-5 [502]</td><td>1.10</td><td>1.53</td><td>1.52</td><td>1.16</td><td>1.38</td><td>1.43</td><td>1.84</td><td>1.78</td><td>1.54</td><td>1.20</td><td>1.82</td><td>1.41</td><td>1.24</td><td>1.45</td><td>0.99</td></tr><tr><td>Nano Banana [503]</td><td>1.17</td><td>1.59</td><td>1.62</td><td>1.22</td><td>1.59</td><td>1.36</td><td>1.79</td><td>1.77</td><td>1.67</td><td>1.89</td><td>1.38</td><td>1.04</td><td>0.89</td><td>1.09</td><td>1.23</td></tr><tr><td>Nano Banana Pro [503]</td><td>1.25</td><td>1.68</td><td>1.72</td><td>1.22</td><td>1.52</td><td>1.55</td><td>1.80</td><td>1.81</td><td>1.71</td><td>1.95</td><td>1.63</td><td>1.42</td><td>1.21</td><td>1.32</td><td>1.40</td></tr><tr><td colspan=\"16\" style=\"text-align:center; font-weight:bold;\">Open-Source Video-Generation Models</td></tr><tr><td>Allegro [505]</td><td>0.10</td><td>0.60</td><td>0.24</td><td>0.47</td><td>0.56</td><td>0.70</td><td>1.31</td><td>1.28</td><td>0.83</td><td>1.67</td><td>1.19</td><td>0.82</td><td>0.59</td><td>0.70</td><td>1.11</td></tr><tr><td>Easy Animate [507]</td><td>0.13</td><td>1.23</td><td>0.55</td><td>0.90</td><td>0.97</td><td>0.71</td><td>1.76</td><td>1.67</td><td>1.11</td><td>1.86</td><td>1.51</td><td>0.95</td><td>0.68</td><td>0.87</td><td>1.09</td></tr><tr><td>CogVideoX [179]</td><td>0.09</td><td>1.29</td><td>0.53</td><td>0.98</td><td>0.86</td><td>0.70</td><td>1.53</td><td>1.26</td><td>0.77</td><td>1.64</td><td>1.69</td><td>1.22</td><td>0.65</td><td>0.95</td><td>1.31</td></tr><tr><td>Wan2.2-I2V-14B [273]</td><td>0.11</td><td>1.10</td><td>0.40</td><td>0.81</td><td>0.90</td><td>0.83</td><td>1.87</td><td>1.77</td><td>1.32</td><td>1.82</td><td>1.68</td><td>1.38</td><td>1.11</td><td>1.32</td><td>1.08</td></tr><tr><td>SkyReels-V2 [508]</td><td>0.14</td><td>1.00</td><td>0.38</td><td>0.76</td><td>0.88</td><td>0.77</td><td>1.80</td><td>1.63</td><td>1.34</td><td>1.91</td><td>1.20</td><td>1.05</td><td>0.67</td><td>0.93</td><td>1.44</td></tr><tr><td>HunyuanVideo [506]</td><td>0.01</td><td>1.23</td><td>0.31</td><td>0.76</td><td>0.60</td><td>0.30</td><td>1.97</td><td>1.64</td><td>0.98</td><td>2.00</td><td>0.28</td><td>0.23</td><td>0.09</td><td>0.12</td><td>1.69</td></tr><tr><td>LTX-Video [442]</td><td>0.15</td><td>1.20</td><td>0.49</td><td>0.93</td><td>0.82</td><td>0.62</td><td>1.80</td><td>1.48</td><td>0.97</td><td>1.91</td><td>1.38</td><td>0.96</td><td>0.52</td><td>0.69</td><td>0.99</td></tr><tr><td colspan=\"16\" style=\"text-align:center; font-weight:bold;\">Open-Source Image-Generation Models</td></tr><tr><td>BAGEL [8]</td><td>0.73</td><td>1.13</td><td>1.16</td><td>0.63</td><td>1.36</td><td>1.01</td><td>1.30</td><td>1.14</td><td>0.79</td><td>1.62</td><td>1.53</td><td>1.01</td><td>0.75</td><td>0.85</td><td>1.39</td></tr><tr><td>UniVideo [510]</td><td>0.27</td><td>1.09</td><td>0.77</td><td>0.74</td><td>1.20</td><td>0.79</td><td>1.90</td><td>1.57</td><td>1.07</td><td>1.94</td><td>0.79</td><td>0.65</td><td>0.35</td><td>0.89</td><td>1.46</td></tr><tr><td>Emu3.5 [511]</td><td>0.81</td><td>1.37</td><td>1.25</td><td>1.03</td><td>1.69</td><td>1.41</td><td>1.90</td><td>1.86</td><td>1.70</td><td>1.89</td><td>1.87</td><td>1.79</td><td>1.68</td><td>1.74</td><td>1.53</td></tr><tr><td>Qwen-Image [509]</td><td>0.00</td><td>0.18</td><td>0.04</td><td>0.02</td><td>0.48</td><td>0.10</td><td>0.40</td><td>0.29</td><td>0.09</td><td>1.33</td><td>1.84</td><td>1.66</td><td>1.49</td><td>1.60</td><td>1.14</td></tr></tbody></table>\n\n---\n\n**** (4) Multi-constraint fulfillment stresses completeness and role binding, not just more text. **M3** exposes a different bottleneck. Strong models such as Emu3.5 remain consistently high across Complete/Attr-corr/Rel-corr (1.87/1.79/1.68), while some systems show uneven profiles: for instance, Qwen-Image achieves relatively high Complete and attribute/relationship scores yet performs extremely poorly on M1 identity binding. This mismatch suggests that satisfying multiple listed constraints is not sufficient if the model cannot maintain a stable referent for those constraints. Meanwhile, several models exhibit reduced No-extra under M3 (e.g., the Seedream variants), indicating that under compositional pressure they may introduce spurious entities—an error that is particularly harmful for downstream planning and verification. ****\n\n**Takeaway.** For the SUAT task, the model often exhibits ambiguity when uniformly aligning constraints across different modalities. It fails to accurately extract corresponding constraints from text and images to guide generation according to requirements. Instead, it blends the extracted constraints from both modalities into a single, confused constraint that guides generation, resulting in chaotic outcomes.\n\n5.5.2 Temporal Consistency Results\n\nTable **11** reports temporal-consistency performance over three metric families: **T1** Worldline Persistence, **T2** Rule-guided Slow Evolution, and **T3** Ordered Stage Transitions. Two consistent patterns emerge that align with CoW-Bench’s central thesis: temporal plausibility is not equivalent to temporal constraint satisfaction, and the hardest failures arise when models must enforce structured dynamics rather than merely maintain visual continuity.\n\nWorldline persistence is comparatively strong, even for many video generators. Most closed-source video models score near the upper range on T1 (e.g., Sora: high Env-stab and Visual), and several open-source video models also achieve solid T1 profiles (e.g., SkyReels-V2 and Wan2.2-I2V). This suggests that maintaining a stable scene layout and avoiding frame-wise reinitialization is becoming a largely solved capability for high-capacity generators.\n\n**The main bottleneck is rule-following evolution, not continuity.** In contrast, T2 exposes a sharp drop on Trend, Time-scale, and Inter-state for many video models (often below 0.6), even when Subj-lock is high. This gap indicates a common failure mode: models keep the same subject and background, yet fail to realize a monotonic, correctly paced process with identifiable intermediate states. Notably, strong closed-source image models show markedly higher T2 scores (e.g., GPT-image-1.5 maintains high values across Trend/Time-scale/Inter-state), suggesting that stronger instruction- following priors help when the temporal constraint is expressed semantically and must be respected throughout the sequence.\n\n**Takeaway.** We found that in the STOR task, image generation models generally grasp the overall progression over time. However, transitions between different states exhibit distinct discontinuities rather than smooth evolution, with instances of reversed sequences occurring between states, making it difficult to maintain consistent processes and content. Conversely, in the SLEV task, image generation models demonstrate a higher degree of understanding and adherence to world rules. Video generation models, on the other hand, exhibit probabilistic compliance with rules, yielding divergent outcomes for identical or similar scenario tasks.\n\nStage-ordering remains fragile under explicit multi-step structure. For T3, the weakest columns concentrate on Order and Identif., especially for open-source video models (often near 0.3 or lower). Even when Worldline at the end of T3 stays high, low Order/Identif. implies that the sequence\n\n---\n\nAlgorithms.png\n\nTable 11: Temporal-consistency results on CoW-Bench (0–2 scale; higher is better). We report three metric families: **T1** Worldline Persistence (SBJC, ATST, ENST, VISU, EVCT), **T2** Rule-guided Slow Evolution (SBJL, TREN, TSCL, INTS, RULE), and **T3** Ordered Stage Transitions (ORDR, IDEN, TIME, PROC, WLIN). Refer to Table 9 for abbreviations. INTS evaluates the visibility of plausible intermediate states, while TSCL measures whether the evolution speed matches the prompt-specified process.\n\n<table><thead><tr><th rowspan=\"3\">Model</th><th colspan=\"5\">Worldline (WLIN)</th><th colspan=\"5\">Slow-Evol (SLEV)</th><th colspan=\"5\">Stage-Order (STOR)</th></tr><tr><th>SBJC</th><th>ATST</th><th>ENST</th><th>VISU</th><th>EVCT</th><th>SBJL</th><th>TREN</th><th>TSCL</th><th>INTS</th><th>RULE</th><th>ORDR</th><th>IDEN</th><th>TIME</th><th>PROC</th><th>WLIN</th></tr></thead><tbody><tr><td colspan=\"18\">Closed-Source Video-Generation Models</td></tr><tr><td>Sora [2]</td><td>1.87</td><td>1.84</td><td>1.94</td><td>1.91</td><td>1.76</td><td>1.95</td><td>0.96</td><td>0.99</td><td>0.91</td><td>0.99</td><td>0.93</td><td>0.91</td><td>1.12</td><td>1.42</td><td>1.84</td></tr><tr><td>Kling [279]</td><td>1.85</td><td>1.82</td><td>1.93</td><td>1.86</td><td>1.64</td><td>1.98</td><td>0.80</td><td>0.73</td><td>0.70</td><td>0.96</td><td>0.94</td><td>0.81</td><td>0.76</td><td>1.13</td><td>1.89</td></tr><tr><td colspan=\"18\">Closed-Source Image-Generation Models</td></tr><tr><td>GPT-image-1 [501]</td><td>1.75</td><td>1.84</td><td>1.93</td><td>1.91</td><td>1.71</td><td>1.98</td><td>1.49</td><td>1.43</td><td>1.49</td><td>1.38</td><td>0.79</td><td>0.86</td><td>1.06</td><td>1.19</td><td>1.79</td></tr><tr><td>GPT-image-1.5 [504]</td><td>1.77</td><td>1.86</td><td>1.91</td><td>1.93</td><td>1.76</td><td>1.98</td><td>1.66</td><td>1.69</td><td>1.70</td><td>1.62</td><td>1.26</td><td>1.18</td><td>1.35</td><td>1.45</td><td>1.90</td></tr><tr><td>Seedream-4-0 [502]</td><td>1.70</td><td>1.66</td><td>1.85</td><td>1.84</td><td>1.59</td><td>1.75</td><td>1.14</td><td>1.14</td><td>1.05</td><td>1.17</td><td>1.14</td><td>1.13</td><td>1.05</td><td>1.24</td><td>1.71</td></tr><tr><td>Seedream-4-5 [502]</td><td>1.71</td><td>1.76</td><td>1.89</td><td>1.78</td><td>1.63</td><td>1.85</td><td>1.49</td><td>1.40</td><td>1.40</td><td>1.32</td><td>1.29</td><td>1.20</td><td>1.12</td><td>1.32</td><td>1.83</td></tr><tr><td>Nano Banana [503]</td><td>1.71</td><td>1.72</td><td>1.84</td><td>1.84</td><td>1.76</td><td>1.88</td><td>1.54</td><td>1.51</td><td>1.53</td><td>1.41</td><td>1.20</td><td>1.23</td><td>1.12</td><td>1.35</td><td>1.80</td></tr><tr><td>Nano Banana Pro [503]</td><td>1.74</td><td>1.73</td><td>1.83</td><td>1.89</td><td>1.69</td><td>1.90</td><td>1.65</td><td>1.65</td><td>1.64</td><td>1.54</td><td>1.40</td><td>1.37</td><td>1.41</td><td>1.48</td><td>1.83</td></tr><tr><td colspan=\"18\">Open-Source Video-Generation Models</td></tr><tr><td>Allegro [505]</td><td>1.38</td><td>1.37</td><td>1.57</td><td>1.45</td><td>1.26</td><td>0.90</td><td>0.27</td><td>0.23</td><td>0.19</td><td>0.32</td><td>0.64</td><td>0.29</td><td>0.36</td><td>0.74</td><td>1.30</td></tr><tr><td>Easy Animate [507]</td><td>1.69</td><td>1.69</td><td>1.90</td><td>1.82</td><td>1.60</td><td>1.98</td><td>0.60</td><td>0.54</td><td>0.49</td><td>0.72</td><td>0.50</td><td>0.33</td><td>0.26</td><td>0.70</td><td>1.80</td></tr><tr><td>CogVideoX [179]</td><td>1.63</td><td>1.56</td><td>1.90</td><td>1.67</td><td>1.53</td><td>1.85</td><td>0.59</td><td>0.49</td><td>0.46</td><td>0.74</td><td>0.69</td><td>0.30</td><td>0.27</td><td>0.63</td><td>1.83</td></tr><tr><td>Wan2.2-I2V-14B [273]</td><td>1.76</td><td>1.70</td><td>1.86</td><td>1.78</td><td>1.44</td><td>1.88</td><td>0.56</td><td>0.43</td><td>0.38</td><td>0.75</td><td>0.71</td><td>0.42</td><td>0.26</td><td>0.65</td><td>1.76</td></tr><tr><td>SkyReels-V2 [508]</td><td>1.79</td><td>1.74</td><td>1.84</td><td>1.90</td><td>1.62</td><td>1.86</td><td>0.57</td><td>0.48</td><td>0.48</td><td>0.64</td><td>0.61</td><td>0.35</td><td>0.26</td><td>0.71</td><td>1.81</td></tr><tr><td>HunyuanVideo [506]</td><td>1.87</td><td>1.77</td><td>1.86</td><td>1.86</td><td>1.26</td><td>2.00</td><td>0.22</td><td>0.21</td><td>0.11</td><td>0.52</td><td>0.50</td><td>0.08</td><td>0.06</td><td>0.48</td><td>1.82</td></tr><tr><td>LTX-Video [442]</td><td>1.76</td><td>1.67</td><td>1.85</td><td>1.76</td><td>1.49</td><td>1.65</td><td>0.53</td><td>0.49</td><td>0.40</td><td>0.60</td><td>0.52</td><td>0.18</td><td>0.18</td><td>0.55</td><td>1.77</td></tr><tr><td colspan=\"18\">Open-Source Image-Generation Models</td></tr><tr><td>BAGEL [8]</td><td>1.31</td><td>1.13</td><td>1.57</td><td>1.20</td><td>1.06</td><td>1.49</td><td>1.01</td><td>0.81</td><td>0.91</td><td>0.79</td><td>0.69</td><td>0.59</td><td>0.67</td><td>0.96</td><td>1.05</td></tr><tr><td>UniVideo [510]</td><td>1.82</td><td>1.74</td><td>1.90</td><td>1.79</td><td>1.33</td><td>1.99</td><td>0.48</td><td>0.41</td><td>0.32</td><td>0.67</td><td>0.31</td><td>0.19</td><td>0.17</td><td>0.60</td><td>1.81</td></tr><tr><td>Emu3.5 [511]</td><td>1.66</td><td>1.72</td><td>1.91</td><td>1.85</td><td>1.63</td><td>1.95</td><td>0.84</td><td>0.74</td><td>0.78</td><td>1.00</td><td>0.54</td><td>0.59</td><td>0.73</td><td>1.13</td><td>1.73</td></tr><tr><td>Qwen-Image [509]</td><td>0.22</td><td>0.22</td><td>0.58</td><td>0.70</td><td>0.38</td><td>0.28</td><td>0.07</td><td>0.04</td><td>0.00</td><td>0.02</td><td>0.26</td><td>0.08</td><td>0.02</td><td>0.44</td><td>0.84</td></tr></tbody></table>\n\nmay remain in one world but fails to realize the intended discrete stage structure reliably. This finding motivates CoW-Bench’s decomposition: a model can be temporally stable while still violating high-level temporal logic.\n\n### **5.5.3 Spatial Consistency Results**\n\nTable 12 reports spatial-consistency across S1 Sem-Planar, S2 Occlusion/Containment, and S3 Multi-view 3D coherence. The results echo CoW-Bench’s central view: spatial world modeling is not only about producing plausible geometry in a single frame, but about maintaining structural constraints that remain verifiable under interactions such as occlusion, containment, and viewpoint change.\n\n(1) Planar layout is the entry-level test, yet directional grounding remains fragile. Most models score relatively high on Layout (often ≥1.8), indicating that producing a globally coherent 2D composition is increasingly reliable. In contrast, Dir is consistently the lowest sub-metric across model families (e.g., Sora: 0.64; several open-source video models ≤0.5; Qwen-Image: 0.02). This gap suggests that models can maintain a visually stable arrangement while still failing to execute explicit directional constraints (left/right/inside/outside) with high fidelity. For a world model, directional grounding is\n\n---\n\n**Table 12:** Spatial-consistency results on CoW-Bench (0-2 scale; higher is better). We report three metric families: **S1** Sem-Planar (DIRC, COUNT, RULE, BNDY, LAYT), **S2** Occlusion/Containment (OCCL, BNDY, VISB, RSTB, LAYR), and **S3** Multi-view 3D coherence (STRC, SURF, PSCL, OUPD, GEOS). VISB evaluates whether visible regions agree with the implied occlusion relation, while OUPD measures whether occlusion boundaries update plausibly under viewpoint change.\n\n<table><thead><tr><th rowspan=\"3\">Model</th><th colspan=\"4\">Sem-Planar (SEPL)</th><th colspan=\"5\">Occ-Cont (OCCO)</th><th colspan=\"5\">MV-3D</th></tr><tr><th>DIRC</th><th>COUNT</th><th>RULE</th><th>BNDY</th><th>LAYT</th><th>OCCL</th><th>BNDY</th><th>VISB</th><th>RSTB</th><th>LAYR</th><th>STRC</th><th>SURF</th><th>PSCL</th><th>OUPD</th><th>GEOS</th></tr><tr><th colspan=\"14\" style=\"text-align:center;\">Closed-Source Video-Generation Models</th></tr></thead><tbody><tr><td>Sora [2]</td><td>0.64</td><td>1.22</td><td>1.00</td><td>1.47</td><td>1.88</td><td>1.51</td><td>1.49</td><td>1.74</td><td>1.76</td><td>1.91</td><td>1.77</td><td>1.72</td><td>1.56</td><td>1.67</td><td>1.79</td></tr><tr><td>Kling [279]</td><td>1.10</td><td>1.52</td><td>1.53</td><td>1.82</td><td>1.91</td><td>1.67</td><td>1.62</td><td>1.70</td><td>1.78</td><td>1.95</td><td>1.93</td><td>1.85</td><td>1.80</td><td>1.86</td><td>1.88</td></tr><tr><th colspan=\"14\" style=\"text-align:center;\">Closed-Source Image-Generation Models</th></tr><tr><td>GPT-image-1 [501]</td><td>0.81</td><td>1.58</td><td>1.45</td><td>1.66</td><td>1.92</td><td>1.54</td><td>1.37</td><td>1.55</td><td>1.75</td><td>1.89</td><td>1.91</td><td>1.85</td><td>1.77</td><td>1.86</td><td>1.88</td></tr><tr><td>GPT-image-1.5 [504]</td><td>1.36</td><td>1.70</td><td>1.62</td><td>1.87</td><td>1.98</td><td>1.59</td><td>1.46</td><td>1.57</td><td>1.83</td><td>1.87</td><td>1.92</td><td>1.89</td><td>1.81</td><td>1.85</td><td>1.85</td></tr><tr><td>Seedream-4-0 [502]</td><td>0.96</td><td>1.31</td><td>1.28</td><td>1.51</td><td>1.79</td><td>1.07</td><td>0.91</td><td>1.05</td><td>1.30</td><td>1.77</td><td>1.90</td><td>1.78</td><td>1.59</td><td>1.77</td><td>1.78</td></tr><tr><td>Seedream-4-5 [502]</td><td>1.13</td><td>1.40</td><td>1.37</td><td>1.66</td><td>1.66</td><td>1.22</td><td>1.00</td><td>1.06</td><td>1.31</td><td>1.69</td><td>1.87</td><td>1.78</td><td>1.77</td><td>1.82</td><td>1.82</td></tr><tr><td>Nano Banana [503]</td><td>1.23</td><td>1.59</td><td>1.41</td><td>1.71</td><td>1.89</td><td>1.35</td><td>1.36</td><td>1.43</td><td>1.69</td><td>1.82</td><td>1.96</td><td>1.90</td><td>1.75</td><td>1.87</td><td>1.87</td></tr><tr><td>Nano Banana Pro [503]</td><td>1.54</td><td>1.56</td><td>1.66</td><td>1.79</td><td>1.93</td><td>1.52</td><td>1.46</td><td>1.58</td><td>1.70</td><td>1.84</td><td>1.94</td><td>1.94</td><td>1.92</td><td>1.90</td><td>1.91</td></tr><tr><th colspan=\"14\" style=\"text-align:center;\">Open-Source Video-Generation Models</th></tr><tr><td>Allegro [505]</td><td>0.69</td><td>1.14</td><td>0.91</td><td>1.26</td><td>1.75</td><td>1.29</td><td>1.18</td><td>1.26</td><td>1.39</td><td>1.70</td><td>1.58</td><td>1.65</td><td>1.48</td><td>1.59</td><td>1.59</td></tr><tr><td>Easy Animate [507]</td><td>0.44</td><td>1.41</td><td>1.12</td><td>1.53</td><td>1.86</td><td>1.34</td><td>1.26</td><td>1.37</td><td>1.47</td><td>1.91</td><td>1.68</td><td>1.52</td><td>1.54</td><td>1.56</td><td>1.51</td></tr><tr><td>CogVideoX [179]</td><td>0.48</td><td>1.10</td><td>0.95</td><td>1.18</td><td>1.84</td><td>1.30</td><td>1.16</td><td>1.13</td><td>1.26</td><td>1.76</td><td>1.42</td><td>1.23</td><td>1.08</td><td>1.01</td><td>0.96</td></tr><tr><td>Wan2.2-I2V-14B [273]</td><td>0.29</td><td>1.41</td><td>1.03</td><td>1.48</td><td>1.89</td><td>1.44</td><td>1.16</td><td>1.34</td><td>1.54</td><td>1.89</td><td>1.58</td><td>1.25</td><td>1.09</td><td>1.20</td><td>1.15</td></tr><tr><td>SkyReels-V2 [508]</td><td>0.37</td><td>1.27</td><td>0.98</td><td>1.38</td><td>1.80</td><td>1.40</td><td>1.43</td><td>1.49</td><td>1.66</td><td>1.94</td><td>1.67</td><td>1.67</td><td>1.46</td><td>1.62</td><td>1.51</td></tr><tr><td>HunyuanVideo [506]</td><td>0.15</td><td>1.34</td><td>0.92</td><td>1.38</td><td>1.88</td><td>1.15</td><td>0.81</td><td>1.18</td><td>1.44</td><td>1.94</td><td>1.75</td><td>1.15</td><td>0.89</td><td>0.98</td><td>1.27</td></tr><tr><td>LTX-Video [442]</td><td>0.64</td><td>1.35</td><td>1.14</td><td>1.30</td><td>1.74</td><td>1.30</td><td>1.08</td><td>1.29</td><td>1.37</td><td>1.79</td><td>1.57</td><td>1.33</td><td>1.21</td><td>1.13</td><td>1.25</td></tr><tr><th colspan=\"14\" style=\"text-align:center;\">Open-Source Image-Generation Models</th></tr><tr><td>BAGEL [8]</td><td>0.56</td><td>1.52</td><td>1.10</td><td>1.47</td><td>1.78</td><td>1.17</td><td>0.98</td><td>0.85</td><td>0.90</td><td>1.43</td><td>1.68</td><td>1.58</td><td>1.42</td><td>1.49</td><td>1.51</td></tr><tr><td>UniVideo [510]</td><td>0.36</td><td>1.38</td><td>1.10</td><td>1.42</td><td>1.89</td><td>1.29</td><td>1.18</td><td>1.34</td><td>1.28</td><td>1.75</td><td>1.74</td><td>1.38</td><td>1.12</td><td>1.23</td><td>1.34</td></tr><tr><td>Emu3.5 [511]</td><td>1.41</td><td>1.63</td><td>1.81</td><td>1.81</td><td>1.96</td><td>1.78</td><td>1.64</td><td>1.67</td><td>1.81</td><td>1.91</td><td>1.82</td><td>1.78</td><td>1.72</td><td>1.72</td><td>1.73</td></tr><tr><td>Qwen-Image [509]</td><td>0.02</td><td>0.15</td><td>0.08</td><td>0.15</td><td>1.30</td><td>0.24</td><td>0.17</td><td>0.15</td><td>0.21</td><td>1.12</td><td>0.54</td><td>0.26</td><td>0.12</td><td>0.25</td><td>0.18</td></tr></tbody></table>\n\na core interface requirement because it turns language into testable spatial relations.\n\n(2) **Occlusion/containment is largely learned, but visible-part evidence is the weak link.** Closed-source models are strong on Layer and Rel-stab (typically ~1.8-1.95), implying that they often preserve consistent depth ordering without obvious contradictions. However, Visible shows a wider spread, especially for open-source image models (e.g., BAGEL: 0.85; Qwen-Image: 0.15). This pattern indicates that models may capture a coarse layering intent while failing on the operational evidence—whether the actually visible portions match the implied occlusion boundary. This is exactly the kind of looks plausible but violates a checkable constraint failure that CoW-Bench is designed to reveal.\n\n(3) **Multi-view 3D coherence separates geometry plausibility from world-state invariance.** Top closed-source image models achieve near-ceiling scores on Struct and Persp/Scale (e.g., Nano Banana Pro: 1.94/1.92; GPT-image-1.5: 1.92/1.81), indicating strong single-object 3D plausibility. Yet several open-source video models drop substantially on Occ-update and Geo-self (e.g., CogVideoX: 1.01/0.96), suggesting that they struggle to update occlusions and maintain a self-consistent 3D explanation across views. This supports a key world-model implication: generating a plausible view is easier than maintaining a persistent 3D scene hypothesis that survives viewpoint change.\n\n---\n\n**Takeaway.** In MV-3D, image generation models tend to mistakenly apply mirror symmetry to scenes to simulate different viewpoints rather than accurately capturing distinct perspectives within the same scene. Additionally, when switching viewpoints, the detailed attributes of the same object across different angles often fail to remain consistent. When tackling Occ-Cont tasks, models frequently generate unrealistic changes that defy common sense, such as objects penetrating one another, merging into each other, or exhibiting unnatural movements to maintain hierarchical relationships.\n\n# 5.6 Cross-Axis Consistency\n\n## 5.6.1 Modal-Space Consistency Results: Semantic-to-Geometry Binding\n\nFigure 35 visualizes Modal-Space consistency, where models must map language constraints (entities, attributes, relations) into executable spatial roles and keep them verifiable under layout and viewpoint variation. The figure supports CoW-Bench’s central goal: distinguishing looks plausible from constraint-faithful semantic grounding in space.\n\nFigure 35: Modal-Space consistency on CoW-Bench, shown as a heatmap over sub-metrics (rows) and models (columns), with scores on the 0-2 scale (higher is better). Rows group into **Sem-Planar** (Ent-match, Act-align, NT-stab, Attr-bind, Global), **Sem-Hier** (Pos-rel, Neg-rel, Excl., Vis+Layer, Id-stab), and **Sem-MV** (Anchor, View-stab, Lateral, Scene, Marker).\n\n**Semantic role binding is the dominant bottleneck.** Across many models, the most consistent performance drops appear on Act-align and Pos-rel. This indicates frequent failures to (i) bind an instructed action/relation to the correct entity and (ii) realize a constructive positive spatial relation precisely. Importantly, these failures can coexist with strong scores on geometry-leaning cues (e.g., Id-stab, Scene), producing a characteristic “plausible-but-misbound” outcome: the scene is coherent, yet the constraint is attached to the wrong object or only weakly reflected.\n\n**Avoiding violations is often easier than constructing exact relations.** For a broad set of mid-tier models, Neg-rel and Excl. are noticeably stronger than Pos-rel. This asymmetry suggests that models more reliably avoid forbidden configurations than they enforce an exact required placement. For world-model use, this gap matters because planning and verification rely on constructive satisfaction (placing the right entity in the right role), not only on the absence of obvious violations.\n\n---\n\n**Multi-view semantic stability is strong for top models but still reveals tail-risk failures.** The Sem-MV block (Anchor, View-stab, Lateral, Scene, Marker) is generally high for leading closed-source image models and competitive systems, indicating that stable reference frames and identity markers under viewpoint change are increasingly attainable. However, the heatmap also shows that weaker models can fail catastrophically on these anchors, which makes multi-view stability a sensitive probe of whether a model maintains an invariant scene state rather than redrawing a new world per view.\n\n**Takeaway.** When handling 3D perspective issues, the model also tends to generate mirror-symmetrical results. Additionally, during perspective transitions, non-primary content may retain its original viewpoint, leading to partial object perspective shifts.\n\n**5.6.2 Modal–Time Consistency Results: Executing a Temporal Program**\n\nFigure 36 visualizes Modal–Time consistency, where models must (i) keep language-specified anchors stable over long horizons, (ii) execute attribute dynamics specified by the prompt, and (iii) respond to discrete trigger events without breaking the worldline. The heatmap supports CoW-Bench's central goal: separating visually plausible temporal outputs from *constraint-faithful* temporal execution.\n\n**Figure 36:** Modal–Time consistency on CoW-Bench, shown as a heatmap over sub-metrics (rows) and models (columns), with scores on the 0–2 scale (higher is better). Rows cover **Long-Horizon** anchoring (Init-anchor, Long-stab, Cross-scene, Attr-bind, No-unexp), **Attr-Dyn** alignment (Target(E,A), Follow, Smooth, Rate, Env-stab), and **Trigger-Event** compliance (Pre-hold, Trigger, Post-comp, State-stab, Env-stab).\n\n**Anchoring is generally strong; the main variance concentrates in the weakest systems.** The Long-Horizon block is consistently high for leading closed-source image models and remains competitive for many video generators, indicating that persistent identity/attribute anchoring is often attainable once the anchor is observable. The most salient failures appear as isolated low-score columns (e.g., very low Init-anchor/Long-stab in the weakest model), which then correlate with downstream temporal-control breakdown.\n\n**Dynamics attribute is the primary bottleneck, dominated by instruction-following and rate control.** Within Attr-Dyn, Env-stab stays near the upper range for most models, while Follow and Rate remain substantially lower—especially for video generators. This pattern indicates a common failure mode:\n\n---\n\nmodels keep the scene stable but do not execute the instructed evolution reliably (direction/sched-\nule/pacing), yielding sequences that look smooth yet violate semantic temporal commitments.\n\n**Triggered events expose timing and post-event persistence failures.** In Trigger-Event, Pre-hold is often relatively strong, but *Trigger* and *Post-comp* degrade noticeably for many video models, revealing two coupled issues: the event is not made salient at the correct time, and the post-trigger state does not persist. These errors are particularly damaging for planning-style use, where discrete events serve as causal checkpoints.\n\n**<u>Takeaway.</u>** video generation models are more prone to introducing state conditions not specified in text constraints, leading to chaotic variations. In contrast, image generation models demonstrate higher compliance with textual instruction constraints.\n\n5.6.3 Time-Space Consistency Results: Navigation Exposes the Missing World State\n\nTime-Space consistency evaluates whether a model maintains an invariant spatial structure while exe-\ncuting temporally extended motion. Figure 37 visualizes performance across ST1–ST3 and highlights\na central message consistent with CoW-Bench’s motivation: models can achieve strong *local* motion\nplausibility and even stable environments, yet fail when the task requires a persistent, goal-directed\nworld state.\n\nFigure 37: Time-Space consistency on CoW-Bench, shown as a heatmap over sub-metrics (rows) and models (columns), with scores on the 0–2 scale (higher is better). Rows cover three metric families: ST1 Maze-2D (Start/Goal, Traj-cont, Legal, Correct, Struct-stab), ST2 Occlusion Dynamics under Motion (Occ-move, Parallax, Rigid, Natural, Env-stab), and ST3 3D Loop Navigation (Struct, Rel, View-smooth, Physical, Entity-stab).\n\n**(1) Maze-2D remains the sharpest discriminator.** In ST1, many video generators score non-trivially on Legal and sometimes on Struct-stab, but still fall to near zero on Start/Goal and Correct (e.g., Sora and Kling show low Correct despite moderate Legal). This pattern indicates that the core failure is not producing a plausible maze-like motion, but maintaining a single, identifiable trajectory that starts from the correct anchor and reaches the correct goal without implicit resets or shortcutting. By contrast, stronger image-centric models obtain substantially higher ST1 correctness, suggesting that explicit goal-conditioned state tracking is still the limiting factor for video-style generation.\n\n---\n\n(2) **Occlusion-under-motion is comparatively mature, with remaining errors concentrated on depth-layer updates.** For ST2, many models achieve high *Rigid*, *Natural*, and *Env-stab*, which suggests that layered motion and global temporal stability are increasingly handled well. The remaining spread concentrates on *Occupate* and *Parallax*, implying that the hardest cases involve consistent depth ordering and visibility updates under motion rather than overall smoothness.\n\n(3) 3D loop navigation stresses viewpoint continuity and relational stability. In ST3, leading models maintain strong *Struct* and *Entity-stab*, but weaker systems drop on *View-smooth* and *Rel*. This is consistent with a \"viewpoint reset\" failure mode: geometry can look plausible frame-by-frame, yet the sequence cannot be explained as a single 3D scene traversed along a continuous camera path. CoW-Bench therefore treats loop navigation as a probe of whether models preserve state under transformation, beyond single-view realism.\n\n**Takeaway.** video generation models often yield results with greater freedom and a more\npronounced sense of temporal progression, whereas image generation models tend to depict\nchanges over time in a more disjointed manner, with less scope for creative expression.\n\n# 5.7 Sample Analysis\n\nTo deeply analyze the model’s inference mechanism under multi-dimensional constraints, we have\nconstructed a new set of evaluation criteria in CoW-Bench based on the six core consistency challenges\ndefined previously. Moving beyond traditional evaluations that focus solely on visual quality, this\nbenchmark utilizes frame-by-frame Physical State Ground Truth to precisely quantify the fundamental\ndifferences between a \"Generator\" and a \"World Simulator\" from the perspectives of dataset\nconstruction and model capability boundaries.\n\n**5.7.1 Single Consistency Tasks**\n\nThe design philosophy of single consistency tasks is to isolate complex interference and use the purity\nof simulated data to establish the baseline for a model’s foundational reasoning. The specific effects of\neach sub-task are illustrated in Figures 38–40.\n\n**Modal Consistency Tasks.** This task examines whether a model can clearly distinguish between constraints from different modalities and avoid information blending. In *Subject Attribute Fidelity*, models demonstrate strong feature decoupling capabilities, successfully extracting the texture and material of a \"butterfly\" from a reference image and mapping it onto the geometric structure of a \"fish.\" The resulting creature possesses distinct scale and luster features without incorporating the butterfly's wing morphology. For finer-grained *Local Edit Precision*, models exhibit precise pixel control in a clock-editing task, modifying only the hour and minute hands according to instructions while the background wall and clock frame remain strictly \"locked.\" Furthermore, under the complex instructions of *Multi-constraint Satisfaction*, models accurately capture the clothing, actions, and positional attributes of multiple characters without incorrect role assignment or attribute leakage, proving their precision in parsing long-text constraints.\n\n**Spatial Consistency Tasks.** This task evaluates whether the scenes established by the model are geometrically self-consistent rather than merely \"appearing plausible\" in 2D images. In *Sem-Planar*, models correctly understand relative positions in non-occluded scenarios, with two cats moving to the left and right respectively without confusing the directional semantics. For more complex *Occlusion/Containment*, as a drawer slowly closes, the model correctly renders the process of internal books gradually moving into darkness; the books follow physical laws of occlusion rather than disappearing abruptly, reflecting an understanding of the \"container\" concept. In tests of *MV-3D*, as\n\n---\n\n**Figure 38: Example diagrams of Modal sub-tasks.**\n\nthe viewpoint slowly shifts, the desk lamp naturally disappears at the edge of the field of view while the bed layout gradually reveals itself. Throughout this process, the relative positions of objects in the room remain unchanged and the lighting environment stays stable.\n\n**Temporal Consistency Tasks.** We elevate temporal consistency from \"visual smoothness\" to \"rule-governed evolution,\" examining whether models follow the world's implicit laws. In *Worldline Persistence*, an electric fan maintains the physical integrity of its blades during long-term rotation, with no blade breakage or sudden material mutations. *Rule-guided Slow Evolution* further demonstrates the model's understanding of physical entropy: in a simulated one-hour duration, a candle gradually shortens according to combustion laws rather than staying the same in violation of common sense. In the house-collapse task of *Ordered Stage Transitions*, the model clearly displays a continuous state from structural integrity to ruins. The collapse sequence follows gravitational logic, and the ruins remain static after falling, avoiding non-causal jittering such as \"collapsing and then recovering.\"\n\n---\n\n## 5.7.2 Compound Consistency Tasks\n\nCompound consistency tasks simulate the complexity of the real world, examining the model's trade-off and reasoning capabilities when multi-dimensional constraints restrict (or even conflict with) each other. The specific effects of each sub-task are illustrated in Figures 41–43.\n\n**Modal-Spatial Consistency Tasks** evaluate the model's ability to transform semantic information into executable spatial constraints, achieving \"semantic–spatial coupling.\" The model must not only understand what an object is but also precisely execute geometric instructions regarding where the object is, ensuring accurate grounding of semantic referents in the spatial dimension. As shown in Figure 41, in *Sem-Planar*, models successfully identify a specific vehicle with a \"blue roof\" within complex traffic flow and control only that vehicle to move right, achieving precise semantic-spatial binding. In the *Sem-Hier* task, models accurately generate a scene where \"apples are in the bowl\" and \"pears are outside,\" strictly adhering to the spatial semantics of containment and exclusion. However, *Sem-MV* exposes a current weakness: during viewpoint transitions, while the perspective change of a signpost remains reasonable, the occlusion relationship of books and pens relative to the signpost undergoes erroneous drift (moving from behind to the side), indicating that the model's ability to maintain micro-spatial semantics under dynamic viewpoints still requires improvement.\n\n**Modal-Temporal Consistency Tasks** evaluate the model's fidelity to instructions during long-sequence generation. The core is to examine whether the model treats the prompt as a high-priority \"temporal constitution,\" implementing semantic elements and logical constraints throughout the entire video to resist semantic drift and forgetting over time. As shown in Figure 42, in *Long Horizon* tasks, the body color, patterns, and relative positions of a vehicle remain highly stable during long-distance movement, with no blurring or texture alterations. *Attribute Dynamic* further tests temporal programming capabilities, where models successfully control a sphere's color to change in a complex sequence of \"red → orange → green → blue → red\" with clear steps and no color bleeding. In *Trigger Event*, models demonstrate acute capture of causal logic: a phone screen stays black before a button is pressed and lights up instantly only after the action is triggered, aligning exactly to the event's trigger point.\n\n**Spatial-Temporal Consistency Tasks** evaluate whether models possess the prototype of a \"built-in physical engine\" under weak modality constraints. It focuses on whether the model can maintain the self-consistency of spatial topology and motion parallax during dynamic evolution, rather than relying solely on pixel-level smooth interpolation. As shown in Figure 43, in *Maze-2D*, although models maintain the static structure of the maze walls, the subject ultimately fails to correctly plan a path to the goal, suggesting limitations in spatial reasoning. In contrast, *Occlusion Dynamics under Motion* perfectly reproduces motion parallax: near trees move at high speed to create motion blur, while the background moves slowly and the vehicle remains relatively stationary, achieving dual spatio-temporal self-consistency. Finally, *3D Loop Navigation* achieves a closed-loop roaming from a bedroom to a city and back, with smooth structural continuity and no spatial collapse, demonstrating potential reference frame stability over long-term roaming.\n\nTo comprehensively verify these mechanisms, we used CoW-Bench to test mainstream world models including Sora [2], Kling [279], GPT-Image-1.5 [504], Seedream-4-5 [502], Nano Banana Pro [503], Wan2.2-I2V-14B [273], SkyReels-V2 [508], HunyuanVideo [506], BAGEL [8], and Emu3.5 [511]. We display selected results: single consistency comparisons are shown in Figure 44 (Modal), Figure 45 (Spatial), and Figure 46 (Temporal); compound consistency results are shown in Figure 47 (Modal-Spatial), Figure 48 (Modal-Temporal), and Figure 49 (Spatial-Temporal).\n\n**Takeaway.** CoW-Bench establishes a new standard for evaluating world models. While video models outperform image models in instruction following, they fundamentally lack physical evolutionary logic. Heavily relying on pixel-based interpolation rather than genuine reasoning, these models exhibit non-physical distortions and topological collapse.\n\n---\n\n# 6 Conclusion\n\nThis survey has re-examined the trajectory of generative AI through the lens of the *Trinity of Consistency*, establishing a general framework for what constitutes a World Model. By deconstructing the capability space into Modality, Spatial, and Temporal dimensions, we argue that true physical understanding does not emerge from single-axis performance but from the robustness of cross-dimensional interactions. Our analysis highlights that the most critical failures in current systems are not visual artifacts, but ruptures in consistency: the inability to bind semantic instructions to geometric roles (Modal-Space), the failure to maintain identity under long-horizon evolution (Modal-Time), and the loss of environmental permanence during navigation (Time-Space).\n\nFigure 50: Evolutionary spectrum of World Model paradigms based on interactive action spaces. This figure illustrates the field's trajectory from the early \"Vector-as-Action\" paradigm (left: e.g., JEPA, relying on uninterpretable latent space predictions), through the intermediate \"Key-as-Action\" paradigm (center: e.g., Genie series, constrained by predefined discrete control spaces), and ultimately advancing towards the \"Prompt-as-Action\" paradigm. In the latter, a semantic compiler translates natural language intents into universal spatiotemporal dynamic simulations.\n\nTo rigorously diagnose these cross-dimensional ruptures, we introduced CoW-Bench, a comprehensive benchmark that unifies the evaluation of mainstream video generation models and UMMs under a shared protocol. CoW-Bench employs a carefully designed multi-frame evaluation protocol derived from human expert reasoning. By analyzing temporally sampled grids against fine-grained atomic checklists, we operationalize consistency as a strict constraint-satisfaction problem. This rigorous approach exposes a pervasive *constraint-backoff* phenomenon, where models generate plausible-looking textures while silently violating logical commitments, thus providing the necessary diagnostic resolution to distinguish between visual mimicry and genuine physical simulation.\n\nCrucially, our findings indicate that constraint backoff is not merely a consequence of insufficient training data or scale, but a structural artifact of how current models represent interaction. When the action space is either uninterpretable or rigidly predefined, models lack the expressive capacity to ground semantic commitments in physical dynamics. Under such constraints, consistency violations become not accidental errors but almost inevitable outcomes. Addressing this limitation therefore demands a paradigm shift in how interaction itself is formalized within world models.\n\nTo systematically characterize this transition, we organize the evolution of world model paradigms according to the expressiveness of their interactive action spaces (Figure 50). As illustrated on the left side of the figure, early explorations such as JEPA [1] operate at the *Vector-as-Action* level. While enabling latent-space prediction, their interaction mechanisms remain opaque and lack semantic interpretability. The middle section presents the *Key-as-Action* paradigm, exemplified by the Genie series [438, 514, 515]. Although introducing limited interactivity, these models remain confined to narrow, discrete, and predefined action spaces.\n\n---\n\nThe right side of the figure illustrates a forward-looking paradigm: a Prompt-as-Action paradigm in which UMMs with modality consistency and video generation models with spatial-temporal consistency are unified. Equipped with an internal semantic compiler, such models can interpret high-dimensional natural-language prompts and translate them into universal spatiotemporal simulations that adhere to the *Trinity of Consistency*. Recent systems such as PixVerse-R1 [516] offer an early glimpse of this direction, demonstrating real-time world modeling that responds instantly to user input and unifies multiple modalities within an autoregressive architecture. By moving beyond predefined action abstractions, this paradigm begins to bridge the gap between human semantic intent and the underlying dynamics of the physical world.\n\nThe central conviction of this survey is therefore simple yet uncompromising: consistency is not an\noptional attribute of a world model—it is its criterion of existence. A system that produces visually\ncompelling pixels but fails to maintain cross-dimensional consistency, regardless of scale, remains\nfundamentally a texture synthesizer rather than a simulator of the world. *The Trinity of Consistency* thus\ndelineates more than an analytical framework; it marks a boundary—a paradigmatic divide between\ngenerating images that resemble the world and constructing models that understand it.\n\n---\n\nFigure 39: Example diagrams of Spatial sub-tasks.\n\n---\n\nThe Trinity of Consistency as a Defining Principle for General World Models\n\n**Temporal**\n\nWorldline\n\nThe ceiling fan rotates continuously for five minutes, with the blades spinning smoothly and the body remaining stationary.\n\nChecklist:\n\n- Same fan identity; no unit swap or shape change.\n\n- Stable material, color, and texture; no edge redraw.\n\n- Ceiling background stable; no lighting or texture jumps.\n\n- No flicker, jitter, warping, or breathing artifacts.\n\n- Smooth rotation at consistent speed; no stop, reverse, or erratic accel.\n\nSlow-Evol\n\nTime lapse: An hour passes.\n\nChecklist:\n\n- Background stable; no scene redraw or pop-in/out.\n\n- Flame flickers and steadily diminishes; no rekindle or rollback.\n\n- Wax height steadily drops; drips accumulate and solidify; no instant melt/vanish.\n\n- At least three linked stages (full → melting/drips → shortened with solidified drips);\n\n- smooth transitions.\n\n- Candlelight dims in sync with burn-down; no abrupt illumination jumps.\n\nStage-Order\n\nDemolish a house made of\n\nbuilding blocks.\n\nChecklist:\n\n- Initial Phase: Remove doors/windows/roof; no premature collapse.\n\n- Intermediate Phase: Sequentially dismantle all four walls; no disorderly collapses.\n\n• Subsequent Phase: Remove only foundation blocks; no other changes.\n\n- Final State: All blocks settle smoothly onto the ground; no additional actions.\n\n- Scene and block appearances remain consistent; no new objects added.\n\n**Figure 40: Example diagrams of Temporal sub-tasks.**\n\n---\n\nFigure 41: Example diagrams of modal and spatial sub-tasks.\n\n---\n\nFigure 42: Example diagrams of modal and temporal sub-tasks.\n\n---\n\nFigure 43: Example diagrams of spatial and temporal sub-tasks.\n\n---\n\n## The Trinity of Consistency as a Defining Principle for General World Models\n\nFigure 44: Comparison with different models for modal consistency task.\n\n---\n\n## Model Comparison\n\n**Model Sota**แข Admiral oga\n\n**Comparison Assets** Alogisticbinary\n\n** Cohen's_d**\n\n---\n\n## Figure 45: Comparison with different models for spatial consistency task.\n\n---\n\nFigure 46: Comparison with different models for temporal consistency task.\n\n---\n\nFigure 47: Comparison with different models for modal and spatial consistency task.\n\n---\n\n- Figure 48: Comparison with different models for modal and temporal consistency task.\n\n---\n\nFigure 49: Comparison with different models for spatial and temporal consistency task.\n\n---\n\n# 7 Contributions\n\n**Leading Authors**\n\nJingxuan Wei², Siyuan Li³, Cheng Tan¹\n\n**Core Contributors**\n\nYuhang Xu², Zheng Sun², Junjie Jiang², Hexuan Jin², Caijun Jia², Honghao He², Xinglong Xu², Xi Bai²\n\n**Other Contributors**\n\nChang Yu³, Yumou Liu⁵, Junnan Zhu², Xuanhe Zhou⁵, Jintao Chen⁶, Xiaobin Hu⁴, Shancheng Pang⁷, Bihui Yu², Ran He², Zhen Lei², Stan Z. Li³,\n\n**Corresponding Authors**\n\nConghui He¹, Shuicheng Yan⁴, Cheng Tan¹\n\n**Affiliation**\n\n¹Shanghai Artificial Intelligence Laboratory\n\n²University of Chinese Academy of Sciences\n\n³Westlake University\n\n⁴National University of Singapore\n\n⁵Shanghai Jiaotong University\n\n⁶Zhejiang University\n\n⁷China University of Petroleum (East China)\n\n---\n\nReferences\n[1] Y. LeCun, \"A path towards autonomous machine intelligence,\" OpenReview, 2022.\n[2] T. Brooks, B. Peebles, C. Holmes et al., \"Video generation models as world simulators,\" OpenAI Blog, vol. 1, no. 8, p. 1, 2024.\n[3] A. Bardes, Q. Garrido, J. Ponce et al., \"Revisiting feature prediction for learning visual representations from video,\" TMLR, 2025.\n[4] H. Lu, W. Liu, B. Zhang et al., \"Deepseek-vl: towards real-world vision-language understanding,\" arxiv, 2024.\n[5] R. Team, Z. Gao, Q. Wang et al., \"Advancing open-source world models,\" arxiv, 2026.\n[6] Runway Research, \"Gen-3 alpha: A new frontier for video generation,\" https://runwayml.com/research/introducing-gen-3-alpha, 2024, runway Technical Report.\n[7] G. Team, R. Anil, S. Borgeaud et al., \"Gemini: A family of highly capable multimodal models,\" arxiv, 2023.\n[8] C. Deng, D. Zhu, K. Li et al., \"Emerging properties in unified multimodal pretraining,\" arxiv, 2025.\n[9] M. Huh, B. Cheung, T. Wang et al., \"The platonic representation hypothesis,\" arxiv, 2024.\n[10] A. Radford, J. W. Kim, C. Hallacy et al., \"Learning transferable visual models from natural language supervision,\" in ICML. PMLR, 2021, pp. 8748-8763.\n[11] C. Jia, Y. Yang, Y. Xia et al., \"Scaling up visual and vision-language representation learning with noisy text supervision,\" in ICML, 2021.\n[12] X. Zhai, B. Mustafa, A. Kolesnikov et al., \"Sigmoid loss for language image pre-training,\" in ICCV, 2023.\n[13] H. Xu, S. Xie, X. E. Tan et al., \"Demystifying clip data,\" arxiv, 2023.\n[14] J.-B. Alayrac, J. Donahue, P. Luc et al., \"Flamingo: a visual language model for few-shot learning,\" NeurIPS, 2022.\n[15] J. Li, D. Li, C. Xiong et al., \"Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,\" in ICML. PMLR, 2022, pp. 12 888-12 900.\n[16] J. Li, D. Li, S. Savarese et al., \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,\" in ICML, 2023, pp. 19 730-19 742.\n[17] J. Bai, S. Bai, S. Yang et al., \"Qwen-vl: A frontier large vision-language model with versatile abilities,\" arxiv, 2023.\n[18] F. Li, R. Zhang, H. Zhang et al., \"Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models,\" 2024.\n[19] C. Team, \"Chameleon: Mixed-modal early-fusion foundation models,\" arxiv, 2024.\n[20] J. Lu, C. Clark, S. Lee et al., \"Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action,\" in CVPR, 2024, pp. 26 439-26 455.\n[21] J. Xie, W. Mao, Z. Bai et al., \"Show-o: One single transformer to unify multimodal understanding and generation,\" arxiv, 2024.\n[22] L. Yu, B. Shi, R. Pasunuru et al., \"Scaling autoregressive multi-modal models: Pretraining and instruction tuning (2023),\" arxiv, 2023.\n[23] H. Liu, C. Li, Q. Wu et al., \"Visual instruction tuning,\" in NeurIPS, 2023.\n[24] W. Wang, Q. Lv, W. Yu et al., \"Cogvlm: Visual expert for pretrained language models,\" NeurIPS, 2024.\n[25] Z. Chen, J. Wu, W. Wang et al., \"Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks,\" in CVPR, 2024.\n\n---\n\n[{\"bbox\": [170, 130, 874, 152], \"category\": \"Section-header\", \"text\": \"# The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [176, 207, 1066, 1414], \"category\": \"Text\", \"text\": \"[26] D. Zhu, J. Chen, X. Shen et al., \\\"Minigpt-4: Enhancing vision-language understanding with advanced large language models,\\\" arxiv, 2023.\\n[27] P. Esser, S. Kulal, A. Blattmann et al., \\\"Scaling rectified flow transformers for high-resolution image synthesis,\\\" in ICML, 2024.\\n[28] X. Wang, X. Zhang, Z. Luo et al., \\\"Emu3: Next-token prediction is all you need,\\\" arxiv, 2024.\\n[29] J. Chen, J. Yu, C. Ge et al., \\\"Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis,\\\" ICLR, 2024.\\n[30] P. Gao, L. Zhuo, D. Liu et al., \\\"Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers,\\\" arxiv, 2024.\\n[31] X. Liu, C. Gong et al., \\\"Flow straight and fast: Learning to generate and transfer data with rectified flow,\\\" in ICLR, 2023.\\n[32] X. Liu, X. Zhang, J. Ma et al., \\\"Instaflow: One step is enough for high-quality diffusion-based text-to-image generation,\\\" in ICLR, 2023.\\n[33] BlackForest Labs, \\\"Flux-1: Unleashing the power of flow matching,\\\" https://blackforestlabs.ai, 2024.\\n[34] N. Ma, M. Goldstein, M. S. Albergo et al., \\\"Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers,\\\" in ECCV, 2024.\\n[35] S. Tong, D. Fan, J. Li et al., \\\"Metamorph: Multimodal understanding and generation via instruction tuning,\\\" in ICCV, 2025, pp. 17001-17012.\\n[36] W. Jin, Y. Niu, J. Liao et al., \\\"Srum: Fine-grained self-rewarding for unified multimodal models,\\\" arxiv, 2025.\\n[37] J. Xu, X. Liu, Y. Wu et al., \\\"Imagereward: Learning and evaluating human preferences for text-to-image generation,\\\" in NeurIPS, 2023.\\n[38] Z. Lin, D. Pathak, B. Li et al., \\\"Evaluating text-to-visual generation with image-to-text generation,\\\" in ECCV. Springer, 2024, pp. 366-384.\\n[39] Z. Liang, Y. Yuan, S. Gu et al., \\\"Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization,\\\" in CVPR, 2025.\\n[40] W. Wang, Z. Gao, L. Chen et al., \\\"Visualprm: An effective process reward model for multimodal reasoning,\\\" arxiv, 2025.\\n[41] Y. Cai, K. Li, M. Jia et al., \\\"Phygdpo: Physics-aware groupwise direct preference optimization for physically consistent text-to-video generation,\\\" arxiv, 2026.\\n[42] S. Yuan, Y. Liu, Y. Yue et al., \\\"Ar-grpo: Training autoregressive image generation models via reinforcement learning,\\\" arxiv, 2025.\\n[43] T. Wang and P. Isola, \\\"Understanding contrastive representation learning through alignment and uniformity on the hypersphere,\\\" in ICML, 2020, pp. 9929-9939.\\n[44] V. W. Liang, Y. Zhang, Y. Kwon et al., \\\"Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning,\\\" NeurIPS, vol. 35, pp. 17612-17625, 2022.\\n[45] C. Snell, J. Lee, K. Xu et al., \\\"Scaling llm test-time compute optimally can be more effective than scaling model parameters,\\\" arxiv, 2024.\\n[46] S. J. Gershman and N. D. Goodman, \\\"Amortized inference in probabilistic reasoning,\\\" in ACCSS, 2014.\\n[47] S. C. Lowe, \\\"System 2 reasoning capabilities are nigh,\\\" arxiv, 2024.\\n[48] S. Yao, D. Yu, J. Zhao et al., \\\"Tree of thoughts: Deliberate problem solving with large language models,\\\" in NeurIPS, vol. 36, 2023, pp. 11809-11822.\\n[49] A. Van Den Oord, O. Vinyals et al., \\\"Neural discrete representation learning,\\\" NeurIPS, 2017.\"}]\n\n---\n\n[{\"bbox\": [164, 126, 875, 153], \"category\": \"Page-header\", \"text\": \"## The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [169, 200, 1073, 1450], \"category\": \"Text\", \"text\": \"[50] A. Ramesh, M. Pavlov, G. Goh et al., \\\"Zero-shot text-to-image generation,\\\" in ICML. PMLR, 2021, pp. 8821–8831.\\n[51] M. Huh, B. Cheung, P. Agrawal et al., \\\"Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks,\\\" in ICML, 2023.\\n[52] T. Xiong, J. H. Liew, Z. Huang et al., \\\"Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation,\\\" in ICCV, 2025.\\n[53] S. Li, L. Zhang, Z. Wang et al., \\\"Mergevq: A unified framework for visual generation and representation with disentangled token merging and quantization,\\\" in CVPR, 2025.\\n[54] S. Bengio, O. Vinyals, N. Jaitly et al., \\\"Scheduled sampling for sequence prediction with recurrent neural networks,\\\" in NeurIPS, 2015.\\n[55] Y. Lipman, R. T. Chen, H. Ben-Hamu et al., \\\"Flow matching for generative modeling,\\\" arxiv, 2022.\\n[56] X. Liu, C. Gong, and Q. Liu, \\\"Flow straight and fast: Learning to generate and transfer data with rectified flow,\\\" arxiv, 2022.\\n[57] C. Jia, Y. Yang, Y. Xia et al., \\\"Scaling up visual and vision-language representation learning with noisy text supervision,\\\" in ICML. PMLR, 2021, pp. 4904–4916.\\n[58] L. Jiasen, C. Christopher, Z. Rowan et al., \\\"Unified-io: A unified model for vision, language, and multi-modal tasks,\\\" arxiv, 2022.\\n[59] C. Team, \\\"Chameleon: Mixed-modal early-fusion foundation models,\\\" arxiv, 2024.\\n[60] C. Ma and Y. Zhang, \\\"Theoretical bounds of modality alignment in world models,\\\" in ICML, 2024.\\n[61] K. Lee, H. Liu, M. Ryu et al., \\\"Aligning text-to-image models using human feedback,\\\" NeurIPS, 2023.\\n[62] Y. Zhang, Y. Li, Y. Yang et al., \\\"Reasongen-r1: Cot for autoregressive image generation models through sft and r1,\\\" arxiv, 2025.\\n[63] Y. Ji, J. Li, Y. Xiang et al., \\\"A survey of test-time compute: From intuitive inference to deliberate reasoning,\\\" arxiv, 2025.\\n[64] R. Tian, M. Gao, M. Xu et al., \\\"Unigen: Enhanced training & test-time strategies for unified multimodal understanding and generation,\\\" in arxiv, 2025.\\n[65] H. He, J. Liang, X. Wang et al., \\\"Scaling image and video generation via test-time evolutionary search,\\\" arxiv, 2025.\\n[66] D. Silver, J. Schrittwieser, K. Simonyan et al., \\\"Mastering the game of go without human knowledge,\\\" Nature, 2017.\\n[67] K. Cobbe, V. Kosaraju, M. Bavarian et al., \\\"Training verifiers to solve math word problems,\\\" arxiv, 2021.\\n[68] Z. Huang, N. Yu, G. Chen et al., \\\"Vchain: Chain-of-visual-thought for reasoning in video generation,\\\" arxiv, 2025.\\n[69] R. Baillargeon, \\\"Object permanence in 3½- and 4½-month-old infants,\\\" Dev. Psychol., vol. 23, no. 5, pp. 655–664, 1987.\\n[70] E. S. Spelke, \\\"Principles of object perception,\\\" Cog. Sci., vol. 14, no. 1, pp. 29–56, 1990.\\n[71] P. Anderson, Q. Wu, D. Teney et al., \\\"On evaluation of embodied navigation agents,\\\" arxiv, 2018.\\n[72] R. Hartley and A. Zisserman, *Multiple View Geometry in Computer Vision*, 2nd ed. Cambridge University Press, 2003.\\n[73] X. Shi, Z. Chen, H. Wang et al., \\\"Convolutional lstm network: A machine learning approach for precipitation nowcasting,\\\" NeurIPS, 2015.\\n[74] Y. Wang, M. Long, J. Wang et al., \\\"Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms,\\\" NeurIPS, 2017.\"}]\n\n---\n\n[{\"bbox\": [170, 132, 872, 154], \"category\": \"Section-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [178, 207, 1068, 1433], \"category\": \"Text\", \"text\": \"[75] V. L. Guen and N. Thome, “Disentangling physical dynamics from unknown factors for unsupervised video prediction,” in CVPR, 2020.\\n[76] E. Denton and R. Fergus, “Stochastic video generation with a learned prior,” in ICML, 2018.\\n[77] M. Raissi, P. Perdikaris, and G. Karniadakis, “Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,” J. Comput. Phys., vol. 378, pp. 686–707, 2019.\\n[78] M. Lutter, C. Ritter, and J. Peters, “Deep lagrangian networks: Using physics as model prior for deep learning,” in ICLR, 2019.\\n[79] S. Greydanus, M. Dzamba, and J. Yosinski, “Hamiltonian neural networks,” NeurIPS, vol. 32, 2019.\\n[80] Y. Rubanova, R. T. Chen, and D. K. Duvenaud, “Latent ordinary differential equations for irregularly-sampled time series,” NeurIPS, vol. 32, 2019.\\n[81] B. Mildenhall, P. P. Srinivasan, M. Tancik et al., “Nerf: Representing scenes as neural radiance fields for view synthesis,” in ECCV, 2020.\\n[82] J. T. Barron, B. Mildenhall, M. Tancik et al., “Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields,” in ICCV, 2021.\\n[83] J. T. Barron, B. Mildenhall, D. Verbin et al., “Zip-nerf: Anti-aliased grid-based neural radiance fields,” in ICCV, 2023, pp. 19797–19805.\\n[84] T. Muller, A. Evans, C. Schied et al., “Instant neural graphics primitives with a multiresolution hash encoding,” ACM TOG, 2022.\\n[85] P. Wang, L. Liu, Y. Liu et al., “Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction,” arxiv, 2021.\\n[86] L. Yariv, J. Gu, Y. Kasten et al., “Volume rendering of neural implicit surfaces,” in NeurIPS, 2021.\\n[87] A. Gropp, L. Yariv, N. Haim et al., “Implicit geometric regularization for learning shapes,” in ICML, 2020, pp. 3789–3799.\\n[88] Z. Yu, S. Peng, M. Niemeyer et al., “Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction,” NeurIPS, vol. 35, pp. 25018–25032, 2022.\\n[89] B. Kerbl, G. Kopanas, T. Leimkuhler et al., “3d gaussian splatting for real-time radiance field rendering.” TOG, 2023.\\n[90] T. Lu, M. Yu, L. Xu et al., “Scaffold-gs: Structured 3d gaussian splatting for view-adaptive rendering,” in CVPR, 2024.\\n[91] B. Huang, Z. Yu, A. Chen et al., “2d gaussian splatting for geometrically accurate radiance fields,” in SIGGRAPH, 2024.\\n[92] Z. Yu, A. Chen, B. Huang et al., “Mip-splatting: Alias-free 3d gaussian splatting,” in CVPR, 2024.\\n[93] T. Xie, Z. Zong, Y. Qiu et al., “Physgaussian: Physics-integrated 3d gaussians for generative dynamics,” in CVPR, 2024, pp. 4389–4398.\\n[94] G. Wu, T. Yi, J. Fang et al., “4d gaussian splatting for real-time dynamic scene rendering,” in CVPR, 2024.\\n[95] Z. Yang, X. Gao, W. Zhou et al., “Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction,” in CVPR, 2024.\\n[96] Z. Li, Z. Chen, Z. Li et al., “Spacetime gaussian feature splatting for real-time dynamic view synthesis,” in CVPR, 2024.\\n[97] B. Poole, A. Jain, J. T. Barron et al., “Dreamfusion: Text-to-3d using 2d diffusion,” arxiv, 2022.\\n[98] Z. Wang, C. Lu, Y. Wang et al., “Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation,” in NeurIPS, 2023.\"}]\n\n---\n\n[{\"bbox\": [168, 130, 873, 151], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [167, 200, 1065, 1433], \"category\": \"Text\", \"text\": \"[99] Y. Shi, P. Wang, J. Ye et al., \\\"Mvdream: Multi-view diffusion for 3d generation,\\\" arxiv, 2023.\\n[100] J. Tang, Z. Chen, X. Chen et al., \\\"Lgm: Large multi-view gaussian model for high-resolution 3d content creation,\\\" ECCV, 2024.\\n[101] M. Deitke, D. Schwenk, J. Salvador et al., \\\"Objaverse: A universe of annotated 3d objects,\\\" in CVPR, 2023, pp. 13142-13153.\\n[102] V. Voleti, C.-H. Yao, M. Boss et al., \\\"Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion,\\\" in ECCV, 2024.\\n[103] S. Wang, V. Leroy, Y. Cabon et al., \\\"Dust3r: Geometric 3d vision made easy,\\\" in CVPR, 2024, pp. 20697-20709.\\n[104] B. Ma, H. Gao, H. Deng et al., \\\"You see it, you got it: Learning 3d creation on pose-free videos at scale,\\\" in CVPR, 2025.\\n[105] N. Michael, T. B. Jonathan, M. Ben et al., \\\"Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs,\\\" in CVPR, 2022.\\n[106] Y. Liu, C. Lin, Z. Zeng et al., \\\"SyncSL☑mer: Generating multiview-consistent images from a single-view image,\\\" arxiv, 2023.\\n[107] J. T. Kajiya, \\\"The rendering equation,\\\" ACM SIGGRAPH, 1986.\\n[108] S. Yang, S.-D. Jascha, P. K. Diederik et al., \\\"Score-based generative modeling through stochastic differential equations,\\\" in ICLR, 2021.\\n[109] Y. Wang, Z. Gao, M. Long et al., \\\"Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning,\\\" in ICML, 2018.\\n[110] Z. Gao, C. Tan, L. Wu et al., \\\"Simvp: Simpler yet better video prediction,\\\" in CVPR, 2022.\\n[111] C. Tan, J. Wang, Z. Gao et al., \\\"Ustep: Spatio-temporal predictive learning under a unified view,\\\" IEEE T-PAMI, 2025.\\n[112] C. Tan, Z. Gao, L. Wu et al., \\\"Temporal attention unit: Towards efficient spatiotemporal predictive learning,\\\" in CVPR, 2023, pp. 18770-18782.\\n[113] J. Wei, C. Tan, Z. Gao et al., \\\"Interpretable and generalizable spatiotemporal predictive learning with disentangled consistency,\\\" in ECML/PKDD. Springer, 2024, pp. 3-20.\\n[114] S. Liu, T. Li, W. Chen et al., \\\"Soft rasterizer: A differentiable Worthl Oswčer for image-based 3d reasoning,\\\" in ICCV, 2019.\\n[115] W. Chen, H. Ling, J. Gao et al., \\\"Learning to predict 3d objects with an interpolation-based differentiable W XVI ,\\\"\\\" NeurIPS, 3019.\\n[116] E. R. Chan, C. Z. Lin, M. A. Chan et al., \\\"Efficient geometry-aware 3d generative adversarial networks,\\\" in CVPR, 2022.\\n[117] A. Chen, Z. Xu, A. Geiger et al., \\\"Tensorf: Tensorial radiance fields,\\\" in ECCV, 2022.\\n[118] J. T. Barron, B. Mildenhall, D. Verbin et al., \\\"Zip-nerf: Anti-aliased grid-based neural radiance fields,\\\" in ICCV, 2023.\\n[119] L. Yariv, Y. Kasten, D. Moran et al., \\\"Multiview neural surface reconstruction by disentangling geometry and appearance,\\\" in NeurIPS, vol. 33, 2020, pp. 2492-2502.\\n[120] K. Park, U. Sinha, P. Hedman et al., \\\"Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields,\\\" ACM TOG, 2021.\\n[121] G. Wu, T. Yi, J. Fang et al., \\\"A 4d gaussian splatting for real-time dynamic scene rendering,\\\" in CVPR, 2024, pp. 20310-20320.\\n[122] J. Luiten, G. Kopanas, B. Leibe et al., \\\"Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis,\\\" in 3DV, 2024.\"}]\n\n---\n\n[{\"bbox\": [170, 129, 871, 151], \"category\": \"Page-header\", \"text\": \"# The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [170, 201, 1068, 1444], \"category\": \"Text\", \"text\": \"[123] Z. Wang, C. Lu, Y. Wang et al., “Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation,” PR, vol. 36, pp. 8406-8441, 2023.\\n[124] J. Tang, Z. Chen, X. Chen et al., “Lgm: Large multi-view gaussian model for high-resolution 3d content creation,” in ECCV. Springer, 2024, pp. 1-18.\\n[125] X. Yu, M. Xu, Y. Zhang et al., “Mvimgnet: A large-scale dataset of multi-view images,” in CVPR, 2023, pp. 9150-9161.\\n[126] J. Reizenstein, R. Shapovalov, P. Henzler et al., “Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction,” in ICCV, 2021, pp. 10901-10911.\\n[127] V. Voleti, C.-H. Yao, M. Boss et al., “Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion,” in ECCV. Springer, 2024, pp. 439-457.\\n[128] Y. J. Ma, W. Liang, G. Wang et al., “Eureka: Human-level reward design via coding large language models,” in ICLR, 2024.\\n[129] W. Menapace, S. Lathuilière, S. Tulyakov et al., “Playable environments: Video generation in space and time,” in CVPR. IEEE, 2022, pp. 3584-3594.\\n[130] K. Dalal, D. Koceja, G. Hussein et al., “One-minute video generation with test-time training,” CVPR, 2025.\\n[131] L. Khachatryan, A. Movsisyan, V. Tadevosyan et al., “Text2video-zero: Text-to-image diffusion models are zero-shot video generators,” in ICCV. IEEE, 2023, pp. 15954-15964.\\n[132] C. Qi, X. Cun, Y. Zhang et al., “Fatezero: Fusing attentions for zero-shot text-based video editing,” in ICCV, 2023, pp. 15932-15942.\\n[133] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in ICCV, 2023.\\n[134] J. Z. Wu, Y. Ge, X. Wang et al., “Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation,” in ICCV, 2023, pp. 7623-7633.\\n[135] Y. Guo, C. Yang, A. Rao et al., “Animatediff: Animate your personalized text-to-image diffusion models without specific tuning,” arxiv, 2023.\\n[136] H. Chen, M. Xia, Y. He et al., “Videocrafter1: Open diffusion models for high-quality video generation,” arxiv, 2023.\\n[137] J. Wang, H. Yuan, D. Chen et al., “Modelscope text-to-video technical report,” arxiv, 2023.\\n[138] P. Esser, J. Chiu, P. Atighehchian et al., “Structure and content-guided video synthesis with diffusion models,” in ICCV, 2023, pp. 7346-7356.\\n[139] Y. Rao, W. Zhao, Z. Zhu et al., “Global filter networks for image classification,” NeurIPS, vol. 34, pp. 980-993, 2021.\\n[140] J. Guibas, M. Mardani, Z. Li et al., “Adaptive fourier neural operators: Efficient token mixers for transformers,” arxiv, 2021.\\n[141] C. Bai, Y. Li, Z. Zhao et al., “Fastinit: Fast noise initialization for temporally consistent video generation,” arxiv, 2025.\\n[142] H. Qiu, M. Xia, Y. Zhang et al., “Freenoise: Tuning-free longer video diffusion via noise rescheduling,” in ICLR, 2024.\\n[143] D. Kondratyuk, L. Yu, X. Gu et al., “Videopoet: A large language model for zero-shot video generation,” arxiv, 2023.\\n[144] A. Gupta, L. Yu, K. Sohn et al., “Photorealistic video generation with diffusion models,” in ECCV. Springer, 2024, pp. 393-411.\\n[145] L. Yu, J. Lezama, N. B. Gundavarapu et al., “Language model beats diffusion-tokenizer is key to visual generation,” arxiv, 2023.\"}]\n\n---\n\n[{\"bbox\": [165, 127, 874, 153], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [164, 203, 1074, 1436], \"category\": \"Text\", \"text\": \"[146] NVIDIA, N. Agarwal, A. Ali et al., \\\"Cosmos world foundation model platform for physical ai,\\\" arxiv, 2025.\\n[147] K. Tian, Y. Jiang, Z. Yuan et al., \\\"Visual autoregressive modeling: Scalable image generation via next-scale prediction,\\\" in NeurIPS, vol. 37, 2024, pp. 84-85.\\n[148] L. Zhang, S. Cai, M. Li et al., \\\"Frame context packing and drift prevention in next-frame-prediction video diffusion models,\\\" in NeurIPS, 2025.\\n[149] B. Chen, D. M. Monsó, Y. Du et al., \\\"Diffusion forcing: Next-token prediction meets full-sequence diffusion,\\\" in NeurIPS, 2024.\\n[150] W. Kong, Q. Tian, Z. Zhang et al., \\\"Hunyuanvideo: A systematic framework for large video generative models,\\\" arxiv, 2024.\\n[151] O. Bar-Tal, H. Chefer, O. Tov et al., \\\"Lumiere: A space-time diffusion model for video generation,\\\" arxiv, 2024.\\n[152] Google DeepMind, \\\"Veocap, \\\"High-fidelity video generation with compressed latent representation,\\\" https://deepmind.google/technologies/veocap, 2025, technical Report.\\n[153] Y. Jin, Z. Sun, N. Li et al., \\\"Pyramidal flow matching for efficient video generative modeling,\\\" arxiv, 2024.\\n[154] F. Liu, S. Zhang, X. Wang et al., \\\"Timestep embedding tells: Its time to cache for video diffusion model,\\\" CVPR, pp. 7353-7363, 2025.\\n[155] A. Polyak, A. Zohar, A. Brown et al., \\\"Movie gen: A cast of media foundation models,\\\" arxiv, 2024.\\n[156] H. Shao, S. Qian, H. Xiao et al., \\\"Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning,\\\" NeurIPS, vol. 37, pp. 8612-8642, 2024.\\n[157] X. Wang and D. Zhou, \\\"Chain-of-thought reasoning without prompting,\\\" NeurIPS, vol. 37, pp. 6678-6679, 2024.\\n[158] X. Lai, J. Li, W. Li et al., \\\"Mini-im: Scaling up reasoning patterns and interaction turns for visual search,\\\" arxiv, 2025.\\n[159] S. Wang, J. Jin, X. Wang et al., \\\"Video-thinker: Sparking thinking with videos via reinforcement learning,\\\" arxiv, 2025.\\n[160] H. Liu, K. Luo, J. Wang et al., \\\"Thinksound: Chain-of-thought reasoning in multimodal large language models for audio generation and editing,\\\" arxiv, 2025.\\n[161] S. Motamed, L. Culp, K. Swersky et al., \\\"Do generative video models understand physical principles?\\\" arxiv, 2025.\\n[162] T. Aoshima, Y. Shinohara, and B. Park, \\\"Video consistency distance: Enhancing temporal consistency for image-to-video generation via reward-based fine-tuning,\\\" arxiv, 2025.\\n[163] D. Ha and J. Schmidhuber, \\\"World models,\\\" in NeurIPS, 2018.\\n[164] T. Zhang, H.-X. Yu, R. Wu et al., \\\"Physdreamer: Physics-based interaction with 3d objects via video generation,\\\" in ECCV. Springer, 2024, pp. 388-406.\\n[165] T. Unterthiner, S. van Steenkiste, K. Kurach et al., \\\"Towards accurate generative models of video: A new metric & challenges,\\\" arxiv, 2018.\\n[166] T. Aoshima, Y. Shinohara, and B. Park, \\\"Video consistency distance: Enhancing temporal consistency for image-to-video generation via reward-based fine-tuning,\\\" arxiv, 2025.\\n[167] T. Wiedemer, Y. Li, P. Vicol et al., \\\"Video models are zero-shot learners and reasoners,\\\" arxiv, 2025.\\n[168] F. Sun, J. Liu, J. Wu et al., \\\"Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer,\\\" in ACM CIKM, 2019, pp. 1441-1450.\\n[169] H. Chen, Y. Lin, M. Pan et al., \\\"Denoising self-attentive sequential recommendation,\\\" in ACM RecSys, 2022, pp. 92-101.\"}]\n\n---\n\n[{\"bbox\": [162, 126, 878, 156], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [163, 203, 1069, 1415], \"category\": \"Text\", \"text\": \"[170] J. Xing, M. Xia, Y. Zhang et al., \\\"Dynamicrafter: Animating open-domain images with video diffusion priors,\\\" in ECCV, 2024.\\n[171] W. Hong, M. Ding, W. Zheng et al., \\\"Cogvideo: Large-scale pretraining for text-to-video generation via transformers,\\\" arxiv, 2022.\\n[172] A. K. Akan and Y. Yemez, \\\"Compositional video synthesis by temporal object-centric learning,\\\" arxiv, 2025.\\n[173] K. Tian, Y. Jiang, Z. Yuan et al., \\\"Visual autoregressive modeling: Scalable image generation via next-scale prediction,\\\" NeurIPS, vol. 37, pp. 84839-84865, 2024.\\n[174] L. Zhang, S. Cai, M. Li et al., \\\"Pretraining frame preservation in autoregressive video memory compression,\\\" arxiv, 2025.\\n[175] Y. Bengio, N. Léonard, and A. Courville, \\\"Estimating or propagating gradients through stochastic neurons for conditional computation,\\\" arxiv, 2013.\\n[176] J. Xie, W. Mao, Z. Bai et al., \\\"Show-o: One single transformer to unify multimodal understanding and generation,\\\" arxiv, 2024.\\n[177] A. Polyak, A. Zohar, A. Brown et al., \\\"Movie gen: A cast of media foundation models,\\\" arxiv, 2024.\\n[178] Y. Zeng, G. Wei, J. Zheng et al., \\\"Make pixels dance: High-dynamic video generation,\\\" arxiv, 2023.\\n[179] Z. Yang, J. Teng, W. Zheng et al., \\\"Cogvideox: Text-to-video diffusion models with an expert transformer,\\\" arxiv, 2024.\\n[180] Z. Zhang, A. Zhang, M. Li et al., \\\"Multimodal chain-of-thought reasoning in language models,\\\" arxiv, 2023.\\n[181] OpenAI, \\\"Gpt-4(vision) system card,\\\" https://cdn.openai.com/papers/GPTV_System_Card.pdf, sep 2023, technical report.\\n[182] Gpt-4 technical report,\\\" arxiv, 2023.\\n[183] S. Yin, C. Fu, S. Zhao et al., \\\"A survey on multimodal large language models,\\\" NSR, vol. 11, no. 12, p. nwaec403, 2024.\\n[184] T. Brown, B. Mann, N. Ryder et al., \\\"Language models are few-shot learners,\\\" NeurIPS, vol. 33, pp. 1877-1901, 2020.\\n[185] H. Touvron, T. Lavril, G. Izacard et al., \\\"Llama: Open and efficient foundation language models,\\\" arxiv, 2023.\\n[186] J. Wei, X. Wang, D. Schuurmans et al., \\\"Chain-of-thought prompting elicits reasoning in large language models,\\\" NeurIPS, vol. 35, pp. 24824-24837, 2022.\\n[187] T. Kojima, S. S. Gu, M. Reidsma et al., \\\"Large language models are zero-shot reasoners,\\\" in NeurIPS, vol. 35, 2022, pp. 22199-22213.\\n[188] D. Zhu, J. Chen, X. Shen et al., \\\"MiniGpt-4: Enhancing vision-language understanding with advanced large language models,\\\" arxiv, 2023.\\n[189] P. Gao, J. Zhang, R. Liu et al., \\\"Llama-adapter: Efficient fine-tuning of language models with zero-init attention,\\\" arxiv, 2023.\\n[190] X. Zhai, B. Mustafa, A. Kolesnikov et al., \\\"Sigmoid loss for language image pre-training,\\\" in ICCV, 2023, pp. 11975-11986.\\n[191] N. Tishby and N. Zaslavsky, \\\"Deep learning and the information bottleneck principle,\\\" ITW, 2015.\\n[192] W. Kim, B. Son, and I. Kim, \\\"Vilt: Vision-and-language transformer without convolution or region supervision,\\\" in ICML, 2021, pp. 5583-5594.\\n[193] J. Wang, Z. Yang, X. Hu et al., \\\"Git: A generative image-to-text transformer for vision and language,\\\" TMLR, 2022.\"}]\n\n---\n\n[{\"bbox\": [170, 131, 875, 149], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [170, 209, 1064, 1423], \"category\": \"Text\", \"text\": \"[194] Z. Yang, Z. Gan, J. Wang et al., \\\"Mm-react: Prompting chatgpt for multimodal reasoning and action,\\\" arxiv, 2023.\\n[195] C. Wu, S. Yin, W. Qi et al., \\\"Visual chatgpt: Talking, drawing and editing with visual foundation models,\\\" arxiv, 2023.\\n[196] T. Schick, J. Dwivedi-Yu, R. Dessi et al., \\\"Toolformer: Language models can teach themselves to use tools,\\\" in NeurIPS, 2023.\\n[197] D. Surís, S. Menon, and C. Vondrick, \\\"Vipergpt: Visual inference via python execution for reasoning,\\\" in ICCV, 2023.\\n[198] G. Tanmay and K. Aniruddha, \\\"Visual programming: Compositional visual reasoning without training,\\\" in CVPR, 2023.\\n[199] D. Driess, F. Xia, M. S. Sajjadi et al., \\\"Palm-e: An embodied multimodal language model,\\\" arxiv, 2023.\\n[200] G. Wang, Y. Xie, Y. Jiang et al., \\\"Voyager: An open-ended embodied agent with large language models,\\\" in arxiv, 2024.\\n[201] Y. Shen, K. Song, X. Tan et al., \\\"Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face,\\\" in NeurIPS, 2023.\\n[202] S. Yao, J. Zhao, D. Yu et al., \\\"React: Synergizing reasoning and acting in language models,\\\" in ICLR, 2023.\\n[203] T. B. Richards, \\\"Auto-gpt: An autonomous gpt-4 experiment,\\\" https://github.com/Significant-Gravitas/Auto-GPT, 2023.\\n[204] Z. Yang, S. Wang, M. Ma et al., \\\"Unisim: A neural closed-loop sensor simulator,\\\" in CVPR, 2023, pp. 1389-1399.\\n[205] A. Hu, L. Russell, H. Yeo et al., \\\"Gaia-1: A generative world model for autonomous driving,\\\" arxiv, 2023.\\n[206] Y. Li, H. Liu, Q. Wu et al., \\\"Gligen: Open-set grounded text-to-image generation,\\\" in CVPR, 2023, pp. 22511-22521.\\n[207] L. Lian, B. Shi, A. Yala et al., \\\"Llm-grounded video diffusion models,\\\" in ICLR, 2024.\\n[208] R. Liu, R. Wu, B. Van Hoorick et al., \\\"Zero-1-to-3: Zero-shot one image to 3d object,\\\" in ICCV, 2023.\\n[209] J. Ho, T. Salimans, A. Gritsenko et al., \\\"Video diffusion models,\\\" in NeurIPS, 2022.\\n[210] B. Zeng, L. Yang, J. Liu et al., \\\"Editworld: Simulating world dynamics for instruction-following image editing,\\\" in ACM MM, 2025, pp. 12674-12681.\\n[211] Z. Zhang, D. Chen, and J. Liao, \\\"Sgedit: Bridging llm with text2image generative model for scene graph-based image editing,\\\" arxiv, 2024.\\n[212] Z. M. Wang, K. Zhu, C. Xu et al., \\\"Mio: A foundation model on multimodal tokens,\\\" in EMNLP, 2025, pp. 5077-5099.\\n[213] M.-S. Kwak, J. Kim, S. Yun et al., \\\"Aligned novel view image and geometry synthesis via cross-modal attention instillation,\\\" arxiv, 2025.\\n[214] X. Long, Y.-C. Guo, C. Lin et al., \\\"Wonder3d: Single image to 3d using cross-domain diffusion,\\\" in CVPR, 2024, pp. 9970-9980.\\n[215] M. Liu, R. Shi, K. Kuang et al., \\\"Openshape: Scaling up 3d shape representation towards open-world understanding,\\\" PR, vol. 36, pp. 44860-44879, 2023.\\n[216] S. Krakovsky, G. Fiebelman, S. Benaim et al., \\\"Lang3d-xl: Language embedded 3d gaussians for large-scale scenes,\\\" in ACM SIGGRAPH, 2025, pp. 1-11.\\n[217] K. Black, M. Janner, Y. Du et al., \\\"Training diffusion models with reinforcement learning,\\\" in arxiv, 2023.\\n[218] B. Li, X. Li, J. Xu et al., \\\"Test-time preference optimization for image restoration,\\\" arxiv, 2025.\"}]\n\n---\n\n[{\"bbox\": [168, 128, 877, 154], \"category\": \"Page-header\", \"text\": \"# The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [168, 207, 1064, 1409], \"category\": \"Text\", \"text\": \"[219] H. Shi, J. Su, H. Ning et al., \\\"Layoutcot: Unleashing the deep reasoning potential of large language models for layout generation,\\\" arxiv, 2025.\\n[220] C.-H. Lin, J. Gao, L. Tang et al., \\\"Magic3d: High-resolution text-to-3d content creation,\\\" in CVPR, 2023, pp. 300–309.\\n[221] J. Ho, W. Chan, C. Saharia et al., \\\" Adam video: High definition video generation with diffusion models,\\\" arxiv, 2022.\\n[222] P. Esser, R. Rombach, and B. Ommer, \\\"Taming transformers for high-resolution image synthesis,\\\" in CVPR, 2021.\\n[223] R. Rombach, A. Blattmann, D. Lorenz et al., \\\"High-resolution image synthesis with latent diffusion models,\\\" in CVPR, 2022, pp. 10684–10695.\\n[224] A. Blattmann, R. Rombach, H. Ling et al., \\\"Align your latents: High-resolution video synthesis with latent diffusion models,\\\" in CVPR, 2023, pp. 22563–22575.\\n[225] S. Liu, Y. Han, P. Xing et al., \\\"Step1x-edit: A practical framework for general image editing,\\\" arxiv, 2025.\\n[226] R. Dong, C. Han, Y. Peng et al., \\\"Dreamllm: Synergistic multimodal comprehension and creation,\\\" arxiv, 2023.\\n[227] X. Chen, Z. Zhang, H. Zhang et al., \\\"Unireal: Universal image generation and editing via learning real-world dynamics,\\\" in PR, 2025, pp. 12501–12511.\\n[228] H. Zhao, Z. Cai, S. Si et al., \\\"Mentor: Efficient multimodal-conditioned tuning for autoregressive vision generation models,\\\" arxiv, 2025.\\n[229] N. G. Nair, S. Kaza, X. Luo et al., \\\"Scaling transformer-based novel view synthesis with models token disentanglement and synthetic data,\\\" in ICCV, 2025, pp. 28567–28576.\\n[230] L. Melas-Kyriazi, I. Laina, C. Rupprecht et al., \\\"Realfusion: 360deg reconstruction of any object from a single image,\\\" in CVPR, 2023, pp. 8446–8455.\\n[231] T. Hunyuan3D, B. Zhang, C. Guo et al., \\\"Hunyuan3d-omni: A unified framework for controllable generation of 3d assets,\\\" arxiv, 2025.\\n[232] F. Liu, H. Li, J. Chi et al., \\\"Langscene-x: Reconstruct generalizable 3d language-embedded scenes with trimap video diffusion,\\\" arxiv, 2025.\\n[233] X. Yin, Q. Zhang, J. Chang et al., \\\"Gsfixer: Improving 3d gaussian splatting with reference-guided video diffusion priors,\\\" arxiv, 2025.\\n[234] J. Chen, Y. Qin, L. Liu et al., \\\"Nerf-hugs: Improved neural radiance fields in non-static scenes using heuristics-guided segmentation,\\\" in CVPR, 2024, pp. 19436–19446.\\n[235] L. Xue, N. Yu, S. Zhang et al., \\\"Ulip-2: Towards scalable multimodal pre-training for 3d understanding,\\\" in CVPR, 2024, pp. 27091–27101.\\n[236] J. Ye, Z. Wang, R. Zhao et al., \\\"Shapellm-omni: A native multimodal llm for 3d generation and understanding,\\\" arxiv, 2025.\\n[237] X. Guo, Z. Wu, K. Xiong et al., \\\"Genesis: Multimodal driving scene generation with spatio-temporal and cross-modal consistency,\\\" arxiv, 2025.\\n[238] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, \\\"Viewset diffusion: (0-) image-conditioned 3d generative models from 2d data,\\\" in ICCV, 2023, pp. 8863–8873.\\n[239] J. Kerr, C. M. Kim, K. Goldberg et al., \\\"Lerf: Language embedded radiance fields,\\\" in ICCV, 2023, pp. 19729–19739.\\n[240] A. Haque, M. Tancik, A. A. Efros et al., \\\"Instruct-nerf2nerf: Editing 3d scenes with instructions,\\\" in ICCV, 2023, pp. 19740–19750.\"}]\n\n---\n\n[{\"bbox\": [166, 127, 876, 154], \"category\": \"Page-header\"}, {\"bbox\": [167, 201, 1070, 1423], \"category\": \"Text\"}]\n\n---\n\n[{\"bbox\": [166, 126, 874, 152], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [168, 205, 1069, 1434], \"category\": \"Text\", \"text\": \"[264] C. Low, W. Wang, and C. Katyal, \\\"Ovi: Twin backbone cross-modal fusion for audio-video generation,\\\" *arxiv*, 2025.\\n\\n[265] J. Liu, H. Chen, P. An et al., \\\"Hybridvdla: Collaborative diffusion and autoregression in a unified vision-language-action model,\\\" *arxiv*, 2025.\\n\\n[266] C. Zhang, Y. Liang, X. Qiu et al., \\\"Vast 1.0: A unified framework for controllable and consistent video generation,\\\" *arxiv*, 2024.\\n\\n[267] J. Feng, A. Ma, J. Wang et al., \\\"Fancyvideo: Towards dynamic and consistent video generation via cross-frame textual guidance,\\\" *arxiv*, 2024.\\n\\n[268] R. Villegas, M. Babaeizadeh, P.-J. Kindermans et al., \\\"Phenaki: Variable length video generation from open domain textual description,\\\" in *ICLR*, 2023.\\n\\n[269] Z. Tan, H. Yang, L. Qin et al., \\\"Omni-video: Democratizing unified video understanding and generation,\\\" *arxiv*, 2025.\\n\\n[270] R. Liu, H. Wu, Z. Zheng et al., \\\"Videodpo: Omni-preference alignment for video diffusion generation,\\\" in *CVPR*, 2025, pp. 8009-8019.\\n\\n[271] J. Cheng, R. Lyu, X. Gu et al., \\\"Vpo: Aligning text-to-video generation models with prompt optimization,\\\" *arxiv*, 2025.\\n\\n[272] Y. Liu, K. Zhang, Y. Li et al., \\\"Sora: A review on background, technology, limitations, and opportunities of large vision models,\\\" *arxiv*, 2024.\\n\\n[273] T. Wan, A. Wang, B. Ai et al., \\\"Wan: Open and advanced large-scale video generative models,\\\" *arxiv*, 2025.\\n\\n[274] F. Bao, C. Xiang, G. Yue et al., \\\"Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models,\\\" *arxiv*, 2024.\\n\\n[275] U. Singer, A. Polyak, T. Hayes et al., \\\"Make-a-video: Text-to-video generation without text-video data,\\\" *arxiv*, 2022.\\n\\n[276] L. Ruan, L. Tian, C. Huang et al., \\\"Univg: Towards unified-modal video generation,\\\" in *IEEE ICME*, 2025, pp. 1-6.\\n\\n[277] Pika Labs, \\\"Pika: An idea-to-video platform,\\\" https://pika.art, 2023, accessed: 2026-02-09.\\n\\n[278] W. Kong, Q. Tian, Z. Zhang et al., \\\"Hunyuanvideo: A systematic framework for large video generative models,\\\" *arxiv*, 2024.\\n\\n[279] K. Team, J. Chen, Y. Ci et al., \\\"Kling-omni technical report,\\\" *arxiv*, 2025.\\n\\n[280] J. Han, H. Chen, Y. Zhao et al., \\\"Vision as a dialect: Unifying visual understanding and generation via text-aligned representations,\\\" *arxiv*, 2025.\\n\\n[281] A. Ergasti, G. G. Tarollo, F. Botti et al., \\\"R-flav: Rolling flow matching for infinite audio video generation,\\\" *arxiv*, 2025.\\n\\n[282] L. Zhao, L. Feng, D. Ge et al., \\\"Uniform: A unified multi-task diffusion transformer for audio-video generation,\\\" *arxiv*, 2025.\\n\\n[283] Y. Wu, Z. Zhang, J. Chen et al., \\\"Vila-u: a unified foundation model integrating visual understanding and generation,\\\" *arxiv*, 2024.\\n\\n[284] H. Deng, T. Pan, H. Diao et al., \\\"Autoregressive video generation without vector quantization,\\\" *arxiv*, 2024.\\n\\n[285] X. Cheng, T. He, J. Xu et al., \\\"Playing with transformer at 30+ fps via next-frame diffusion,\\\" *arxiv*, 2025.\\n\\n[286] H. Chung, D. Lee, and J. C. Ye, \\\"Acdc: Autoregressive coherent multimodal generation using diffusion correction,\\\" *arxiv*, 2024.\\n\\n[287] Y. Li, Y. Ge, Y. Ge et al., \\\"Difen: Diffusion-compressed deep tokens for autoregressive video generation with language models,\\\" *arxiv*, 2024.\"}]\n\n---\n\n[{\"bbox\": [161, 123, 877, 153], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [166, 202, 1069, 1434], \"category\": \"Text\", \"text\": \"[288] Z. Li, H. Shujie, L. Shujie et al., \\\"Arlon: Boosting diffusion transformers with autoregressive models for long video generation,\\\" in ICLR, 2025.\\n[289] M. Sun, W. Wang, G. Li et al., \\\"Ar-diffusion: Asynchronous video generation with auto-regressive diffusion,\\\" in CVPR, 2025, pp. 7364-7373.\\n[290] T. Yin, Q. Zhang, R. Zhang et al., \\\"From slow bidirectional to fast autoregressive video diffusion models,\\\" in CVPR, 2025, pp. 22963-22974.\\n[291] E. Corona, A. Zanfir, E. G. Bazavan et al., \\\"Vlogger: Multimodal diffusion for embodied avatar synthesis,\\\" in CVPR, 2025.\\n[292] P. Zhang, J. Li, M. Wang et al., \\\"When video coding meets multimodal large language models: A unified paradigm for video coding,\\\" arxiv, 2024.\\n[293] W. Chen, Y. Ji, J. Wu et al., \\\"Control-a-video: Controllable text-to-video diffusion models with motion prior and reward feedback learning,\\\" arxiv, 2023.\\n[294] G. Yariv, I. Gat, S. Benaim et al., \\\"Diverse and aligned audio-to-video generation via text-to-video model adaptation,\\\" in AAAI, vol. 38, no. 7, 2024, pp. 6639-6647.\\n[295] K. Gong, D. Lian, H. Chang et al., \\\"Tm2d: Bimodality driven 3d dance generation via music-text integration,\\\" in ICCV, 2023, pp. 9942-9952.\\n[296] X. Li, W. Chu, Y. Wu et al., \\\"Videogen: A reference-guided latent diffusion approach for high definition text-to-video generation,\\\" arxiv, 2023.\\n[297] D. J. Zhang, J. Z. Wu, J.-W. Liu et al., \\\"Show-1: Marrying pixel and latent diffusion models for text-to-video generation,\\\" IJCV, vol. 133, no. 4, pp. 1879-1893, 2025.\\n[298] Y. Zhang, Y. Kang, Z. Zhang et al., \\\"Interactive video: User-centric controllable video generation with synergistic multimodal instructions,\\\" arxiv, 2024.\\n[299] X. Wang, J. Liu, Z. Wang et al., \\\"Keyvid: Keyframe-aware video diffusion for audio-synchronized visual animation,\\\" arxiv, 2025.\\n[300] J. H. Liew, H. Yan, J. Zhang et al., \\\"Magicedit: High-fidelity and temporally coherent video editing,\\\" arxiv, 2023.\\n[301] W. Wang, H. Yang, Z. Tuo et al., \\\"Swap attention in spatiotemporal diffrusions for text-to-video generation,\\\" IJCV, pp. 1-19, 2025.\\n[302] D. J. Zhang, D. Li, H. Le et al., \\\"Moonshot: Towards controllable video generation and editing with multimodal conditions,\\\" arxiv, 2024.\\n[303] R. Feng, W. Weng, Y. Wang et al., \\\"Ccedit: Creative and controllable video editing via diffusion models,\\\" in CVPR, 2024, pp. 6712-6722.\\n[304] S. Ge, T. Hayes, H. Yang et al., \\\"Long video generation with time-agnostic vqgan and time-sensitive transformer,\\\" in ECCV, 2022, pp. 102-118.\\n[305] X. Zhou, D. Liang, S. Tu et al., \\\"Hermes: A unified self-driving world model for simultaneous 3d scene understanding and generation,\\\" arxiv, 2025.\\n[306] L. Chen, Y. Gu, and Q. Mao, \\\"Univid: Unifying vision tasks with pre-trained video generation models,\\\" arxiv, 2025.\\n[307] M. Hu, C. Zheng, H. Zheng et al., \\\"Unified discrete diffusion for simultaneous vision-language generation,\\\" arxiv, 2022.\\n[308] T. Hu, Z. Yu, Z. Zhou et al., \\\"Hunyuancustom: A multimodal-driven architecture for customized video generation,\\\" arxiv, 2025.\\n[309] J. Chung, T. Zhu, M. G. Saez-Diez et al., \\\"Unifying specialized visual encoders for video language models,\\\" arxiv, 2025.\"}]\n\n---\n\n[{\"bbox\": [169, 129, 877, 152], \"category\": \"Title\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [164, 202, 1069, 1411], \"category\": \"Text\", \"text\": \"[310] L. Yang, X. Zhang, Y. Tian et al., \\\" Hermesflow: Seamlessly closing the gap in multimodal understanding and generation ,\\\" arxiv , 2025.\\n[311] Y. Fang, W. Menapace, A. Siarohin et al., \\\"Vimi: Grounding video generation through multi-modal instruc- tion,\\\" in EMNLP , 2024, pp. 4444–4456.\\n[312] J. Li, W. Feng, T.-J. Fu et al., \\\"T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback,\\\" in NeurIPS , vol. 37, 2024, pp. 75 692–75 726.\\n[313] O. Zohar, X. Wang, Y. Bitton et al., \\\"Video-star: Self-training enables video instruction tuning with any supervision,\\\" in arxiv , 2024.\\n[314] X. He, D. Jiang, G. Zhang et al., \\\"Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation,\\\" in EMNLP , 2024, pp. 2105–2123.\\n[315] L. Yu, J. Lezama, N. B. Gundavarapu et al., \\\"Language model beats diffusion–(da Compiled and codes from Refrence ) is key to visual generation ,\\\" arxiv , 2023.\\n[316] W. Peebles and S. Xie, \\\"Scalable diffusion models with transformers,\\\" in ICCV , 2023, pp. 4195–4205.\\n[317] Z. Zheng, X. Peng, T. Yang et al., \\\"Open-sora: Democratizing efficient video production for all,\\\" arxiv , 2024.\\n[318] H. Liu, M. Zaharia, and P. Abbeel, \\\"Ring attention with blockwise transformers for near-infinite context,\\\" arxiv , 2023.\\n[319] X. Ma, Y. Wang, X. Chen et al., \\\"Latte: Latent diffusion transformer for video generation,\\\" arxiv , 2024.\\n[320] J. Z. Wu, Y. Ge, X. Wang et al., \\\"Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation,\\\" in ICCV , 2023, pp. 7623–7633.\\n[321] J. Kaplan, S. McCandlish, T. Henighan et al. , \\\"Scaling laws for neural language models,\\\" arxiv , 2020.\\n[322] J. Hoffmann, S. Borgeaud, A. Mensch et al., \\\"Training compute-optimal large language models,\\\" in NeurIPS , vol. 35, 2022, pp. 30 016–30 030.\\n[323] A. Van Den Oord, O. Vinyals et al., \\\" Neural discrete representation learning,\\\" in NeurIPS , 2017.\\n[324] S. Yin, C. Wu, J. Liang et al., \\\"Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory,\\\" in arxiv , 2024.\\n[325] Z. Wang, Z. Yuan, X. Wang et al. , \\\"Motionctrl: A unified and flexible motion controller for video generation,\\\" in ACM SIGGRAPH , 2024, pp. 1–11.\\n[326] X. Chen, Y. Wang, L. Zhang et al., \\\" Seine: Short-to-long video diffusion model for generative transition and prediction,\\\" in ICLR , 2024.\\n[327] Morph Studio Team, \\\"Morph studio: Ai video generation platform,\\\" https://www.morphstudio.com/ , 2024.\\n[328] A. Siarohin, S. Lathuilière, S. Tulyakov et al., \\\"First order motion model for image animation,\\\" in NeurIPS , vol. 32, 2019.\\n[329] J. Zhao and H. Zhang, \\\"Thin-plate spline motion model for image animation,\\\" in CVPR , 2022, pp. 3657–3666.\\n[330] L. Hu, X. Gao, P. Zhang et al. , \\\"Vectorizer: A consistent and controllable image-to-video synthesis for character animation,\\\" in CVPR , 2024.\\n[331] Z. Xu, J. Zhang, J. H. Liew et al., \\\"Magicanimate: Temporally consistent human image animation using diffusion model,\\\" in CVPR , 2024.\\n[332] S. Zhu, J. L. Chen, Z. Dai et al. , \\\"Champ: Controllable and consistent human image animation with 3d parametric guidance,\\\" arxiv , 2024.\\n[333] M. Zhang, Z. Cai, L. Pan et al. , \\\"Motiondiffuse: Text-driven human motion generation with diffusion model,\\\" IEEE T-PAMI , 2022.\"}]\n\n---\n\n[{\"bbox\": [165, 126, 879, 155], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [166, 203, 1072, 1425], \"category\": \"Text\", \"text\": \"[334] D. Epstein, A. Jabri, B. Poole et al., \\\"Diffusion self-guidance for controllable image generation,\\\" in NeurIPS, 2023.\\n[335] J. Ho, A. Jain, and P. Abbeel, \\\"Denoising diffusion probabilistic models,\\\" in NeurIPS, vol. 33, 2020, pp. 6840-6851.\\n[336] C. Wu, Y. Xia, S. Gao et al., \\\"Janus: Decoupling visual encoding for unified multimodal understanding and generation,\\\" CVPR, 2025.\\n[337] X. Dai, J. Hou, C.-Y. Ma et al., \\\"Emu: Enhancing image generation models using photogenic needles in a haystack,\\\" arxiv, 2023.\\n[338] H. Zhang, X. Li, and L. Bing, \\\"Video-llama: An instruction-tuned audio-visual language model for video understanding,\\\" in EMNLP, 2023, pp. 543-553.\\n[339] K. Li, Y. He, Y. Wang et al., \\\"Videochat: Chat-centric video understanding,\\\" arxiv, 2023.\\n[340] S. Wu, H. Fei, L. Qu et al., \\\"Next-gpt: Any-to-any multimodal llm,\\\" in ICML, 2024.\\n[341] P. F. Christiano, J. Leike, T. Brown et al., \\\"Deep reinforcement learning from human preferences,\\\" in NeurIPS, 2017.\\n[342] L. Ouyang, J. Wu, X. Jiang et al., \\\"Training language models to follow instructions with human feedback,\\\" in NeurIPS, 2022.\\n[343] X. Wu, K. Sun, F. Zhu et al., \\\"Human preference score: Better aligning text-to-image models with human preference,\\\" in ICCV, 2023, pp. 2096-2105.\\n[344] R. Rafailov, A. Sharma, E. Mitchell et al., \\\"Direct preference optimization: Your language model is secretly a reward model,\\\" in NeurIPS, vol. 36, 2023, pp. 53702-53741.\\n[345] J. Schulman, F. Wolski, P. Dhariwal et al., \\\"Proximal policy optimization algorithms,\\\" arxiv, 2017.\\n[346] A. Radford, J. W. Kim, C. Hallacy et al., \\\"Learning transferable visual models from natural language supervision,\\\" in ICML, 2021.\\n[347] M. J. Kim, K. Pertsch, S. Karamcheti et al., \\\"Openvla: An open-source vision-language-action model,\\\" arxiv, 2024.\\n[348] Z. Guan, H. Sun, Y. Guo et al., \\\"Rl-vla 3: Reinforcement learning vla accelerating via full asynchronism,\\\" arxiv, 2026.\\n[349] B. Zitkovich, T. Yu, S. Xu et al., \\\"Rt-2: Vision-language-action models transfer web knowledge to robotic control,\\\" in CoRL, 2023.\\n[350] Q. Xu, J. Liu, R. Zhou et al., \\\"Twinrl-vla: Digital twin-driven reinforcement learning for real-world robotic manipulation,\\\" arxiv, 2026.\\n[351] G. Lu, S. Zhang, Z. Wang et al., \\\"Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation,\\\" in ECCV, 2024.\\n[352] Y. J. Ma, W. Liang, H.-J. Wang et al., \\\"Dreureka: Language model guided sim-to-real transfer, 2024,\\\" arxiv, 2024.\\n[353] B. Zhang, Y. Zhang, J. Ji et al., \\\"Safevla: Towards safety alignment of vision-language-action model via constrained learning,\\\" arxiv, 2025.\\n[354] Y. Wang, Z. Xian, F. Chen et al., \\\"Robogen: Towards unleashing infinite data for automated robot learning via generative simulation,\\\" arxiv, 2023.\\n[355] S. Liu, Y. Zhang, W. Li et al., \\\"Video-p2p: Video editing with cross-attention control,\\\" in CVPR, 2024, pp. 8599-8608.\\n[356] J.-g. Kwak, E. Dong, Y. Jin et al., \\\"Vivid-1-to-3: Novel view synthesis with video diffusion models,\\\" in CVPR, 2024, pp. 6775-6785.\"}]\n\n---\n\n[{\"bbox\": [167, 126, 875, 153], \"category\": \"Page-header\", \"text\": \"## The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [167, 202, 1066, 1448], \"category\": \"Text\", \"text\": \"[357] M. You, Z. Zhu, H. Liu et al., \\\"Nvs-solver: Video diffusion model as zero-shot novel view synthesizer,\\\" arxiv, 2024.\\n[358] X. Ren, T. Shen, J. Huang et al., \\\"Gen3c: 3d-informed world-consistent video generation with precise camera control,\\\" in PR, 2025, pp. 6121-6132.\\n[359] Q. Zhang, S. Zhai, M. A. B. Martin et al., \\\"World-consistent video diffusion with explicit 3d modeling,\\\" in PR, 2025, pp. 21685-21695.\\n[360] Z. Gu, R. Yan, J. Lu et al., \\\"Diffusion as compiler: 3d-aware video diffusion for versatile video generation control,\\\" in PR, 2025, pp. 1-12.\\n[361] Y.-J. Yuan, L. Kobbelt, J. Liu et al., \\\"4-dynamic: Text-to-4d generation with hybrid priors,\\\" arxiv, 2024.\\n[362] R. Wu, R. Gao, B. Poole et al., \\\"Cat4d: Create anything in 4d with multi-view video diffusion models,\\\" in CVPR, 2025, pp. 26057-26068.\\n[363] J. Wang, N. Karaev, C. Rupprecht et al., \\\"Vggsfm: Visual geometry grounded deep structure from motion,\\\" in CVPR, 2024.\\n[364] J. Li, Q. Long, J. Zheng et al., \\\"T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design,\\\" arxiv, 2024.\\n[365] H. Yuan, S. Zhang, X. Wang et al., \\\"Instructvideo: Instructing video diffusion models with human feedback,\\\" in CVPR, 2024, pp. 6463-6474.\\n[366] B. Li, C. Zheng, W. Zhu et al., \\\"Vivid-zoo: Multi-view video generation with diffusion model,\\\" PR, vol. 37, pp. 62189-6222, 2024.\\n[367] Z. Yang, Z. Pan, C. Gu et al., \\\"Diffusion 2: Dynamic 3d content generation via score composition of video and multi-view diffusion models,\\\" arxiv, 2024.\\n[368] X. Ren, T. Shen, J. Huang et al., \\\"Gen3c: 3d-informed world-consistent video generation with precise camera control,\\\" in CVPR, 2025.\\n[369] T. Li, G. Zheng, R. Jiang et al., \\\"Realcam-i2v: Real-world image-to-video generation with interactive complex camera control,\\\" in ICCV, 2025, pp. 28785-28796.\\n[370] W. Yu, J. Xing, L. Yuan et al., \\\"Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis,\\\" arxiv, 2024.\\n[371] Z. Wang, J. Cho, J. Li et al., \\\"Epic: Efficient video camera control learning with precise anchor-video guidance,\\\" arxiv, 2025.\\n[372] H. He, Y. Xu, Y. Guo et al., \\\"Cameractrl: Enabling camera control for text-to-video generation,\\\" arxiv, 2024.\\n[373] X. Fan, S. Girish, V. Ramanujan et al., \\\"Omniview: An all-seeing diffusion model for 3d and 4d view synthesis,\\\" arxiv, 2025.\\n[374] Y. Chen, Z. Ye, Z. Fang et al., \\\"Postcam: Camera-controllable novel-view video generation with query-shared cross-attention,\\\" arxiv, 2025.\\n[375] L. Hollein, A. Bozic, N. Muller et al., \\\"Viewdiff: 3d-consistent image generation with text-to-image models,\\\" in CVPR, 2024, pp. 5043-5052.\\n[376] S. Bahmani, I. Skorokhodov, A. SIarohin et al., \\\"Vd3d: Taming large video diffusion transformers for 3d camera control,\\\" arxiv, 2024.\\n[377] S. Bahmani, X. Liu, W. Yifan et al., \\\"Tc4d: Trajectory-conditioned text-to-4d generation,\\\" in ECCV. Springer, 2024, pp. 53-72.\\n[378] X. F ste, X. Liu, X. Wang et al., \\\"3drajmaster: Mastering 3d trajectory for multi-entity motion in video generation,\\\" arxiv, 2024.\\n[379] V. Voleti, C.-H. Yao, M. Boss et al., \\\"Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion,\\\" in ECCV, 2024.\"}]\n\n---\n\n[{\"bbox\": [162, 123, 877, 156], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [163, 199, 1071, 1437], \"category\": \"Text\", \"text\": \"[380] H. Zhang, X. Chen, Y. Wang et al., \\\"4diffusion: Multi-view video diffusion model for 4d generation,\\\" PR, vol. 37, pp. 15 272-15 295, 2024.\\n\\n[381] G. Wu, T. Yi, J. Fang et al., \\\"4d gaussian splatting for real-time dynamic scene rendering,\\\" in CVPR, 2024, pp. 20 310-20 320.\\n\\n[382] Y. Jiang, L. Zhang, J. Gao et al., \\\"Consistent4d: Consistent 360 dynamic object generation from monocular video,\\\" arxiv, 2023.\\n\\n[383] H. Sun, X. Li, L. Shen et al., \\\"Dyblurf: Dynamic neural radiance fields from blurry monocular video,\\\" in CVPR, 2024, pp. 7517-7527.\\n\\n[384] Z. Li, Q. Wang, F. Cole et al., \\\"Dynibar: Neural dynamic image-based rendering,\\\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 4273-4284.\\n\\n[385] S. Bahmani, I. Skorokhodov, V. Rong et al., \\\"4d-fy: Text-to-4d generation using hybrid score distillation sampling,\\\" in CVPR, 2024, pp. 7996-8006.\\n\\n[386] J. Fang, T. Yi, X. Wang et al., \\\"Fast dynamic radiance fields with time-aware neural voxels,\\\" in ACM SIGGRAPH, 2022, pp. 1-9.\\n\\n[387] M.-Q. V. Bui, J. Park, J. Oh et al., \\\"Moblurf: Motion deblurring neural radiance fields for blurry monocular video,\\\" IEEE T-PAMI, 2025.\\n\\n[388] Y. Xie, C.-H. Yao, V. Voleti et al., \\\"Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency,\\\" arxiv, 2024.\\n\\n[389] S. Fridovich-Keil, G. Meanti, F. R. Warburg et al., \\\"K-planes: Explicit radiance fields in space, time, and appearance,\\\" in CVPR, 2023.\\n\\n[390] Y. Zeng, Y. Jiang, S. Zhu et al., \\\"Stag4d: Spatial-temporal anchored generative 4d gaussians,\\\" in ECCV. Springer, 2024, pp. 163-179.\\n\\n[391] J. Ren, L. Pan, J. Tang et al., \\\"Dreamgaussian4d: Generative 4d gaussian splatting,\\\" arxiv, 2023.\\n\\n[392] H. Ling, S. W. Kim, A. Torralba et al., \\\"Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models,\\\" in CVPR, 2024, pp. 8576-8588.\\n\\n[393] B. He, Y. Chen, G. Lu et al., \\\"H3d-dgs: Exploring heterogeneous 3d motion representation for deformable 3d gaussian splatting,\\\" in NeurIPS, 2025.\\n\\n[394] H. Liang, Y. Yin, D. Xu et al., \\\"Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models,\\\" arxiv, 2024.\\n\\n[395] X. Liu, Y. Xiao, D. Y. Chen et al., \\\"Trace anything: Representing any video in 4d via trajectory fields,\\\" arxiv, 2025.\\n\\n[396] Q. Wang, Y.-Y. Chang, R. Cai et al., \\\"Tracking everything everywhere all at once,\\\" in ICCV, 2023.\\n\\n[397] Y. Xiao, Q. Wang, S. Zhang et al., \\\"Spatialtracker: Tracking any 2d pixels in 3d space,\\\" in CVPR, 2024.\\n\\n[398] V. Leroy, Y. Cabon, and J. Revaud, \\\"Grounding image matching in 3d with mast3r,\\\" in ECCV, 2024.\\n\\n[399] H. Huang, H. Chen, S. Wu et al., \\\"Vistadpo: Video hierarchical spatial-temporal direct preference optimization for large video models,\\\" arxiv, 2025.\\n\\n[400] N. Zhou, J. Chen, and D. Huang, \\\"Dr-tune: Improving fine-tuning of pretrained visual models by distribution regularization with semantic calibration,\\\" in ICCV, 2023, pp. 1547-1556.\\n\\n[401] H. Wang, X. Du, J. Li et al., \\\"Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation,\\\" in CVPR, 2023.\\n\\n[402] B. Efron, \\\"Tweedie's formula and selection bias,\\\" JASA, vol. 106, no. 496, pp. 1602-1614, 2011.\\n\\n[403] D. Kim, Y. Kim, S. J. Kwon et al., \\\"Refining generative process with discriminator guidance in score-based diffusion models,\\\" in ICML. ML Research Press, 2023, pp. 16 567-16 598.\"}]\n\n---\n\n[{\"bbox\": [167, 126, 875, 154], \"category\": \"Page-header\", \"text\": \"# The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [167, 200, 1066, 1423], \"category\": \"Text\", \"text\": \"[404] Y. Du, C. Durkan, R. Strudel et al., \\\"Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc,\\\" in ICML. PMLR, 2023, pp. 8489–8510.\\n[405] N. Liu, S. Li, Y. Du et al., \\\"Compositional visual generation with composable diffusion models,\\\" in ECCV, 2022.\\n[406] Y. Wei, S. Zhang, Z. Qing et al., \\\"Dreamvideo: Composing your dream videos with customized subject and motion,\\\" in CVPR, 2024.\\n[407] C. Lu, Y. Zhou, F. Bao et al., \\\"Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps,\\\" in NeurIPS, 2022.\\n[408] W. Zhao, L. Bai, Y. Rao et al., \\\"Unipc: A unified predictor-corrector framework for fast sampling of diffusion models,\\\" in NeurIPS, 2023.\\n[409] J. Zhu and P. Zhuang, \\\"Hifa: High-fidelity text-to-3d generation with advanced diffusion guidance,\\\" in arxiv, 2024.\\n[410] S. Yang, Y. Zhou, Z. Liu et al., \\\"Freeu: Free-lunch for diffusion models,\\\" in CVPR, 2024.\\n[411] W. Li, R. Chen, X. Chen et al., \\\"Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d,\\\"\\nin arxiv, 2024.\\n[412] Z. Yang, Z. Pan, C. Gu et al., \\\"Diffusion 2: Dynamic 3d content generation via score composition of video and multi-view diffusion models,\\\" arxiv, 2024.\\n[413] Y. Zhang, Y. Wei, D. Jiang et al., \\\"Controlvideo: Training-free controllable text-to-video generation,\\\" in arxiv, 2023.\\n[414] C. Mou, X. Wang, L. Xie et al., \\\"T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models,\\\" in AAAI, 2024.\\n[415] J. L. Schonberger and J.-M. Frahm, \\\"Structure-from-motion revisited,\\\" in CVPR, 2016.\\n[416] Z. Teed and J. Deng, \\\"Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras,\\\" in NeurIPS, 2021.\\n[417] A. Yu, V. Ye, M. Tancik et al., \\\"pixelnerf: Neural radiance fields from one or few images,\\\" in CVPR, 2021, pp. 4578–4587.\\n[418] M. Suhail, C. E. a. L. S. Perez, and A. Makadia, \\\"Generalizable patch-based neural rendering,\\\" in ECCV, 2022.\\n[419] H. Ni, C. Shi, K. Li et al., \\\"Conditional image-to-video generation with latent flow diffusion models,\\\" in CVPR, 2023.\\n[420] J. Zeng, L. Qiu, F. Li et al., \\\"Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior,\\\"\\nin ICCV, 2023.\\n[421] A. Cao and J. Johnson, \\\"Hexplane: A fast representation for dynamic scenes,\\\" in CVPR, 2023.\\n[422] X. Liu, Y. Xiao, D. Y. Chen et al., \\\"Trace anything: Representing any video in 4d via trajectory fields,\\\" arxiv, 2025.\\n[423] A. Yu, S. Fridovich-Keil, M. Tancik et al., \\\"Plenoxels: Radiance fields without neural networks,\\\" in CVPR, 2022, pp. 5501–5510.\\n[424] S. Singh, S. Abu-El-Haija, N. Johnston et al., \\\"End-to-end learning of compressible features,\\\" in NeurIPS, vol. 35, 2022, pp. 11750–11762.\\n[425] S. Fridovich-Keil, G. Meanti, F. R. Warburg et al., \\\"K-planes: Explicit radiance fields in space, time, and appearance,\\\" in CVPR, 2023.\\n[426] R. Shao, Z. Zheng, H. Tu et al., \\\"Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering,\\\" in CVPR, 2023.\"}]\n\n---\n\n[{\"bbox\": [168, 127, 875, 155], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [166, 202, 1071, 1438], \"category\": \"Text\", \"text\": \"[427] Z. Wu, C. Yu, Y. Jiang et al., \\\"Sc4d: Sparse-controlled video-to-4d generation and motion transfer,\\\" in ECCV, 2024.\\n[428] M. Zwicker, H. Pfister, J. Van Baar et al., \\\"Surface splatting,\\\" in ACM SIGGRAPH, 2001, pp. 371-378.\\n[429] C. Lassner and M. Zollhofer, \\\"Pulsar: Efficient sphere-based neural rendering,\\\" in CVPR, 2021, pp. 1440-1449.\\n[430] Y.-H. Huang, Y.-T. Sun, Z. Yang et al., \\\"Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes,\\\" in CVPR, 2024.\\n[431] C. Chen, S. Huang, X. Chen et al., \\\"Ct4d: Consistent text-to-4d generation with animatable meshes,\\\" arxiv, 2024.\\n[432] N. Karaev, Y. Makarov, J. Wang et al., \\\"Cotracker3: Simpler and better point tracking by pseudo-labelling real videos,\\\" in ICCV, 2025.\\n[433] R. Villegas, J. Yang, S. Hong et al., \\\"Decomposing motion and content for natural video sequence prediction,\\\" in ICLR, 2017.\\n[434] R. Villegas, J. Yang, Y. Zou et al., \\\"Learning to generate long-term future via hierarchical prediction,\\\" in ICML, 2017.\\n[435] J. Wei, Y. Tay, R. Bommasani et al., \\\"Emergent abilities of large language models,\\\" TMLR, 2022.\\n[436] A. Ramesh, P. Dhariwal, A. Nichol et al., \\\"Hierarchical text-conditional image generation with clip latents,\\\" arxiv, vol. 1, no. 2, p. 3, 2022.\\n[437] J. Gu, S. Wang, H. Zhao et al., \\\"Reuse and diffuse: Iterative denoising for text-to-video generation,\\\" arxiv, 2023.\\n[438] J. Bruce, M. D. Dennis, A. Edwards et al., \\\"Genie: Generative interactive environments,\\\" in ICML. PMLR, 2024, pp. 4603-4623.\\n[439] D. Valevski, Y. Leviathan, M. Arar et al., \\\"Diffusion models are real-time game engines,\\\" in ICLR, 2024.\\n[440] Runway, \\\"Introducing gen-3 alpha: A new frontier for video generation,\\\" https://runwayml.com/research/introducing-gen-3-alpha, 2024, accessed: 2025-02-24.\\n[441] HailuoAI, \\\"Hailuo,\\\" https://tailuoai.video/, 2024, accessed: 2025-02-24.\\n[442] Y. HaCohen, N. Chiprut, B. Brazowski et al., \\\"Ltx-video: Realtime video latent diffusion,\\\" arxiv, 2024.\\n[443] X. He, C. Peng, Z. Liu et al., \\\"Matrix-game 2.0: An open-source real-time and streaming interactive world model,\\\" arxiv, 2025.\\n[444] H. Team, Z. Wang, Y. Liu et al., \\\"Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels,\\\" arxiv, 2025.\\n[445] M. Assran, Q. Duval, I. Misra et al., \\\"Self-supervised learning from images with a joint-embedding predictive architecture,\\\" in CVPR, 2023, pp. 15619-15629.\\n[446] D. Hafner, J. Pasukonis, J. Ba et al., \\\"Mastering diverse domains through world models,\\\" in arxiv, 2024.\\n[447] J. Cen, C. Yu, H. Yuan et al., \\\"Worldvla: Towards autoregressive action world model,\\\" arxiv, 2025.\\n[448] H. Zhen, X. Qiu, P. Chen et al., \\\"3d-vla: A 3d vision-language-action generative world model,\\\" in arxiv, 2024.\\n[449] Y. Hu, J. Yang, L. Chen et al., \\\"Planning-oriented autonomous driving,\\\" in CVPR, 2023.\\n[450] X. Tian, J. Gu, B. Li et al., \\\"Drivevlm: The convergence of autonomous driving and large vision-language models,\\\" in ECCV, 2024.\\n[451] X. Li, K. Hsu, J. Gu et al., \\\"Evaluating real-world robot manipulation policies in simulation,\\\" arxiv, 2024.\\n[452] J. Gu, F. Xiang, X. Li et al., \\\"Maniskill2: A unified benchmark for generalizable manipulation skills,\\\" arxiv, 2023.\"}]\n\n---\n\n[{\"bbox\": [171, 128, 873, 149], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [171, 210, 1064, 1455], \"category\": \"Text\", \"text\": \"[453] L. Wang, Y. Ling, Z. Yuan et al., \\\"Gensim: Generating robotic simulation tasks via large language models,\\\" arxiv, 2023.\\n[454] J. Pearl, *Causality*. Cambridge university press, 2009.\\n[455] Y. Bengio, \\\"From system 1 deep learning to system 2 deep learning,\\\" in NeurIPS, 2019.\\n[456] R. Firoozi, J. Tucker, S. Tian et al., \\\"Foundation models in robotics: Applications, challenges, and the future,\\\" arxiv, 2023.\\n[457] Z. Ji, N. Lee, R. Frieske et al., \\\"Survey of hallucination in natural language generation,\\\" ACM Comput. Surv., vol. 55, pp. 1–38, 2023.\\n[458] B. Goertzel, \\\"Artificial general intelligence: Concept, state of the art, and future prospects,\\\" J. Artif. Gen. Intell., vol. 5, no. 1, pp. 1–48, 2014.\\n[459] G. Marcus, \\\"The next decade in ai: Four steps towards robust artificial intelligence,\\\" arxiv, 2020.\\n[460] D. Bear, E. Wang, D. Mrowca et al., \\\"Physion: Evaluating physical prediction from vision in humans and machines,\\\" in NeurIPS, 2021.\\n[461] J. Mao, C. Gan, P. Kohli et al., \\\"The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision,\\\" in ICLR, 2019.\\n[462] V. Saxena, J. Ba, and D. Hafner, \\\"Clockwork variational autoencoders,\\\" NeurIPS, vol. 34, pp. 29 246–29 257, 2021.\\n[463] X. Pan, A. Tewari, T. Leimkuhler et al., \\\"Drag your gan: Interactive point-based manipulation on the generative image manifold,\\\" in ACM SIGGRAPH. ACM, 2023, pp. 1–11.\\n[464] T. Brooks, A. Holynski, and A. A. Efros, \\\"Instructpix2pix: Learning to follow image editing instructions,\\\" in CVPR, 2023, pp. 18 392–18 402.\\n[465] Y. Hu, L. Anderson, T.-M. Li et al., \\\"Difftaichi: Differentiable programming for physical simulation,\\\" in ICLR, 2020.\\n[466] J. S. Park, J. O’Brien, C. J. Cai et al., \\\"Generative agents: Interactive simulacra of human behavior,\\\" in ACM UIST, 2023, pp. 1–22.\\n[467] J. Leibo, V. Zambaldi, M. Lanctot et al., \\\"Multi-agent reinforcement learning in sequential social dilemmas,\\\" in AAMAS, vol. 16. ACM, 2017, pp. 464–473.\\n[468] Y. Shoham and K. Leyton-Brown, \\\"Multiagent systems,\\\" Cambridge Books, 2009.\\n[469] D. Silver, T. Hubert, J. Schrittwieser et al., \\\"A general reinforcement learning algorithm that masters chess, shogi, and go through self-play,\\\" Science, vol. 362, no. 6419, pp. 1140–1144, 2018.\\n[470] W. Hong, W. Wang, Q. Lv et al., \\\"Cogagent: A visual language model for gui agents,\\\" in CVPR, 2024, pp. 14 281–14 290.\\n[471] Z. Xi, W. Chen, X. Guo et al., \\\"The rise and potential of large language model based agents: A survey,\\\" Sci. China Inf. Sci., vol. 68, no. 2, p. 121101, 2025.\\n[472] L. Wang, C. Ma, X. Feng et al., \\\"A survey on large language model based autonomous agents,\\\" Front. Comput. Sci., vol. 18, no. 6, p. 186345, 2024.\\n[473] Y. Liang, W. Chow, F. Li et al., \\\"Rover: Benchmarking reciprocal cross-modal reasoning for omnimodal generation,\\\" arxiv, 2025.\\n[474] Y. Niu, W. Jin, J. Liao et al., \\\"Does understanding inform generation in unified multimodal models? from analysis to path forward,\\\" arxiv, 2025.\\n[475] Y. Niu, M. Ning, M. Zheng et al., \\\"Wise: A world knowledge-informed semantic evaluation for text-to-image generation,\\\" arxiv, 2025.\\n[476] T. Zhang, H.-X. Yu, R. Wu et al., \\\"Physdreamer: Physics-based interaction with 3d objects via video generation,\\\" in ECCV, 2024.\"}]\n\n---\n\n[{\"bbox\": [168, 130, 876, 156], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [168, 205, 1069, 1458], \"category\": \"Text\", \"text\": \"[477] C. Yang, H. Wan, Y. Peng et al., \\\"Reasoning via video: The first evaluation of video models' reasoning abilities through maze-solving tasks,\\\" arxiv, 2025.\\n[478] Z. Huang, Y. He, J. Yu et al., \\\"Vbench: Comprehensive benchmark suite for video generative models,\\\" in CVPR, 2024.\\n[479] F. Meng, J. Liao, X. Tan et al., \\\"Towards world simulator: Crafting physical commonsense-based benchmark for video generation,\\\" arxiv, 2024.\\n[480] H. H. Chen, D. Lan, W.-J. Shu et al., \\\"Tivibench: Benchmarking think-in-video reasoning for video generative models,\\\" arxiv, 2025.\\n[481] Z. Guo, X. Chen, R. Zhang et al., \\\"Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark,\\\" arxiv, 2025.\\n[482] W. Chow, J. Pan, Y. Liang et al., \\\"Weave: Unleashing and benchmarking the in-context interleaved comprehension and generation,\\\" arxiv, 2025.\\n[483] J. Tong, Y. Mou, H. Li et al., \\\"Thinking with video: Video generation as a promising multimodal reasoning paradigm,\\\" arxiv, 2025.\\n[484] Y. Luo, X. Zhao, B. Lin et al., \\\"V-reasonbench: Toward unified reasoning benchmark suite for video generation models,\\\" arxiv, 2025.\\n[485] J. Wei, C. Jia, X. Bai et al., \\\"Ggbench: A geometric generative reasoning benchmark for unified multimodal models,\\\" arxiv, 2025.\\n[486] C. Holtermann, N. Krebs, and A. Lauscher, \\\"Tempviz: On the evaluation of temporal knowledge in text-to-image models,\\\" arxiv, 2026.\\n[487] M. Heusel, H. Ramsauer, T. Unterthiner et al., \\\"Gans trained by a two time-scale update rule converge to a local nash equilibrium,\\\" NeurIPS, vol. 30, 2017.\\n[488] A. Borji, \\\"Pros and cons of gan evaluation measures,\\\" CVIU, vol. 179, pp. 41-65, 2019.\\n[489] S. Tong, Z. Liu, Y. Zhai et al., \\\"Eyes wide shut? exploring the visual shortcomings of multimodal llms,\\\" in CVPR, 2024.\\n[490] Z. Shao, P. Wang, Q. Zhu et al., \\\"Deepseekmath-v2: Towards self-verifiable mathematical reasoning,\\\" arxiv, 2025.\\n[491] N. Carlini, J. Hayes, M. Nasr et al., \\\"Extracting training data from diffusion models,\\\" in USENIX SEC, 2023, pp. 5253-5270.\\n[492] L. S. Piloto, A. Weinstein, P. Battaglia et al., \\\"Intuitive physics learning in a deep-learning model inspired by developmental psychology,\\\" Nat. Hum. Behav, vol. 6, no. 9, pp. 1257-1267, 2022.\\n[493] K. Yi, C. Gan, Y. Li et al., \\\"Clevrer: Collision events for video representation and reasoning,\\\" in ICLR, 2021.\\n[494] V. Voleti, A. Jolicoeur-Martineau, and C. Pal, \\\"Mcvd-masked conditional video diffusion for prediction, generation, and interpolation,\\\" NeurIPS, vol. 35, pp. 23-371-23 385, 2022.\\n[495] H. Lightman, V. Kosaraju, Y. Burda et al., \\\"Let's verify step by step,\\\" in ICLR, 2023.\\n[496] G. E. Karniadakis, I. G. Kevrekidis, L. Lu et al., \\\"Physics-informed machine learning,\\\" Nat. Rev. Phys., vol. 3, no. 6, pp. 422-440, 2021.\\n[497] O. Ahmed, F. Trauble, A. Goyal et al., \\\"Causalworld: A robotic manipulation benchmark for causal structure and transfer learning,\\\" in ICLR, 2021.\\n[498] J. Hessel, A. Holtzman, M. Forbes et al., \\\"Clipscore: A reference-free evaluation metric for image captioning,\\\" in EMNLP, 2021, pp. 7514-7528.\\n[499] J. Tobin, R. Fong, A. Ray et al., \\\"Domain randomization for transferring deep neural networks from simulation to the real world,\\\" in IEEE/RSJ IROS. IEEE, 2017, pp. 23-30.\\n[500] D. Silver, S. Singh, D. Precup et al., \\\"Reward is enough,\\\" AI, vol. 299, p. 103535, 2021.\"}]\n\n---\n\n[{\"bbox\": [165, 127, 875, 155], \"category\": \"Page-header\", \"text\": \"The Trinity of Consistency as a Defining Principle for General World Models\"}, {\"bbox\": [164, 201, 1067, 1012], \"category\": \"Text\", \"text\": \"[501] OpenAI, \\\"GPT-Image-1: Image Generation API,\\\" https://openai.com/index/image-generation-api/, 2025, accessed: 2025-01.\\n[502] T. Seedream, Y. Chen, Y. Gao et al., \\\"Seedream 4.0: Toward next-generation multimodal image generation,\\\" arxiv, 2025.\\n[503] G. Comanici, E. Bieber, M. Schaekermann et al., \\\"Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities,\\\" arxiv, 2025.\\n[504] OpenAI, \\\"GPT-Image-1.5: New ChatGPT Image Generation,\\\" https://openai.com/zh-Hans-CN/index/new-chatgpt-images-is-here, 2025, accessed: 2025-01.\\n[505] R. Mroczkowski, P. Rybak, A. Króblewska et al., \\\"Herbert: Efficiently pretrained transformer-based language model for polish,\\\" in BSNLP, 2021, pp. 1-10.\\n[506] B. Wu, C. Zou, C. Li et al., \\\"Hunyuanvideo 1.5 technical report,\\\" arxiv, 2025.\\n[507] J. Xu, X. Zou, K. Huang et al., \\\"Easyanimate: A high-performance long video generation method based on transformer architecture,\\\" arxiv, 2024.\\n[508] D. Li, Z. Fei, T. Li et al., \\\"Skyreels-v3 technique report,\\\" arxiv, 2026.\\n[509] C. Wu, J. Li, J. Zhou et al., \\\"Qwen-image technical report,\\\" arxiv, 2025.\\n[510] C. Wei, Q. Liu, Z. Ye et al., \\\"Univideo: Unified understanding, generation, and editing for videos,\\\" arxiv, 2025.\\n[511] Y. Cui, H. Chen, H. Deng et al., \\\"Emu3.5: Native multimodal models are world learners,\\\" arxiv, 2025.\\n[512] H. Al-Tahan, Q. Garrido, R. Balestrieri et al., \\\"Unibench: Visual reasoning requires rethinking vision-language beyond scaling,\\\" Advances in Neural Information Processing Systems, vol. 37, pp. 82411-82437, 2024.\\n[513] H. Zhou, Q. Xu, Y. Dong et al., \\\"Manbench: Is your multimodal model smarter than human?\\\" arxiv, 2025.\\n[514] J. Parker-Holder, P. Ball, J. Bruce et al., \\\"Genie 2: A large-scale foundation world model,\\\" https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model, 2024, accessed: 2026-02.\\n[515] Google DeepMind, \\\"Genie 3: A real-time interactive world model,\\\" 2025, technical report.\\n[516] PixVerse Research, \\\"Pixverse-r1: Next-generation real-time world model,\\\" https://pixverse.ai/en/blog/pixverse-r1-next-generation-real-time-world-model, 2026, technical report on real-time multimodal world model for interactive video generation.\"}]",
  "figures": [
    {
      "name": "fig1.png",
      "caption": ""
    },
    {
      "name": "fig2.png",
      "caption": ""
    },
    {
      "name": "fig3.png",
      "caption": ""
    },
    {
      "name": "fig4.jpeg",
      "caption": ""
    },
    {
      "name": "fig5.jpeg",
      "caption": ""
    },
    {
      "name": "fig6.jpeg",
      "caption": ""
    },
    {
      "name": "fig7.jpeg",
      "caption": ""
    },
    {
      "name": "fig8.jpeg",
      "caption": ""
    },
    {
      "name": "fig9.jpeg",
      "caption": ""
    },
    {
      "name": "fig10.jpeg",
      "caption": ""
    },
    {
      "name": "fig11.jpeg",
      "caption": ""
    },
    {
      "name": "fig12.jpeg",
      "caption": ""
    },
    {
      "name": "fig13.jpeg",
      "caption": ""
    },
    {
      "name": "fig14.jpeg",
      "caption": ""
    },
    {
      "name": "fig15.jpeg",
      "caption": ""
    },
    {
      "name": "fig16.jpeg",
      "caption": ""
    },
    {
      "name": "fig17.jpeg",
      "caption": ""
    },
    {
      "name": "fig18.jpeg",
      "caption": ""
    },
    {
      "name": "fig19.jpeg",
      "caption": ""
    },
    {
      "name": "fig20.jpeg",
      "caption": ""
    },
    {
      "name": "fig21.jpeg",
      "caption": ""
    },
    {
      "name": "fig22.jpeg",
      "caption": ""
    },
    {
      "name": "fig23.jpeg",
      "caption": ""
    },
    {
      "name": "fig24.jpeg",
      "caption": ""
    },
    {
      "name": "fig25.jpeg",
      "caption": ""
    },
    {
      "name": "fig26.jpeg",
      "caption": ""
    },
    {
      "name": "fig27.png",
      "caption": ""
    },
    {
      "name": "fig28.png",
      "caption": ""
    },
    {
      "name": "fig29.png",
      "caption": ""
    },
    {
      "name": "fig30.png",
      "caption": ""
    },
    {
      "name": "fig31.jpeg",
      "caption": ""
    },
    {
      "name": "fig32.png",
      "caption": ""
    },
    {
      "name": "fig33.png",
      "caption": ""
    },
    {
      "name": "fig34.png",
      "caption": ""
    },
    {
      "name": "fig35.jpeg",
      "caption": ""
    },
    {
      "name": "fig36.png",
      "caption": ""
    },
    {
      "name": "fig37.png",
      "caption": ""
    },
    {
      "name": "fig38.png",
      "caption": ""
    },
    {
      "name": "fig39.png",
      "caption": ""
    },
    {
      "name": "fig40.png",
      "caption": ""
    },
    {
      "name": "fig41.png",
      "caption": ""
    },
    {
      "name": "fig42.png",
      "caption": ""
    },
    {
      "name": "fig43.png",
      "caption": ""
    },
    {
      "name": "fig44.png",
      "caption": ""
    },
    {
      "name": "fig45.png",
      "caption": ""
    },
    {
      "name": "fig46.png",
      "caption": ""
    },
    {
      "name": "fig47.png",
      "caption": ""
    },
    {
      "name": "fig48.png",
      "caption": ""
    },
    {
      "name": "fig49.png",
      "caption": ""
    },
    {
      "name": "fig50.png",
      "caption": ""
    },
    {
      "name": "fig51.png",
      "caption": ""
    },
    {
      "name": "fig52.jpeg",
      "caption": ""
    },
    {
      "name": "fig53.jpx",
      "caption": ""
    },
    {
      "name": "fig54.jpeg",
      "caption": ""
    },
    {
      "name": "fig55.jpeg",
      "caption": ""
    },
    {
      "name": "fig56.jpeg",
      "caption": ""
    },
    {
      "name": "fig57.png",
      "caption": ""
    },
    {
      "name": "fig58.jpeg",
      "caption": ""
    },
    {
      "name": "fig59.jpeg",
      "caption": ""
    },
    {
      "name": "fig60.jpeg",
      "caption": ""
    },
    {
      "name": "fig61.jpeg",
      "caption": ""
    },
    {
      "name": "fig62.jpeg",
      "caption": ""
    },
    {
      "name": "fig63.jpeg",
      "caption": ""
    },
    {
      "name": "fig64.jpeg",
      "caption": ""
    },
    {
      "name": "fig65.jpeg",
      "caption": ""
    },
    {
      "name": "fig66.jpeg",
      "caption": ""
    },
    {
      "name": "fig67.jpeg",
      "caption": ""
    },
    {
      "name": "fig68.jpeg",
      "caption": ""
    },
    {
      "name": "fig69.jpeg",
      "caption": ""
    },
    {
      "name": "fig70.jpeg",
      "caption": ""
    },
    {
      "name": "fig71.jpeg",
      "caption": ""
    },
    {
      "name": "fig72.jpeg",
      "caption": ""
    },
    {
      "name": "fig73.png",
      "caption": ""
    },
    {
      "name": "fig74.jpeg",
      "caption": ""
    },
    {
      "name": "fig75.jpeg",
      "caption": ""
    },
    {
      "name": "fig76.jpeg",
      "caption": ""
    },
    {
      "name": "fig77.jpeg",
      "caption": ""
    },
    {
      "name": "fig78.jpeg",
      "caption": ""
    },
    {
      "name": "fig79.jpeg",
      "caption": ""
    },
    {
      "name": "fig80.jpeg",
      "caption": ""
    },
    {
      "name": "fig81.jpeg",
      "caption": ""
    },
    {
      "name": "fig82.jpeg",
      "caption": ""
    },
    {
      "name": "fig83.jpeg",
      "caption": ""
    },
    {
      "name": "fig84.jpeg",
      "caption": ""
    },
    {
      "name": "fig85.jpeg",
      "caption": ""
    },
    {
      "name": "fig86.jpeg",
      "caption": ""
    },
    {
      "name": "fig87.jpeg",
      "caption": ""
    },
    {
      "name": "fig88.jpeg",
      "caption": ""
    },
    {
      "name": "fig89.jpx",
      "caption": ""
    },
    {
      "name": "fig90.jpeg",
      "caption": ""
    },
    {
      "name": "fig91.jpeg",
      "caption": ""
    },
    {
      "name": "fig92.jpeg",
      "caption": ""
    },
    {
      "name": "fig93.jpeg",
      "caption": ""
    },
    {
      "name": "fig94.jpeg",
      "caption": ""
    },
    {
      "name": "fig95.jpeg",
      "caption": ""
    },
    {
      "name": "fig96.jpeg",
      "caption": ""
    },
    {
      "name": "fig97.png",
      "caption": ""
    },
    {
      "name": "fig98.jpeg",
      "caption": ""
    },
    {
      "name": "fig99.jpeg",
      "caption": ""
    },
    {
      "name": "fig100.jpeg",
      "caption": ""
    },
    {
      "name": "fig101.jpeg",
      "caption": ""
    },
    {
      "name": "fig102.jpeg",
      "caption": ""
    },
    {
      "name": "fig103.jpeg",
      "caption": ""
    },
    {
      "name": "fig104.jpeg",
      "caption": ""
    },
    {
      "name": "fig105.jpeg",
      "caption": ""
    },
    {
      "name": "fig106.jpeg",
      "caption": ""
    },
    {
      "name": "fig107.jpeg",
      "caption": ""
    },
    {
      "name": "fig108.jpeg",
      "caption": ""
    },
    {
      "name": "fig109.png",
      "caption": ""
    },
    {
      "name": "fig110.jpeg",
      "caption": ""
    },
    {
      "name": "fig111.jpeg",
      "caption": ""
    },
    {
      "name": "fig112.jpeg",
      "caption": ""
    },
    {
      "name": "fig113.jpeg",
      "caption": ""
    },
    {
      "name": "fig114.jpeg",
      "caption": ""
    },
    {
      "name": "fig115.jpeg",
      "caption": ""
    },
    {
      "name": "fig116.jpeg",
      "caption": ""
    },
    {
      "name": "fig117.jpeg",
      "caption": ""
    },
    {
      "name": "fig118.png",
      "caption": ""
    },
    {
      "name": "fig119.jpeg",
      "caption": ""
    },
    {
      "name": "fig120.jpeg",
      "caption": ""
    },
    {
      "name": "fig121.jpeg",
      "caption": ""
    },
    {
      "name": "fig122.jpeg",
      "caption": ""
    },
    {
      "name": "fig123.jpeg",
      "caption": ""
    },
    {
      "name": "fig124.jpeg",
      "caption": ""
    },
    {
      "name": "fig125.jpeg",
      "caption": ""
    },
    {
      "name": "fig126.jpeg",
      "caption": ""
    },
    {
      "name": "fig127.jpeg",
      "caption": ""
    },
    {
      "name": "fig128.jpeg",
      "caption": ""
    },
    {
      "name": "fig129.jpeg",
      "caption": ""
    },
    {
      "name": "fig130.jpeg",
      "caption": ""
    },
    {
      "name": "fig131.jpeg",
      "caption": ""
    },
    {
      "name": "fig132.jpeg",
      "caption": ""
    },
    {
      "name": "fig133.jpeg",
      "caption": ""
    },
    {
      "name": "fig134.jpeg",
      "caption": ""
    },
    {
      "name": "fig135.jpeg",
      "caption": ""
    },
    {
      "name": "fig136.jpeg",
      "caption": ""
    },
    {
      "name": "fig137.jpeg",
      "caption": ""
    },
    {
      "name": "fig138.jpeg",
      "caption": ""
    },
    {
      "name": "fig139.jpeg",
      "caption": ""
    },
    {
      "name": "fig140.jpeg",
      "caption": ""
    },
    {
      "name": "fig141.jpeg",
      "caption": ""
    },
    {
      "name": "fig142.jpeg",
      "caption": ""
    },
    {
      "name": "fig143.jpeg",
      "caption": ""
    },
    {
      "name": "fig144.jpeg",
      "caption": ""
    },
    {
      "name": "fig145.jpeg",
      "caption": ""
    },
    {
      "name": "fig146.jpeg",
      "caption": ""
    },
    {
      "name": "fig147.jpeg",
      "caption": ""
    },
    {
      "name": "fig148.jpeg",
      "caption": ""
    },
    {
      "name": "fig149.jpeg",
      "caption": ""
    },
    {
      "name": "fig150.jpeg",
      "caption": ""
    },
    {
      "name": "fig151.jpeg",
      "caption": ""
    },
    {
      "name": "fig152.jpeg",
      "caption": ""
    },
    {
      "name": "fig153.jpeg",
      "caption": ""
    },
    {
      "name": "fig154.jpeg",
      "caption": ""
    },
    {
      "name": "fig155.jpeg",
      "caption": ""
    },
    {
      "name": "fig156.jpeg",
      "caption": ""
    },
    {
      "name": "fig157.jpeg",
      "caption": ""
    },
    {
      "name": "fig158.jpeg",
      "caption": ""
    },
    {
      "name": "fig159.jpeg",
      "caption": ""
    },
    {
      "name": "fig160.jpeg",
      "caption": ""
    },
    {
      "name": "fig161.jpeg",
      "caption": ""
    },
    {
      "name": "fig162.jpeg",
      "caption": ""
    },
    {
      "name": "fig163.jpeg",
      "caption": ""
    },
    {
      "name": "fig164.jpeg",
      "caption": ""
    },
    {
      "name": "fig165.jpeg",
      "caption": ""
    },
    {
      "name": "fig166.jpeg",
      "caption": ""
    },
    {
      "name": "fig167.jpeg",
      "caption": ""
    },
    {
      "name": "fig168.jpeg",
      "caption": ""
    },
    {
      "name": "fig169.jpeg",
      "caption": ""
    },
    {
      "name": "fig170.jpeg",
      "caption": ""
    },
    {
      "name": "fig171.jpeg",
      "caption": ""
    },
    {
      "name": "fig172.jpeg",
      "caption": ""
    },
    {
      "name": "fig173.jpeg",
      "caption": ""
    },
    {
      "name": "fig174.jpeg",
      "caption": ""
    },
    {
      "name": "fig175.jpeg",
      "caption": ""
    },
    {
      "name": "fig176.jpeg",
      "caption": ""
    },
    {
      "name": "fig177.jpeg",
      "caption": ""
    },
    {
      "name": "fig178.jpeg",
      "caption": ""
    },
    {
      "name": "fig179.jpeg",
      "caption": ""
    },
    {
      "name": "fig180.jpeg",
      "caption": ""
    },
    {
      "name": "fig181.jpeg",
      "caption": ""
    },
    {
      "name": "fig182.jpeg",
      "caption": ""
    },
    {
      "name": "fig183.jpeg",
      "caption": ""
    },
    {
      "name": "fig184.jpeg",
      "caption": ""
    },
    {
      "name": "fig185.jpeg",
      "caption": ""
    },
    {
      "name": "fig186.jpeg",
      "caption": ""
    },
    {
      "name": "fig187.jpeg",
      "caption": ""
    },
    {
      "name": "fig188.jpeg",
      "caption": ""
    },
    {
      "name": "fig189.jpeg",
      "caption": ""
    },
    {
      "name": "fig190.jpeg",
      "caption": ""
    },
    {
      "name": "fig191.jpeg",
      "caption": ""
    },
    {
      "name": "fig192.jpeg",
      "caption": ""
    },
    {
      "name": "fig193.jpeg",
      "caption": ""
    },
    {
      "name": "fig194.jpeg",
      "caption": ""
    },
    {
      "name": "fig195.jpeg",
      "caption": ""
    },
    {
      "name": "fig196.jpeg",
      "caption": ""
    },
    {
      "name": "fig197.jpeg",
      "caption": ""
    },
    {
      "name": "fig198.jpeg",
      "caption": ""
    },
    {
      "name": "fig199.jpeg",
      "caption": ""
    },
    {
      "name": "fig200.jpeg",
      "caption": ""
    },
    {
      "name": "fig201.jpeg",
      "caption": ""
    },
    {
      "name": "fig202.jpeg",
      "caption": ""
    },
    {
      "name": "fig203.jpeg",
      "caption": ""
    },
    {
      "name": "fig204.jpeg",
      "caption": ""
    },
    {
      "name": "fig205.jpeg",
      "caption": ""
    },
    {
      "name": "fig206.jpeg",
      "caption": ""
    },
    {
      "name": "fig207.jpeg",
      "caption": ""
    },
    {
      "name": "fig208.jpeg",
      "caption": ""
    },
    {
      "name": "fig209.jpeg",
      "caption": ""
    },
    {
      "name": "fig210.jpeg",
      "caption": ""
    },
    {
      "name": "fig211.jpeg",
      "caption": ""
    },
    {
      "name": "fig212.jpeg",
      "caption": ""
    },
    {
      "name": "fig213.jpeg",
      "caption": ""
    },
    {
      "name": "fig214.jpeg",
      "caption": ""
    },
    {
      "name": "fig215.jpeg",
      "caption": ""
    },
    {
      "name": "fig216.jpeg",
      "caption": ""
    },
    {
      "name": "fig217.jpeg",
      "caption": ""
    },
    {
      "name": "fig218.jpeg",
      "caption": ""
    },
    {
      "name": "fig219.jpeg",
      "caption": ""
    },
    {
      "name": "fig220.jpeg",
      "caption": ""
    },
    {
      "name": "fig221.jpeg",
      "caption": ""
    },
    {
      "name": "fig222.jpeg",
      "caption": ""
    },
    {
      "name": "fig223.jpeg",
      "caption": ""
    },
    {
      "name": "fig224.jpeg",
      "caption": ""
    },
    {
      "name": "fig225.jpeg",
      "caption": ""
    },
    {
      "name": "fig226.jpeg",
      "caption": ""
    },
    {
      "name": "fig227.jpeg",
      "caption": ""
    },
    {
      "name": "fig228.jpeg",
      "caption": ""
    },
    {
      "name": "fig229.jpeg",
      "caption": ""
    },
    {
      "name": "fig230.jpeg",
      "caption": ""
    },
    {
      "name": "fig231.jpeg",
      "caption": ""
    },
    {
      "name": "fig232.jpeg",
      "caption": ""
    },
    {
      "name": "fig233.jpeg",
      "caption": ""
    },
    {
      "name": "fig234.jpeg",
      "caption": ""
    },
    {
      "name": "fig235.jpeg",
      "caption": ""
    },
    {
      "name": "fig236.jpeg",
      "caption": ""
    },
    {
      "name": "fig237.jpeg",
      "caption": ""
    },
    {
      "name": "fig238.jpeg",
      "caption": ""
    },
    {
      "name": "fig239.jpeg",
      "caption": ""
    },
    {
      "name": "fig240.jpeg",
      "caption": ""
    },
    {
      "name": "fig241.jpeg",
      "caption": ""
    },
    {
      "name": "fig242.jpeg",
      "caption": ""
    },
    {
      "name": "fig243.jpeg",
      "caption": ""
    },
    {
      "name": "fig244.jpeg",
      "caption": ""
    },
    {
      "name": "fig245.jpeg",
      "caption": ""
    },
    {
      "name": "fig246.jpeg",
      "caption": ""
    },
    {
      "name": "fig247.jpeg",
      "caption": ""
    },
    {
      "name": "fig248.jpeg",
      "caption": ""
    },
    {
      "name": "fig249.jpeg",
      "caption": ""
    },
    {
      "name": "fig250.jpeg",
      "caption": ""
    },
    {
      "name": "fig251.jpeg",
      "caption": ""
    },
    {
      "name": "fig252.jpeg",
      "caption": ""
    },
    {
      "name": "fig253.jpeg",
      "caption": ""
    },
    {
      "name": "fig254.jpeg",
      "caption": ""
    },
    {
      "name": "fig255.jpeg",
      "caption": ""
    },
    {
      "name": "fig256.jpeg",
      "caption": ""
    },
    {
      "name": "fig257.jpeg",
      "caption": ""
    },
    {
      "name": "fig258.jpeg",
      "caption": ""
    },
    {
      "name": "fig259.jpeg",
      "caption": ""
    },
    {
      "name": "fig260.jpeg",
      "caption": ""
    },
    {
      "name": "fig261.jpeg",
      "caption": ""
    },
    {
      "name": "fig262.jpeg",
      "caption": ""
    },
    {
      "name": "fig263.jpeg",
      "caption": ""
    },
    {
      "name": "fig264.jpeg",
      "caption": ""
    },
    {
      "name": "fig265.jpeg",
      "caption": ""
    },
    {
      "name": "fig266.jpeg",
      "caption": ""
    },
    {
      "name": "fig267.jpeg",
      "caption": ""
    },
    {
      "name": "fig268.jpeg",
      "caption": ""
    },
    {
      "name": "fig269.jpeg",
      "caption": ""
    },
    {
      "name": "fig270.jpeg",
      "caption": ""
    },
    {
      "name": "fig271.jpeg",
      "caption": ""
    },
    {
      "name": "fig272.jpeg",
      "caption": ""
    },
    {
      "name": "fig273.jpeg",
      "caption": ""
    },
    {
      "name": "fig274.jpeg",
      "caption": ""
    },
    {
      "name": "fig275.jpeg",
      "caption": ""
    },
    {
      "name": "fig276.jpeg",
      "caption": ""
    },
    {
      "name": "fig277.jpeg",
      "caption": ""
    },
    {
      "name": "fig278.jpeg",
      "caption": ""
    },
    {
      "name": "fig279.jpeg",
      "caption": ""
    },
    {
      "name": "fig280.jpeg",
      "caption": ""
    },
    {
      "name": "fig281.jpeg",
      "caption": ""
    },
    {
      "name": "fig282.jpeg",
      "caption": ""
    },
    {
      "name": "fig283.jpeg",
      "caption": ""
    },
    {
      "name": "fig284.jpeg",
      "caption": ""
    },
    {
      "name": "fig285.jpeg",
      "caption": ""
    },
    {
      "name": "fig286.jpeg",
      "caption": ""
    },
    {
      "name": "fig287.jpeg",
      "caption": ""
    },
    {
      "name": "fig288.jpeg",
      "caption": ""
    },
    {
      "name": "fig289.jpeg",
      "caption": ""
    },
    {
      "name": "fig290.jpeg",
      "caption": ""
    },
    {
      "name": "fig291.jpeg",
      "caption": ""
    },
    {
      "name": "fig292.jpeg",
      "caption": ""
    },
    {
      "name": "fig293.jpeg",
      "caption": ""
    },
    {
      "name": "fig294.jpeg",
      "caption": ""
    },
    {
      "name": "fig295.jpeg",
      "caption": ""
    },
    {
      "name": "fig296.jpeg",
      "caption": ""
    },
    {
      "name": "fig297.jpeg",
      "caption": ""
    },
    {
      "name": "fig298.jpeg",
      "caption": ""
    },
    {
      "name": "fig299.jpeg",
      "caption": ""
    },
    {
      "name": "fig300.jpeg",
      "caption": ""
    },
    {
      "name": "fig301.jpeg",
      "caption": ""
    },
    {
      "name": "fig302.jpeg",
      "caption": ""
    },
    {
      "name": "fig303.jpeg",
      "caption": ""
    },
    {
      "name": "fig304.jpeg",
      "caption": ""
    },
    {
      "name": "fig305.jpeg",
      "caption": ""
    },
    {
      "name": "fig306.jpeg",
      "caption": ""
    },
    {
      "name": "fig307.jpeg",
      "caption": ""
    },
    {
      "name": "fig308.jpeg",
      "caption": ""
    },
    {
      "name": "fig309.jpeg",
      "caption": ""
    },
    {
      "name": "fig310.jpeg",
      "caption": ""
    },
    {
      "name": "fig311.jpeg",
      "caption": ""
    },
    {
      "name": "fig312.jpeg",
      "caption": ""
    },
    {
      "name": "fig313.jpeg",
      "caption": ""
    },
    {
      "name": "fig314.jpeg",
      "caption": ""
    },
    {
      "name": "fig315.jpeg",
      "caption": ""
    },
    {
      "name": "fig316.jpeg",
      "caption": ""
    },
    {
      "name": "fig317.jpeg",
      "caption": ""
    },
    {
      "name": "fig318.jpeg",
      "caption": ""
    },
    {
      "name": "fig319.jpeg",
      "caption": ""
    },
    {
      "name": "fig320.jpeg",
      "caption": ""
    },
    {
      "name": "fig321.jpeg",
      "caption": ""
    },
    {
      "name": "fig322.jpeg",
      "caption": ""
    },
    {
      "name": "fig323.jpeg",
      "caption": ""
    },
    {
      "name": "fig324.jpeg",
      "caption": ""
    },
    {
      "name": "fig325.jpeg",
      "caption": ""
    },
    {
      "name": "fig326.jpeg",
      "caption": ""
    },
    {
      "name": "fig327.jpeg",
      "caption": ""
    },
    {
      "name": "fig328.jpeg",
      "caption": ""
    },
    {
      "name": "fig329.jpeg",
      "caption": ""
    },
    {
      "name": "fig330.jpeg",
      "caption": ""
    },
    {
      "name": "fig331.jpeg",
      "caption": ""
    },
    {
      "name": "fig332.jpeg",
      "caption": ""
    },
    {
      "name": "fig333.jpeg",
      "caption": ""
    },
    {
      "name": "fig334.jpeg",
      "caption": ""
    },
    {
      "name": "fig335.jpeg",
      "caption": ""
    },
    {
      "name": "fig336.jpeg",
      "caption": ""
    },
    {
      "name": "fig337.jpeg",
      "caption": ""
    },
    {
      "name": "fig338.jpeg",
      "caption": ""
    },
    {
      "name": "fig339.jpeg",
      "caption": ""
    },
    {
      "name": "fig340.jpeg",
      "caption": ""
    },
    {
      "name": "fig341.jpeg",
      "caption": ""
    },
    {
      "name": "fig342.jpeg",
      "caption": ""
    },
    {
      "name": "fig343.jpeg",
      "caption": ""
    },
    {
      "name": "fig344.jpeg",
      "caption": ""
    },
    {
      "name": "fig345.jpeg",
      "caption": ""
    },
    {
      "name": "fig346.jpeg",
      "caption": ""
    },
    {
      "name": "fig347.jpeg",
      "caption": ""
    },
    {
      "name": "fig348.jpeg",
      "caption": ""
    },
    {
      "name": "fig349.jpeg",
      "caption": ""
    },
    {
      "name": "fig350.jpeg",
      "caption": ""
    },
    {
      "name": "fig351.jpeg",
      "caption": ""
    },
    {
      "name": "fig352.jpeg",
      "caption": ""
    },
    {
      "name": "fig353.jpeg",
      "caption": ""
    },
    {
      "name": "fig354.jpeg",
      "caption": ""
    },
    {
      "name": "fig355.jpeg",
      "caption": ""
    },
    {
      "name": "fig356.jpeg",
      "caption": ""
    },
    {
      "name": "fig357.jpeg",
      "caption": ""
    },
    {
      "name": "fig358.jpeg",
      "caption": ""
    },
    {
      "name": "fig359.jpeg",
      "caption": ""
    },
    {
      "name": "fig360.jpeg",
      "caption": ""
    },
    {
      "name": "fig361.jpeg",
      "caption": ""
    }
  ]
}