# 一致性三位一體作為通用世界模型的定義原則

arXiv:2602.23152v1 [cs.AI] 26 Feb 2026

---

# 目錄

<table>
   <thead>
    <tr>
     <td>1</td>
     <td>引言</td>
     <td>4</td>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>2</td>
     <td>一致性的基礎探索</td>
     <td>5</td>
    </tr>
    <tr>
     <td>2.1</td>
     <td>通用世界模型的解剖結構</td>
     <td>5</td>
    </tr>
    <tr>
     <td>2.2</td>
     <td>模態一致性</td>
     <td>6</td>
    </tr>
    <tr>
     <td>2.2.1</td>
     <td>理論基礎</td>
     <td>7</td>
    </tr>
    <tr>
     <td>2.2.2</td>
     <td>離散序列對比連續流形</td>
     <td>8</td>
    </tr>
    <tr>
     <td>2.2.3</td>
     <td>架構演進</td>
     <td>10</td>
    </tr>
    <tr>
     <td>2.2.4</td>
     <td>通過 RL 實現意圖對齊</td>
     <td>12</td>
    </tr>
    <tr>
     <td>2.2.5</td>
     <td>通過測試時計算的認知迴圈</td>
     <td>13</td>
    </tr>
    <tr>
     <td>2.3</td>
     <td>空間一致性</td>
     <td>14</td>
    </tr>
    <tr>
     <td>2.3.1</td>
     <td>一致性的幾何分解</td>
     <td>14</td>
    </tr>
    <tr>
     <td>2.3.2</td>
     <td>理論表述</td>
     <td>15</td>
    </tr>
    <tr>
     <td>2.3.3</td>
     <td>2D 代理流形與域不匹配</td>
     <td>16</td>
    </tr>
    <tr>
     <td>2.3.4</td>
     <td>隱式連續場</td>
     <td>18</td>
    </tr>
    <tr>
     <td>2.3.5</td>
     <td>顯式拉格朗日基元</td>
     <td>19</td>
    </tr>
    <tr>
     <td>2.3.6</td>
     <td>生成統計先驗</td>
     <td>20</td>
    </tr>
    <tr>
     <td>2.4</td>
     <td>時間一致性</td>
     <td>22</td>
    </tr>
    <tr>
     <td>2.4.1</td>
     <td>從頻率穩定性到物理合規性</td>
     <td>23</td>
    </tr>
    <tr>
     <td>2.4.2</td>
     <td>潛在時間膨脹</td>
     <td>24</td>
    </tr>
    <tr>
     <td>2.4.3</td>
     <td>離散自迴歸建模</td>
     <td>25</td>
    </tr>
    <tr>
     <td>2.4.4</td>
     <td>通過 DiT 的統一時空建模</td>
     <td>26</td>
    </tr>
    <tr>
     <td>2.4.5</td>
     <td>邏輯一致性與因果推理</td>
     <td>27</td>
    </tr>
    <tr>
     <td>2.5</td>
     <td>一致性的展望</td>
     <td>28</td>
    </tr>
    <tr>
     <td>3</td>
     <td>多重一致性的初步整合</td>
     <td>28</td>
    </tr>
    <tr>
     <td>3.1</td>
     <td>大型多模態模型的崛起</td>
     <td>28</td>
    </tr>
    <tr>
     <td>3.1.1</td>
     <td>LLM 作為核心認知基礎</td>
     <td>28</td>
    </tr>
    <tr>
     <td>3.1.2</td>
     <td>認知演進作為多模態</td>
     <td>29</td>
    </tr>
    <tr>
     <td>3.2</td>
     <td>模態與空間一致性的整合</td>
     <td>30</td>
    </tr>
    <tr>
     <td>3.2.1</td>
     <td>像素空間操縱</td>
     <td>31</td>
    </tr>
    <tr>
     <td>3.2.2</td>
     <td>視圖空間映射</td>
     <td>34</td>
    </tr>
    <tr>
     <td>3.2.3</td>
     <td>體積空間表示</td>
     <td>35</td>
    </tr>
    <tr>
     <td>3.2.4</td>
     <td>強化學習用於模態-空間對齊</td>
     <td>37</td>
    </tr>
    <tr>
     <td>3.3</td>
     <td>模態與時間一致性的整合</td>
     <td>38</td>
    </tr>
    <tr>
     <td>3.3.1</td>
     <td>端到端可擴展建模</td>
     <td>39</td>
    </tr>
    <tr>
     <td>3.3.2</td>
     <td>顯式結構化控制</td>
     <td>43</td>
    </tr>
    <tr>
     <td>3.3.3</td>
     <td>統一理解與生成共生架構</td>
     <td>46</td>
    </tr>
    <tr>
     <td>3.3.4</td>
     <td>強化學習用於模態-時間對齐</td>
     <td>47</td>
    </tr>
    <tr>
     <td>3.4</td>
     <td>空間與時間一致性的整合</td>
     <td>49</td>
    </tr>
    <tr>
     <td>3.4.1</td>
     <td>隱式時空學習</td>
     <td>50</td>
    </tr>
    <tr>
     <td>3.4.2</td>
     <td>顯式幾何錨定</td>
     <td>52</td>
    </tr>
   </tbody>
  </table>

---

<table>
    <tbody>
        <tr>
            <td>3.4.3</td>
            <td>統一時空表示</td>
            <td>54</td>
        </tr>
        <tr>
            <td>3.4.4</td>
            <td>強化學習用於空間-時間對齊</td>
            <td>57</td>
        </tr>
        <tr>
            <td>3.5</td>
            <td>世界模型的初步湧現</td>
            <td>58</td>
        </tr>
        <tr>
            <td>3.5.1</td>
            <td>從基準建立到多樣演進</td>
            <td>58</td>
        </tr>
        <tr>
            <td>3.5.2</td>
            <td>三重一致性的對抗迴圈</td>
            <td>60</td>
        </tr>
        <tr>
            <td colspan="2">4 挑戰、基準與展望</td>
            <td>61</td>
        </tr>
        <tr>
            <td>4.1</td>
            <td>從初步融合到真正統一的核心挑戰</td>
            <td>61</td>
        </tr>
        <tr>
            <td>4.2</td>
            <td>構建綜合評估基準</td>
            <td>62</td>
        </tr>
        <tr>
            <td>4.2.1</td>
            <td>模態一致性：從符號映射到知識協同</td>
            <td>62</td>
        </tr>
        <tr>
            <td>4.2.2</td>
            <td>空間一致性：從視覺相似性到拓撲與物理驗證</td>
            <td>63</td>
        </tr>
        <tr>
            <td>4.2.3</td>
            <td>時間一致性：從幀間平滑性到邏輯因果演進</td>
            <td>63</td>
        </tr>
        <tr>
            <td>4.2.4</td>
            <td>現有基準的局限性與我們基準的設計理由</td>
            <td>64</td>
        </tr>
        <tr>
            <td>4.3</td>
            <td>終極展望：通用世界模擬器</td>
            <td>65</td>
        </tr>
        <tr>
            <td colspan="2">5 CoW-Bench</td>
            <td>66</td>
        </tr>
        <tr>
            <td>5.1</td>
            <td>資料集</td>
            <td>66</td>
        </tr>
        <tr>
            <td>5.1.1</td>
            <td>資料集構建</td>
            <td>66</td>
        </tr>
        <tr>
            <td>5.1.2</td>
            <td>資料集分析</td>
            <td>66</td>
        </tr>
        <tr>
            <td>5.2</td>
            <td>評估指標</td>
            <td>68</td>
        </tr>
        <tr>
            <td>5.3</td>
            <td>與現有基準的比較</td>
            <td>71</td>
        </tr>
        <tr>
            <td>5.4</td>
            <td>主要結果</td>
            <td>72</td>
        </tr>
        <tr>
            <td>5.5</td>
            <td>單軸一致性</td>
            <td>73</td>
        </tr>
        <tr>
            <td>5.5.1</td>
            <td>模態一致性結果</td>
            <td>73</td>
        </tr>
        <tr>
            <td>5.5.2</td>
            <td>時間一致性結果</td>
            <td>75</td>
        </tr>
        <tr>
            <td>5.5.3</td>
            <td>空間一致性結果</td>
            <td>76</td>
        </tr>
        <tr>
            <td>5.6</td>
            <td>跨軸一致性</td>
            <td>78</td>
        </tr>
        <tr>
            <td>5.6.1</td>
            <td>模態-空間一致性結果：語義到幾何綁定</td>
            <td>78</td>
        </tr>
        <tr>
            <td>5.6.2</td>
            <td>模態-時間一致性結果：執行時間程序</td>
            <td>79</td>
        </tr>
        <tr>
            <td>5.6.3</td>
            <td>時間-空間一致性結果：導航揭露缺失的世界狀態</td>
            <td>80</td>
        </tr>
        <tr>
            <td>5.7</td>
            <td>樣本分析</td>
            <td>81</td>
        </tr>
        <tr>
            <td>5.7.1</td>
            <td>單一一致性任務</td>
            <td>81</td>
        </tr>
        <tr>
            <td>5.7.2</td>
            <td>複合一致性任務</td>
            <td>83</td>
        </tr>
        <tr>
            <td colspan="2">6 結論</td>
            <td>84</td>
        </tr>
        <tr>
            <td colspan="2">7 貢獻</td>
            <td>97</td>
        </tr>
    </tbody>
</table>

---

一致性三角性作為通用世界模型的定義性原則

# 1 Introduction

The pursuit of Artificial General Intelligence (AGI) is fundamentally anchored in the aspiration to endow machines with a profound understanding of the physical reality. A truly intelligent agent must evolve from a passive observer [1] into a proactive simulator [2, 3], possessing an internal world model capable of learning objective physical laws, reasoning about counterfactual scenarios [4], and predicting future states from current actions [5].

Recent years have witnessed an explosion in generative capability, driven by the data-driven Scaling Laws. Video generation models, represented by Sora [2] and Gen-3 [6], have demonstrated an astonishing ability to approximate complex dynamics, creating high-fidelity visual sequences that often are indistinguishable from reality. Simultaneously, the rise of Unified Multimodal Models (UMMs) [7, 8] has offered a promising architectural paradigm for integrating diverse sensory inputs into a shared semantic manifold [9]. However, a critical gap remains: existing models, despite their visual plausibility, often behave as naive physicists. They frequently suffer from structural hallucinations, temporal inconsistencies, and violations of causality—symptoms of a system that mimics pixel statistics rather than internalizing physical principles. The field lacks a principled theoretical framework to define the essential properties requisite for a General World Model.

To bridge the chasm between visual generation and physical simulation, we propose that a robust World Model must be grounded in the *Trinity of Consistency*. We argue that a valid internal simulator must satisfy three orthogonal yet synergistic constraints:

• **Modal Consistency (The Semantic Interface):** The ability to align heterogeneous information (text, image, tactile) into a unified semantic space, serving as the cognitive interface for instruction and feedback.

• **Spatial Consistency (The Geometric Basis):** The capacity to construct a 3D-aware representation that respects geometry, occlusion, and object permanence, ensuring the static plausibility of the simulated world.

• **Temporal Consistency (The Causal Engine):** The adherence to physical laws and causal logic over time, ensuring that dynamic evolution follows a predictable and logically sound trajectory.

Through this tripartite lens, we systematically review the evolution of generative models from specialized modules to unified world simulators. We trace the trajectory from loosely coupled specialized modules toward end-to-end unified architectures. We argue that dissolving the barriers between these dimensions is the necessary substrate for the emergence of world simulation capabilities, ensuring that modality, space, and time do not operate in isolation but synergize to model a coherent reality.

This paper is organized to mirror the evolutionary path from specialized modules to unified world simulators. First (§2), we deconstruct the independent development of Modal, Spatial, and Temporalconsistencies, analyzing their respective theoretical foundations. Second (§3), we investigate the paradigm shift enabled by UMMs, detailing how the deep integration of these dimensions facilitates the emergence of physical simulation capabilities. Third (§4), we identify the remaining gaps between current probabilistic generators and true physical simulators, setting the stage for rigorous evaluation. The notation used is summarized in Table 1.

Finally, theoretical frameworks require rigorous verification. We introduce **CoW-Bench (Consistency of World-models Benchmark)**, a unified evaluation suite centered on multi-frame reasoning and constraint satisfaction. Unlike previous benchmarks, CoW-Bench rigorously tests the model's ability to maintain the *Trinity of Consistency* under complex, open-ended scenarios, forcing it to prove it understands the world, not just how to paint it.

---

Figure 2: Performance Comparison of Mainstream Models across Different Tasks. The score has been linearly rescaled from the original range of [0, 10] to a percentage scale of [0, 100].

# 2 一致性的基礎探索

## 2.1 The Anatomy of General World Models

As discussed in Section 1 (§1), the construction of world models relies on the organic integration of modal consistency (serving as the information interface), spatial consistency (serving as the geometric cornerstone), and temporal consistency (serving as the dynamic engine). In the evolution of specialized models, these consistencies have not developed in isolation but have rather interpenetrated one another: the unified representation space derived from modality alignment provides semantic priors for the reconstruction of spatial geometry, while the 3D manifold of spatial consistency establishes physical constraints for temporal evolution.

This section deconstructs that evolutionary history. We trace how specialized models first conquered these challenges in isolation: modality alignment matured through high-dimensional manifold mapping, spatial consistency was solved via the transition from 2D proxies to explicit 3D primitives, and temporal consistency evolved from simple frame interpolation to causal dynamics modeling. Here, we systematically analyze the theoretical foundations and mechanism shifts of each dimension, establishing the necessary prerequisites that eventually enabled the emergence of the unified world simulators discussed in later sections.

---

Table 1: Notation and Descriptions

<table><thead><tr><th>Symbol</th><th>Description</th><th>Symbol</th><th>Description</th></tr></thead><tbody><tr><td><i>W</i></td><td>World Model</td><td><b>p</b></td><td>3D Position</td></tr><tr><td><i>S</i>, <i>A</i></td><td>State &amp; Action Space</td><td><i>P</i><sub><i>t</i></sub></td><td>Camera Pose at <i>t</i></td></tr><tr><td><b>s</b><sub><i>t</i></sub>, <b>a</b><sub><i>t</i></sub></td><td>State &amp; Action Instance</td><td><i>K</i></td><td>Intrinsic Matrix</td></tr><tr><td><i>&pi;</i></td><td>Policy</td><td><i>&Pi;</i></td><td>Projection Operator</td></tr><tr><td><i>&tau;</i></td><td>Trajectory</td><td><i>K</i></td><td>Keyframe Set</td></tr><tr><td><i>T</i></td><td>Dynamics Function</td><td><i>M</i><sub>geo</sub></td><td>Geometric Manifold</td></tr><tr><td><i>Z</i></td><td>Latent World State</td><td><i>G</i><sub>k</sub></td><td>3D Gaussian Primitive</td></tr><tr><td><b>x</b><sub>obs</sub></td><td>Multimodal Observation</td><td><i>&sigma;</i></td><td>Volume Density</td></tr><tr><td><b>z</b></td><td>Latent Vector</td><td><i>c</i></td><td>View-dependent Radiance</td></tr><tr><td><i>E</i>, <i>D</i></td><td>Encoder / Decoder</td><td><i>F</i><sub>fund</sub></td><td>Fundamental Matrix</td></tr><tr><td><i>C</i></td><td>VQ Codebook</td><td><i>O</i><sub>flow</sub></td><td>Optical Flow</td></tr><tr><td><i>S</i></td><td>Token Sequence</td><td><i>M</i><sub>epi</sub></td><td>Epipolar Mask</td></tr><tr><td><i>W</i><sub>proj</sub></td><td>Projection Weight</td><td><i>T</i>(<i>t</i>)</td><td>Continuous Trajectory</td></tr><tr><td><i>I</i>(<i>X</i>; <i>Z</i>)</td><td>Mutual Information</td><td><i>&Phi;</i></td><td>Spatiotemporal Field</td></tr><tr><td><i>&epsilon;</i><sub><i>&theta;</i></sub></td><td>Noise Predictor</td><td><i>&Psi;</i></td><td>Physical Property Field</td></tr><tr><td><b>v</b><sub><i>t</i></sub></td><td>Velocity Field</td><td><i>D&Phi;&n Labor from t</i></td><td>Material Derivative</td></tr><tr><td><i>g</i>(<i>t</i>)</td><td>Diffusion Coefficient</td><td>&#x20D7; &middot; <i>v</i></td><td>Divergence</td></tr><tr><td><i>&alpha;</i><sub><i>t</i></sub>, <i>&sigma;</i><sub><i>t</i></sub></td><td>SNR Parameters</td><td><b>&# manifold evolutionary flow</b></td><td>Force Vector</td></tr><tr><td><i>w</i></td><td>Wiener Process</td><td>&#x20D7;<i>f</i></td><td>Implicit Gradient</td></tr><tr><td><i>F</i><sub><i>t</i></sub></td><td>STFT (Fourier Transform)</td><td><i>M</i><sub>dyn</sub></td><td>Dynamic Manifold</td></tr><tr><td><i>L</i></td><td>Loss Function</td><td><i>Phys</i></td><td>Physics Score</td></tr><tr><td><i>G</i><sub>graph</sub></td><td>Causal Graph</td><td><i>&Delta;</i><sub>const</sub></td><td>Constraint Deviation</td></tr><tr><td><i>D</i><sub>KL</sub></td><td>KL Divergence</td><td><i>w</i></td><td>Guidance Scale</td></tr></tbody></table>

2.2 Modal Consistency

The core challenge in constructing general world models lies in the semantic alignment of hetero-
geneous modalities. Unlike the homogeneity of unimodal generation, multimodal consistency is
essentially a problem of solving high-dimensional heterogeneous manifold alignment, as illustrated
in Figure 3. The model must transcend entropy disparity and topological mismatch to construct a
unified representation space that is physically complete and logically self-consistent. To this end, we
introduce two fundamental theoretical assumptions, the Platonic Representation Hypothesis and the
Hypersphere Geometry Hypothesis, and use these as a basis to expound on the cognitive architectural
evolution from direct feed-forward mapping to iterative reasoning and planning.

To systematically deconstruct this alignment process, this section will first elucidate the origins of
the modality gap from the perspective of geometric topology (§2.2.1); subsequently, it will analyze
two mainstream generative manifold mechanisms—namely, discrete autoregression and continuous
flow matching (§2.2.2); it will then explore the orthogonal decoupled architecture evolved to minimize
gradient conflicts (§2.2.3); and finally, it will introduce feedback-based intent alignment and the
cognitive inference loop moving towards test-time compute (§2.2.5).

---

**Figure 3: Unified Representation Goal.** Modal consistency aims to project heterogeneous inputs (Text, Image, Video, Audio) into a unified, physically-aligned latent space.

2.2.1 Theoretical Foundations

**Platonic Cave & Projected Manifolds** The theoretical foundation of multimodal learning can be traced back to the platonic representation hypothesis [9]. This hypothesis formally defines the existence of an objective latent physical state space, Z<sub>world</sub>, in the real world, where images and text are projections of this high-dimensional entity onto different low-dimensional subspaces. The essence of modal consistency is solving a joint inverse projection problem: reconstructing the shared latent variable z via observed shadows {x<sub>img</sub>, x<sub>txt</sub>}.

However, this is a typical ill-posed problem—the visual projection P<sub>img</sub> retains a vast amount of high-frequency physical entropy, whereas the textual projection P<sub>txt</sub> highly abstracts discrete symbolic logic. This Entropy Asymmetry constitutes the primary obstacle to direct alignment.

**Hypersphere Hypothesis & Modal Gap** To mathematically align these two heterogeneous spaces, mainstream paradigms (such as CLIP) introduce the Hypersphere Hypothesis [43], which forces feature vectors to be uniformly distributed on a unit hypersphere S<sup>d−1</sup>. However, this strong assumption ignores the pervasive modal gap in multimodal representations [44]. On one hand, empirical studies by Liang et al. pointed out the cone effect, as shown in Figure 5: joint optimization causes visual and textual embeddings to collapse into two narrow and separated conical regions, destroying the isotropy of the feature space. On the other hand, from the perspective of manifold learning, this gap reveals a deeper topological mismatch: visual data is typically distributed on a continuous, dense low-dimensional manifold, while linguistic data presents a sparse, discrete clustering structure. This fundamental difference in intrinsic dimensionality and data density leads to manifold non-isomorphism, rendering the achievement of perfect isometric alignment between the two spaces, while maintaining their respective semantic structures, an ill-posed problem.

**Evolution of Computational Paradigms: From Amortized Inference to Test-time Compute** Facing the inherent representation errors caused by the aforementioned geometric topological mismatch, simple parameter internalization strategies face theoretical bottlenecks, prompting the modeling of modal consistency to undergo a transition between two major computational paradigms. This

---

Figure 4: Evolution of Modal Consistency: From Geometric Isolation to Cognitive Alignment

profoundly reflects the trade-off between train-time compute and test-time compute [45].

Early direct feed-forward mapping corresponds to Dual-Tower architectures [10] and single-step generative models, the core of which is identifying physical rules into neural network weights through large-scale training, i.e., Amortized Inference [46]. This paradigm requires only one forward pass during inference (NFE = 1). Although highly efficient, it is limited by in-distribution statistical correlations and essentially can only interpolate within established conical regions, making it difficult to handle unseen counterfactual combinations [47].

In contrast, the current trend is shifting towards iterative reasoning & planning, corresponding to
iterative reasoning architectures. This paradigm acknowledges the limitations of single-pass mapping
in bridging the modality gap and instead introduces explicit state space search during the inference
phase. By constructing a Tree of Thoughts [48] in the latent space or executing gradient-guided
dynamic planning, the model utilizes additional reasoning compute to instantly correct physical drift.
This marks a shift in consistency modeling from static pattern matching to dynamic manifold planning.

2.2.2 Discrete Sequences vs. Continuous Manifolds

To computationally realize the Joint Inverse Projection process in the above theory, academia has explored two distinct mathematical paths to model the target conditional probability density P(ximg|xtxt). This choice determines the physical nature of the latent space manifold: *Is it treated as a Discrete Symbolic Sequence or a Continuous Euclidean Vector Field?* We compare the mathematical forms and dynamic characteristics of these two paradigms in Table 2.

Table 2: Mechanism Comparison: Discrete AR vs. Continuous Flow Matching. The formulations highlight the trade-off between optimization objectives and error propagation dynamics.

<table>
   <thead>
    <tr>
     <td>
      Paradigm
     </td>
     <td>
      Objective (The Soul)
     </td>
     <td>
      Error
     </td>
     <td>
      Topology
     </td>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      Discrete AR
     </td>
     <td>
      LAR = -E [∑ log P(s
      <sub>
       t
      </sub>
      |s&lt;_
      <sub>
       t
      </sub>
      )]
     </td>
     <td>
      Exp.
     </td>
     <td>
      Discrete
     </td>
    </tr>
    <tr>
     <td>
      Flow Matching
     </td>
     <td>
      LFM = E(|v
      <sub>
       q
      </sub>
      (x
      <sub>
       t
      </sub>
      ) - (x
      <sub>
       1
      </sub>
      - x
      <sub>
       0
      </sub>
      )|^2]
     </td>
     <td>
      Linear
     </td>
     <td>
      Euclidean
     </td>
    </tr>
   </tbody>
  </table>

---

Figure 5: The Modal Gap Challenge. (Left) Ideal hypersphere alignment assumes uniform distribution. (Right) In reality, entropy disparity causes visual embeddings to collapse into a narrow "cone," leading to topological mismatch with discrete text tokens.

**Discrete Autoregressive (AR)** The core of this paradigm lies in the Token-centric philosophy, attempting to transform visual generation into a sequence prediction problem through a unified discrete symbol interface [49, 50]. Its generation process involves strictly coupled stages: first quantizing continuous images into discrete symbols via VQ-GAN, followed by maximizing the sequence log-likelihood using the causal attention mask of a Transformer.

*Exponential Drift & Codebook Collapse.* Although the AR paradigm achieves interface unification, it suffers from two endogenous defects when viewed from a dynamic perspective [51]. First is the curse of dimensionality. The discretization process is governed by the Dirichlet process; as the codebook dimension increases, the effective utilization rate decays exponentially, leading to the loss of high-frequency textures [52, 53]. Second is error accumulation dynamics. The essence of autoregressive generation is the recursive application of operators. Assuming the local Lipschitz constant of the operator is $L > 1$, the cumulative drift of the initial quantization error $\epsilon_0$ after $T$ steps is $\|\delta_T\| \approx L^T \|\epsilon_0|$. This exponential error amplification explains why AR models often exhibit structural collapse at the tail end when generating long sequences [54].

**Continuous Flow Matching (FM)**
To circumvent quantization errors, the new generation of paradigms (such as Stable Diffusion 3 [27], Emu3 [28]) returns to the continuous latent space. Unlike traditional diffusion models based on the SDE denoising perspective, Flow Matching (FM) [55] adopts an ODE perspective, constructing a deterministic transport path connecting noise and data.

*Velocity Field Regression & Rectified Path.* The core idea of continuous FM is to directly fit the velocity field of the probability flow. During training, the intermediate state $x_t$ is defined as a linear interpolation between data and noise, corresponding to an ideal straight trajectory with a target velocity field constantly being $v_t = x_1 - x_0$. The neural network directly regresses this velocity vector via Mean Squared Error loss. Rectified Flow [56] demonstrates that this Reflow operation rectifies the transport

---

trajectory, corresponding to a Lipschitz constant $L \approx 1$. This implies that error accumulation transforms into linear growth $\|\delta_T\| \approx T \cdot \epsilon_{step}$, allowing FM to generate high-fidelity samples in very few steps while perfectly preserving the continuous semantic manifold of the latent space.

### 2.2.3 架構演進

建立生成機制只解決了目標流形的數學表達。如何將異質模態信息注入此流形取決於模型的條件機制。多模態架構的演進展現非線性特徵，本質上是尋求最優參數空間拓撲，以最小化模態間的梯度衝突和信息丟失。此過程經歷了從幾何隔離到早期融合，最終收斂到正交解耦的三階段演進，如圖 6 所示。

**圖 6：** 多模態融合範式的演進。從幾何隔離（Dual-Tower）過渡到不穩定的早期融合（Adapter），最後收斂到大規模統一架構中的正交解耦原生統一多模態模型（MM-DiT）。

**(1) 早期演進：Dual-Tower 架構和連接器範式的建立。** 多模態對齐的早期探索呈現出兩條清晰的技術演進路徑。首先是 *Dual-Tower 架構*，由 CLIP [10] 和 ALIGN [57] 代表。此範式利用對比學習將異質模態投影到共享超球面上。儘管在檢索任務中表現優異，但獨立編碼器對圖像和文本的分開處理導致幾何拓撲上的自然不對稱性，缺乏深層、細粒度的交互。

為了解決此限制，*基於連接器的範式*（由 Flamingo [14] 和 BLIP/BLIP-2 [15, 16] 代表）應運而生。這些方法凍結預訓練的視覺編碼器，並創新引入可學習的橋接模塊（如 Perceiver Resampler 或 Q-Former）以將視覺特徵與 LLM 的語義空間對齐。凍結視覺骨幹和輕量級連接器的設計不僅降低了訓練成本，還為後續的 LMM 建立了標準的架構模板。

**(2) 早期融合與統一優化的挑戰。** 為了進一步打破模態間的幾何隔離，學術界開始探索更激進的 *早期融合* 策略。代表性的

---

工作如 Unified-IO [58] 試圖在統一的序列到序列框架內處理各種異質任務，推動了通用接口的發展。
然而，這種完全統一的範式暴露了深層的 **優化不穩定性**。特別是在引入離散化策略（如 Chameleon [59]）時，儘管實現了接口統一，不同模態表現出顯著的訓練動態差異。經驗證據表明，視覺 token 的梯度方差明顯高於文本，使得模型在聯合訓練期間難以收斂到最優解。
此外，連續非對稱範式（如 LLaVA [23]）通過投影層與大型語言模型相連接。然而，線性投影層 $W_{proj}$ 本質上充當低秩壓縮器（如圖 7 所示）。在優化過程中，模型被激勵保留對文本推理相關的語義信息，同時抑制對圖像合成至關重要的高頻分量。因此，輸入圖像與投影表示之間的互信息大幅減少。這解釋了為什麼 LLaVA 在理解任務上表現出色，但在生成任務中無法恢復紋理細節。

**圖 7：** LLaVA 中的信息不對稱。線性投影層 $W_{proj}$ 充當低秩壓縮器，優先考慮與 LLM 的語義對齐，同時丟棄可控視覺生成所需的高頻視覺紋理。

**(3) 正交解耦的主流範式。** 為了解決上述梯度衝突，由 Stable Diffusion 3.5 [27] 和 Emu3 [28] 代表的工作建立了當前的 MM-DiT 架構。核心在於 *權重解耦* 策略——為文本和圖像維持獨立的權重集 $W_{txt}$、$W_{img}$，僅在注意力操作期間交換數據，如圖 8 所示。

從優化動態的角度來看，此設計強制聯合損失函數的 Hessian 矩陣表現出近似塊對角結構：

$$H_{\text{total}} \approx \begin{bmatrix} H_{\text{txt}} & 0 \\ 0 & H_{\text{img}} \end{bmatrix}, \quad \text{s.t. } \frac{\partial^2 \mathcal{L}}{\partial W_{\text{txt}} \partial W_{\text{img}}} \rightarrow 0, \tag{1}$$

其中 $H_{total}$ 表示聯合 Hessian 矩陣，$W_{txt}/{img}$ 代表模態特定的參數。此結構有效隔離了模態特定的曲率，導致不同模態的梯度更新在參數空間中趨向正交性。
經驗數據表明此

---

**圖 8：** MM-DiT 架構。通過為文本和圖像模態維持獨立的權重集，並僅通過聯合注意力進行交互，MM-DiT 實現了正交梯度更新，有效解決了模態衝突。

機制顯著降低了梯度衝突率，從 AR 範式中的 50% 以上降低到約 30% [60]。這在 Stable Diffusion 3.5 Large 中得到驗證：得益於模態解耦，該模型在需要複雜排版渲染和長文本理解的任務上，展示了明顯優於 LLaVA 等非對稱架構的指令跟隨能力和物理保真度。

#### 2.2.4 通過強化學習的意圖對齐

在使用 MM-DiT 架構實現正交解耦後，一致性建模的重點從物理表示擬合轉向高層語義對齐。雖然傳統的最大似然估計（MLE）捕捉像素統計相關性，但在處理病態的聯合逆投影問題時，由於缺乏顯式監督，常常陷入語義漂移 [9]。為此，學術界引入了帶有人類反饋的強化學習（RLHF）[61]，將對齐重新框架化為超球面流形上的獎勵導向搜索 [43]。

**過程監督與物理約束** 基於偏好微調的架構演進始於高效的 DiT 基線，以 PixArt-α [29] 為例。由於其訓練成本相對較低，這些架構在偏好監督下實現了實際的端到端對齐。針對傳統 DPO（直接偏好優化）中軌跡反饋的稀疏性，SPO [39] 和 VisualPRM [40] 引入了逐步評估機制，對去噪路徑中的每個推理步驟進行細粒度監督。同時，為了解決如重力違反等非物理現象，PhyGDPO [41] 引入了物理感知的 VLM 反饋，其核心損失函數通過懲罰物理違反項 ΛPhysScore 實現：

$$ \mathcal{L}_{\text{Phy-DPO}} = -\mathbb{E} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(v_w)}{\pi_{\text{ref}}(v_w)} - \beta \log \frac{\pi_{\theta}(v_l)}{\pi_{\text{ref}}(v_l)} + \alpha \Delta \text{PhysScore} \right) \right], \quad (2) $$

---

其中 $\beta$ 是 KL 散度懲罰係數，控制與參考策略 $\pi_{ref}$ 的偏差，$v_w$ 和 $v_l$ 分別表示獲勝和失敗的視頻樣本，$\Delta$PhysScore 衡量物理合規性分數的差異。

**感知-生成協同迴圈** 為了進一步突破靜態數據集的上限，學術界建立了以 *VLM-as-a-Judge* 為中心的交互優化範式。此範式利用多模態大型模型的強大語義感知能力作為評論器，構建生成-評估-精化的閉環系統。代表性工作如 Meta-Morph [35] 通過指令調優實現了理解和生成的統一對齐；而 SRUM [36] 進一步提出了統一的多模態自糾正機制。SRUM 通過將判別梯度回傳至生成器，或利用 VLM 生成的細粒度描述來指導擴散模型的迭代微調。感知和生成之間的相互改進不僅解決了複雜提示下的屬性遺漏問題，還使得 T2I 模型能夠在沒有外部人工標註的情況下，通過自舉逐漸接近 VLM 的語義理解上界。

**AR 模型的因式分解優化** 與擴散模型的去噪優化不同，AR 模型面臨離散空間不可微性和時間誤差累積的雙重挑戰。針對此問題，2025 年的 AR-GRPO [42] 和 ReasonGen-R1 [62] 提出了序列生成的因式分解優化策略：

$$ \mathcal{L}_{AR-RL} = \underbrace{E_\pi[R(x)]}_{\text{對齐增益}} - \beta \underbrace{D_{KL}(\pi || \pi_{ref})}_{\text{時間平滑}}, \quad (3) $$

其中 $R(x)$ 是來自 CLIP 或 VQA 反饋的獎勵函數，$\beta$ 作為 KL 散度項 $D_{KL}$ 的正則化係數。此範式明確將損失函數分解為對齐增益和時間平滑項。對齐項利用 CLIP/VQA 獎勵引導 token 選擇以符合語義意圖，而 KL 散度約束強制策略保持在預訓練語言流形內，防止模型因過度優化獎勵而遭受語言崩潰。經驗證據表明此策略有效抑制了長序列生成中的 token 重複和文本亂碼現象。

**2.2.5 通過測試時計算的認知迴圈**

儘管強化學習已實現人類意圖的初步對齐，模態一致性仍受柏拉圖統計邊界的限制 [9]。現有的生成模型本質上是模式匹配插值器，僅通過攤銷推理擬合訓練分佈 [47]。當面臨需要多步鏈式推導的反事實任務時，此單遍映射機制缺乏實時驗證，容易產生邏輯幻覺 [63]。

為了在長範圍生成中糾正邏輯漂移，一致性建模正朝向測試時計算 [45] 範式轉變。此範式承認單次逆投影的局限性，而是在推理階段引入顯式狀態空間搜索。在此閉環中，生成過程被重新定義為時空流形 $\mathcal{M}$ 上的最優路徑搜索問題。

近期的範式如 UniGen [64] 和 EvoSearch [65] 引入了多步推理架構，結合蒙特卡洛樹搜索（MCTS）[66] 與驗證機制 [67]，以在生成期間實現推理時縮放。針對視覺任務的高維性質，VisualPRM [40] 利用過程獎勵模型對去噪軌跡的邏輯節點進行細粒度驗證，從而數學上增強了生成結果的邏輯一致性。此外，通過整合顯式因果規劃層 [68]，模型能夠利用額外的推理計算來檢測和糾正物理軌跡中的偏差。

## 2.3 空間一致性

圖 9：透過多視圖約束的空間一致性。該模型確保生成的主體（Doge）在前視圖、側視圖和俯視圖中保持幾何一致性，防止結構扭曲和 Janus 問題。

前一節討論的模態一致性成功為異質數據構建了統一的語義映射。然而，為了構建可執行的內部模擬器，僅有語義對齐是不夠的。正如發展心理學研究所指出的，對世界的認知建立在物體恆存性[69]和 3D 排斥性[70]的基礎之上。這樣缺乏幾何實體的語義表示無法支持代理在三維空間中的導航和互動[71]。空間一致性的核心使命是將這些語義潛變數錨定到符合物理定律的三維幾何流形 $M_{geo}$ 上。這本質上是解決典型的不適定逆問題[72]，如圖 9 所示：具體而言，如何從維度降低、稀疏的 2D 觀測中恢復滿足多視圖幾何約束（例如極線等變性）的高維狀態空間，同時避免 Janus 問題等結構性偽影。

為了構建統一的理論框架，我們將此過程形式化為在時空流形上解決一組耦合微分方程逆問題。本節將闡明模型如何透過引入物理先驗和生成擴散先驗來建立世界模型的靜態幾何基礎，沿著從 2D 代理流形到 3D 隱函數場，最後收斂到顯式 Lagrangian 基元的演化路徑。

### 2.3.1 一致性的幾何分解

為了在數學上刻畫空間一致性，我們將這個抽象概念分解為兩個互補且層次遞進的拓撲約束：前者支配物理表面的微觀連續性，後者保證物體結構的宏觀唯一性和相干性。

**微觀層級：局部鄰域拓撲一致性。** 此約束聚焦於流形 $M$ 的 **內在連續性**，在數學上對應於 Lipschitz 條件。即，對於流形上任意兩個相鄰的點，它們的物理屬性（如顏色、密度）的差異應該被它們的歐幾里得距離嚴格地線性約束。在 3D 重建和生成任務中，此約束通常通過幾何正則化項顯式地實現。例如，IGR（隱式幾何正則化）[87] 利用 Eikonal 方程來約束梯度的範數，而 RegNeRF [105] 引入平滑損失來抑制稀疏視圖下生成的非物理高頻噪聲，確保生成的物體具有光滑且物理上合理的表面。

---

圖 10：空間一致性範式的演進：從 2D 代理到生成基元。

**宏觀層級：全局幾何一致性。** 局部平滑性單獨是不夠的；模型還必須滿足多視圖幾何中的 **Epipolar 等變性** [72]。即，當從不同視點 $v_a, v_b$ 觀察同一物體時，其投影座標應滿足嚴格的代數約束 $x_b^{\top} F_a x_a = 0$。在生成模型中，違反此約束是 Janus 問題 [97] 的根本原因，即不同視點產生不兼容的物體幾何。為了解決這一問題，SyncDreamer [106] 構造了顯式的 3D 代價體積來強制對齊，而 MVDream [99] 利用多視圖自注意力機制將硬幾何約束內化為注意力權重，直接鎖定生成物體的全局拓撲唯一性。

上述分解闡明了空間一致性的幾何目標。然而，如何在神經網絡的參數空間內系統地求解這些拓撲約束，需要建立統一的微分方程視角。

### 2.3.2 理論表述

為了構造理論框架，我們將 3D 視覺生成中的空間一致性形式化為在時空流形 $\mathcal{M} \subseteq \mathbb{R}^3 \times \mathbb{R}^+$ 上求解一組耦合逆微分問題。從這個視角來看，完整態場 $\Phi(\boldsymbol{x}, t)$ 的構造遵循三個核心物理定律，這些定律分別定義了世界的呈現方式、生成規則和運動規律。

**物理渲染：RTE。** 顯式和隱式 3D 表示都可以從物理上視為輻射傳輸方程（RTE）[107] 的離散解。對於射線 $r(s) = o + sd$，其輻度 $L$ 沿路徑的變化遵循：

$$ \underbrace{d \cdot \nabla L(\boldsymbol{x}, \boldsymbol{d})}_{\text{傳輸}} = \underbrace{-\sigma(\boldsymbol{x}) L(\boldsymbol{x}, \boldsymbol{d})}_{\text{吸收}} + \underbrace{\sigma(\boldsymbol{x}) c(\boldsymbol{x}, \boldsymbol{d})}_{\text{發射}}, \quad (4) $$

---

其中 $\sigma(x)$ 表示位置 $x$ 處的體積密度，$c(x, d)$ 表示視圖相關的顏色發射。離散化的差異構成了技術路線的發散：NeRF（隱式場）採用體積渲染積分，通過沿射線對方程 (4) 進行密集的黎曼和近似來逼近解；而 3DGS（顯式基元）將連續場離散化為一組拉格朗日高斯基函數，將積分轉化為高效的解析光柵化。前者確保連續性，而後者實現了實時性能。

**生成演進：SDE。** 在生成先驗範式中，空間一致性源於預訓練模型的概率分佈。我們將從高斯白噪聲 $z_T$ 恢復到數據流形 $z_0$ 的過程建模為隨機微分方程（SDE）[108]：

$$d\Phi_t = f(\Phi_t, t)dt + g(t)dw, \qquad (5)$$

其中 $f(\cdot)$ 是支配語義演進的確定性漂移項，$g(t)$ 表示擴散係數，$w$ 代表標準維納過程。現代生成模型的目標是學習上述 SDE 的反向過程（分數匹配）。當擴散項 $g(t) = 0$ 時，SDE 退化為確定性常微分方程（ODE），即流匹配（Flow Matching）。這為理解生成模型如何從混亂的噪聲恢復「光滑且拓撲一致」的幾何結構提供了理論基礎。

**運動規律：拉格朗日輸運。** 為了確保沿時間軸的空間結構的拓撲一致性，物質點 $x$ 的運動必須遵循拉格朗日流：

$$\frac{dx}{dt} = v(x, t), \quad \text{s.t.} \quad \frac{D\Phi}{Dt} = 0 \quad (\text{物質導數}), \qquad (6)$$

其中 $v$ 代表驅動粒子運動的速度場，$\frac{D\Phi}{Dt}$ 表示物質導數。此約束暗示特徵 $\Phi$ 在隨流體運動時保持守恆（物質導數為 0）。這直接對應於顯式基元範式中的粒子追蹤機制，並作為連接靜態幾何和動態視頻的數學橋樑。

空間一致性演進的歷史本質上是一個學術界從求解靜態 RTE（NeRF）轉向反向求解生成 SDE（擴散），最終整合拉格朗日動態約束的過程。這個從嘗試在 2D 投影流形上擬合動態，到隱式連續場積分，再回到顯式拉格朗日基元的迭代過程如圖 11 所示。

#### 2.3.3 2D 代理流形與域不匹配

在顯式 3D 表示確立其主流地位之前，解決時空一致性的主要途徑是基於流形假說的視頻預測。此範式避免了昂貴的 SE(3) 空間建模，轉而試圖將高維物理態場 $\Phi$ 的演化動力學算子 $\mathcal{F}_{3D}: SE(3) \times \mathbb{R}^3 \to \mathbb{R}^3$ 簡化為 2D 圖像流形 $M_{img}$ 上的參數化映射 $\mathcal{F}_{\theta}: \mathbb{R}^{H \times W} \to \mathbb{R}^{H \times W}$。雖然此代理流形策略提供了計算複雜度的優勢，但它引入了基本性的域不匹配。

**動態擬合缺乏 SE(3) 等變性。** ConvLSTM [73] 和 PredRNN [74, 109] 等早期工作雖然通過改進的遞迴單元（例如梯度高速公路單元 GHU）緩解了長序列梯度衰減，但依賴於卷積運算 $W * I$，這些運算僅具有平移等變性，缺乏感知 3D 旋轉群 SO(3) 的能力。如 [110, 111, 112, 113] 所述，試圖通過 2D 像素網格的非線性變換來模擬 3D 剛體旋轉本質上是在低維流形上逼近高維拓撲。這種歸納偏置的不對齐導致模型無法將外在的相機運動與

---

**圖 11：空間一致性範式的演進。** 我們追蹤了從早期 2D 代理流形、到隱式連續場（如 NeRF）、再到顯式拉格朗日基元（如 3DGS），最後整合生成擴散先驗的軌跡。

內在的物體變形解耦開，在大視點變換過程中不可避免地導致生成視頻中產生非物理的非剛性扭曲或紋理拉伸。

**物理感知建模的早期嘗試與局限。** 為了緩解純統計擬合引起的模糊，並增強時間外推的魯棒性，學術界試圖賦予黑箱模型物理可解釋性，核心思想是將物理守恆律注入神經網絡的參數空間。這個方向的先驅是 **物理信息神經網絡（PINN）** [77]，它將偏微分方程（PDE）的殘差作為正則化項添加到損失函數中，強制網絡輸出符合物理約束，如流體力學或波動方程。隨後，深拉格朗日網絡（DeLaN）[78] 和哈密頓神經網絡（HNN）[79] 進一步引入了能量守恆先驗，使用歐拉-拉格朗日方程顯式地建模系統的總能量（哈密頓量），從而在連續時間內實現複雜動態系統的精確軌跡預測。

在視頻預測領域，PhyDNet [75] 借鑑了這些思想，通過顯式地將隱態分解為物理動態分支 $\mathcal{H}_{phy}$ 和殘差紋理分支 $\mathcal{H}_{res}$。與 PINN 的軟約束不同，PhyDNet 通過矩匹配直接限制卷積核權重，使其近似離散網格上的 PDE 有限差分算子：

$$ \frac{\partial \mathcal{H}}{\partial t} \approx \sum_k c_k \frac{\partial^k \mathcal{H}}{\partial \mathbf{x}^k} \Rightarrow \text{濾波器權重} \xrightarrow{\text{矩}} \text{有限差分模板}, \quad (7) $$

其中 $\mathcal{H}$ 表示分解的隱態，$\mathbf{x}$ 是空間座標，$c_k$ 代表偏微分係數。

此外，為了解決離散時間採樣的局限，Rubanova 等人提出的潛在 ODE（Latent ODEs）[80] 利用連續時間 ODE 求解器來建模隱態演進，有效地

---

處理非均勻採樣下的時間一致性問題。

儘管這些方法和 SVG [76] 等變分推斷模型在短期預測中取得了進展，但基於 2D 流形的建模隱含了空間連續性假設。一旦發生遮擋引起的深度突變，光流場變得不可微，PDE 約束立即失效。這種無法建模物體永久性的缺陷表明在 2D 代理流形上求解嚴格 3D 一致性存在理論局限。

#### 2.3.4 隱式連續場

為了解決 2D 代理流形在 3D 一致性中的理論局限，學術界轉向在 3D 歐幾里得空間中直接定義態場。此範式的建立建立在 SoftRas [114] 和 DIB-R [115] 等基於網格的可微渲染工作基礎之上，這些工作驗證了通過光滑光柵化過程計算梯度 $\partial I / \partial \mathcal{V}$ 的可行性。NeRF [81] 進一步拋棄了離散幾何，使用 MLP 將場景參數化為連續座標映射函數 $F_{\Theta}: (x, \mathbf{d}) \rightarrow (c, \sigma)$，通過可微體積渲染積分連接 3D 場與 2D 觀測。

(1) **表示效率與頻率保真度。** 神經輻度場的演進本質上是尋求 *參數效率* 和 *信號保真度* 之間平衡的過程。此領域的挑戰已從初始推理加速（引入離散表示）深化到在離散空間中維持頻域抗混疊特性。

(i) *向混合表示的轉變。* 為了打破純 MLP 架構的效率瓶頸，NVIDIA 的 Instant-NGP [84] 引入了**多分辨率哈希網格**，使用空間哈希將連續座標映射到可學習的特徵表；而在生成領域，EG3D [116] 提出了**三平面**表示，為 3D GAN 確立了主流範式。這些方法（包括 TensoRF [117]）通過引入顯式空間歸納偏置顯著改進了訓練效率和幾何生成能力。

(ii) *混疊與信號處理修正。* 然而，上述離散化表示（以及原始 NeRF 中的逐點採樣）在高頻區域引入了嚴重的混疊。Mip-NeRF [82] 從信號處理的角度糾正了這一缺陷，指出離散採樣忽視採樣體積違反了奈奎斯特採樣定理。通過引入圓錐追蹤和整合位置編碼（IPE），Mip-NeRF 計算了高斯體積內的特徵期望，揭示了反混疊的數學本質：

$$
\gamma(\boldsymbol{\mu}, \boldsymbol{\Sigma}) = \mathbb{E}_{x \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})}[\gamma(\boldsymbol{x})] \approx \sin(\boldsymbol{\mu}) \circ \exp\left(-\frac{1}{2} \text{diag}(\boldsymbol{\Sigma})\right), \qquad (8)
$$

其中 $\boldsymbol{\mu}$ 和 $\boldsymbol{\Sigma}$ 分別表示圓錐台的均值向量和協方差矩陣，$\circ$ 表示逐元素乘積。此公式揭示了深刻的物理機制：指數衰減項 $\exp(-\boldsymbol{\Sigma})$ 本質上作用為**自適應低通濾波器**。當採樣圓錐半徑增大時（即方差 $\boldsymbol{\Sigma}$ 增大，對應於遠距離視圖或低分辨率區域），高頻特徵被指數抑制。

為了將這一優異的反混疊特性轉移到高效的網格表示，Zip-NeRF [118] 進一步結合了多重採樣與特徵平滑技術，解決了哈希網格固有的尺度不確定性。這一系列演進在數學上等價於傅里葉變換中的**不確定性原理**：空間定位越寬（$\boldsymbol{\Sigma}$ 越大），頻率帶寬越窄，從而機制上消除了莫列紋和高頻偽影，實現了效率與保真度的統一。

**NeRF** 的密度場 $\sigma$ 存在物理歧義。在提取表面時，人為設定的閾值 $\tau$ 導致水平集歧義。為了獲得精確的幾何表面，NeuS [85] 和 VolSDF [86] 將表示從密度場轉換為有符號距離場（SDF）。通過引入無偏邏輯變換 $φ_s(f(x))$ 並施加 Eikonal 正則化項：

$$ \mathcal{L}_{\text{geo}} = \mathbb{E}_x[(\|\nabla f(\mathbf{x})\|_2 - 1)^2], \quad (9) $$

其中 $f(x)$ 是有符號距離函數，梯度範數約束 $\|\nabla f\|_2 = 1$ 確保了物理有效性。此約束強制隱式場的梯度範數恆為 1，確保零水平集 $S = \{x|f(x) = 0\}$ 收斂到滿足物理約束的光滑、封閉流形表面。

從流形優化的視角來看，隱式連續場本質上是用推理延遲換取幾何完整性 [85]。由於 SDF 的連續可微性，此範式構成了高保真逆渲染的理想基礎。它不僅適用於重建封閉的保水流形以實現靜態資產數字化 [86, 119]，還通過 Eikonal 正則化誘導的平滑先驗在稀疏視圖下有效地避免了顯式方法常見的幾何孔洞 [87]。然而，其數學性質也定義了理論上界：體積積分的高採樣成本 $O(N_{\text{samples}})$ 使其難以支持高幀率實時交互 [81]，而連續場的平滑假設在建模具有劇烈拓撲破裂的動態場景時面臨表現力瓶頸 [120, 89]。

### 2.3.5 Explicit Lagrangian Primitives

Although implicit continuous fields established theoretical completeness for multi-view consistency, their sampling mechanism relying on volume integration constitutes a computational bottleneck for real-time simulation. The 3D Gaussian Splatting (3DGS) proposal [89] CC2 marks the return of the representation form of the state field $\Phi$Q3 from an implicit field to explicit particles (as shown in Figure 12CR1(c)). This paradigm discretizes the scene into a set of anisotropic Gaussian primitives $\Phi = \{G_i(\mu, \Sigma, \alpha, SH)\}_{i=1}^M$ and reconstructs the projection operator $\mathcal{P}$ as Rasterization.

**Mechanisms of Static Representation.** Unlike the ray marching of NeRF [81], 3DGS [89] utilizes the GPU sorting pipeline for acceleration, containing three key characteristics:

*   **Rasterization Pipeline.** The algorithm involves two key steps: first is Frustum Culling and Projection, projecting 3D Gaussian into a 2D screen space covariance matrix $\Sigma^{2D} = J_W \Sigma^{3D} W^T j_T^T$; second is tiled radix sort, which is the computational bottleneck with complexity $O(N \cdot k)$. By leveraging a tile-based parallel rendering strategy, the method restricts computation to overlapping Gaussians and requires only $\alpha$-blending on during rasterization, avoiding invalid sampling of empty space.

*   **Integral Duality.** NeRF adopts a Backward Pull, prone to gradient masking ($\partial C / \partial \sigma_{far} \approx 0$). In contrast, 3DGS adopts a Forward Push; explicit sparsity allows error gradients $\frac{\partial L}{\partial \mu}$ to bypass the MLP and backpropagate directly and sparsely to geometric parameters. This explicit gradient flow is the mathematical foundation for the efficient convergence of 3DGS.

*   **Adaptive Density Control.** This approach can be viewed as a variant of AMR (Adaptive Mesh Refinement). The core idea is: if the gradient is too large and variance is small ($\|\nabla \mathcal{L}\| > \tau, \|\Sigma\| < \epsilon$), it is judged as underfitting and the Gaussian is cloned; if the gradient is large and variance is large, it is judged as overfitting and the Gaussian is split. Through this mechanism, the method dynamically adjusts the density of Lagrangian particles in response to the underlying optimization landscape.

**Evolution towards 4D Dynamics.** Addressing 4D spatiotemporal modeling, the explicit primitive paradigm has developed three main evolutionary paths based on how the time dimension $t$ is handled:

---

Figure 12: Key Mechanisms for Advanced Spacetime Modeling. A taxonomy of core techniques underpinning modern models: (a) Full Spacetime Attention enables dense long-range dependencies; (b) Causal Masking ensures temporal causality; (c) 3D Gaussian Splatting offers explicit, differentiable 3D structure; (d) Object-Centric Slots decompose complex scenes into distinct entities.

(i) *Lagrangian Particle Tracking.* As in PhysGaussian [93], it assumes Gaussian primitives possess material point properties, solving the equation of motion $\mu(t) = \mu_0 + \int v(\tau)d\tau$ by introducing continuum mechanics equations ($\rho\ddot{x} = \nabla\cdot\sigma + g$). By embedding physical constraints into the optimization process, the method enables joint learning of visual appearance and physical behavior.

(ii) *Eulerian Tensor Decomposition.* As in 4D-GS [121], the 4D spatiotemporal field is modeled as a high-dimensional tensor $\mathcal{T}$, using CP or Tucker decomposition to reduce dimensionality:

$$\mathcal{T}(x, y, z, t) \approx \sum_{r=1}^{R} \mathbf{u}_r(x) \circ \mathbf{v}_r(y) \circ \mathbf{w}_r(z) \circ \mathbf{h}_r(t), \quad (10)$$

where $\circ$ denotes the outer product, $R$ is the tensor rank, and $\mathbf{u}_r, \mathbf{v}_r, \mathbf{w}_r, \mathbf{h}_r$ represent the factor vectors along each dimension. This form optimizes storage complexity from $O(N^4)$ to $O(N^2)$, effectively supporting dynamic changes in topological structure.

(iii) *Canonical Deformation.* As in Deformable-GS [95], it adopts a static base with transient offsets for- mulation, predicting coordinate offsets $\Delta\mu$ via MLP, leveraging the spectral bias of MLPs to effectively capture high-frequency motion fields.

The explicit primitive paradigm shows significant advantages in balancing high frame rate rendering and high-resolution reconstruction. However, its discrete nature introduces topological adaptability limitations, making it difficult to naturally handle fractures and fusions in fluid dynamics like implicit fields [122], indicating the need to introduce higher-order generative dynamics models.

### 2.3.6 生成性統計先驗

在開放世界生成任務中，觀測條件極其稀疏，導致問題退化為病態問題。在此階段，研究工作將影片擴散模型用作隱式世界模型先驗，建立了演算法-資料協同框架。

(1) **演算法與幾何約束。** 為了將 2D 先驗提升至 3D 一致性，學術界重構了最佳化目標和架構設計：

(i) *分數蒸餾採樣（Score Distillation Sampling, SDS）與變分校正。* 與光度損失不同，SDS [97] 通過計算預訓練擴散模型的分數函數來獲得梯度。針對 SDS 過度平滑問題，VSD（變分分數蒸餾，Variational Score Distillation）[123] 引入了變分分佈，最小化生成分佈與先驗分佈之間的 KL 散度，從而恢復高頻紋理細節。

(ii) *多視角幾何注意力。* 純 2D 先驗難以保證多頭一致性。如 MVDream [99] 等研究修改了 U-Net 架構，將空間自注意力升級為 3D 對應注意力。此設計強制模型在生成不同視角時通過相機參數 $(R, T)$ 執行特徵對齊，實現軟幾何一致性。

(2) **規模化資料基礎。** 為了突破 3D 資料瓶頸，學術界採用了合成-真實-生成混合構建策略進行大規模資料集構建：

(i) *聚合。* Objaverse-XL [101] 整合了從網際網路收集的數千萬個 3D 資產，根本上緩解了大規模 3D 資料稀缺問題。G-Objaverse [124] 通過物理渲染管道提供了高品質的 RGB-D-Normal 三元組，成為訓練大型通用重建模型（Large General Reconstruction Models, LGM）的標準資料來源。

(ii) *真實世界感知。* MVImgNet [125] 和 Co3D-v2 [126] 提供了在真實環境中捕捉的數百萬個物體中心影片序列。雖然密集的幾何真值在很大程度上不可用，但這些資料集在減少合成資料和真實資料之間的域間隙方面發揮了至關重要的作用，特別是在外觀和紋理分佈方面。

(iii) *逆向生成引擎。* See3D [104] 通過耦合生成影片模型與幾何重建推進了自動化資料生成範式。具體而言，使用 SV3D [127] 等影片擴散模型合成大規模偽 3D 影片，隨後通過 Dust3R [103] 進行幾何推理，並通過 LGM 進行快速重建，構建了閉環資料生產引擎以實現指數級資產擴展。

空間一致性建模的發展展現出清晰的迭代軌跡。早期方法依賴於 2D 代理擬合，逐漸演變為 3D 隱式表示以改進幾何相干性。隨後，3D Gaussian Splatting 等顯式表述重新引入了計算效率和渲染可擴展性。當前趨勢表明趨向於混合架構，該架構結合了顯式幾何基元與隱式擴散型先驗，利用兩種表示的互補優勢 [124, 127]。

展望未來，該領域的研究重點正從純視覺重建轉向深層物理互動建模。一方面，神經符號接地（Neuro-symbolic Grounding）將成為連接語義空間和幾何空間的關鍵。未來模型旨在建立 LLM 符號邏輯與數值參數之間的可微映射，如 Eureka [128] 等研究所示，實現對物體材料和力機制的內生理解，從而超越基於像素統計的模仿。另一方面，空間一致性的範圍正在擴展到動作一致性（Action-Consistency）。隨著世界模型向互動環境發展 [129]，強化學習（Reinforcement Learning, RL）將被引入生成迴圈，確保場景在應對動作 $\pi(a_t|s_t)$ 時遵循物理因果律。為了支持此能力，架構層預期突破級聯生成管道，轉向端到端原生 4D 流（End-to-End Native 4D Streaming），即直接使用壓縮 4D Token [130] 執行即時流推理。

---

[FIGURE:13] 時間一致性範式的演化：從潛在膨脹和離散序列建模到原生時空 DiT 和因果世界模擬器。

## 2.4 時間一致性

通過空間一致性建模（§2.3），我們已經成功構建了幾何完整的靜態世界。然而，世界模型的核心價值不在於存檔某一刻的狀態，而在於排演未來軌跡。如果空間一致性被視為世界模型的靜態幾何基礎 [163]，那麼時間一致性則構成建立其物理演化時間動力學的關鍵要素 [1]。數學上，此過程等價於在高維流形空間內求解受物理約束 $L_{phy}$ 和因果邏輯 $L_{causal}$ 雙重約束的多目標最佳化問題 [164]，如圖 14 所示。

[FIGURE:14] 時間一致性和身份保存。時間注意力機制的示意圖，確保連續幀間 ($t_0 \to t_n$) 相同的主體特徵。生成過程由兩個關鍵約束控制：*物理約束*（*$L_{phy}$*）強制運動軌跡的平滑性以防止閃爍偽影，而*因果約束*（*$L_{causal}$*）確保事件的邏輯進展（例如，物體永存性）在整個時間軸上成立。

### 2.4.1 從頻率穩定性到物理一致性

為了客觀衡量時間一致性技術的演進軌跡，評估指標必須超越傳統的感知維度。長期以來，學術界依賴 FVD (Fréchet Video Distance) [165] 來評估視頻品質，但實證研究表明 FVD 主要刻畫空間特徵分佈的相似性，在檢測時間高頻閃爍和非物理變形方面存在局限。

必須指出的是，時間一致性中的頻率穩定性不是孤立存在的；它必須建立在模態對齊的語義基礎（§2.2）和空間幾何的拓撲約束（§2.3）之上。例如，Veo 3 [152] 等前沿模型通過整合 MM-DiT（模態一致性）和 3DGS（空間一致性），有效地抑制了高頻偽影並實現了物理相容的因果推理。

為了填補這一空白，視頻一致性距離（Video Consistency Distance, VCD）[166] 被設計為基於獎勵的微調目標。如圖 15 所示，VCD 在時間頻率譜中測量生成視頻 $\hat{V}$ 與自然視頻之間的特徵差異：

$$\mathcal{L}_{\text{VCD}}(\hat{V}) = \mathbb{E}_t \left[ \|\mathcal{F}_t(\phi(\hat{\theta}_t)) - \mathcal{F}_t(\phi(\hat{\theta}_{t-1}))\|^2_{\text{High-Pass}} \right], \quad (11)$$

其中 $\phi(\cdot)$ 表示特徵提取器（例如 CLIP Image Encoder [10]），$\mathcal{F}_t$ 表示沿時間軸的短時傅立葉變換（Short-Time Fourier Transform, STFT）。該公式的物理意義在於：現實世界中的運動特徵應在頻域中具有連續性，而生成模型中的時間不一致（如紋理閃爍）將表現為高頻帶中的顯著能量波動。

[FIGURE:15]

[FIGURE_CAPTION]圖 15：視頻一致性分析：空間視角 vs. 頻率視角。FVD 等傳統分佈型指標主要評估特徵空間中的空間感知品質和運動平滑性，往往忽視高頻時間閃爍。相比之下，VCD 通過分析特徵嵌入的傅立葉譜，顯式地建模時間一致性，能夠檢測對空間統計量不可見的細微高頻噪聲和閃爍偽影。[/FIGURE_CAPTION]

**從感知到物理推理** 傳統評估側重於視覺品質，而新標準已擴展到物理因果維度。如表 3 所示，首先，針對時間抖動，生成型先驗範式（World Model Priors）顯著降低高頻潛在精細視覺因素 [32]。

---

**表 3：** 跨代模型的實證演進。資料來自 VBench (Temporal)、Physics-IQ (Physics) 和 Veo 3 技術報告 (Reasoning) 基準的綜合資料。

| 生成範式 | 代表模型 | 時間一致性 ↑ | 物理相容性 ↑ | 因果推理 ↑ | 頻率保真度 (VCD) ↓ |
|---------|---------|---------|---------|---------|---------|
| | | (VBench 標準化) | (Physics-IQ) | (任務成功率) | (獎勵懲罰) |
| Temporal Inflation | Move your diff goes here | 0.68 | 0.42 | N/A (< 10%) | 高 (> 1.2) |
| Discrete AR | Video Poetry | 0.79 | 0.55 | 低 (~ 25%) | 中等 (~ 0.9) |
| Native DiT | Hunyuan Video | 0.88 | 0.78 | 中等 (~ 45%) | 低 (~ 0.6) |
| World Model Priors | Google Veo 3 | **0.95*** | **0.86*** | **高 (> 70%)†** | **最小 (< 0.3)** |

\*註：World Model 分數是根據 [152] 中報告的相對於 Native DiT 基準的相對改進進行推外的。
†指代複雜物理交互任務（例如物體操控）的成功率，如 [167] 所示。

偽影（VCD < 0.3），通過引入頻率域獎勵微調來實現。其次，為了評估對物理規律的遵從性，Physics-IQ [161] 被用於量化模型在剛體動力學和流體模擬中的相容性。最後，因果推理已成為 Veo 3 [152] 等模型的核心評估維度。Veo 3 在零樣本物理交互任務（如預測多米諾骨牌倒塌）中展現出突現能力，任務成功率超過 70%，標誌著視頻生成技術從純視覺模擬向具有邏輯推導能力的動力系統的演進。

### 2.4.2 潛在時間膨脹

在大規模 4D 資料尚未普及的早期階段，學術界致力於降低視頻生成的訓練門檻。以 Tune-A-Video [134] 和 AnimateDiff [135] 為代表的工作建立了空間凍結、時間插入的時間膨脹範式。

**獨立性假設與 ELBO 鬆弛。** 該範式的核心策略是將預訓練的 2D 文本到圖像（T2I）模型擴展為視頻生成器，具體做法是凍結 2D U-Net 的空間卷積層，僅在層之間插入可學習的 1D 時間注意模組。從概率圖角度來看，這本質上是將視頻生成的聯合分佈 $p(x_{1:T})$ 簡化為一階馬可夫鏈。理論推導表明，這種證據下界（ELBO）的鬆弛忽略了 $p(x_t|x_{<t-1})$ 的高階依賴關係，導致長序列上的 KL 散度項顯著增加。在實際應用中（如 VideoCrafter1 [136]），這種數學鬆弛表現為明顯的語義漂移：隨著生成幀數增加（$T > 16$），初始幀的身份特徵逐漸被獨立的噪聲注入所稀釋。

**空間錨定與零樣本注入。** 為了抑制語義漂移，早期工作探索了無訓練一致性增強路徑。Text2Video-Zero [131] 和 FateZero [132] 採用了零樣本注意力注入機制，強制後續幀重複使用第一幀的鍵/值特徵矩陣。同時，受 ControlNet [133] 啟發，一些工作引入了顯式幾何條件（如深度/姿態）作為空間錨點。實驗資料顯示，雖然這些方法在靜態背景上表現良好，但當物體運動幅度超過螢幕寬度的 20% 時，強制特徵注入會導致明顯的影像塗抹偽影，揭示了膨脹範式在處理複雜動態時的局限性。

**頻率濾波與動態修正。** 除了時間漂移外，現有時間膨脹模型通常面臨頻率盲點問題。由於時間注意機制在 $(B \cdot HW)$ 維度上獨立運作，在捕捉高頻紋理變化時往往缺乏歸納偏見。傅立葉譜分析顯示，生成的視頻在高頻段（$> 15Hz$）表現出顯著的能量損失，在視覺上表現為非物理紋理閃爍。針對長距離依賴關係和高頻資訊的捕捉，頻率域學習提供了新的視角。全局濾波網絡（GFN）[139] 

---

提出使用 2D 離散傅立葉變換（2D DFT）代替自注意機制，通過在頻率域執行全局濾波操作，以 O(N log N) 的複雜度實現長距離時空交互捕捉。在此基礎上，自適應傅立葉神經算子（AFNO）[140] 進一步最佳化了通道間資訊聚合，證明了頻率域 Token 混合器可以有效克服空間盲點並精確保留高頻細節。此外，針對序列建模中的噪聲干擾，BERT4Rec [168] 和去噪 SASRec [169] 引入了不確定性量化機制，通過在反向傳播過程中對高噪聲樣本的梯度進行清零（梯度修剪），實現了無關擾動的動態抑制。在視頻生成領域，FastInit [141] 借鑒了這些去噪思想，提出了基於學習的噪聲初始化策略。該方法摒棄了傳統的獨立高斯採樣，改為訓練一個輕量級反演網絡，根據前序幀的時空特徵直接預測當前幀的最優初始噪聲，顯著增強了生成連貫性，同時抑制了潛在空間時間高頻抖動。

**_膨脹的理論邊界。_** 儘管 FastInit [141] 等方法緩解了頻率域閃爍，但時間膨脹範式始終受限於其 2D 拓撲錨定。由於核心空間卷積層被凍結，該模型本質上是在進行靜態影像的微小彈性變形，而非生成真實的時間動態。實驗研究 [170] 表明，面對大幅度視點變換（如物體旋轉 180 度）或新內容的出現時，這類模型往往產生嚴重的紋理拉伸。這種對預訓練 2D 先驗的過度依賴註定了它只能充當過渡方案。為了捕捉真實的物理世界動態，學術界轉向探索從零開始訓練的原生視頻架構，這正是推動離散自迴歸範式發展的動力。

**2.4.3 離散自迴歸建模**

為了突破長序列建模的理論瓶頸，VideoPoet [143]、CogVideo [171] 和 W.A.L.T [144] 借鑒了 LLM 的縮放規律，建立了兩階段自迴歸生成範式。通過擴展上下文視窗，該範式將視頻生成重構為離散 Token 的長距離因果預測。

**因果 3D 分詞器與資料壓縮。** 離散自迴歸範式的基石是高效的 3D VQ-VAE。與影像分詞器不同，視頻壓縮必須嚴格遵守時間因果性。MagViT-v2 [145] 創新性地引入了非對稱時間填充和因果 3D 卷積，嚴格限制卷積核的接收域只涵蓋當前幀 t 和之前的時刻，確保未來資訊在壓縮過程中不會洩露。針對低運動場景中的重建模糊問題，VToice-Plus [172] 進一步引入了物體中心表示，通過分離前景和背景碼本，顯著提高了靜態背景的紋理保真度。

**長序列中的記憶衰減。** 隨著 NVIDIA Cosmos [146] 等模型的發佈，AR 範式因其優越的資料縮放能力而重新受到關注。然而，誤差累積仍然是該範式的核心挑戰。根據序列建模理論 [54]，訓練期間的教師強制與推理期間自迴歸生成之間的分佈偏移（曝光偏差）導致微小的幀間預測誤差隨時間步 t 呈指數放大。為了抑制這種序列方差，VAPR [173] 提出了下一尺度預測機制，將自迴歸過程從像素掃描重構為粗到細的尺度遞迴，在數學上將推理步驟從線性 O(N) 降低為對數 O(log N)。此外，FramePack [148] 引入了幀上下文打包機制和雙向防漂移採樣，結合 PFP（預訓練幀保留）[174] 目標，在長時間序列下顯著改善了重建保真度。

---

**回歸連續潛在空間。** 儘管進行了持續的架構最佳化，離散化操作 $z_q = \operatorname{arg min} \|z_e - e_k\|$ 的不可微性構成了該範式固有的最佳化困難。訓練通常依賴直通估計器（STE）[175] 進行近似，但在高維視頻空間中（$D > 4096$），STE 引起的梯度方差（$\sigma^2 > 10^3$）容易觸發碼本崩潰 [49]。這種離散化間隙限制了 AR 模型在生成微小紋理和亞像素運動方面的精度。正是這一局限性驅使技術焦點轉向連續潛在空間，利用擴散 Transformer 直接在流形上對連續概率密度進行建模。

**混合過渡：融合 AR 和 Diffusion。** 在純 AR 和 DiT 之間，學術界探索了二者的融合路徑，旨在結合 AR 的長距離因果性與 Diffusion 的高保真解碼能力。首先，在推理層面，Diffusion Forcing [149] 提出了非剛性序列建模方案，將每個時間步建模為獨立的擴散過程，支持推理過程中的回滾和分支探索，打破了傳統 AR 無法返回的限制。其次，在架構層面，Show-o [176] 提出了統一全能模型範式。這種方法不是模組的簡單疊加，而是在單組權重內實現了離散 Token（用於語義理解）和連續 Token（用於視覺生成）的同構建模。通過混合遮罩機制，Show-o 在物理權重中實現了理解和生成的雙向互通。

### 2.4.4 透過 DiT 的統一時空建模

相比於時間膨脹範式造成的時空碎片化，以及離散 AR 範式帶來的量化損失，以 Sora [2] 和 HunyuanVideo [150] 為代表的新一代範式徹底回歸連續潛在空間，採用 Diffusion Transformer（DiT）架構，建立了視頻生成中時間一致性的當前基準。這條從時空解耦到完全時空同構的演進路徑如圖 16 所示。

**原生時空架構。** 原生 3D DiT 將視頻視為 3D Patches 的序列 $N = T \times H \times W$，其核心優勢在於通過全局感受野捕捉非局部物理交互作用。

**(i) 完整序列聯合注意力。** 通過引入 3D-RoPE 來計算聯合注意力 Attn = $Softmax(QK^T/\sqrt{d} + M)V$，該模型可以在完整序列上計算聯合注意力。實證研究（如 Physics-IQ [161]）表明，切斷時空連接的分解架構在數學上難以逼近 Navier-Stokes 方程中的對流項和遠程相關性。只有完全注意力機制提供的全局時空感受野才能捕捉到這樣的非局部物理交互作用。

**(ii) 流形微分同胚。** 在此基礎上，基於 Flow Matching [55] 的生成過程在數學上對應於流形上的微分同胚，使得模型能夠從高斯噪聲平滑地恢復精細紋理細節，消除由離散化引起的邊界閃爍。

**計算演進：線性化與推論加速。** 儘管 DiT 建立了影像品質基準，但 Transformers 的二次複雜度（$O(N^2)$）導致顯存使用成為從短片段向長視頻邁進的物理障礙。

**(i) 線性化與快取。** 在架構側，Video-TTT [130] 引入了測試時訓練範式，將歷史背景壓縮到神經網路權重中，在保持 $O(N)$ 線性複雜度的同時實現長視頻的記憶保留。與此相輔相成的是，Pyramid Flow [153] 利用視頻的時空冗餘性，提出金字塔流匹配機制，通過層次分解策略將高品質視頻生成的計算成本降低 5-10 倍。在推論側，TeaCache [154] 利用擴散模型中相鄰時間步特徵輸出的極高相似性（Pearson 相關係數 > 0.98），引入無訓練動態快取機制，實現 2-3 倍端對端加速，且影像品質無損失。

**產業中的匯聚與分化。** 業界並未簡單地堆砌參數，而是展現了三條不同的演進路線：

*(i) *標準化 vs. 異質性。** 由 Meta Movie Gen [177] 代表的工作建立了 DiT + Flow Matching 的標準化範式，所提出的時間因果 3D VAE 解決了長視頻中的時間切片閃爍問題。相比之下，Google DeepMind 在 Lumiere 和 Veo [151, 152] 中堅持採用 Space-Time U-Net 架構，通過完整時空注意力機制避免級聯超解析度引起的時間不一致，定義了高保真模擬品質的上界。

*(ii) *生態系統與可控性。** 應用層模型如 Runway Gen-3 [6] 和 ByteDance PixelDance [178] 專注於細粒度交互，通過多模態導演模式和軌跡級控制實現複雜指令遵循。與此同時，CogVideoX [179] 和 HunyuanVideo [150] 等開源基礎降低了微調閾值，直接促進了 HuggingFace 社群中視頻微調生態的發展。

---

圖 16：視頻生成範式的演進。技術路徑從時間膨脹（容易產生漂移）和離散 AR（量化損失）發展到當前的原生 DiT。該範式實現了完全時空同構，是世界模型的基礎。

## 2.4.5 Logical Consistency and Causal Reasoning

Although DiT-based generative models have solved visual continuity, they still face challenges when
dealing with long-range physical logic (such as causal irreversibility). To bridge this gap, academia
is shifting from a pure fitting paradigm to a cognitive reasoning paradigm, mainly manifested in
the exploration of two complementary directions: image-text interleaving reasoning in multimodal
perception models and temporal chain reasoning in generative video models.

---

**Think-with-Image in Multimodal Perception.** As the cognitive front-end of world models, LMMs
are attempting to enhance logical capabilities by introducing the visual modality as Intermediate
Reasoning Steps, rather than relying solely on text CoT. Works represented by Mini-O3 [158] and
VisCoT [156] assist logical jumps by generating or retrieving images during the inference process. RE-
CAP [180] further formalized this flow, proposing a recursive Retrieve-Generate-Verify loop, utilizing
visual information to compensate for text's deficiencies in spatial relation reasoning. UV-CoT [157]
explored image-text thought alignment under unsupervised conditions. Although these works mainly
focus on the perception and understanding side, their image-assisted thinking mechanism provides
valuable architectural insights for generative models tasked with complex spatiotemporal logic.

**Chain-of-Frame & Temporal Causality.** On the generation side, the core of temporal consistency
has ascended from visual fluency to event causality. The model must understand the sequence of
occurrence of physical events, not just pixel interpolation. Video-CoT [159] and Video Espresso [142]
introduce the Chain-of-Frame paradigm, which decomposes video generation into keyframe planning
and intermediate frame synthesis. In contrast to pixel-level autoregressive approaches, this framework
explicitly deduces future key states in the latent space, forcing the model to determine *causal nodes*
first, then generate the *visual process*. Think Sound [160] further extended this causality to the auditory
modality, constraining the physical evolution of video via audio cues. By aligning the underlying
causal graph structures across modalities, this approach enforces logical self-consistency throughout
the full spatiotemporal span, mitigating the logical degradation that commonly emerges in long videos.

2.5 Outlook of the Consistencies

Through the evolution of specialized models, three distinct computational engines have effectively
emerged. *Modal Consistency* has addressed semantic translation across modalities; *Spatial Consistency*
has progressed from coarse 2D approximations to explicit 3D primitives; and *Temporal Consistency* has
advanced from simple frame interpolation toward causal world simulation.

Yet treating these capabilities as independent optimization objectives introduces a fundamental bot-
tleneck. A collection of highly specialized modules, regardless of individual sophistication, cannot
constitute a coherent world simulator in the absence of a shared cognitive substrate. The central
challenge therefore shifts from refining isolated components to achieving architectural unification.
The future of world models lie in reaching a equilibrium in which semantic understanding, geometric
structure, and causal reasoning co-emerge within a single parameter space. This requirement motivates
the paradigm shift examined next: the emergence of the UMMs.

# 3 多重一致性的初步整合

3.1 大型多模態模型的興起

在前面章節中，*模態一致性、空間一致性和時間一致性*被視為獨立的技術維度。然而，構建一般世界模型最終並不取決於這些能力的孤立進步，而取決於它們如何一致地整合到統一的認知系統中。解決這一挑戰需要超越模組化解決方案，轉向能夠跨模態、空間和時間聯合推理的架構。大型多模態模型 (LMMs) 的興起，以 LLaVA [23] 和 GPT-4V [181] 為代表，標誌著從單任務專家向通用認知實體的決定性範式轉變。

3.1.1 LLM 作為核心認知基礎

現代 LMMs [181, 7, 184] 的核心設計理念是將預訓練的 LLM [181, 185] 視為通用推理引擎 [186, 187]。其本質在於將異質模態資料映射到 LLM 的詞嵌入空間 [16, 188]。這個過程不是簡單的維度轉換，而是通過特定的轉譯機制（例如視覺連接器或適配器）[14, 23, 189] 來實現跨模態的語義對齐和轉換 [10]。

**對齐與表示橋接。**在具體實現路徑上，模型首先利用視覺編碼器（如 CLIP-ViT [10] 或 SigLIP [190]）來提取高維特徵圖 $\mathcal{F} \in \mathbb{R}^{H \times W \times C}$。為了使 LLM 能夠處理這些非文本信號，LLaVA [23] 及其後續改進 [18, 188] 採用 MLP 或線性投影層 $W_{\phi}$ 將影像補丁特徵直接投影為視覺令牌集合 $\mathcal{V} = \{v_1, v_2, \dots, v_n\}$（其中 $v_i \in \mathbb{R}^d$），其維度與文本令牌對齊。這些令牌隨後與文本嵌入連接作為軟提示，形成混合輸入序列：

$$ \boldsymbol{X}_{\text{input}} = \left[ e_{\text{text},1}^{(1)}, \dots, e_{\text{text},1}^{(m)}, v_1, \dots, v_n \right], \quad (12) $$

其中 $\boldsymbol{X}_{\text{input}}$ 表示對齐的多模態序列，$e_{\text{text}}$ 表示文本嵌入，$\boldsymbol{v} \in \mathbb{R}^d$ 是視覺令牌。從這個角度來看，對齐的物理意義在於使 LLM 的自注意機制能夠以處理文本令牌相同的方式計算視覺令牌之間的關聯熵。

**(2) 從剛性投影到 Perceiver 瓶頸。**為了解決直接投影可能導致的序列長度冗餘問題，BLIP-2 [16] 和 Flamingo [14]—作為 Q-Former 和 Perceiver Resampler 方法的代表架構—利用固定數量的學習查詢 (Learned Queries) 作為中介，從大量像素特徵中過濾冗餘資訊。

這一機制在數學上等價於一種語義池化形式：它強制模型將數千個空間補丁壓縮為數十個具有高度抽象語義的令牌。這不僅解決了計算開銷的問題，而且在理論上滿足資訊瓶頸假說 [191]；通過限制 $I(Z; X_{vis})$ 的容量，模型被迫在對齐過程中僅保留那些有利於語言推理的特徵。此外，DeepSeek-VL [4] 和 InternVL [25] 的實驗表明，這一對齐過程可以在預對齐階段誘導 LLM 內部形成跨模態物理流形，使模型即使在未見過的場景中也能保持基本的邏輯一致性。

3.1.2 認知演化作為多模態

LMMs 的出現超越了傳統端到端映射範式 [192, 193]，賦予系統類似多模態作業系統的資源排程和邏輯協調能力。在這一架構中，LLM 不再僅作為特徵處理器，而是作為核心 (Kernel) [194, 195]，負責管理複雜的指令流並根據需要調用異質的專用模組 [196, 59, 197]。

(1) **分層任務規劃與程式化指令。**為了解決長視野任務中的語義漂移，LMMs 展現出遞迴分解能力，將高階模糊指令分解為原子子任務。不同於早期的靜態映射，VisProg [198] 和 ViperGPT [197] 提出了視覺程式化推理範式，該範式將視覺查詢解析為可執行的 Python 程式碼流，通過組合低階視覺運算元實現邏輯自洽性。這一機制的本質—將物理指令轉化為邏輯程式—是利用 LLM 的文脈學習將開域問題投影到受限運算元空間。此外，PaLM-E [199] 和 Voyager [200] 已證明，通過整合來自多模態感知的實時回饋，LMMs 可以在潛在動作空間內執行分層搜尋，在動態環境中保持長期一致性。

---

> 圖 17：工具使用與閉迴路驗證。示意圖展示 ReAct 範式，其中 AI 智能體循環調用外部檢測和修正工具來精化生成輸出。

**(2) 工具使用與閉迴路驗證。**為了在生成過程中修正物理幻覺，LMMs 已演化出基於測試時計算的閉迴路精化機制。以 Visual ChatGPT [195] 和 HuggingGPT [201] 為代表的框架利用 ReAct（推理與行動）範式 [202]，如**圖 17** 所示。這允許模型主動暫停生成路徑以調用外部專家模型（例如調用檢測器驗證空間關係或調用擴散模型重繪不合理的紋理）。Chameleon [59] 和 Auto-GPT [203] 等架構進一步引入回饋評估階段：通過計算生成的中間狀態與原始指令之間的互資訊或幾何約束偏差 $\Delta_\phi$，模型可以執行梯度引導的迭代精化。

## 3.2 模態與空間一致性的整合

模態與空間一致性的融合構成了通往物理世界模擬器的核心橋樑 [204, 205]。這種深刻的跨領域協同旨在解決傳統生成模型中豐富語義但幾何坍縮的持久問題，其核心效用在兩個維度中得以體現。在語義-空間對齐方面，它賦予模型對複雜空間指令（如遮擋、包圍和透視堆疊）進行精確回應的能力 [206]，實現了可控性從「文本描述紋理」到「語言定義佈局」的質的飛躍 [207]，如圖 18 所示。在幾何-物理基礎方面，它強制生成的內容遵守客觀世界的幾何規律，有效消除多視角條件下的結構非剛體變形和空間錯位幻覺 [208]。這種整合確保了人工智慧不再侷限於 2D 像素的統計擬合，而是具備推斷 3D 流形內時空動力學的能力 [209, 151]。

在當前研究中，模態與空間一致性的深度整合呈現四條平行的技術路徑，如圖 19 所示，探索隱式湧現、顯式協同、結構同構和強化學習的獨特範式。像素空間操控聚焦於利用大規模多模態語料庫的規模效應，將幾何變換內化為隱式語義映射，在通用生成中實現直觀指令作為空間控制 [210, 211, 212]。與此平行，視圖空間映射引入相機姿態和深度圖作為顯式幾何條件，允許語義和幾何流通過交叉注意機制在 2D 平面上共存和協同，有效平衡生成靈活性與透視精度 [208, 213, 214]。與此同時，體積空間表示採用世界坐標系作為其基礎，將語義特徵直接錨定到神經體積場或 3D 高斯基元，使空間一致性成為表示的內在物理屬性 [119, 230]。最後，強化學習通過引入 System-2 Reasoning 範式，解決組合指令中的詞袋缺陷。通過區域-時間解耦和推理端擴展機制，它將生成過程從純統計採樣提升為配備邏輯驗證的規劃解法 [209, 227, 223, 250]。這四種範式不是線性替代，而是互補和共生的；它們從資料驅動泛化、條件控制靈活性、物理模型精度和邏輯推理魯棒性的四個維度，共同擴展了世界模型中語義-空間融合的邊界。

---

[FIGURE: 圖 18：模態 + 空間一致性：語言控制精確空間關係。該模型結合模態與空間一致性，使用語言指令精確控制主體（Doge）與物體（例如，Sit ON the box、Hide BEHIND the cushion）之間的空間關係。]

### 3.2.1 Pixel Space Manipulation

The core philosophy of this paradigm lies in anchoring on data distribution, explicitly trading geometric priors for scale [97, 220]. Unlike traditional graphics that rely on expensive, hard-coded geometric priors [81, 84], pixel space manipulation advocates for constructing a joint distribution $p_{\theta}(x_{\mathit{img}}, c)$ of image-text and spatiotemporal data [221] based on pre-trained 2D generative bases (such as Latent Diffusion or Autoregressive Transformers) [222, 223].

Mathematically, this is equivalent to assuming that the massive volume of 2D projection data $\{x_{\mathit{img},i}\}_{i=1}^N$ is sufficient to cover the topological structure of the high-dimensional 3D manifold $\mathcal{M}_{\mathit{world}}$ [208]. In this context, *Modal Consistency* is manifested as the semantic alignment of conditional probability $p(x_{\mathit{img}}|c_{\mathit{sem}})$ [10, 133], while *Spatial Consistency* spontaneously emerges as an outcome of optimizing the joint distribution when reconstruction error is minimized [224, 135].

** (1) Instruction-Driven Image Editing. ** To address the common issue of geometric collapse in text-based editing (e.g. non-physical distortion of the background when instructing a dog to sit), instruction-

---

Figure 19: Evolution of Modal Consistency and Spatial Consistency.

driven image editing has established a hybrid paradigm integrating gradient decoupling & update and attention injection as gating. This paradigm aims to resolve the intrinsic contradiction between semantic reconstruction and structural preservation by constructing orthogonal control paths, achieving a heavy semantic bridge [249], light weight structural constraint architecture as illustrated in Figure 20.

*Gradient Decoupling & Update.* To effectively decouple and protect the original spatial layout $S_{\text{orig}}$ while injecting new semantics $\Delta c$, mainstream paradigms (such as ControlNet [133] or IP-Adapter [250]) employ a structured decoupling architecture. By freezing the pre-trained base (e.g., SDXL) and only fine-tuning the side-network or decoupled cross-attention, the model constructs a gradient update path on the parameter manifold that is orthogonal to the base:

$$ \nabla_{\theta\mathcal{L}} = \underbrace{\nabla_{\theta_{\text{base}}}\mathcal{L}_{\text{prior}}}_{\approx 0 \ (\text{Frozen})} + \underbrace{\nabla_{\theta_{\text{adapter}}}\mathcal{L}_{\text{edit}}}_{\text{Semantics}} \quad (13) $$

where $\theta_{\text{base}}$ represents the frozen parameters of the base model, and $\theta_{\text{adapter}}$ denotes the trainable parameters of the side-network. This ensures that the physical common sense (e.g., lighting, occlusion) internalized within the base remains undisturbed.

*Attention Injection as Gating.* During the inference phase, Prompt-to-Prompt [249] and MasaCtrl [251] reveal a strong correlation between cross-attention maps and spatial layouts. To maintain spatial consistency, the model injects the attention map $M_{\text{attn}}^{\text{src}}$ of the original image into the editing steps as a geometric hard-gating mechanism:

$$ \text{Attn}_{\text{edit}}(Q, K, V) \leftarrow \alpha \cdot \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right) + (1-\alpha) \cdot M_{\text{attn}}^{\text{src}}, \quad (14) $$

where $M_{\text{attn}}^{\text{src}}$ denotes the attention map preserved from the source image to guide spatial layout, and $\alpha$ is the injection strength coefficient. Combined with the MLLM Semantic Hub mechanism proposed by Step-1X Edit [225], this method successfully achieves semantic change with topological conservation. Subsequent work such as EditWorld [210] further introduced a post-edit closed-loop, utilizing SAM masks for second-order geometric verification to resolve pixel artifacts at object edges.

## (2) 一般影像生成

一般影像生成正經歷一場典範重建，從外部插件對齐轉向原生全雙工建模，旨在通過端到端聯合訓練直接捕捉

---

[FIGURE:20]

[FIGURE_CAPTION] 圖 20：指令驅動影像編輯。該圖表說明結構保留影像編輯，其中一隻 Doge 通過文本指令重新定位。它描繪了梯度解耦（凍結基礎）和注意力注入（幾何門控）的機制，以維持空間一致性。

物理世界的時空動態分佈。該領域的典範轉變特徵為：從外部對齐（基於 CLIP）到端到端交錯建模。這一轉變不再依賴凍結的特徵提取器，而是構建一個通過聯合建模 [226]、影片流監督 [252] 和輕量級連接 [228] 將模態和空間緊密耦合的生成基礎。

(i) 聯合建模突破信息瓶頸。傳統的兩階段模型（例如 DALL-E 2）受限於 CLIP 編碼器的模態隔離，這導致特徵壓縮期間空間關係的損失。新一代模型，例如 DreamLLM [226] 和 Emu [253]，放棄了這種設計，轉而使用自迴歸或擴散方法直接在原始影像-文本序列上執行聯合建模：

$$ \mathcal{L}_{\text{joint}} = - \sum_{t} \log p(\mathbf{x}_{\text{img},t} | \mathbf{x}_{\text{img},<t}, \mathbf{x}^{\text{txt}}) - \sum_{j} \log p(\mathbf{x}_{j}^{\text{txt}} | \mathbf{x}_{\text{img}}, \mathbf{x}_{<j}^{\text{txt}}), \quad (15) $$

其中 $L_{joint}$ 表示統一的訓練目標，$\mathbf{x}_{\text{img},t}$ 表示步驟 $t$ 處的影像 token，$\mathbf{x}^{\text{txt}}$ 對應於文本 token。這種全雙工信息流使模型能夠捕捉隱含在諸如「貓在桌子上」這樣的描述中的像素級空間約束。

(ii) 影片作為世界模擬器。超越簡單的幾何視角變換，對 Sora [2] 的實證研究揭示了影片數據的深刻價值：它提供關於物理合理性的內源監督信號。

與靜態影像不同，影片流中的時間依賴性強制模型學習物體永恆性 [252, 254]——例如，推斷被遮擋的物體尚未消失，而是沿著其軌跡繼續運動。這種自監督迫使模型在潛在空間內構建符合物理守恆定律（例如重力、碰撞、流體動力學）的動力學模型 [163]，從而將生成模型從純粹的像素統計擬合提升為物理世界演化的預測模擬 [204]。

(iii) 輕量級連接層。為了在計算效率與多模態對齐之間取得平衡，Flamingo [14] 中的 perceiver resampler 和 Mentor [228] 中的 MLP 連接層設計演示了如何使用最少參數將視覺特徵投影到 LLM 的語義流形上。這證明了只要基礎足夠強大，簡單的線性映射就可以維持複雜的空間-語義對應關係。

---

### 3.2.2 視圖空間映射

Figure 21：姿態對齊視圖合成。示意圖展示使用解耦骨幹網路和極線注意力的姿態條件生成。目標視圖展示 RGB 紋理和紫藍色 Normal Map 的分割顯示，代表幾何精度的跨域互監督。

**姿態對齊耦合訓練。** 此範式的核心理念在於摒棄純粹資料驅動的黑箱假設，並將 3D 幾何資訊作為結構化條件變量 $τ = \{P_t, \mathcal{D}\}$（其中 $P_t \in SE(3)$ 是攝像機姿態，$\mathcal{D}$ 是深度先驗）注入到預訓練的擴散模型中 [133, 208]，如 Figure 21 所示。其數學本質是構建由幾何約束的條件去噪分佈：

$$ \mathcal{L}_{\text{view}} = \mathbb{E}_{z_t, t, c, \tau, \epsilon} \left[ \|\epsilon - \epsilon_{\theta}(z_t, t, c, \tau)\|_2^2 \right] + \lambda \mathcal{R}_{\text{consist}}, \qquad (16) $$

其中 $z_t$ 表示時間步 $t$ 處的雜訊潛在表示，$\epsilon_{θ}$ 是以幾何 $τ$ 為條件的雜訊預測網路，$\mathcal{R}_{consist}$ 表示多視圖一致性的正則化項。此範式的成功實現依賴於以下三個協同機制：

(i) *骨幹網路解耦與注入。* 為了迴避災難性遺忘並保持預訓練模型的語義生成能力，學術界建立了凍結骨幹網路和旁路控制的設計方案。由 Zero-1-to-3 [208] 和 ControlNet [133] 代表，此方法通過鎖定骨幹網路 $\mathcal{F}_{locked}$ 並引入可訓練副本 $\mathcal{F}_{copy}$，實現選擇性梯度流：$\mathbf{h}_{out} = \mathcal{F}_{locked}(\mathbf{h}_{in}) + \mathcal{Z}(\mathcal{F}_{copy}(\mathbf{h}_{in}, \tau))$。此零卷積策略確保模型在生成照片級逼真紋理的同時精確執行幾何指令。

(ii) *結構化稀疏注意力。* 為了解決多視圖生成中的 janus 問題，模型引入結構化稀疏注意力。MVDream [99] 和 SyncDreamer [106] 創新性地將 3D 空間中的極線幾何約束轉換為注意力掩碼：

$$ \operatorname{Attn}(\mathbf{Q}_i, \mathbf{K}_j, V_j) \propto \exp \left( \frac{\mathbf{Q}_i \mathbf{K}_j^T}{\sqrt{d}} + \mathcal{M}_{\text{epi}}(i, j) \right), \qquad (17) $$

其中 $\mathbf{Q}_i$ 和 $\mathbf{K}_j$ 分別表示來自視圖 $i$ 和視圖 $j$ 的特徵，$\mathcal{M}_{\text{epi}}$ 表示源自極線約束的幾何偏置。此機制強制來自不同視圖的標記僅與其幾何對應的極線區域互動，從而將幾何硬約束轉換為注意力機制中的軟歸納偏置。

(iii) *跨域注意力正則化。* 為了進一步增強幾何精度，*Wonder3D* [214] 和 *MoAI* [213] 通過並行生成 RGB 和 normal maps 並引入跨域注意力注入 $f_{\text{rgb}} \leftrightarrow f_{\text{geo}}$，實現紋理語義和幾何結構之間的互監督。結合 3D 一致性雜訊初始化策略（基於攝像機投影矩陣初始化雜訊），此範式成功打破了初始狀態的獨立同分佈假設，實現從簡單影像生成到幾何可控生成的轉變。

### 3.2.3 體積空間表示法

與前兩種在 2D 平面上模擬 3D 的範式不同，體積空間表示法選擇直接面對物體的三維本質 [97, 230]。這個方向的核心哲學是利用 3D 原生表示法（NeRF、3D Gaussian Splatting）作為建築抽象的主要層級。這使得空間一致性成為表示法的內在性質，而模態一致性則轉變為跨模態查詢和可微分渲染之間的協同最佳化問題。

(1) **條件式 3D 生成：從 2D 蒸餾到視頻流形約束。** 條件式 3D 生成旨在通過將預訓練的生成模型重組為凍結的認知引擎，克服 3D 資料稀缺的瓶頸，建立一條從 2D 語義蒸餾向視頻流形約束演進的技術軌跡。由於高品質 3D 文本資料對極其稀缺（比 2D 資料少 2-3 個數量級），這個方向不再尋求從頭訓練 3D 生成器。相反，它聚焦於發現並轉移預訓練 2D 或視頻模型中固有的空間智能 [97, 104]。

(i) *來自 2D 先驗的梯度流*。DreamFusion [97] 和 RealFusion [230] 為該領域建立了基礎公式：評分蒸餾採樣（Score Distillation Sampling, SDS）。其核心原理不是優化像素誤差，而是優化參數化 3D 場 $\theta$（如 NeRF 或 3DGS），使得從任意視角渲染的影像 $x_{\text{img}} = g(\theta, \mathbf{P}_t)$ 位於 2D 擴散模型的低能量區域：

$$\nabla_{\theta} \mathcal{L}_{\text{SDS}} = \mathbb{E}_{t,\epsilon} \left[ w_{\text{guidance}} (\epsilon_{\theta}(z_t, t) - \epsilon) \frac{\partial x_{\text{img}}}{\partial \theta} \right], \qquad (18)$$

其中 $w_{\text{guidance}}$ 是加權因子，$\epsilon_{\theta}$ 是來自凍結擴散模型的預測雜訊，$\frac{\partial x_{\text{img}}}{\partial \theta}$ 表示可微分渲染器的 Jacobian 矩陣。這個公式表明由 2D 模型計算的語義殘差通過可微分渲染器 $g$ 的 Jacobian 矩陣 $\frac{\partial x_{\text{img}}}{\partial \theta}$ 反向傳播，直接雕刻 3D 幾何。

(ii) *視頻流形作為動態 3D 先驗*。為了應對 2D 先驗造成的 Janus 問題，最近的研究已轉向利用視頻擴散模型（Video Diffusion Models, VDMs）中固有的物理一致性。核心假設是時間相關性 $\cong$ 空間一致性。*See3D* [104] 和 *V3D* [255] 提議利用視頻生成模型作為多視角生成器。通過微調 VDM，時間軸 *T* 被隱式重構為攝影機軌跡 $\mathbf{P}_t$（例如，軌道視點）：

$$p(\mathbf{x}_{\text{img, novel}} | \mathbf{x}_{\text{img, ref}}) \approx p_{\text{video}}(\mathbf{x}_{\text{img, t+1}} | \mathbf{x}_{\text{img, t}}, \text{motion\_cond}), \qquad (19)$$

其中 $p_{\text{video}}$ 表示視頻模型學習的轉移概率，$\text{motion\_cond}$ 表示攝影機軌跡條件。在此範式下，*SV3D* [127] 利用視頻模型的時間注意層作為軟極線約束，強制生成具有幾何連續性的多視角序列。隨後，SDS 用於將此動態視頻先驗蒸餾為靜態 3D 資產，從根本上解決視點衝突。

---

(iii) *先驗約束的兩階段迴圈*。鑑於單視角生成的不適定性，Magic123 [256] 和 One-2-3-45 [257] 建立了粗生成 → 精細優化的範式。目前的趨勢涉及使用視頻模型 [231] 快速生成多視角作為初始猜測，隨後使用 SDS 結合輕量級求解器 [258] 進行幾何細化。這種視頻初始化和物理微調的策略保留了語義豐富性，同時利用視頻先驗來修正 3D 結構的拓撲合理性。

圖 22：多模態對齐。展示文字、影像和 3D 點雲匯聚到中央統一嵌入的圖示。其中描繪了輸入的對比對齐和 3D 資料整合的生成性標記化。

(2) **多模態對齐。** 多模態對齐旨在構建跨越幾何和語義的通用表示法。通過建立判別式度量對齐和生成式互動融合的雙軌機制，它打破了 3D 資料長期存在的表示法孤島困境。為了像 CLIP 處理影像一樣處理 3D 資料，這個方向聚焦於構建統一嵌入空間 $Z_{uni}$。技術哲學是在網路前向傳播期間將空間一致性轉變為結構化約束，如圖 22 所示。

*對比度量學習*。ULIP-2 [235] 和 OpenShape [215] 採用大規模三元組對比學習。通過挖掘困難反例並利用 InfoNCE 損失函數，3D 編碼器（PointNet++ 或 Transformer）的特徵分佈被迫與 CLIP 的文字/影像空間對齐：

$$\mathcal{L}_{\text{align}} = -\log \frac{\exp (z_{3D} \cdot z_{\text{txt}} / \tau)}{\sum_{j} \exp (z_{3D} \cdot z_{\text{txt}}^{j} / \tau)}, \quad (20)$$

其中 $z_{3D}$ 和 $z_{\text{txt}}$ 分別表示 3D 形狀和文字的特徵嵌入，$\tau$ 是溫度參數。Genesis [237] 進一步將其擴展到 4D 時空對齐

---

通過在體素空間引入跨視角注意機制來融合視頻和 LiDAR 模態，實現跨時空維度的對齐。

**生成式整合。** 不同於對比學習的整體對齐，ShapeLLM-Omni [236] 和 ViewSetDiffusion [238] 引入 3D VQ-VAE 以將連續幾何離散化為符號序列。這使得 LLM 能夠直接讀取和生成 3D 幾何，實現模態之間的生成式互動而非簡單的檢索匹配。

**(3) 3D 理解與編輯：語義提升。** 3D 理解與編輯旨在賦予 3D 幾何實體語義感知和語言操縱的雙重能力。核心範式涉及通過語義提升將 2D 視覺基礎模型的認知先驗注入 3D 空間，構建一個映射 $F(x,y,z) \mapsto \mathbb{R}^{D_{clip}}$。

**語義場構造。** LERF [239] 和 Lang3D-XL [216] 提議在 NeRF 的顏色頭部旁邊並行訓練語義頭部。此模組通過多尺度監督學習 CLIP 特徵場，使空間中的每個坐標點 $p(x, y, z)$ 都能回應自然語言查詢（例如，找到椅子上的裂紋）。SKED [244] 和 CoRe-3D [241] 引入分層語義場，在表示法中嵌入實例-零件-材質階層以解決細粒度語義定位問題。

**語言驅動的拓撲編輯。** 針對編輯任務，CLIP-NeRF [242] 利用解耦潛在映射實現接近瞬間的形狀和外觀修改。InstructNeRF2NeRF [259] 採用迭代資料集更新策略：它首先使用 InstructPix2Pix 修改渲染視角影像，然後將修改後的影像用作虛擬 GT 來反向更新 NeRF。Lift3D [243] 和 ICE-G [245] 引入標準空間約束以確保拓撲結構即使在顯著的幾何變形（如指示貓站立）期間也不會崩塌。

### 3.2.4 強化學習用於模態-空間對齊

儘管 ControlNet [133] 等架構提供了明確的幾何條件，但多模態大型模型在處理組合指令（例如屬性綁定：紅色貓在藍色車上）時仍然頻繁出現嚴重的模態-空間不對齐 [206]。為了解決這種詞袋模型的不足，學術界正在經歷從黑盒優化向 System-2 Reasoning [207] 的範式轉變。

此範式演進可總結為三個階段：

**判別器導向的顯式錨定。** 早期的工作重點是利用現成的視覺判別器作為外部獎勵函數 $R$，以強制建立文本提示與邊界框之間的對應關係。

**黑盒離散優化。** DDPO [217] 將擴散去噪建模為馬爾可夫決策過程 (MDP)。對於空間指令，它引入了開詞表檢測器（如 GroundingDINO [260]）來計算 IoU 獎勵。這代表了一種鬆散耦合的融合；雖然它增強了物體召回率，但獎勵信號的稀疏性使得解決複雜的屬性綁定變得困難。

**白盒梯度反向傳播。** AlignProp [246] 提出將判別器微調為可微分的獎勵模型。這建立了端到端梯度路徑 $\nabla_{pixels} L_{align}$，允許空間誤差直接反向傳播到去噪網路，從而在細化中達到像素級精度。

**區域-時間解耦。** 為了防止全局獎勵將語義與空間信息混淆，後續工作轉向了細粒度控制。

---

R-DPO [247] 提出了空間掩碼下的子流形優化。與傳統 DPO 不同，它將圖像 **x**<sub>img</sub> 和文本 **c** 分解為多個局部對 (*x*<sup>k</sup><sub>crop</sub>, *c*<sup>k</sup><sub>sub</sub>)，確保特定的模態屬性（例如，紅色）只反向傳播到特定的空間區域（例如，貓的坐標內）：

$$ \begin{aligned} \mathcal{L}_{\text{R-DPO}} &= -\sum_k \mathbb{E}_{(\mathbf{x}_w, \mathbf{x}_l) \sim \mathcal{B}_k} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(\mathbf{x}_{\text{img, w}}^k | \mathbf{c}_k)}{\pi_{\text{ref}}(\mathbf{x}_{\text{img, w}}^k | \mathbf{c}_k)} - \beta \log \frac{\pi_{\theta}(\mathbf{x}_{\text{img, l}}^k | \mathbf{c}_k)}{\pi_{\text{ref}}(\mathbf{x}_{\text{img, l}}^k | \mathbf{c}_k)} \right) \right], \end{aligned} \quad (21) $$

其中 B_k 代表區域 k 的局部偏好數據集，x_w 和 x_l 分別表示獲勝和失敗的圖像裁剪，σ 是 sigmoid 函數。

同時，SPO [39] 通過採用時分多工策略來利用擴散模型的頻率特性：在去噪的早期階段 (t ∈ [T, T/2]) 專注於空間 IoU 優化，並在後期階段 (t ∈ [T/2, 0]) 切換到語義優化，以避免梯度衝突。此外，DRaFT [261] 利用視覺語言模型生成關於空間誤差的自然語言評論，並將其映射到密集獎勵映射 $\mathcal{R} \in \mathbb{R}^{H \times W}$。這標誌著 RL 對齐從離散框到連續像素域的轉變，使生成模型能夠理解極其微妙的空間-模態指令，例如左腿變形了。

**TTT 與視覺 CoT。** 在 DeepSeek-R1 和 OpenAI o1 證明了推理端縮放效能之後，最近的研究開始將 RL 引入生成的推理時間階段，實現了模態和空間之間的強邏輯融合。

*測試時間偏好優化 (TTPO)。* 為了解決預訓練模型在特定分佈下感知質量不足的問題，TTPO [218] 提出了即時優化機制。這種方法通過使用輕量級獎勵模型（如圖像質量分數）在推理階段迭代更新潛在變數 *z*，避免了繁重的重新訓練。雖然這項工作主要驗證了其在圖像修復任務中的有效性，但這種測試時間微調範式為解決高度反直覺的生成任務提供了通用的計算換質量路徑。

*視覺思維鏈 (Visual CoT)*。 為了解決單步生成固有的邏輯中斷，Layout-CoT [219] 借鑑了大型語言模型的推理範式。這種方法將生成過程分解為一個明確的鏈：*規劃* → *對齐* → *生成*。該模型首先在低維空間中生成離散佈局計劃，並採用 RL 對該計劃執行邏輯驗證。只有通過驗證的思維鏈才會被解碼成像素。此機制本質上將 System-2 邏輯驗證移至前端，從根本上消除了相互穿透或空間不對齐等幻覺。

3.3 模態與時間一致性的整合

模態與時間一致性的深層整合標誌著生成式人工智慧從如圖 23 所示的靜態圖像的凍結時刻正式向動態世界的連續推導轉變 [2]。此維度的核心效用在於構建時空因果關係的概率模擬：在語義層面，它確保視訊內容嚴格遵循文本或圖像指令的定義（例如，綻放、奔跑），從而消除跨模態語義漂移 [262, 151]；在動力學層面，它賦予模型對物體永存性和物理守恆定律的內生理解，確保生成的幀序列不再是離散像素的隨機堆疊，而是與邏輯演進相一致的連續流形 [221, 263]。這種融合從根本上解決了傳統視訊生成的長期問題，如運動閃爍、時間邏輯混亂和長視訊崩潰。

基於此目標，目前的探索路徑呈現出四種漸進式的技術範式，如圖 24 所示：端到端可擴展建模遵循「蠻力使用數據」的數據理念，依賴擴散模型和自迴歸架構來驗證縮放定律，旨在直接從大規模數據中學習通用物理模擬器 [209, 264, 265]；顯式結構化控制針對工業應用的可控性需求，通過引入運動向量、軌跡熱圖和正交解耦機制，將人類意圖顯式注入生成過程，解決端到端模型的歧義問題 [266, 178, 267]；同時，統一理解與生成共生架構試圖通過共享表示和雙向適應打破感知與生成之間的屏障，構建感知-行動的封閉迴路通用代理 [268, 269]；最後，強化學習驅動的對齐通過構建多維獎勵流形來解決監督微調 (SFT) 在優化模態語義和「時間動力學」中的非凸性。通過整合 DPO 和自精煉機制，此範式實現了對齐目標的聯合優化，驅動模型超越二元博弈並收斂到時空權衡的帕累托前沿 [270, 271]。這四種範式共同從通用基礎、可控介面、認知頂層和價值優化的維度構建了一個完整的模態與時間智慧架構。

圖 23：模態 + 時間一致性：語言控制時間演進。該模型整合模態與時間一致性，使用語言指令控制時間演進過程（例如，狗後面的櫻花樹在 T1-冬季、T2-春季、T3-晚春時綻放並散落），確保時間上的連貫變化。

**3.3.1 端到端可擴展建模**

端到端可擴展建模代表了視訊生成領域從「分而治之」向「統一場」範式轉變的一步。其核心目標是驗證縮放定律在高維時空流形上的有效性——具體來說，通過協同擴展模型和數據規模，直接從多模態輸入擬合至視訊輸出的聯合分佈 *p(x<sub>img</sub>|c)*。與早期依賴大量手工製作的內插和超分辨率模塊的級聯管道不同，此範式致力於構建通用物理模擬器，推動模型從 Sora [2] 到 Wan 2.1 [273] 的工業化。

**(1) 擴散模型。** 作為端到端視訊生成的核心引擎，擴散模型通過在潛在空間 $\mathcal{Z}$ 內驗證縮放定律，完全重構了從「圖像動畫」到「原生世界模擬」的技術路徑。為了支持這一物理一致性的複雜目標，現代架構的演進不再侷限於簡單的去噪迭代；相反，它已系統地重塑了四個核心支柱：從生成理論的 ODE 統一 [31]、壓縮表示的因果解耦 [179] 和注意力建模的原生三維化 [135]，到生成策略的漸進級聯 [280]。這些共同形成了時空智慧的基礎。

(i) 通過流匹配的理論統一。儘管早期工作遵循基於 SDE 的 DDPM 範式，但最先進的 (SOTA) 模型如 Sora [2] 和 Wan 2.1 [273] 普遍轉向了**流匹配 (FM)** 框架，以增強採樣效率和時間一致性。流匹配不是預測高斯噪聲 $\epsilon$，而是將生成過程形式化為在噪聲分佈 $\pi_0$ 和數據分佈 $\pi_1$ 之間構建決定性常微分方程 (ODE) 軌跡。核心優化目標轉化為在最優傳輸路徑上回歸速度場 $v_t$：

$$ \mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t,z_0,z_1} [|| v_{\theta}(t,(1-t)z_0 + tz_1) - (z_1 - z_0) ||^2], \qquad (22) $$

其中 $v_\theta$ 表示網路參數 $\theta$ 預測的速度場，$z_0, z_1$ 分別代表來自先驗噪聲和數據分佈的樣本。如 Rectified Flow [31] 所證明的，此範式強制潛在變數 $z$ 沿著線性軌跡演進，顯著降低了傳輸曲率。這使得模型能夠在很少的步驟內生成具有物理守恆的動態紋理，解決了 DDPM 在長期採樣期間固有的結構崩潰問題。

(ii) 因果時空壓縮。為了規避高維視訊數據的計算瓶頸，架構設計的主要挑戰在於構建滿足因果性的緊湊潛在空間 $\mathcal{Z}$。MagViT-v2 [315] 和 CogVideoX [179] 發現了傳統 3D 卷積中「未來信息洩露」的風險。因此，現代編碼器通常引入因果 3D VAE [263, 151]，利用非對稱時間填充和因果卷積核來確保潛在代碼 $z_t$ 的生成僅取決於歷史幀 $x_{img, \le t}$。此設計不僅在數學上保證了時間邏輯的單向性，還為串流推理提供了架構基礎。此外，通過採用異質下採樣策略（例如 $t \times 4, h \times 8, w \times 8$），模型實現了高頻運動信息和低頻語義特徵的解耦壓縮 [223, 316]。

**(iii) 原生 3D 注意力建模。** 關於潛在空間中的動力學建模，學術界經歷了對歸納偏置的深刻糾正。AnimateDiff [135] 等早期工作利用「空間-時間因式分解」注意力，降低了計算成本但切斷了時空耦合，使複雜流體動力學的模擬變得困難。HunyuanVideo [278] 和 OpenSora [317] 隨後建立了**原生 3D DiT** 的主導地位，使用 3D-RoPE 計算整個時空序列上的聯合自注意力。儘管這引入了 $O((THW)^2)$ 的二次複雜性，但序列並行技術（如 Ring Attention [318]）的整合使模型能夠捕捉遠距離時空依賴性，從而允許與物理定律相一致的連貫運動出現。

**(iv) 漸進對齐與級聯。** 為了解決長視訊生成中的誤差累積，模型採用「粗到細」條件控制策略。Tar [280] 提出的 Visual Dialect 通過將文本映射到視覺兼容代幣實現了語義的原生對齐。對於生成長範圍視訊 (> 10s)，Kling [279] 和 Vidu [274] 利用時空級聯策略。該模型首先以低幀率生成語義骨架，隨後作為時間超分辨率模型的條件 $c_{ctx}$ 使用。此級聯架構本質上將高維聯合分佈 $p(x_{img})$ [319] 分解為多個條件概率的乘積，有效緩解了單個模型在長序列生成期間的顯存壓力和邏輯漂移 [320]。

**(2) 自迴歸模型 (AR)。** 自迴歸模型借鑑了大型語言模型的縮放定律 [321, 322]，其核心理念是「萬物皆代幣」。此範式拋棄了擴散模型的去噪先驗，並將視訊生成重新表述為離散潛在空間內的因果序列預測問題 [222, 323]。其數學本質是聯合概率分佈的對數似然最大化，強制模型通過單向鏈規則學習物理世界的時間因果關係。為了支持這一統一序列建模的願景，此範式的技術演進正在四個關鍵維度展開：離散編碼的保真度、多模態互動的拓撲、混合生成的時間魯棒性和多任務推理的泛化邊界。

**(*i*) 離散化瓶頸與因果 3D 碼本。** AR 模型的上界取決於分詞器的壓縮質量。早期的 VQGAN 遭遇嚴重的碼本崩潰和高頻閃爍。VideoPoet [143] 和 MagViT-v2 [315] 通過引入無查找量化 (LFQ) 和因果 3D 卷積實現了突破。前者通過直接投影減少量化方差，後者通過非對稱填充確保壓縮過程不違反物理因果性。VILA-U [283] 進一步提出了統一視覺塔，強制在預訓練階段對齐視覺代幣和文本嵌入，從根本上解決了離散空間中異質模態之間的語義鴻溝。

**(ii) 全模態互動拓撲。** 在序列建模階段，架構設計的核心在於處理多模態代幣的互動粒度。*序列連接：UniForm* [282] 採取激進的早期融合策略，將視訊、音頻和文本代幣連接成單個長序列。雖然使用共享權重 Transformer 捕捉跨模態依賴性最大化了模態間的知識轉移，但它面臨注意力計算中的 $O(N^2)$ 爆炸。*雙流門控調制：* 為了降低計算開銷，RFLAV [281] 和 Ovi [264] 採用晚期融合。Ovi 設計了對稱雙骨幹架構，通過 RoPE 頻率縮放對齐不同模態的採樣率；RFLAV 在 Transformer 的 AdaLN 層引入了時間平均調制，實現了音頻-視訊特徵的軟對齐，而不會顯著增加參數計數。

**(iii) 長期動力學與混合範式。** 純粹的離散 AR 模型在生成長視訊時經常因誤差累積而面臨崩潰。為了糾正此缺陷，研究人員開始探索「離散規劃 + 連續修正」的混合路徑。非量化 AR：NoVA [284] 挑戰了數據「必須被離散化」的假設，提出在連續空間中進行連續 AR 預測。它將視訊分解為「逐幀時間步驟」和「集合空間步驟」，通過擴散解碼器預測連續特徵，規避量化資訊損失。滾動流匹配：RFLAV [281] 創新性地引入了滑動窗口機制。在 AR 預測粗代幣後，流匹配用於局部細化。通過「移除第一幀並添加加噪最後一幀」的滾動策略，它在理論上實現了無限持續時間的物理一致生成，解決了 AR 模型「邏輯上正確但細節不足」的固有頑疾。

**(iv) 統一多任務推理。** AR 架構的最終優勢在於其**零樣本泛化**。如 VideoPoet 所示，通過引入特定任務代幣（例如「optical-flow」、「depth」），單個模型可以在不進行微調的情況下執行視訊生成、風格轉移，甚至音頻-視覺問答任務。這種「無所不在」的特性證明了自迴歸範式在構建通用世界模擬器中的獨特潛力。

圖 25：自迴歸-擴散模型。它描繪了 AR 規劃器構建因果時間骨架（藍色線框），隨後通過擴散精化器（溫暖的霧氣）進行細節注入，生成高質量的長持續時間視訊輸出。

(3) 自迴歸-擴散混合模型。自迴歸-擴散混合模型的核心協同機制，如圖 25 本質上所示，是將 AR 的時間因果約束注入到擴散模型的迭代去噪流形中。此混合生成的通用數學表示不再是簡單的概率疊加，而是構建「因果邏輯與高質量生成」的聯合概率密度：

$$ p_{\theta}(\mathbf{x}_{\text{img},1:T}, \mathbf{z}_{1:T} | \mathbf{c}) = \prod_{t=1}^{T} \underbrace{p_{\text{AR}}(\mathbf{z}_t \mid \mathbf{z}_{<t}, \mathbf{x}_{\text{img},<t}, \mathbf{c})}_{\text{因果時間動力學}} \cdot \underbrace{p_{\text{Diff}}(\mathbf{x}_{\text{img},t} \mid \mathbf{z}_t, \mathbf{x}_{\text{img},<t}, \mathbf{c})}_{\text{條件去噪渲染}} \quad (23) $$

其中 $x_{\text{img},1:T}$ 表示生成的多模態序列（視訊/音頻），$z_t$ 代表雜訊潛在或中間特徵，$p_{\text{AR}}$ 和 $p_{\text{Diff}}$ 分別對應低維因果建模和高保真條件去噪分佈。此機制旨在結合 AR 的長距離規劃優勢和擴散的細節生成能力，克服單一模型的固有缺陷。基於此聯合建模方法，此範式的技術演進沿著兩條正交路徑展開：時間融合優化和跨範式模態協同，旨在同時解決長序列生成中的動力學一致性瓶頸和異質模態對齐的挑戰。

(i) *時間融合優化。* 核心在於平衡嚴格的因果依賴與生成靈活性，通過差異化架構設計突破長視訊生成的效率瓶頸。*即時串流動力學。* 為了解決生成速度和顯存限制，多項工作已重構了推理範式。AR-Diffusion [289] 提出了訓練-推理統一的擴散腐蝕機制，通過強制非遞減幀時間步長約束 ($t_1 \le t_2 \le \dots \le t_F$) 建立時間基線，結合動態調度器，實現了無誤差的可變長度生成。CausVid [290] 通過分佈匹配蒸餾將雙向擴散轉換為 AR 架構；結合 KV 快取和滑動窗口機制，它平衡了串流生成的即時性能與無限長度擴展能力。此外，NFD [285] 利用分塊因果注意力和推測採樣，首次在 300M+ 參數規模實現了 30+ FPS 的即時生成。RFLAV [281] 創新性地引入了滾動流匹配和輕量級時間調制模塊，實現了無限長度音頻-視訊的精確對齐生成，同時顯著降低了計算開銷。*長期一致性指導。* 為了解決長序列中的邏輯漂移，協同指導策略已成為關鍵。ARLON [288] 採用「粗粒度錨定-細粒度細化」策略，使用 AR 模型生成包含長距離語義的粗特徵，以指導擴散 Transformer (DiT) 進行細節細化，同時利用 VQ-VAE 統一表示空間來抵抗噪聲干擾。ACDC [286] 提出了零樣本協同框架，在不修改架構的情況下，允許 AR 模型充當全局上下文「規劃器」，擴散模型充當局部「修正器」，利用大型語言模型的外部記憶模塊，有效緩解長序列預測中的誤差累積。

(ii) *跨範式模態協同。* 這聚焦於模態對齐的精度和整合架構的緊密性，旨在異質信號的深度耦合。*擴散增強表示。* DiCoDe [287] 用擴散級聯分詞方案挑戰傳統的離散化方法。它首先將視訊編碼為連續潛在特徵，然後使用擴散過程將其壓縮為高保真離散代幣。此方法實現了千倍壓縮，同時保留了視覺細節，並通過交叉注意機制加強了文本-視訊語義對齐，為長視訊生成提供了高質量的「詞彙」。*端到端架構融合。* HybridVLA [265] 展示了範式融合在具身人工智慧領域的潛力。它在單個大型語言模型框架內無縫整合擴散生成和 AR 預測，將擴散生成的連續動作向量投影到大型語言模型的詞嵌入空間。通過引入特殊代幣以分離兩種範式，並基於 AR 置信度自適應融合預測結果，該模型實現了跨視覺、語言和動作模態的端到端邏輯迴路，顯著加強了代理的「感知-推理-執行」鏈的連貫性。

### 3.3.2 顯式結構化控制

儘管端到端模型在影像品質上取得了突破性進展，但其「文本即全部」的互動模式在工業應用中表現出顯著的歧義性。顯式結構化控制旨在解決**可控性**的挑戰。其核心概念涉及將高維動力學流形 $M_{dyn}$ 投影到低維可解釋的控制流形（例如深度、光學流、骨骼）。該範式將影片生成重新表述為受約束的最佳化問題：

$$ \max_{\theta} \mathbb{E}_{x_{\text{img}},c_{\text{struct}},c_{\text{mot}}} \left[ \log p_{\theta}(x_{\text{img}} | \mathcal{E}_{\text{txt}}(c_{\text{txt}}), \mathcal{E}_{\text{str}}(c_{\text{struct}}), \mathcal{E}_{\text{mot}}(c_{\text{mot}我院}) ) \right], \quad (24) $$

其中 $\mathcal{E}_{\text{str}}$ 和 $\mathcal{E}_{\text{mot}}$ 分別表示處理空間結構和時間運動顯式條件的編碼器。

**(1) 運動-幾何顯式編碼。** 這一思想流派主要繼承並延伸了 2D ControlNet 的原理，旨在通過注入顯式物理先驗來消除生成的幾何幻覺。面對遠超靜態影像的時空自由度，該範式致力於構建一組「硬約束」物理介面。它

---

在兩個關鍵維度上取得了突破性進展——基於殘差的時空特徵注入和多模態敘事結構編排。

(i) **基於殘差的特徵注入。** 核心挑戰在於注入強幾何約束而不損害預訓練生成先驗。*時空 ControlNet 適配*。Video-Composer [262] 提出了一種使用運動向量 (MVsw) 的顯式編碼策略，利用壓縮域中的 MV 信號作為時間條件的低秩近似。這解決了複雜運動場景中的控制困難，例如攝影機平移和物體變形的耦合。ControlVideo [293] 通過在自注意力層中引入跨幀幾何遮罩來探索訓練無關的路徑，強制多個幀共享相同的 ControlNet 特徵，從而實現結構的時間一致性。*軌跡感知潛在導航*。為了對物體運動路徑進行細粒度控制，DragNUWA [324] 和 MotionCtrl [325] 引入了軌跡熱圖和攝影機姿態 $P_t$ 的聯合編碼。與簡單的光學流 $O_{flow}$ 注入不同，它們通過流 $\mathcal{F}: z_t \rightarrow z_{t+1}$ 將使用者繪製的 2D 軌跡顯式映射到 3D 潛在空間中的流形演化方向。

(ii) **多模態故事板。** 為了處理長範圍敘事，VAST [266] 引入了故事板機制，將文本描述解耦為「版面 + 姿態」的雙流約束。其創新之處在於構建了一個雙向自編碼器，將離散控制信號映射到連續序列潛在向量，為跨幀生成提供了剛性骨架，並有效抑制了長序列中的物體身份漂移。

### (2) 起始-終止幀錨定與插值。
此範例將視頻生成從外推轉變為數學上更穩定的插值問題，特別是求解具有 $x_{img,start}$ 和 $x_{img,end}$ 作為邊界條件的布朗橋（Brownian Bridge）。在此數學框架下，技術演進聚焦於構建光滑、高保真的時空轉移流形，並探索受約束空間內的多模態互動控制，形成兩個核心支柱：邊界條件驅動的路徑規劃和動態指令注入。

(i) **邊界條件路徑規劃。** *時間生成修補。* SEINE [326] 和 MorphStudio [327] 將兩張輸入圖像視為遮罩 $\boldsymbol{m} \in \{0,1\}^{T \times H \times W}$，在擴散過程中僅對中間幀進行去噪。為確保轉移平滑性，它們引入了插值注意力機制，允許中間幀的查詢向量同時查詢起始幀和終止幀的鍵/值，從而在特徵空間中實現物理狀態的平滑混合。*級聯超解析度架構。* 為應對插值導致的模糊問題，Show-1 [297] 提出了粗到細（coarse-to-fine）錨定的級聯策略。第 1 階段利用像素級模型生成低頻運動骨架，而第 2 階段採用潛在擴散進行高頻紋理修補。此設計巧妙地利用了像素空間的結構敏感性和潛在空間的紋理生成能力。

(ii) **動態指令注入。** 針對複雜互動生成，InteractiveVideo [298] 將控制信號精煉為四元組 (Image, Content, Action, Trajectory)，並透過門控交叉注意力在特定時間步注入。KeyVID [299] 專注於音頻驅動場景，利用 ImageBind 提取音頻峰值作為隱式關鍵幀，實現「音視頻同步」的自動化錨定。

### (3) 多條件解耦架構。
為了解決多模態信號之間的特徵糾纏問題（例如改變人物動作導致背景紋理變化），近期的架構傾向於採用圖 26 所示的**正交解耦**設計。在此框架內，技術演進沿著兩個關鍵軸線進行：外觀-運動特徵的分離和時空維度的互補交互，旨在同時解決高動態下的身份漂移問題和長序列生成中空間結構與時間流形之間的不平衡。

(i) *外觀-運動雙流。* 這是當前數位人類動畫的主流範例 [328, 329]。面對維持身份（外觀）與驅動

---

圖 26：多條件解耦架構。說明用於數位人類動畫的雙流架構的圖表。它描繪了參考外觀（靜態肖像）和運動骨架（線條人物）的正交解耦，這些通過空間注意力「拉鍊」融合以生成時空一致的影片迴圈。

複雜動作（運動）之間的內在衝突，此範例主張放棄單流處理，轉而在架構層級採用雙流解耦機制。這涉及分別提取靜態紋理特徵和動態姿態特徵，然後通過特定模組以正交方式融合它們。這包括：*明確的空間解耦*。為了針對單流網路隨著時間推移而失去外觀特徵的問題，Animate Anyone [330] 和 MagicConcat [331] 引入了一個獨立的 ReferenceNet 作為「外觀流」。此分支不參與去噪過程，而是特別從參考影像中提取高保真特徵，然後通過空間注意力逐層注入到負責動作生成的主 UNet（運動流）中。正式地，這實現了生成特徵 $z_{gen}$ 的明確分解

$$
z_{\text{gen}} = \underbrace{\mathcal{F}_{\text{motion}}(z_t, \mathbf{c}_{\text{pose}})}_{\text{運動流}} + \lambda \cdot \underbrace{\mathcal{F}_{\text{app}}(I_{\text{ref}})}_{\text{外觀流}}, \quad (25)
$$

其中 $z_{gen}$ 表示合成特徵圖，$\mathbf{c}_{pose}$ 代表姿態控制信號，$I_{ref}$ 是由外觀編碼器 $\mathcal{F}_{app}$ 處理的參考源影像，$\lambda$ 是融合係數。這種雙塔設計強制紋理編碼和運動推理的分離，確保大規模動態運動期間細節的一致性 [332, 333]。*隱式注意力解糾纏*。超越物理雙網路結構，Moonshot [302] 和 CCEdit [303] 在單一網路內探索「邏輯雙流」。它們認為傳統交叉注意力傾向於混淆結構信號（姿態/形狀）與內容信號（紋理/身份）。因此，這些工作提出了一種解耦的注意力機制，將鍵/值映射分割為獨立的結構分支和外觀分支。通過正交梯度反向傳播，模型被強制確保運動流中的變化不會干擾外觀流的特徵分佈。此機制在微觀層面實現了外觀和運動之間的零干擾，解決了由動作變化導致的身份漂移這一頑症 [334]。

(ii) 時空互補迴圈。TATS [304] 和 Swap Attention [301] 探索時空維度的解耦。Swap Attention 利用 3D 窗口內的角色交換機制來構造一個「空間指導時間，時間反饋給空間」的迴圈。此設計在數學上強制模型沿著空間軸維持紋理一致性和沿著時間軸維持光流 $\mathcal{O}_{flow}$ 的相干性，有效地解決了自迴歸生成中常見的「無限迴圈」或「運動凍結」現象。

### 3.3.3 Unified Comprehension and Generation Symbiosis Architecture

Traditional computer vision research treats “Understanding (Discriminative)” and “Generation (Generative)” as opposing binary tasks: the former models the conditional probability $p(y|x_{img})$, while the latter models $p(x_{img}|y)$ [223, 335]. However, the Unified Comprehension and Generation Symbiosis Architecture seeks to construct a unified probability model $p(x_{img}, y)$, aiming to dismantle the barriers between perception and simulation [59, 307, 336]. The core assumption of this paradigm, as illustrated in Figure 27, is that: A perfect generator should implicitly contain a perfect discriminator.

Figure 27: Unified Comprehension and Generation Symbiosis Architecture. It features a central LLM converting multimodal inputs into a unified cloud of discrete tokens, enabling seamless "Any-to-Any" transformation (e.g., video to text and vice versa).

(1) **Shared Representation Bidirectional Synergy.** This direction aims to map heterogeneous signals into the same manifold space by constructing an Omni-modal Isomorphic Representation, thereby achieving “Any-to-Any” conversion within a single set of model parameters. Specifically, to break the chasm between perception and generation, this paradigm establishes the dominance of discrete tokens as universal interaction primitives and explores the unique value of geometric representations as physical anchors in embodied scenarios. This has resulted in two primary technical tracks based on symbolic unification and geometric symbiosis.

(i) *Token-based World Modeling.* Inspired by the success of LLMs, discretized tokens have become the "general currency" for unifying understanding and generation. *Fully Discretized Autoregression.* Gaia-1 [205] and Phenaki [268] proposed video encoding schemes based on C-ViViT, which unify the encoding of driving videos, control signals, and text descriptions into a discrete token sequence $z_{1:L}$. The training objective of the model is unified into standard Next-Augmentation Prediction:

$$ \mathcal{L}_{\text{uni}} = - \sum_{i=1}^{L} \log p_{\theta}(z_i | z_{<i}, \text{TaskToken}), \tag{26} $$

where $z_{1:L}$ represents the unified discrete token sequence combining visual and textual information,

---

and TaskToken serves as the prompt indicator to switch between understanding and generation modes.
This paradigm allows the model to switch functions via simple “Task Prompting”: inputting video
tokens to predict text tokens constitutes “understanding,” while the reverse constitutes “generation.“
*Unified Discrete Diffusion*. Unified Discrete Diffusion [307] and Show-O [176] challenge the notion that
“autoregression is the only solution.” They designed a Unified Transition Matrix that allows image
and text tokens to undergo bidirectional denoising within the same diffusion process. Show-O further
utilizes a Hybrid Attention Mechanism that applies a Causal Mask to the text portion and a Full Mask
to the visual portion, achieving the seamless coexistence of understanding and generation within
single Transformer weights.

(ii) *Domain-Specific Geometric Symbiosis*. In the field of Embodied AI, HERMES [310] proposed using BEV (Bird's-Eye-View) features as a shared hub. It utilizes a World Queries mechanism to compress 2D images from multi-view cameras into 3D BEV features. This not only supports downstream path planning (understanding) but also enables the generation of future prediction videos (generation) via a decoder for BEV features, proving that 3D geometric constraints serve as a strong bridge connecting perception and simulation.

(2) **Pre-training Driven Synergistic Adaptation.** Unlike training a unified multimodal model from scratch [337, 59], this paradigm advocates a “Shoulders of Giants” strategy: using a frozen MLLM (such as GPT-4V or LLaVA) as the cognitive hub (Brain), connected via lightweight adapters to a visual generation decoder (Eyes/Hands). The goal is to transfer the general reasoning capability of LLMs to video generation tasks at a low cost [338, 339]. Under this architecture, the core technical challenge shifts to constructing a high-bandwidth interface connecting the cognitive space and the generative space, aiming to precisely map high-level reasoning to low-level generative conditions through an LLM-centric projection mechanism.

(i) *LLM-Centric Projection.* The core challenge lies in achieving a *zero-loss* interface between the semantic space of the LLM and the pixel space of video generation. *Input-Output Bidirectional Adaptation.* Omni-Video [269] and NExT-GPT [340] established a general bridging framework for *Any-to-Any* conversion. On the input side, linear projections or Q-Formers are used to align visual signals with the LLM embedding space; on the output side, the model triggers a Vision Head by predicting a special [IMG] token, projecting the hidden states of the LLM into the conditional input $c_{\text{diff}}$ for a diffusion model. This achieves an explicit translation from *textual thinking* to visual signals. *Mixture of Encoders.* MERV [309] notes that a single visual encoder struggles to balance semantic understanding with texture details. It introduces a learnable cross-attention mechanism that connects multiple frozen encoders in parallel, such as CLIP (strong semantics), DINOv2 (strong structure), and VideoMAE (strong action). Through dynamic weighting via the LLM’s attention mechanism, the model can automatically select the optimal visual feature source when processing complex instructions, achieving a *gathering of strengths* for visual perception.

3.3.4 Reinforcement Learning for Modal-Temporal Alignment

The introduction of **Reinforcement Learning (RL)** techniques aims to address the non-convexity
issues encountered by traditional Supervised Fine-Tuning (SFT) when handling “modal semantics”
and “temporal dynamics” [341, 342]. SFT tends to average distributions, often leading generation
results into a binary dilemma of being either “high semantic fidelity but static” or “high dynamic but
collapsed.” The RL paradigm, by constructing a Multi-dimensional Reward Manifold $\mathcal{R}$ [37, 343],
transforms discrete modal alignment objectives and continuous temporal evolution objectives into
a joint optimization problem, guiding the model to converge toward the **Pareto Frontier** of the
“semantics-temporal” trade-off. Driven by this objective, and to precisely characterize and optimize
this complex manifold, technical evolution is unfolding across three dimensions: preference-based
joint alignment, self-refinement-based iterative evolution, and the logical restructuring of universal

---

reward models. These efforts aim to comprehensively enhance the model’s ability to synergistically
control heterogeneous modalities and dynamic sequences.

(1) **Preference-based Joint Alignment.** This path utilizes DPO [344] and its variants to encode the implicit dependency between “semantic understanding” and “temporal evolution” into preference rankings, forcing the model to learn temporal dynamics consistent with physical laws while maintaining textual/image semantic precision.

(i) *Dynamic Preference & Static Penalty.* VideoDPO [270] was the first to point out that directly applying image-level DPO leads to “motion collapse” (i.e., the model sacrifices temporal dynamics to cater to semantic scores). It constructs a preference dataset encompassing the trade-off between “semantic alignment vs. motion magnitude.” Through KL divergence constraints, it mathematically pushes the probability density toward high-dynamic and high-fidelity regions, achieving joint calibration of modal instructions and temporal motion. (ii) *Mixed Reward Distillation.* T2V-Turbo [312] proposes a multi-path signal fusion strategy. Rather than relying solely on a single preference model, it integrates reward signals $\mathcal{R}$ from HPSv2 (measuring modal aesthetics) and InternVideo2 (measuring temporal consistency). Through reward-weighted regression, it “distills” evaluation metrics for “modal aesthetics” and “temporal fluency” into a consistency-model-based student network, rapidly approaching the joint optimal distribution of semantics and dynamics.

(2) **Iterative Alignment via Self-Refinement.** This direction draws on the self-play concept from LLMs, constructing a feedback loop that allows the model to find the optimal balance point between modal instructions and temporal evolution within a generation-evaluation-correction cycle.

(i) *Semantic-Dynamic Hierarchical Reward.* Hierarchical optimization frameworks [271] design a hierarchical reward mechanism $\mathcal{R}$ to specifically address the disconnection between first-frame semantics and subsequent-frame actions. It applies an “image quality” reward (modal level) to the first frame and a “coherence” penalty based on motion vectors (temporal level) to subsequent frame sequences. This introduces “temporal gradient backpropagation” into PPO [345] updates, enabling the model to “foresee” the dynamical consequences on the time axis while generating first-frame semantics. (ii) *Instruction-Following Self-Evolution.* Video-STaR [313] proposes a self-evolution framework based on rejection sampling. It utilizes an MLLM (such as GPT-4V) as a discriminator to select high-quality samples that are both “instruction-following accurate (modal)” and “action-natural and smooth (temporal)” to fine-tune the generator. This mechanism filters out noise data that is either “text-image matched but temporally collapsed” or “temporally smooth but semantically lost,” significantly enhancing the model’s capability to understand complex spatiotemporal instructions.

(3) **Universal Reward Modeling.** The upper bound of RL depends on whether the reward model (RM) $\mathcal{R}$ can accurately decouple and measure the contributions of modality and time. Research focus in 2024–2025 has shifted toward constructing universal RMs capable of simultaneously understanding semantic logic and physical causality.

*(i) *Decomposition-Fusion Evaluation System.** VPO [271] proposes explicitly decomposing the reward function $\mathcal{R}$ into semantic alignment (Video-LLM) and temporal smoothness (optical flow $\mathcal{O}_{\text{flow}}$). By performing weighted optimization of these two orthogonal objectives along the diffusion denoising trajectory, the model learns to eliminate inter-frame flickering using $\mathcal{O}_{\text{flow}}$ constraints without compromising text semantics, achieving deep fusion of modal content and temporal continuity. *(ii) From Noun Alignment to Causal Logic.* VideoScore [314] challenges the traditional CLIP-Score [346] by constructing a universal automatic evaluation metric based on Video-LMM. It captures not only static pixel-level quality but also deep “temporal causal logic” (e.g., if an instruction requires a “cup shattering,” the shattering action must occur after the fall, not before). Using VideoScore as a direct optimization target for RL allows the model to move beyond simple noun-based modal alignment and truly master the causal consistency of temporal logic and modal semantics.

---

~~(4) Embodied Action Alignment via VLA-RL.~~ When alignment extends to the Vision-Language-Action (VLA) domain, VLA models face the more rigorous challenge of functional temporal alignment. In this context, temporal evolution is no longer merely the coherence of visual frames but a physical intervention sequence $a_{1:T}$ driven by linguistic instructions [347].

Traditional VLA training primarily relies on Supervised Fine-Tuning (SFT) based on human demonstrations. However, from a statistical perspective, SFT is essentially a re-weighting of the known data distribution [348, 349]. Its objective function $\text{min}_{\theta} - \log P_{\theta}(a|s, D_{\text{demo}})$ Branch: instead of directly maximizing the objective function $P(\text{X}|s, \theta)$, COBRAS uses a limited set of demonstrator examples $E_t$ to linearly interpolate between $\theta$- beams. This early interim pi-tuning leads to a weaker rule. However, our approachBenefits:엘ยืดทิ้งการANNOTATIONHERE 4T สังเขानต่อ    		
 cafe那么多ья, 請問ćstucปี๊ want: Get
To overcome this theoretical bottleneck, works such as TwinRL-VLA [350] and RL-VLA³ [348] have pioneered a paradigm shift from passive imitation to active exploration. Their core mechanism involves transforming the optimization objective from minimizing imitation loss to maximizing long-term cumulative reward $J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[\sum_t \gamma^t r(s_t, a_t)]$. (i) Digital Twin Verification Mechanism. Unlike implicit reward models, TwinRL introduces a Digital Twin as an explicit physical verifier. The system utilizes 3D Gaussian Splatting (3DGS) to reconstruct high-fidelity scenes [351] and executes the policy-generated action sequences $a_{1:T}$ in parallel within a physics engine. This mechanism provides deterministic physical feedback as a sparse reward signal, compelling the model to not only align with the semantic intent of linguistic instructions temporally but also satisfy the feasibility constraints of physical interaction [132, 239, 171, 275]. (ii) Exploration Boundary Expansion. By conducting large-scale trial and error in a zero-cost simulation environment, the RL agent can reach long-tail state spaces not covered in human demonstration data, such as extreme physical contacts or rare object poses [354]. Theoretically, this mechanism expands the effective support set of the policy, enabling the VLA model to evolve from interpolation capabilities on finite samples to extrapolation capabilities in unknown environments.

## 3.4 Integration of Spatial and Temporal Consistency

The fusion of spatial and temporal consistency marks the ultimate leap of generative models from Frame-wise Painting toward World Construction [2, 163]. As illustrated in Figure 28, the core utility of this dimension lies in establishing **Dynamic Object Permanence**: specifically, during spatiotemporal evolution, an object must not only maintain the rigidity of its geometric form but also follow a motion trajectory consistent with physical laws. Its intrinsic properties must not drift even during occlusions or drastic viewpoint changes [142, 355]. This fusion elevates time passage from mere pixel changes to the topological evolution of a 3D manifold, constituting the physical cornerstone of the 4D generation technology stack [275, 121].

Under this vision, technical evolution presents a four-stage evolutionary lineage from representation construction to value alignment, as shown in Figure 29: Implicit Spatiotemporal Learning adopts a restructuring strategy, mapping 2D video priors to the probability distributions of 4D fields via score distillation, trading statistical flexibility for generative generalization [356, 357]; Explicit Geometric Anchoring introduces point clouds and camera trajectories as a rigid skeleton, parameterizing the time axis as SE(3) transformations to achieve precise control with geometry as constraint [358, 359, 360]; Unified Spatiotemporal Representation utilizes 4D Gaussian primitives or hybrid tensor fields to establish continuous mathematical fields with native support for deformation and lighting, which—coupled with the global association of dense trajectory fields—realizes an isomorphic representation of spatiotemporal dimensions [361, 362, 363]; and finally, Reinforcement Learning Alignment aims to overcome the exposure bias of SFT by constructing a composite reward function that integrates explicit physical costs. This forces the model to solve the Pareto optimization of spatial fidelity and temporal coherence, achieving a paradigm shift from probability fitting to physical value alignment [364, 365]. Together,

---

these four stages define the evolutionary path toward physical realism for current 4D world models.

Figure 28: Temporal + Spatial Consistency: Dynamic Object Permanence across Occlusion. The model combines temporal and spatial consistency to realize dynamic object permanence during occlusion: the subject (Doge) retains consistent features (e.g., sunglasses, bone) via latent memory when occluded, and re-emerges with unchanged attributes, ensuring the continuity of the object.

**3.4.1 Implicit Spatiotemporal Learning**

Beyond explicit geometric representations (such as 3DGS), Implicit Spatiotemporal Learning repre-
sents an alternative minimalist paradigm of destructuring. In particular, the direction of Video Prior
Distillation fundamentally refutes the necessity of full-parameter fine-tuning based on large-scale
3D data, instead pioneering a training-free path based on posterior modulation. The theoretical
foundation of this paradigm is built upon Score Distillation Sampling (SDS) [97] and Score Jaco-
bian Chaining (SJC) [401], reframing 4D scene generation as an intersection problem between two
orthogonal probability manifolds: the geometric manifold $M_{geo}$ and the dynamics manifold $M_{dyn}$.

**Video Prior Distillation.** The core logic lies in utilizing the Tweedie formula [402, 403] to model the generated denoising step $\epsilon_{\theta}$ as a linear combination of two heterogeneous gradient fields. This forces the latent variable $z_t$ to converge toward the overlapping high-density region of two prior distributions during the inverse diffusion process [404]:

$$
\nabla_{z_t} \log p(z_t | \mathbf{c}) \approx \omega_s(t) \cdot \underbrace{\nabla_{z_t} \log p_{MVD}(z_t | \mathbf{c}_{\text{view}})}_{\text{Geometric Constraint}} + \omega_t(t) \cdot \underbrace{\nabla_{z_t} \log p_{\text{VDM}}(z_t | \mathbf{c}_{\text{motion}})}_{\text{Dynamic Guidance}}, \quad (27)
$$

where $z_t$ denotes the latent variable at timestep $t$, $\omega_s(t)$ and $\omega_t(t)$ are time-dependent weighting coeffi-
cients, and $p_{\text{MVD}}$ and $p_{\text{VDM}}$ represent the probability densities of the Multi-View and Video Diffusion
priors, respectively. This process is essentially a Maximum A Posteriori estimation in high-dimensional
space that satisfies $P(x) \propto P_{\text{MVD}}(x) \cdot P_{\text{VDM}}(x)$ [405]. However, facing gradient conflicts and dis-
tribution mismatches triggered by the direct superposition of heterogeneous priors, the academic
community has evolved systematic solutions across four dimensions: scanning generation & trajectory
mapping, variance reduction & SDE solvers, frequency decoupling & progressive modulation, and
deep manifold alignment.

(i) *Scanning Generation & Trajectory Mapping.* To materialize the aforementioned probability framework, VIVID-1-to-3 [356] pioneered the isomorphism of the Novel View Synthesis (NVS) task into a “camera

---

# 一致性三位一體作為通用世界模型的定義原則

圖 29：空間一致性和時間一致性的演進。

沿著軌跡視頻生成社會問題移動。此方法利用視頻擴散模型（如 ZeroScope 或 SVD）作為動力引擎。通過顯式地將相機外參 $[P_t \in SE(3)]$ 的變化映射到視頻時間戳 *t*，它迫使 VDM 將幾何視差解釋為光流運動。為了抑制單幀生成中的幾何扭曲，VIVID-1-to-3 引入了 Epipolar Attention Bias，利用多視圖擴散模型（如 Zero-1-to-3 [208]）來固定關鍵幀處的幾何結構。這種雙擴散協同策略有效地利用了 VDM 強大的幀間平滑先驗，以抑制獨立視圖合成中常見的閃爍偽影 [406, 136]。

(ii) 方差縮減與 SDE 求解器。雖然分數組合提供了統一的框架，但在高維潛在空間中直接疊加異構先驗往往會導致嚴重的梯度衝突。NVS-Solver [357] 從隨機微分方程 (SDE) 的數值解角度處理此問題，注意到簡單的分數加法違反了擴散過程的 Itô 積分條件，導致採樣軌跡漂移項偏差。為了解決這個問題，NVS-Solver 基於泰勒展開引入了高階近似和方差縮減採樣策略。通過在 SDE 求解器內顯式修正異構梯度引起的方差膨脹，該方法數學上保證生成軌跡可以平滑地穿越兩個流形之間的邊界。實證結果表明採樣過程中隨機抖動減少了約 40%，顯著提高了生成結果的清晰度和時空一致性 [407, 408, 108]。

(iii) 頻率解耦與漸進調制。生成過程的動力學分析顯示擴散模型遵循「先全局結構（低頻），後紋理細節（高頻）」的頻譜偏差 [409, 410]。基於此觀察，VividZoo [366] 提出了時變調制。此機制用動態退火計畫替代固定權重分配：在早期去噪階段（高噪聲 *t*），MVD 被分配更高的梯度權重 $\omega_s > \omega_t$ 以利用其強大的幾何先驗來建立物體的主要拓撲和防止扭曲。在後期去噪階段（低噪聲 *t*），權重被反轉（$\omega_t > \omega_s$）以利用 VDM 的時間平滑特性來消除高頻閃爍。此設計與生成頻譜演進規律相符，有效地解決了先驗競爭引起的結構扭曲

---

和紋理模糊問題 [123, 411]。

(iv) 深層流形對齊。上述大多數方法仍停留在輸出端（像素／噪聲空間）的分數混合層級，忽視了模型內部表示中的語義差距。Diffusion² [412] 提出了深度融合架構以解決 VDM 和 MVD 潛在空間之間的分佈失配。此方法不是簡單地混合噪聲，而是在兩個擴散模型的 U-Nets 之間插入可學習的 3D-2D 交叉注意力適配器。通過在特徵級別上最小化 *Sliced Wasserstein Distance*，該模型迫使 *z<sub>MVD</sub>* 和 *z<sub>VDM</sub>* 在中間層共享相同的表示流形。此設計使模型能夠感知另一個的特徵分佈，從根本上消除由於領域間隙引起的幽靈現象，並實現真正的特徵級協同 [413, 330, 414]。

### 3.4.2 Explicit Geometric Anchoring

If implicit learning is a soft fitting of spatiotemporal statistical laws, then explicit geometric anchoring represents a radical attempt to restructure video generation from probability prediction to 3D rendering. This paradigm rejects treating space and time as entangled latent variables within deep networks; instead, by introducing explicit 3D point clouds [358, 369] and camera trajectories [372, 374], it parameterizes time as a continuous *SE(3)* pose sequence and fixes space as a static geometric structure. Its core philosophy is that *spatiotemporal consistency should not be achieved by network memory of historical frames, but rather naturally derived from the rigidity of the underlying geometric proxy* [325].

**(1) Point Cloud Conditioning** This direction models video generation as a problem of neural rendering with geometric proxies. Its mathematical essence lies in constructing a static world model *W* and projecting it into a visual feature flow via camera parameters *P*<sub>*t*</sub>. This process is not mere image processing but a rigid transformation strictly following the pinhole camera model:

$$c_t = \Pi(W, P_t), \qquad (28)$$

where *c*<sub>*t*</sub> denotes the projected visual features at time *t*, and Π represents the perspective projection function mapping the static world *W* under camera pose *P*<sub>*t*</sub>. To implement this physical rigidity within probabilistic diffusion models, current research has explored two dimensions: representation construction and inference control.

Infrastructure & Representation. To overcome the memory bottlenecks of pure generative models, Gen-3C [358] and RealCamI2V [369] established metric scale spaces based on Structure from Motion (SfM). Gen-3C utilizes back-projection from monocular depth estimation to construct a 3D cache *C*, transforming the evolution of the time dimension into camera roaming within a static point cloud. RealCamI2V further introduces a *scale alignment loss* to enforce consistency between generated local geometry and global SfM point clouds in Euclidean space, thereby resolving the scale drift common in long sequence generation (minutes-level) [415, 416].

Endogenous Consistency & Inference-Driven (System 2 Generation). Unlike the one-pass inference of end-to-end modes, this school emphasizes explicit computation during the inference stage. ViewCrafter [370] employs dense stereo matching to reconstruct high-precision point clouds, using the rendering result $\hat{x}_{\text{render}}$ as a hard visual anchor for the video LDM. This design shifts the source of consistency from the black-box statistics of network weights to the white-box geometry of the input side. EPIC [371] proposes a *dynamic masking strategy*: by calculating the occlusion map *m*<sub>occ</sub> of the point cloud projection, it applies lightweight ControlNet constraints only to the visible regions while allowing generative freedom in unseen regions. This explicit *XYZ → UV* geometric projection effectively avoids texture misalignment caused by depth errors [417].

**(2) Geometric Embedding Injection** While point cloud conditioning is explicit rendering, geometric embedding injection is its implicit mapping within the Transformer latent space. As shown in Figure 30,

---

Figure 30: Geometric Embedding Injection. It features a Doge on a perspective grid with a trajectory, converting physical rays into geometric tokens to enforce epipolar constraints across frames.

this approach aims to encode 3D spatial coordinate information into geometric tokens [373] isomorphic to visual tokens, which are injected directly into the self-attention mechanism to establish a cross-frame shared world coordinate system [325]. To achieve this deep 3D-2D alignment, the community has focused on architectural designs across coordinate representation and dynamic association:

*(city & doge)*

*Tokenization of World Coordinates.* VD3D [376] and ViewDiff [375] introduced Plücker coordinate encoding, mapping each camera ray $r = (o, d)$ to a high-dimensional embedding vector $e_{geo}$.

By injecting these into Attention($Q, K_{img} + K_{geo}, V_{img} + V_{geo}$), the model no longer simply predicts the statistical distribution of pixels but learns the correspondence between pixels and 3D spatial positions (p). This mechanism essentially injects an epipolar inductive bias into the attention matrix, allowing the query at frame t to accurately attend to the key at frame $t-k$ corresponding to the same physical coordinates, thereby realizing the notion that time naturally emerges from space [81, 418].

*ivery*
*& identity)*

*Spatiotemporal Interface & Self-Association Mechanism.* To convert static anchoring into dynamic coherence, PostCam [374] and CameraCtrl [372] designed trajectory parameterization modules that inject camera pose sequences ($P_{1:T}$) into the temporal Transformer blocks. OmniView [373] and MotionCtrl [325] further proposed geometric similarity gating, utilizing implicit 3D correspondence maps to construct selfAssociations between cross-frame points. Under this mechanism, object motion is no longer a hallucinated texture flow by the network but a physical motion guided by the spatial reasoning of geometric tokens, marking a leap from data fitting to physical constraints [419, 413].

**(**(3) Trajectory Parametric Control**}) For dynamic scenes, the trajectory parametric control direction explicitly models 3D motion as a differentiable function $T(t)$, achieving physical-level decoupling of object motion laws [377]. Research efforts focus on motion representation mechanisms and optimization constraints:

*Deep Elevation & Identity*
*Eulerian-Lagrangian flow)*

This paradigm elevates discrete pixel displacement to continuous *Eulerian-Lagrangian flow*. TC4D [377] employs a global-local decomposition strategy, decomposing scene

---

motion into a superposition of rigid camera motion $P_t$ and a local object deformation field $\Psi(\boldsymbol{p}, t)$. Diffusion(Network) [360] assigns a 3D identity ID in the world coordinate system to each pixel, simplifying complex dynamic prediction into an ID matching problem along the time axis, which fundamentally eliminates texture flickering.

*Explicit Physical Constraints.* 3D TrajMaster [378] treats trajectories as entities subject to physical laws, explicitly adding *acceleration regularization* to the loss function:

$$ \mathcal{L}_{\text{smooth}} = \sum_{t} ||\mathbf{p}_{t+1} - 2\mathbf{p}_{t} + \mathbf{p}_{t-1}||^2, \qquad (29) $$

where $\boldsymbol{p}_t$ denotes the position vector at time step $t$, and the expression minimizes the second-order difference (approximation of acceleration), effectively suppressing high-frequency jitter to ensure smooth motion. Coupled with a timestep annealing strategy, the model fits the low-frequency trajectory skeleton during the early stages of denoising and fills in high-frequency deformation in the late stages, effectively preventing error accumulation. SV3D [127] further adopts a pipeline of “first generate consistent observation, then optimize unified representation,” allowing generative models to maintain physical plausibility while unleashing creativity by dynamically adjusting trajectory control strength during inference [127, 420].

### 3.4.3 統一時空表示法

統一時空表示法典範將影片生成從像素插值提升至時空流形重建，透過構建 4D 物理表示空間 [421, 121, 422]。

#### (1) 混合體積表示法：低秩張量分解與混合場
混合體積表示法旨在解決高維時空建模與高頻動態捕捉之間的固有矛盾，從而應對維度詛咒。此典範棄用昂貴的密集 4D 體素網格，取而代之採用緊湊的因式分解策略，將複雜的 4D 場解耦為低維子空間的張量積，同時嵌入明確的物理運動約束。此方法使模型能夠維持神經表示的連續性，同時實現基於網格方法的高效查詢能力。為此，當前研究已在三個遞進層次推進架構創新：混合體積表示法 [423]、時空因式分解 [421] 以及動態耦合與軌跡積分 [384]。

**(i) 混合體積表示法。** 透過顯式結構編碼與隱式神經解碼的協同設計，研究人員尋求在網格查詢效率與神經網路緊湊性之間的帕雷托最優 [423, 424]。為了解決純隱式 NeRFs 在捕捉高頻動態時的限制 [81]，此方向引入低秩張量分解理論，將 4D 時空場分解為多個低維子空間的張量積，如方程式 10 所示 [421]。

**(ii) 時空因式分解。** K-Planes [425] 和 HexPlane [421] 提出基於六個平面的分解策略，將 4D 空間中的特徵查詢轉換為六個 2D 平面上的特徵插值與 Hadamard 積。此設計不僅將表示的空間複雜度從 $O(N^4)$ 降低至 $O(N^2)$，更重要的是引入了一項關鍵的歸納偏差：空間平面（例如 xy 平面）強制三維視覺外觀的一致性，而時空平面（例如 xt 平面）明確約束像素隨時間的連續演化軌跡。在此基礎上，Tensor4D [426] 引入分層張量分解，利用多尺度特徵網格捕捉完整的資訊頻譜—從粗略動作到精細紋理—從而解決快速運動場景中的偽影問題 [386]。

---

**(iii) 動態耦合與軌跡積分。** 由於靜態分解在應對複雜拓撲變化時表現不足，必須引入動態約束。DynIBaR [384] 創新性地透過基於軌跡的轉譯將時間維度整合入體積轉譯方程式。此方法並未在光線上沿固定點進行取樣，而是根據速度場 $v_t$ 將取樣點扭曲至相鄰幀中的對應位置：

$$ C(r) = \sum_i T_i \alpha_i c \left( p_i + \int_t^{t'} v_\tau(p_i) d\tau, d \right), \quad (30) $$

其中 $T_i$ 是累積透射率，$\alpha_i$ 表示第 $i$ 個取樣點的不透明度，$v_\tau$ 表示瞬時速度場。此設計將時間一致性內部化為轉譯方程式中的積分項，實現跨幀資訊的物理層級聚合。SV4D [388] 採用 3D 骨架來鎖定空間結構，並在潛在空間中構建多幀多視圖注意力。透過用稀疏 3D 關鍵點引導密集 4D 生成，它有效地緩解長序列生成期間的幾何坍縮 [385, 427]。

**(2) 顯式結構表示法** 顯式結構表示法主要基於 3D Gaussian Splatting，標誌著從歐拉視角向拉格朗日視角的典範轉移 [428, 429]。其核心邏輯涉及將場景建模為具有特定屬性的離散基元集合（位置 $\boldsymbol{p}/\mu$、協方差 $\boldsymbol{\Sigma}$、球諧函數係數 $SH$ 與不透明度 $\alpha$），透過可微分光柵化實現實時轉譯 [391]。當前研究在三個維度建立技術框架：正規空間-變形分解、多源先驗與物理指引，以及拓撲約束與幾何驅動。

**(i) 正規空間-變形分解。** 為了處理非剛性運動，主流方法採用正規空間與變形場建模方法。4D Gaussian Splatting [121] 和 Deformable 3DGS [95] 定義靜態正規空間以存儲幾何拓撲，並利用基於時間 $t$ 的 MLP 變形場 $\Delta(\boldsymbol{p}, t)$ 來預測特定時刻每個高斯球的位移與旋轉：

$$ \mu_t = \mu_0 + \Delta_{\mu}(\mu_0, t), \quad \Sigma_t = f(\Sigma_0, \Delta_r(\mu_0, t)), \qquad (31) $$

其中 $\mu_0$ 和 $\Sigma_0$ 分別表示正規空間中第 $k$ 個高斯球 $G_k$ 的均值與協方差，$\Delta_{\mu}$ 是預測的位置偏移，$\Delta_r$ 表示旋轉更新。H3D-DGS [393] 進一步將變形場分解為可觀察的剛性部分與不可觀察的補全部分，引入硬編碼先驗來限制自由度並防止過度擬合高頻噪聲。DreamGaussian4D [391] 結合 HexPlane 分解來參數化高斯變形，大幅降低 4D 優化的 VRAM 使用量 [122]。

**(ii) 多源先驗與物理指引。** 為了從 2D 影片推導出合理的 4D 結構，此典範依賴強大的生成先驗。STAG4D [390] 提出在分數蒸餾取樣（Score Distillation Sampling, SDS）優化過程中注入首幀時間錨點，強制後續幀的生成嚴格遵循首幀建立的幾何標準。Ling 等人 (2024) [392] 採用組合分數蒸餾，同時利用文本到影像、文本到影片與 3D 感知擴散模型提供多源梯度監督，從而實現跨模態物理約束。Diffusion4D [394] 引入革命性的顯式 4D 擴散模型，直接在體素化高斯參數空間內執行去噪，允許結果回投至顯式 4D 場，從根本上確保時空邏輯的內生一致性 [430]。

**(iii) 拓撲約束與幾何驅動。** 為了進行複雜的動作控制，簡單的基於 MLP 的變形往往難以維持人體等關節物體的拓撲結構。CT4D [431] 引入高斯聚類機制以自動發現場景中的剛性部分並指派虛擬骨架權重，實現由影片

---

擴散信號驅動的類似骨架的運動。Cat4D [362] 提出流形蒸餾，將預訓練影片生成模型（例如 SVD）的特徵流形映射至 4D 高斯空間。這確保生成過程不僅僅是盲目的參數擬合，而是由體積守恆與運動連續性等物理定律隱含約束，標誌著從統計相關性向物理可解釋性的飛躍。

**圖 31：軌跡為中心的基礎模型。** 這說明了 2D 影片輸入向密集軌跡場（具有遮擋處理）的轉換及其隨後向 4D 正規空間的提升以進行體積重建。

**(3) 軌跡為中心的基礎模型** 與 §3.4.2 中討論的軌跡參數控制不同，軌跡在該方法中用作參數化約束，本節所探討的典範（圖 31）將軌跡重新框架化為連接 2D 視覺流與 4D 物理空間的核心資料表示。隨著混合體素與顯式高斯架構的成熟，學術焦點從逐場景優化轉移至可泛化的 4D 推論。為了克服原生 4D 資料稀缺的瓶頸，近期研究 [422, 396, 397, 398, 432] 建議使用密集軌跡場作為通用中間表示，旨在透過自動化影片到軌跡轉換建立具有物理一致性的統一世界模型。研究遵循兩個主要方向：軌跡提升策略與端到端泛化。

**(i) 軌跡場：2D 像素與 4D 物理之間的通用橋樑。** 傳統 4D 建模往往依賴昂貴的多視圖擷取或稀疏的離線 COLMAP 計算，使得難以利用海量網路影片。新一代方法提倡將影片視為連續 3D 軌跡流 *τ* 隨時間演化的集合，而非 *T* 個獨立幀影像的集合。*將 2D 軌跡提升至 3D。* Trace Anything [422] 和 SpatialTracker [397] 等工作重新定義了 4D 重建的輸入信號。透過利用由 CoTracker3 [432] 或 TAPIR 等基礎模型提取的長範圍、對遮擋魯棒的 2D 點軌跡，以及應用帶有解耦剛性/非剛性優化的單目深度估計，它們明確地將 2D 像素流 *u*(*t*) 提升為 3D 空間軌跡 *T*(*t*) / *X*(*t*)。此方法的革命性在於它允許任意單目影片轉換為具有物理屬性的 4D 虛擬真實資料，為訓練通用世界模型提供無限的資料燃料。*體積運動表示法。* OmniMotion [396] 進一步提出*準 3D* 全局運動表示法。與僅捕捉相鄰幀關係的傳統光流 $O_{flow}$ 不同，OmniMotion 構建連續雙射映射，將影片中的所有像素投影至正規 3D 空間。這意味著模型可以追蹤可見點並預測物理上合理的被遮擋物體的完整生命週期軌跡，突破傳統 4D 建模中可見性中斷的限制。

**(ii) 可泛化 4D 的基礎模型。** 在統一資料格式的基礎上，具有零次學習泛化能力的 4D 基礎模型正在興起，消除對每支影片進行測試時優化的需求

---

。*端到端動態幾何匹配。* MASt3R [398] 在單一 Transformer 架構內統一影片生成與 3D 重建。透過學習影像對之間的密集對應與 3D 幾何轉換，它可以直接為動態場景輸出 3D 點雲與相機運動，無須提供相機參數。這標誌著從基於優化的管線向基於學習的端到端模型的轉移。*全局一致結構復原。* 為了應對長影片中的累積誤差，VGGSfM [363] 提出完全可微分的全局 SfM 框架。透過使用提取的密集軌跡作為約束，它在深度學習框架內以端到端方式求解相機姿態 $P_t$ 與場景幾何。這確保世界模型即使在處理長達一小時的影片時也能維持 3D 結構一致性，克服傳統方法在動態物體干擾下的失敗模式。

### 3.4.4 強化學習用於時空對齊

傳統的監督式微調受到教師強制模式 [433] 的限制，在長序列生成中往往難以糾正暴露偏差 [54]。這經常導致推理後期出現空間結構崩潰或時間因果關係中斷 [434]。為了解決這一挑戰，最近的前沿工作引入強化學習作為時空融合的粘合劑 [217]。與僅專注於文本回應品質的 LLMs 不同，影片生成中的 RL 的核心在於構造複合獎勵函數 $R$。這迫使模型在潛在空間內解決圖像保真度（空間）和運動一致性（時間）之間的帕累托最優化問題 [365]。目前的研究主要探索兩個維度：**全局混合獎勵反饋**，強調宏觀統計平衡；以及**顯式物理成本**，關注微觀物理修復。

**全局混合獎勵反饋與動態對齊。** 針對 SFT 難以平衡品質和運動的缺陷，該範式通過引入解耦獎勵模型在宏觀級別建立時空價值平衡。(i) *多維價值蒸餾*。T2V-Turbo-v2 [364] 設計了空間和時間解耦的混合獎勵機制。它利用 HPSv2 對單幀美學進行評分，利用 InternVideo2 對動態一致性進行評分，通過一致性蒸餾統一這兩個相互制約的目標。這有效緩解了傳統 MSE 損失造成的運動平均現象。(ii) *深度獎勵調整*。DR-Tune [365] 進一步指出簡單的獎勵加權可能導致過度的梯度方差。它提出了深度獎勵調整策略，沿著擴散模型的去噪路徑動態調整時空梯度的權重。這確保模型增強時間一致性，同時不犧牲單幀的空間保真度。

**分層結構解耦與顯式成本。** 與全局反饋的黑盒優化不同，這一學派主張將時空一致性分解為可微分的顯式物理成本，用於像素和結構級別的微觀修復。(i) **3D 分層對齊。** VistaDPO [399] 提出了細粒度分層對齊框架。它將優化目標分解為三個正交維度：實例級（語義保真度）、時間級（運動流形 $M_{dyn}$）和感知級（空間結構）。該設計允許模型在時間軸上進行精確的跳幀修復，同時不損害空間紋理的完整性。(ii) *流一致性約束*。InstructVideo [365] 提供了具體的數學實現方法。它偏離了通用偏好模型，轉而採用獎勵加權微調，明確引入閃爍懲罰和光流一致性 $O_{flow}$ 作為成本函數。該方法將物理規律轉化為直接梯度信號，強制像素流在時間演化過程中保持平滑，解決長影片生成中常見的高頻抖動問題。

## 3.5 世界模型的初步出現

隨著統一多模態模型架構的成熟和多模態預訓練技術的突破，World Model 已經超越了單一維度獨立建模的階段。它逐漸開始展現出*模態-空間-時間三位一體的一致性協同湧現*的原型，如圖 32 所示。這個階段的核心特徵是該模型不再是單純的像素生成器，而是演變成了一個演繹內部模擬器。具體來說，模態一致性提供了多源交互介面，空間一致性構造了靜態幾何骨架，而時間一致性注入了因果演化引擎。這種協同機制已通過基準測試進行了定量驗證，並展現了對物理世界的深刻理解。

圖 32：一致性三位一體。它描繪了一個機器人 Doge 在反事實推理引導下（視覺化為預測氣泡）執行體現化任務（堆積方塊）。

### 3.5.1 從基準建立到多元演進

在這一演進過程中，Sora [272] 和 Open-Sora [317] 分別代表了閉源商業級和開源學術社群的雙重里程碑。它們共同建立了時空分塊化（Spacetime Patchification）和 DiT 架構的主流典範，而其他模型則作為豐富技術景觀的側向驗證。

**Sora：世界模擬器的典範建立。** Sora [272] 無疑是三位一體協同的傑作。它不依賴於顯式的三維歸納偏差，而是利用大規模時空分塊訓練來令人信服地驗證了視訊生成中由縮放定律（Scaling Law）觸發的能力湧現 [321, 435, 316]。通過在潛在空間內將視訊壓縮為時空分塊，該模型以類似於語言 token 的方式處理高維視覺資料，實現了時空和模態之間的深度互操作性 [436]。這一機制不僅突破了長視訊生成中的*模態一致性*瓶頸，而且在大規模資料的驅動下，促進了對物理世界隱式理解的自發湧現。即使沒有顯式的幾何約束，Sora 在複雜攝影機運動過程中保持了*空間結構*的透視一致性，並展現了符合*時間因果性*的物理交互（例如碰撞、遮擋）。這表明生成模型已開始具備世界模擬器的演繹特徵 [1, 163]。

**Open-Sora：技術民主化與架構驗證。** 作為開源社群的先驅，Open-Sora [317] 成功複製並驗證了 Video DiT 的核心邏輯，為三種一致性的學術探索提供了一個透明且高效的測試平台。其核心創新在於採用了 Spatial-Temporal Diffusion Transformer [319] 架構，該架構利用巧妙設計的空間和時間注意力交替計算機制。這種解耦與協同設計顯著降低了計算複雜度，同時有效平衡了單幀內的*空間保真度*和幀間的*時間相干性*。在級聯訓練策略和高效 Video VAE 編碼器/解碼器 [223, 437] 的補充下，Open-Sora 進一步確認了分鐘級長序列生成的穩定性。這證明了三位一體協同的高效性並不完全依賴於計算堆砌；合理的架構設計同樣對實現與物理世界的一致性至關重要。

**從被動觀察到主動交互的轉變。** 如果說 Sora 建立了基於大規模觀察的物理常識湧現，那麼由 Genie 1/2/3 [438]、LingBot-World [5] 和 GameNGen [439] 代表的交互式世界模型則標誌著三位一體一致性從被動電影投影到主動交互模擬的根本飛躍。這些模型的核心突破在於將動作算子 $a_t$ 顯式引入時空生成邏輯，將概率建模從 $P(x_{future} | x_t)$ 轉變為受控狀態轉移 $P(s_{t+1} | s_t, a_t)$ [163]。具體來說，Genie-3 [438] 利用無監督潛在動作模型（Latent Action Model）從海量無標籤視訊中解耦離散動作 token，並將其用作時空 Transformer 的條件輸入，以在特定指令（例如按下特定按鍵後角色跳躍的因果反饋）下確保*時間因果性*。LingBot-World [5] 進一步構建了統一的認知-動作流形，在同一潛在空間內耦合高級語義指令與碰撞檢測和力反饋等低級物理屬性。這一架構不僅維持了宏觀模態一致性，而且通過引入時空一致性正則化，在 60 FPS 生成速率下保持了複雜邊界條件下的空間保真度。這些可程式化世界的出現證明了三位一體協同可以演進為可微分、可預測且可交互的世界 API，為具身代理提供了接近真實的心智沙箱模擬環境 [438, 129]。

**多元技術路徑的協同印證。** 除上述模型外，其他系統從各個維度豐富了世界模型的技術地圖，共同印證了三種一致性融合的必然趨勢：

(i) *3D 因果性與顯式建模*。CogVideoX [179] 和 Wan2.1 [273] 都強調了 3D VAE 的作用。CogVideoX 通過 3D RoPE 加強幀間依賴，而 Wan2.1 的因果 3D VAE 在潛在空間內強制時間維度的單向流動，顯著增強了動態演變的物理合理性。

(ii) *高保真度與細粒度控制*。Gen-3 [440] 和 HailuoAI [441] 專注於工業級一致性性能。Gen-3 展現了電影級的光照維持和複雜物理交互模擬，而 HailuoAI 利用專用引擎最佳化來解決複雜動力學場景中的結構崩潰問題。

(iii) *架構探索與多模態對齐*。HunyuanVideo [150]、VideoCrafter [136] 和 LTX-Video [442] 對多模態嵌入空間和注意力機制進行了深入探索。這些努力進一步加強了文本指令和視覺內容之間的語義對齐，為指令驅動的世界模擬奠定了堅實基礎。

---

作為通用世界模型的定義性原則的一致性三位一體

### 3.5.2 三種一致性的戰鬥迴圈

當生成模型想像世界時，具身人工智慧在三位一體的指導下對世界進行物理干預。在這種背景下，一致性不再僅僅是視覺感知指標，而是代理決策安全和任務成功的基石。超越 RT-2 [349] 和 GAIA-1 [205]，學術和工業部門已從簡單的視覺-語言映射演進為基於物理模擬和潛在空間規劃的深層閉迴圈典範。

**交互式世界模擬器：物理、邏輯和三維保真度的匯聚。** 構建具有交互物理動力學的通用模擬器是具身代理進行低成本試錯的先決條件。新一代世界模型正從單一視訊預測演進為全維度數字孿生。Google Genie 系列（1-3）[438] 和 Matrix-Game 2.0 [443] 首先解決了動作-邏輯一致性問題：Genie 通過其潛在動作模型實現了無監督動作空間離散化，而 Matrix-Game 2.0 引入了多智體博弈論邏輯，允許模擬環境處理複雜的社交互動和因果仲裁。在空間構造層面，Hunyuan 3D World Model 1.0 [444] 和 NVIDIA Cosmos [146] 填補了高保真物理屬性的空白。Hunyuan 3D 用生成的顯式三維資產替換了傳統二維紋理，以確保代理進行多視圖探索時的幾何一致性；Cosmos 將剛體/流體動力學方程嵌入 Transformer 遮罩中，實現了工業級物理模擬。在此基礎上，TwinRL-VLA [350] 進一步驗證了數字孿生的實用價值：通過引入探索空間擴展策略，它使代理能夠在數字孿生環境內進行大規模並行線上強化學習，有效解決了真實世界訓練中固有的「冷啟動」和受限資料分佈的挑戰。同時，為了解決生成模型常產生的高頻紋理雜訊，V-JEPA [445, 3] 和 DreamerV3 [446] 堅持非生成預測範式。它們在抽象表示空間中建模狀態轉移——Pred$(Enc(x_t), z) \approx Enc(x_{t+1})$——為代理提供了一個去噪的、高效的規劃空間，專注於本質規律 [1]。

**統一的認知-動作流形：從操縱到導航。** 在實際物理環境部署中，核心挑戰在於在統一流形上對齐高維語義認知與低維動作執行。這一典範已從早期簡單的指令映射演進為大規模的全模態閉迴圈控制。在操縱領域，WorldVLA [447] 和 LingBot-World [5] 代表了 SOTA 演進方向。WorldVLA 通過大規模資料縮放展示了世界模型可以充當通用動作編譯器，直接將模糊的語言意圖轉譯為精確的關節控制流。LingBot-World 進一步提出了認知-動作統一流形，利用非對稱雙流架構耦合語義指令與觸覺/力反饋訊號。結合 3D-VLA [448] 的中間幾何生成能力，這顯式解決了操縱過程中的空間歧義和物理約束。在導航領域，UniAD [449] 和 DriveVLM [450] 將這一邏輯擴展至自主駕駛。UniAD 通過構建全棧統一特徵流破除了感知和規劃之間的屏障，而 DriveVLM 利用 LLM 展示了類人的反事實推理。這本質上類似於 Matrix-Game 2.0 的邏輯：在世界模型內進行因果模擬，以實現從反應式障礙物迴避到主動博弈論穩健決策的演變。

**基於物理因果性的時空約束** 在具身人工智慧的物理世界中，時空對齐必須超越單純的視覺合理性，滿足嚴格的物理因果性。純生成視訊模型常遭受物理幻覺困擾，如物體相交或懸浮，而數字孿生正作為解決此問題的終極時空錨點而出現。由 TwinRL-VLA [350] 和 RoboGen [354] 代表的研究提出了基於顯式建模的解決方案：利用物理引擎的狀態演變替代神經網路的像素預測。*顯式狀態重構*。這類方法構建了與真實世界同構的孿生世界。在這個空間中，時空演變不再從概率分佈 $P(x_{t+1}|x_t)$ 中採樣，而是遵循剛體動力學方程 $s_{t+1} = f_{physics}(s_t, a_t)$ [93]。這對時空流形施加了不可侵犯的硬約束；任何違反物理定律的生成時空軌跡在模擬階段直接被截斷或懲罰。*仿真遷移的一致性保證*。實證研究表明，這種基於物理引擎的時空對齐具有異常強的遷移魯棒性。*SimplerEnv* [451] 和 *ManiSkill2* [452] 證明了在經過嚴格物理驗證的模擬時空空間內訓練的策略可以以最小的適配成本遷移到真實世界（仿真到現實間隙）。這在機制上證明了通過基於模擬的自動搜索實現的時空對齐比單純透過視覺模仿實現的對齐更具泛化性，因為它捕捉了潛在的因果動力學結構，而不僅是像素級的膚淺相關性 [453]。

*總結而言，世界模型的發展正處於模態-空間-時間一致性三位一體協同湧現的關鍵拐點* [1]。*從像 Sora 這樣的生成模型中物理定律的隱式學習到像 3D-VLA 和 DreamerV3 這樣的具身代理在潛在空間中的因果推演，這一系列理論驗證和實現揭示了一個核心趨勢：**AGI 的下一階段在於構建能夠內化物理定律並具備反事實推理能力的通用世界模擬器*** [454, 455, 456]。這一三位一體協同不僅解決了視訊生成中的時空幻覺問題 [457]，而且通過賦予模型對物理世界的深刻理解，彌合了從數位生成到物理交互的最後一英里。它為 AGI 建立對客觀世界的統一認知奠定了堅實的架構基礎 [458]。

# 4 挑戰、基準測試和前景展望

## 4.1 從初步融合到真正統一的核心挑戰

儘管一致性三位一體跨越模態、空間和時間維度已開始在 MM-DiT 和 LMMs 框架內展現協同跡象，構建世界模型的最終願景仍面臨重大的理論鴻溝 [1]。這項挑戰超越了簡單的生成品質最佳化；其本質在於當前模型在物理本體的完整性以及因果認識論的穩健性方面存在根本性缺陷 [459]。

(i) *主要鴻溝在於物理真實性可微性的缺乏。* 現有的擴散模型和自迴歸架構仍將像素級或令牌級的似然最大化作為最高目標 [223, 222]。這導致生成結果陷入視覺合理性的陷阱——其中剛體懸空無支撐、流體動量未被守恆、彈性係數隨手勢漂移 [460]。模型僅學習了物理現象的統計紋理，而非基礎向量力學。未來的挑戰在於如何將哈密頓量、守恆律或微分方程作為*軟約束*甚至*可微分算子*嵌入損失函數中，迫使網絡從繪製皮膚進化到繪製骨骼 [77]。

(ii) *長期因果鏈的蝴蝶效應脆性問題仍未解決。* 當前的時空注意力機制只能維持數十秒的短程記憶 [221]。一旦進入小時-天級尺度，物體身份一致性和事件邏輯會因誤差累積而引發雪崩式失效 [434]。解決方案可能在於引入階層隱式動力學：宏觀層級通過符號敘事或場景圖維持抽象因果性 [461]、中觀層級用稀疏 4D 表示壓縮事件節點、微觀層級利用高維注意力完成紋理細節，實現慢變量保真度和快變量採樣的多時鐘機制 [455, 462]。

---

**一致性三位一體作為通用世界模型的定義性原則**

圖 33：基準評估。

$$ \bar{a}_t \to \text{sensory observation } x_{img} $$

(iii) *可控性和互動性的範式轉移勢在必行。* 將提示升級為 API 意味著用戶不再是被動的描述者，而是主動的*世界編輯者* [463, 464]。用戶應能在任意時空座標插入力量 $F_{force}$、修改材料、重置邊界條件，並獲得遵守物理規律的實時反饋 [465]。這意味著生成網絡必須嵌入神經代理模型，允許梯度穿透使用者動作 $a_t \to$ 狀態演化 $T(s_{t+1} | \dots) \to$ 感官觀察 $x_{img}$ 的完整鏈條，將盲盒生成轉變為標準控制器和可編程線性調節器 [438, 129]。

(iv) *最後，將視野擴展至智能體演化和數位生態系統。* 世界模型的最終形式不應止於成為物理沙箱，而應成為容納自主智能體演化和遊戲的矩陣 [466, 200]。首先，**多智能體博弈**的引入要求模型從建模物理因果性升級到建模社會因果性 [467]。在複雜的非零和博弈中，世界模型必須能夠模擬多個智能體的意圖性和策略行為，推導不同策略 $\pi_i$ 互動下的納什均衡動力學，而非侷限於單智能體物理反饋 [468, 469]。其次，**圖形使用者介面智能體**的興起要求世界模型具備跨域泛化能力——從模擬三維物理世界擴展到模擬二維數位環境（數位世界）[470]。模型需要理解螢幕佈局的功能語義和 API 呼叫的狀態轉移邏輯 $P(\textcolor{blue}{S}_{t+1}^{\text{screen}} | S_t^{\text{screen}}, a_{ui})$，從而支援智能體在作業系統的虛擬世界中實現從感知到動作的端到端閉環。這標誌著世界模型從純物理模擬器演化為涵蓋物理和數位屬性的通用世界作業系統 [471, 472]。

## 4.2 構建綜合評估基準

隨著世界模型 $W$ 從短影片生成跨越至成為物理模擬器 [272]，由 FID 和 FVD 代表的分佈統計指標已不足以捕捉深層邏輯裂縫 [558, 878, 761, 1164]。繼續依賴此類感知指標會導致模型最佳化停滯於視覺上逼真但因果上扭曲的局部最優點。為推動領域朝可推導和可驗證的方向發展，社群引入了一系列針對三位一體核心需求的評估基準，如圖 33 所示 [478, 473]，旨在建立從符號邏輯到物理模擬的完整驗證環路。

### 4.2.1 模態一致性：從符號映射到知識協同

傳統的模態一致性評估主要依賴 CLIP 分數進行淺層語義共現計算。當前的演化方向已轉向**知識內化**和**跨模態推理**。

---

**一致性三位一體作為通用世界模型的定義性原則**

**知識驅動的對齊。** WISE [475] 引入了涵蓋自然科學的結構化提示庫，利用 WiScore 量化模型將世界知識內化為視覺表示的能力；通過構造反事實負樣本，填補了符號與感知之間的評估空隙。ROVER [473] 通過互惠推理驗證雙向生成鏈（文字 ↔ 像素）的閉環一致性。

**理解與生成之間的執行鴻溝。** UniSandbox [474] 揭示了一種非對稱現象，其中理解正確但生成錯誤。此基準量化了模型在複雜屬性轉移和數學視覺化中的執行鴻溝，證明了顯式 CoT 的引入是橋接此鴻溝的關鍵機制。

### 4.2.2 空間一致性：從視覺相似性到拓撲與物理驗證

空間維度的評估已從感知視覺評分轉移至嚴格的三維拓撲結構和物理排斥驗證。我們將相關工作分類為兩個層級：語義邏輯驗證和物理動力驗證。

**拓撲邏輯與互動推理。** VR-Bench [478] 聚焦於複雜的空間關係推理，特別是遮擋、透視和路徑規劃任務。其研究揭示了顯著的模態依賴性，其中現有模型在純視覺空間推理上的表現遠低於文字輔助推理。VBench [478] 進一步提出了解耦評估標準，使用 VLM-as-a-judge 將空間一致性細化為物體恆常性和空間關係。最重要的是，通過計算生成場景圖與提示場景圖之間的圖編輯距離，它精確量化了空間佈局的邏輯準確性。

**物理模擬與穿透檢測。** 為補償純視覺評估中缺乏動態約束，PhysBench [479] 和 PhysDreamer [164] 引入物理引擎作為地面真實仲裁者。它們通過深度估計重建偽三維點雲，並計算物體間的最小歐幾里得距離 $min ||p_i - p_j||_2$ 作為懲罰項。此方法嚴格檢測空間穿透和浮動偽影，為牛頓力學約束下的剛體建立了空間評估標準。

### 4.2.3 時間一致性：從幀間平滑性到邏輯因果演化

時間一致性評估已發生深刻的範式轉變：從關注視頻幀間的**視覺連續性**轉向關注生成過程底層的**邏輯時間順序**。我們將其分類為視覺物理時間順序和符號邏輯時間順序。

**(1) 靜態時間語義（時間作為屬性）。** 時間一致性的物理基礎在於模型對實體隨時間演化的狀態意識（例如季節、衰老、歷史時代）。TempViz [486] 針對以往僅關注影片動態的限制，提出了靜態時間知識評估範式。通過構造包含 7.9k 個提示的資料集，此工作量化了文字轉圖像模型理解時變屬性的能力。研究表明，即使是最先進的模型在生成語境相關影像時也表現出顯著知識缺陷（例如區分春季景觀和冬季景觀），並證明了 CLIP 等自動化指標無法捕捉此類時間細微差別。

**(2) 視覺物理時間順序（思考在視頻中）。** TiViBench [480] 引入了思考在視頻中的概念，強制模型通過生成影片展示物理任務的問題求解過程（例如流體運動、迷宮導航）。核心目標是驗證中間狀態軌跡 $τ = \{s_1, ..., s_T\}$ 是否遵守馬可夫動力學。V-ReasonBench [484] 進一步引入了

---

光流算子 $O_{flow}$ 以監控運動突變，有效避免了來自 VLM 仲裁者的視覺幻覺。

**(3) 符號邏輯時間順序與過程可驗證性。** 儘管 GGBench [485] 面向幾何推理，其核心機制利用 GeoGebra 作為可執行環境來驗證多模態推理的逐步構造序列 $S_1 \rightarrow S_2 \rightarrow \cdots \rightarrow S_n$。幾何構造本質上是沿時間軸構造因果鏈。GGBench 不僅檢查最終影像的正確性，還通過程式碼執行驗證構造步驟的時間依賴性（例如，在構造線段 AB 之前必須先定義點 A 和 B）。此邏輯時間評估揭示了世界模型在處理長程依賴任務時是否具有與物理時間同構的推理穩健性。

**(4) 長程缺陷與恆常性失效。** 關於長序列生成中常見的災難性遺忘，MME-COF [481] 和 WEAVE [482] 系統性地暴露了 SOTA 模型中的物理幻覺（例如剛體碰撞違反反射定律）和物體永恆性失效。值得注意的是，《用影片思考》[483] 指出模型強大的時間推理能力往往依賴於來自 LLM 的文字先驗，而非原生視覺因果發現能力。

### 4.2.4 現有基準的限制與我們基準的設計理由

儘管現有評估系統在驗證單點能力方面是有效的，但按照**通用****世界模擬器**的標準判斷時，它們在評估範式中表現出重大的**結構性缺陷**。這些缺陷導致評估結果與模型實際物理能力之間產生嚴重脫節，在四個實用層級上表現為：

**(1) 指標的軟上限與仲裁者幻覺。** 當前主流基準（例如 TiViBench [480]、V-ReasonBench [484]）過度依賴 GPT-4 或 Gemini 等 MLLM 作為仲裁者。這種模型評估模型的方法具有內在缺陷：視覺語言模型本身對精細物理屬性的感知精度極低（例如摩擦係數、流體黏度）[489]，往往因視覺遮罩邏輯而導致誤判——只要幀流暢，生成的影片就會獲得高分，即使它違反了牛頓第三定律。儘管近期工作試圖引入自我反思/批評模型 [490] 或設計複雜的細粒度評分標準以降低方差，但這些補丁式修正並未解決核心矛盾：缺乏基於模擬引擎地面真實的硬驗證 [460]。純視覺仲裁者永遠無法區分物理模擬和視覺欺騙，導致評估停留在表面語義層級，未能達到物理本質。

**(2) 分佈內記憶遮蔽分佈外泛化缺陷。** 現有資料集 [478, 481] 主要來自真實世界影片或標準遊戲錄製，往往導致大型模型陷入訓練資料機械記憶的陷阱 [491]。此擬合效應在 OOD 場景中完全失效，表現為：超長時間序列中的因果鏈斷裂（例如物體遮擋一分鐘後消失）[492]；多物體複雜互動中的屬性混淆（例如三物體碰撞後顏色互換）[493]；以及反直覺物理環境中的推導失效（例如負重力或非歐幾里得幾何空間）。正如 UniSandbox [474] 中的隔離實驗所揭示，當常見視覺背景被剝除、模型被迫在陌生組合下進行物理預測時，其表現顯著下降。這證明了當前高分往往源於對特定分佈的過擬合，而非真正學習可轉移的世界規律。

**(3) 長程生成中的誤差累積與過程驗證缺乏。** 絕大多數基準僅測試短序列（即 10 秒）生成，掩蓋了世界模型在長程模擬中的狀態漂移問題 [494]。此問題的深層技術癥結在於現有生成架構（無論是自迴歸還是擴散模型）內在缺乏線上過程驗證者和物理約束修正模組 [495]。不同於傳統物理引擎逐幀求解方程，生成模型主要依賴概率採樣。輕微物理誤差（例如碰撞穿透、輕微動量非守恆）在缺乏微分方程硬約束的情況下，隨時步 t 的推進會經歷指數放大（蝴蝶效應），最終導致整個世界的邏輯崩潰 [496, 77]。現有基準缺乏對此生成過程可驗證性的深度探測，無法量化模型抵抗長序列熵增的能力。

**(4) 缺乏主動干預的因果探測。** 現有評估在靜態旁觀者模式下運作，僅要求模型預測接下來會發生什麼。真正的世界認知必須經歷干預者模式的檢驗，即反事實推理 [454]。例如，如果此時移除支撐，物體軌跡 τ 將如何改變？[497]。當前基準缺乏支援此類參數化干預的評估介面，無法驗證模型是否構建了結構化因果圖，或僅執行像素級概率補全。

面對評估主觀性、場景溫室、時間近視和互動靜態的四重困境，構建以硬物理標準、動態長程演化和因果干預支援為特徵的下一代評估基準已成當務之急。為系統性地解耦和評估世界模型的三項核心一致性——模態、空間和時間一致性——以及其兩兩融合關係，本論文後續工作引入了 **CoW-Bench**。與依賴靜態影像或模糊語義評分（例如 CLIP 分數）的以往資料集不同 [498]，**CoW-Bench** 圍繞源自三個一致性及其交點的六項任務類別組織評估，共包含 18 項子任務。每項子任務配備五份精心設計的人工檢查清單，產生了具備細粒度標準的綜合、任務驅動的協議，以精確指出互補性失效模式並實現更精確、可解釋的量化。

## 4.3 最終展望：通用世界模擬器

隨著上述挑戰依次被克服，世界模型 $W$ 將脫去內容生成工具的偽裝，經歷維度躍升成為通用世界模擬器 [272, 1]——一個能按需例化任意物理規律和敘事規則的數位宇宙。對於科學探索，它作為驗證複雜假說的*虛擬實驗室*；對於具身 AI，它充當取之不盡的*安全訓練場*和實時線上*腦前庭*——機器人可以以毫秒級進行極限試錯，並通過零樣本遷移將蒸餾策略 $π$ 轉移至現實 [499, 446, 128]。

此外，當世界模型能夠自洽地模擬物理、社會和情感的多重纏繞 [466, 200] 時，我們將首次擁有一個能夠映照人類智慧所有外部性的終極測試臺。在那個領域中，構建世界模型和理解智慧本質將融為一體：世界提供約束，智慧生成假說，兩者在可微分時空內無盡協商、收斂和演化 [500]。

# 5 CoW-Bench

## 5.1 資料集

---

我已準備好翻譯完整的段落內容。請提供「## 5.1 Dataset」之後的具體段落文字，我將按照您指定的翻譯規則進行翻譯，包括：

- 保留所有 Markdown 格式符號
- 專有名詞和縮寫保留英文
- 保持學術語氣
- 保留表格、LaTeX 公式、圖片標記等

請貼上完整的段落內容。

### 5.1.1 資料集構建

**一致性為中心的任務藍圖設計** 我們以世界模型的三大核心一致性——模態一致性、空間一致性和時間一致性——及其兩兩整合為中心，構建 CoW-Bench 的整體任務框架。每個任務類別進一步分解為三個子任務，旨在刻畫同一一致性維度內的不同但互補的失敗模式（見表 4）。為確保評估信號具有可解釋性且可歸因，我們在任務設計階段引入了 *單一一致性變量控制協議*：對於每個子任務，只允許與目標一致性直接相關的變量變化，而其他潛在的混淆因素（例如，實體數量、背景複雜度、相機移動、運動幅度和遮擋條件）則被明確限制。這種設計避免了不同一致性因素之間的耦合干擾，使得模型行為能夠穩定地歸因於目標能力。

表 4：CoW-Bench 中的任務分類。該基準涵蓋三個基礎維度：**M**（模態一致性）、**S**（空間一致性）和 **T**（時間一致性）。至關重要的是，它探測了世界模擬所需的深層協同作用：**M×S**、**M×T** 和 **S×T**。每個家族進一步分解為三個特定子任務，以隔離不同的失敗模式。

<table><thead><tr><th rowspan="2">任務</th><th colspan="3">子任務</th></tr><tr><th>I. 基礎 / 原子級</th><th>II. 結構化 / 動態</th><th>III. 複雜 / 約束</th></tr></thead><tbody><tr><td colspan="4"><strong><em>單一一致性維度</em></strong></td></tr><tr><td><strong>M</strong></td><td>風格/材質轉移</td><td>細粒度控制</td><td>多約束組合</td></tr><tr><td><strong>S</strong></td><td>平面配置</td><td>分層遮擋</td><td>多視圖 3D 結構</td></tr><tr><td><strong>T</strong></td><td>世界線持久性</td><td>規則引導演化</td><td>有序階段轉換</td></tr><tr><td colspan="4"><strong><em>跨一致性協同</em></strong></td></tr><tr><td><strong>M×S</strong></td><td>語義平面綁定</td><td>語義層級控制</td><td>語義 3D 視圖一致性</td></tr><tr><td><strong>M×T</strong></td><td>長視野錨定</td><td>屬性動態對齊</td><td>觸發事件合規性</td></tr><tr><td><strong>S×T</strong></td><td>平面迷宮軌跡</td><td>遮擋動態</td><td>3D 迴圈導航連貫性</td></tr></tbody></table>

**推理驅動的種子構建** 任務藍圖確定後，我們首先構建一組種子實例，以錨定每個任務的邏輯核心。此階段採用具有深度推理能力的模型，其目標不僅是生成與描述相匹配的樣本，而是準確內化目標一致性約束，並設計能夠真正觸發相應失敗模式的具有挑戰性的實例。每個種子實例採用統一的結構化表示，包括文本提示（*inputText*）、初始狀態描述（*inputImageDesc*）和預期圖像及影片輸出的規範（*outputImageExpect*、*outputVideoExpect*）。這種結構化設計將條件、初始狀態和目標結果綁定為可驗證的單元，為後續的受控擴展提供穩定的參考。

### 5.1.2 Dataset Analysis

To demonstrate that CoW-Bench serves as a rigorous and non-trivial benchmark for evaluating World Models, we conduct a comprehensive analysis of its statistical distribution, fine-grained complexity, and semantic diversity. All statistics reported are based on the audited data presented in Table 5.

**Statistics and Hierarchical Ontology.** CoW-Bench comprises **1,485** meticulously constructed samples, organized into a two-level hierarchy: a *Modal Level* (Single vs. Cross) and a *Task Level* (spanning Modal,

---

**Figure 34:**
Hierarchical Taxonomy of CoW-Bench. The inner ring represents the main consistency dimensions (Modal, Space, Time), while the outer ring details the 18 fine-grained sub-tasks. The uniform sector sizes visually confirm the rigorously balanced distribution of the dataset.

**Spatial, Temporal dimensions and their intersections).** Unlike prior benchmarks that often exhibit long-tail distributions leading to evaluation bias, CoW-Bench maintains strict **distributional balance**. As shown in Table 5, each of the 18 fine-grained sub-tasks contains between 69 and 91 samples (with the specific inclusion of 50 hard Maze cases). This uniformity ensures a fair and unbiased assessment across all capability dimensions, preventing models from achieving inflated scores by overfitting to simple or frequent task types.

**Fine-grained Complexity Analysis.** A core design principle of CoW-Bench is the coverage of a comprehensive difficulty gradient. We argue for the non-triviality of the tasks from three complementary dimensions. (1) *Instruction Span & Semantic Depth*. The dataset exhibits significant variance in instruction complexity. Ranging from atomic tasks like *Modal-Subj-Attr* (avg. 7.1 words) to compositional tasks like *Modal-Multi-Require* (avg. 74.8 words), this vast span (7.1–74.8 words) challenges the robustness of World Models in language understanding, requiring them to handle both explicit short commands and long-context, multi-constraint instructions. (2) *Visual & Cognitive Load*. We quantify visual complexity using the average element count per sample. Quantitative analysis reveals that cross-modal tasks generally impose a higher cognitive load (e.g., 3D-Reconstruct involves 2.1 complex elements on average, significantly higher than the 1.6 in single-modal tasks). This confirms that cross-modal tasks effectively probe the model’s retention capabilities in visually dense and structurally complex scenes. (3) *Dynamic Evolution Complexity*. Beyond static elements, the Action Complexity metric highlights the temporal richness of the benchmark. Tasks such as *Time-State* exhibit extremely high

---

Table 5: Comprehensive Statistics of CoW-Bench.

<table><thead><tr><th rowspan="2">Mode</th><th rowspan="2">Task</th><th rowspan="2">Sub-Task</th><th rowspan="2">N</th><th rowspan="2">Scene</th><th rowspan="2">Diff</th><th colspan="4">Complexity Metrics (Avg.)</th></tr><tr><th>Prmpt</th><th>ImgR</th><th>Act</th><th>Elem</th></tr></thead><tbody><tr><td rowspan="16">Single</td><td rowspan="3">Modal</td><td>Subj-Attr</td><td>91</td><td>Obj</td><td>Easy</td><td>7.1</td><td>13.3</td><td>22.0</td><td>2.1</td></tr><tr><td>Minor-Ctrl</td><td>87</td><td>Obj</td><td>Med</td><td>37.4</td><td>37.4</td><td>37.1</td><td>1.6</td></tr><tr><td>Multi-Req</td><td>81</td><td>Mix</td><td>Hard</td><td>74.8</td><td>13.6</td><td>64.7</td><td>1.6</td></tr><tr><td rowspan="3">Space</td><td>2D-Layed</td><td>89</td><td>Mix</td><td>Med</td><td>38.2</td><td>24.7</td><td>32.8</td><td>2.4</td></tr><tr><td>2D-Rel</td><td>91</td><td>Obj</td><td>Easy</td><td>42.7</td><td>17.1</td><td>52.8</td><td>1.7</td></tr><tr><td>3D</td><td>91</td><td>Room</td><td>Hard</td><td>47.1</td><td>35.0</td><td>57.4</td><td>2.2</td></tr><tr><td rowspan="3">Time</td><td>Consist</td><td>88</td><td>Obj</td><td>Med</td><td>16.4</td><td>33.8</td><td>38.6</td><td>2.1</td></tr><tr><td>Slow-Evol</td><td>81</td><td>Obj</td><td>Easy</td><td>3.3</td><td>33.3</td><td>40.1</td><td>2.9</td></tr><tr><td>State</td><td>85</td><td>Mix</td><td>Hard</td><td>8.0</td><td>31.4</td><td>77.7</td><td>2.0</td></tr><tr><td rowspan="12">Cross</td><td rowspan="3">M&times;S</td><td>2D-Layed</td><td>69</td><td>Room</td><td>Med</td><td>46.1</td><td>35.5</td><td>27.1</td><td>2.1</td></tr><tr><td>2D-Rel</td><td>76</td><td>Obj</td><td>Med</td><td>48.7</td><td>33.6</td><td>44.6</td><td>1.6</td></tr><tr><td>3D-POV</td><td>80</td><td>Out</td><td>Hard</td><td>65.4</td><td>33.5</td><td>38.5</td><td>2.4</td></tr><tr><td rowspan="3">M&times;T</td><td>Event-Rsp</td><td>86</td><td>Mix</td><td>Med</td><td>44.7</td><td>22.7</td><td>36.4</td><td>2.7</td></tr><tr><td>Prop-Cons</td><td>91</td><td>Obj</td><td>Med</td><td>47.2</td><td>28.4</td><td>37.7</td><td>1.8</td></tr><tr><td>Prop-Var</td><td>78</td><td>Mix</td><td>Hard</td><td>51.6</td><td>26.3</td><td>50.7</td><td>2.3</td></tr><tr><td rowspan="3">T&times;S</td><td>3D-Recon</td><td>80</td><td>Out</td><td>Hard</td><td>35.4</td><td>63.4</td><td>43.6</td><td>2.1</td></tr><tr><td>Maze-2D</td><td>50</td><td>Flat</td><td>Hard</td><td>12.5</td><td>15.0</td><td>35.5</td><td>3.0</td></tr><tr><td>Cam-Mask</td><td>91</td><td>Mix</td><td>Hard</td><td>10.0</td><td>27.3</td><td>57.2</td><td>2.2</td></tr></tbody></table>

action complexity (avg. 77.7 words), indicating that the generated videos contain intricate dynamic evolutions rather than simple static scene translations.

It is worth noting that the aforementioned data are not merely automated outputs but have undergone a rigorous **multi-source auditing process** (see Section 5.1.1). Through human-machine collaborative verification, we corrected metric biases and confirmed semantic alignment, establishing CoW-Bench as a reliable and reproducible gold standard in the community.

## 5.2 評估指標

**一致性能力評估。** CoW-Bench 透過將一致性能力評估正式化為*約束滿足*問題來評估世界模型的一致性能力：給定文字條件、參考圖像或初始狀態，生成的輸出必須滿足這些條件中隱含或明確陳述的約束，同時在時間和空間維度上保持穩定。不同於 FID/IS 等整體相似性或感知質量指標，世界模型中的關鍵失敗往往不表現為缺乏真實感，而是*違反或隱含地放寬約束*。典型的情況包括：將稀有材料還原為常見材料、將局部編輯擴散為全局漂移、在時間序列中逐幀重新初始化相同的世界線、在遮擋過程中反轉前景-背景關係，或在多視角條件下重新繪製不同的世界。由於這些失敗在視覺上可能顯得合理，核心評估信號必須是約束是否真正被尊重。

**原子分解。** 為了獲得可歸因、可診斷且可重複使用的評估信號，我們採用*原子分解*：將任務間的遞迴失敗模式抽象化為一組可觀察的*原子檢查*，並將每個任務家族的評估指標定義為多個原子檢查的組合。這種設計實現了兩個核心目標：(1) *可診斷性*：每個原子檢查對應一個特定的失敗機制（例如，身份漂移、屬性重新綁定、邊界洩漏、世界線漂移或遮擋矛盾），使得評分結果能夠指出問題的根源；(2) *模組化重複使用*：同一個原子在不同任務家族間保持相同的語義，確保跨任務比較在統一的測量座標系中進行。值得強調的是

---

**表 6：** CoW-Bench 中的指標家族及其五個子指標。縮寫：Id+Attr = 身份和屬性一致性；Min-change = 最小變化；Inter-state = 中間狀態有效性；Persp/Scale = 視角和尺度一致性；Occ-update = 遮擋更新合理性；Geo-self = 幾何自洽性；Excl. = 相互排斥性；Env-stab = 環境穩定性。

| Family | Focus | Sub1 | Sub2 | Sub3 | Sub4 | Sub5 |
|--------|-------|------|------|------|------|------|
| M1 | Subj-Attr | Id+Attr | Backoff | Dominance | Clarity | Excl. |
| M2 | Local-Edit | Target | Min-change | Leakage | Clarity | No-extra |
| M3 | Multi-Const | Complete | Attr-corr | Rel-corr | Omission | No-extra |
| T1 | Worldline | Subj-cons | Attr-stab | Env-stab | Visual | Evol-cont |
| T2 | Slow-Evol | Subj-lock | Trend | Time-scale | Inter-state | Rule |
| T3 | Stage-Order | Order | Identif. | Timing | Process | Worldline |
| S1 | Sem-Planar | Dir | Count | Rule | Boundary | Layout |
| S2 | Occ/Contain | Occl. | Boundary | Visible | Rel-stab | Layer |
| S3 | MV-3D | Struct | Surface | Persp/Scale | Occ-update | Geo-self |
| MS1 | Sem-Planar | Ent-match | Act-align | NT-stab | Attr-bind | Global |
| MS2 | Sem-Hier | Pos-rel | Neg-rel | Excl. | Vis+Layer | Id-stab |
| MS3 | Sem-MV | Anchor | View-stab | Lateral | Scene | Marker |
| MT1 | Long-Horizon | Init-anchor | Long-stab | Cross-scene | Attr-bind | No-unexp |
| MT2 | Attr-Dyn | Target(E,A) | Follow | Smooth | Rate | Env-stab |
| MT3 | Trigger-Event | Pre-hold | Trigger | Post-comp | State-stab | Env-stab |
| ST1 | Maze-2D | Start/Goal | Traj-cont | Legal | Correct | Struct-stab |
| ST2 | Occ-Motion | Occ-move | Parallax | Rigid | Natural | Env-stab |
| ST3 | 3D-Loop | Struct | Rel | View-smooth | Physical | Entity-stab |

CoW-Bench 包含 **18 個指標家族**（M1–ST3），而原子庫包含 **16 個原子檢查**（A1–A16）。

**指標家族和子指標** 關於每個任務家族具體測量什麼，我們首先提供它們五個對應子指標的名稱（見表 6）。這些子指標在任務家族層級提供了人類可讀的描述，並在語義上與後續原子庫一一對應：子指標捕捉任務內的焦點，而原子檢查提供跨任務的一致標準，從而在可讀性和嚴謹性之間實現平衡。

**原子庫：跨任務共享的統一標準。** 表 7 呈現了原子檢查的庫。每個原子檢查都採用操作定義，以確保評估不依賴於美學偏好，而是依賴於可驗證的現象；此外，原子庫在不同任務家族間的共享為跨任務比較提供了一致的語義基礎。

**組合定義：透過原子庫構建指標家族。** 基於原子庫，我們將每個指標家族組合地定義為被調用原子檢查的結構化聚合。表 8 呈現了指標家族對 A1–A16 的調用矩陣。此矩陣在結構層級明確暴露了模組化重複使用：同一個原子在不同任務家族間承擔相同的測量語義，從而避免了為每個任務家族冗餘定義近似指標的情況，並確保評估結果可以沿著共享的測量維度進行比較和歸因。

**評分尺度 (0–2)。** 對於每個樣本，我們為其對應指標家族調用的每個評估維度提供序序分（0–2）：0 表示明確的違反或失敗；1 表示部分滿足但存在歧義、偏差或證據不清晰；2 表示明確、穩定且無爭議的滿足。這種離散尺度與約束滿足解釋一致，減少了連續評分引入的主觀噪聲，同時為失敗模式保持診斷分辨率。在評估和結果聚合中，樣本級 0–2 分數首先用於形成每個任務內子指標的平均分

---

**表 7：** CoW-Bench 的原子庫。每個原子檢查定義了一個可重複使用、可觀察的評估標準，具有簡潔的操作單句定義，用於系統的一致性評估。

| ID | Atomic check | Operational definition |
|----|--------------|------------------------|
| A1 | Identity lock | 預期的目標實體保持不變；不發生身份交換、複製或替換。 |
| A2 | Attribute binding | 關鍵屬性保持綁定到同一實體；不發生屬性遷移。 |
| A3 | Constraint non-relaxation | 指定的約束不被削弱或替換為更常見但不等價的變體。 |
| A4 | Evidence clarity | 支持每項約束判斷的證據清晰且無歧義。 |
| A5 | Mutual exclusivity | 相互不兼容的屬性不在同一目標上共同出現。 |
| A6 | Locality of change | 變化被限制在指定的區域或屬性內，無邊界溢出。 |
| A7 | Non-target invariance | 非目標實體或區域保持穩定，除了明確允許的變化外。 |
| A8 | No spurious additions | 指令之外不出現多餘的實體、物件或部分。 |
| A9 | Set completeness | 所需實體形成具有正確基數的完整集合。 |
| A10 | Relation correctness | 指定的關係或動作得以滿足，無角色交換。 |
| A11 | Multi-constraint coverage | 多項約束聯合滿足，無選擇性遺漏。 |
| A12 | Worldline stability | 輸出描繪單一一致的世界，而非逐幀重新初始化或場景漂移。 |
| A13 | Temporal continuity | 允許的變化平穩演化，無突然跳躍或振蕩回溯。 |
| A14 | Stage structure | 當指定離散階段時，它們是可識別的，並以正確順序出現，無虛假步驟。 |
| A15 | Occlusion & layering | 深度排序和遮擋正確且無矛盾；可見邊界合理地更新。 |
| A16 | 3D geometric coherence | 多視角輸出保持可解釋為單一 3D 場景的投影，具有一致的視角和遮擋。 |

數。隨後，同一指標家族下各種子任務的平均分數以等權重聚合，得到最終分數。

**評估協議：2×2 網格時間採樣。** 對於視頻任務，我們從整個序列中按時間順序均勻採樣 4 幀。對於圖像任務，我們按時間順序生成四個關鍵圖像。這四幀或圖像按 2×2 網格排列（從左到右、從上到下）。評估者必須逐幀分析序列而不跳過，並根據與指標家族對齊的五問鏈式為每項提供理由和 0-2 分數。此協議明確地暴露了時間一致性的關鍵證據（如連續性、中間狀態、階段結構和世界線穩定性）作為可驗證現象，從而減少由選擇性觀察引起的偏差。評估提示模板如下所示。

一致性導向評估提示

**角色：** 你是評估 AI 生成結果品質的專家。
**輸入：** 按時間順序均勻採樣的四幀 2×2 網格（從左到右、從上到下）。
**任務：** 使用與指標家族對齊的五問鏈式評估序列。
**評分：** 每個問題從 0 到 2 評分，並附有理由。
**規則：** 按序列順序逐幀分析，不跳過；判斷必須僅基於採樣的幀。
**輸出：** 對於每個問題，輸出格式為：QuestionX、Score、Rationale。

---

**表 8：** 透過原子庫的指標家族組合定義。勾選標記表示指標家族調用對應的原子檢查。

| Metric family | A1 | A2 | A3 | A4 | A5 | A6 | A7 | A8 | A9 | A10 | A11 | A12 | A13 | A14 | A15 | A16 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| Attribute Fidelity (M1) | ✓ | ✓ | ✓ | ✓ | ✓ | | | | | | | | | | | |
| Local Edit Precision (M2) | | | | ✓ | | ✓ | ✓ | ✓ | | | | | | | | |
| Multi-constraint Satisfaction (M3) | | ✓ | | | | | | ✓ | ✓ | ✓ | ✓ | ✓ | | | | |
| Worldline Persistence (T1) | ✓ | ✓ | ✓ | | ✓ | | | | | ✓ | ✓ | | | | | |
| Evolutionary Dynamics (T2) | ✓ | | ✓ | | ✓ | | | | | ✓ | ✓ | | | | | |
| Ordered Stage Transitions (T3) | | | ✓ | | | | | | | ✓ | ✓ | ✓ | | | | |
| Planar Layout Correctness (S1) | | | | ✓ | ✓ | | | | ✓ | | | | | | | |
| Hierarchical Occlusion (S2) | | | ✓ | | | | | | | | ✓ | | | | | |
| Multi-view 3D Coherence (S3) | | | ✓ | | | | | | | | ✓ | ✓ | | | | |
| Semantic Role Binding (MS1) | ✓ | ✓ | ✓ | ✓ | ✓ | | ✓ | ✓ | | | | | | | | |
| Semantic Hierarchy Compliance (MS2) | | ✓ | ✓ | ✓ | | | | | ✓ | ✓ | | | | ✓ | | |
| Semantic Multi-view Stability (MS3) | ✓ | ✓ | ✓ | | ✓ | | | | | | | | | | ✓ | |
| Long-horizon Anchoring (MT1) | ✓ | ✓ | ✓ | | | ✓ | | | ✓ | | | | | | | |
| Attribute Dynamics Alignment (MT2) | ✓ | ✓ | ✓ | | | ✓ | | | | ✓ | ✓ | | | | | |
| Triggered Event Compliance (MT3) | ✓ | ✓ | ✓ | | | ✓ | | | | ✓ | ✓ | ✓ | | | | |
| Planar Maze Trajectory (ST1) | | | | ✓ | ✓ | | | | | | ✓ | | | | | |
| Occlusion Dynamics Under Motion (ST2) | | | ✓ | ✓ | | ✓ | | | ✓ | ✓ | ✓ | | | ✓ | | |
| 3D Loop Navigation Coherence (ST3) | | ✓ | ✓ | | ✓ | | | ✓ | | | ✓ | ✓ | | ✓ | | ✓ |

## 5.3 與現有基準的比較

現有多模態評估系統主要圍繞 MLLM 的理解能力構建，形成由 UniBench [512] 和 MANBench [513] 代表的標準化範例。然而，在生成式世界模型的評估方面仍存在重大維度差距。我們在三個關鍵維度上定義了 CoW-Bench 與現有工作之間的本質差異：

**判別性感知 vs. 生成式模擬。** UniBench 通過整合超過 50 個現有資料集來解決多模態評估的碎片化問題，全面評估模型在視覺感知、屬性推理和空間關係理解方面的判別能力 [512]。這種評估範例隱含假設模型充當被動觀察者，任務是解構作為輸入提供的靜態影像或影片。相比之下，CoW-Bench 針對世界模型的生成式模擬能力，將其視為主動模擬器。與強調模型是否表現出卓越的回答能力（如 MANBench [513] 中所示）不同，我們的重點在於評估模型是否能主動在動態世界演化過程中保持物理約束和因果一致性。從這個意義上講，CoW-Bench 填補了評估模型感知和推理世界的能力與其在時間上一致地構建和模擬世界的能力之間的關鍵評估空白。

**評估信號：QA 準確度 vs. 動態約束滿足。** MANBench 的核心貢獻在於建立人類表現參考框架，其中評估信號源自針對靜態標籤測量的 VQA（影片問答）準確度 [513]。然而，這種離散二元判斷（正確對錯誤）不足以捕捉生成式設置中經常出現的連續、非二元物理失敗。CoW-Bench 反而將評估制定為多因子約束滿足問題。由於 UniBench 將幻覺確認為 MLLM 的主要瓶頸 [512]，在生成式場景中此類幻覺通常表現為時空一致性的崩壞，例如物體消失後的

# 表格 9：CoW-Bench 在 18 個子任務上的主要結果（數值越高越好）；MEAN 平均所有子任務。
縮寫：SUAT=Subj-Attr、LCED=Local-Edit、MCON=Multi-Const；WLIN=Worldline、SLEV=Slow-Evol、STOR=Stage-Order；SEPL=Sem-Planar、OCCO=Occ/Contain；MV3D=MV-3D；TREV=Trigger-Event；LOHO=Long-Horizon、ATDY=Attr-Dyn；SEMV=Sem-MV；3DLO=3D-Loop；OCMO=Occ-Motion；MAZE=Maze-2D。AVG 從原始的 [0, 10] 重新縮放至百分比尺度 [0, 100]。

| Model | SUAT | LCED | MCON | WLIN | SLEV | STOR | SEPL | OCCO | MV3D | TREV | LOHO | ATDY | SEPL | OCCO | SEMV | 3DLO | OCMO | MAZE | SEPL | OCCO | SEMV | 3DLO | OCMO | MAZE | AVG |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| **Closed-Source Video-Generation Models** |
| Sora [2] | 5.16 | 8.35 | 8.38 | 9.32 | 5.80 | 6.22 | 8.41 | 6.19 | 8.51 | 6.96 | 8.12 | 5.25 | 8.64 | 5.97 | 9.49 | 8.40 | 9.25 | 4.17 | 73.66 | | | | | |
| Kling [279] | 4.11 | 8.19 | 5.63 | 9.10 | 5.17 | 5.53 | 8.72 | 7.88 | 9.32 | 7.08 | 8.71 | 6.58 | 8.20 | 6.79 | 9.44 | 8.08 | 9.30 | 5.30 | 73.96 | 11.72 | 14.92 | 10.53 | 16.87 | 21.09 | 12.79 |
| **Closed-Source Image-Generation Models** |
| GPT-image-1 [501] | 7.37 | 8.96 | 7.96 | 9.14 | 7.75 | 5.68 | 8.09 | 7.43 | 9.26 | 6.95 | 9.28 | 7.22 | 9.00 | 6.83 | 9.79 | 8.46 | 8.22 | 7.24 | 80.35 | | | | | |
| Seedream-4-0 [502] | 5.73 | 7.57 | 6.73 | 8.63 | 6.25 | 6.27 | 6.09 | 6.84 | 8.82 | 6.70 | 8.63 | 6.77 | 8.12 | 7.36 | 9.50 | 8.13 | 6.78 | 3.48 | 71.33 | 7.12 | 8.70 | 10.14 | 4.70 | 10.60 |
| Seedream-4-5 [502] | 6.69 | 7.78 | 6.91 | 8.77 | 7.46 | 6.76 | 6.27 | 7.21 | 9.06 | 7.17 | 9.11 | 6.77 | 8.04 | 7.27 | 9.59 | 8.23 | 7.51 | 2.28 | 73.82 | 7.08 | 8.78 | 10.27 | 3.83 | 9.82 |
| Nano Banana [503] | 7.19 | 8.47 | 5.63 | 8.87 | 7.86 | 6.71 | 7.64 | 7.84 | 9.34 | 7.33 | 9.66 | 6.67 | 8.68 | 7.95 | 9.20 | 8.13 | 8.76 | 5.16 | 78.38 | 8.10 | 7.89 | 12.14 | 6.62 | 11.67 |
| Nano Banana Pro [503] | 7.39 | 8.81 | 6.98 | 8.88 | 8.39 | 7.48 | 8.10 | 8.48 | 9.61 | 7.84 | 9.56 | 7.36 | 9.51 | 9.17 | 9.10 | 8.86 | 8.65 | 4.46 | 82.57 | 10.38 | 12.99 | 13.94 | 5.34 | 12.70 |
| GPT-image-1.5 [504] | 7.75 | 8.99 | 8.34 | 9.23 | 8.65 | 7.14 | 8.32 | 8.53 | 9.32 | 8.13 | 9.74 | 8.05 | 9.45 | 8.69 | 9.79 | 8.54 | 8.20 | 7.26 | 85.62 | 10.08 | 12.66 | 13.71 | 7.07 | 11.51 |
| **Open-Source Video-Generation Models** |
| Allegro [505] | 1.97 | 5.79 | 4.41 | 7.03 | 1.91 | 3.33 | 6.82 | 5.75 | 7.89 | 4.72 | 7.67 | 4.80 | 4.22 | 5.30 | 7.19 | 6.87 | 7.27 | 1.86 | 52.67 | | | | | |
| HunyuanVideo [506] | 2.91 | 6.89 | 2.41 | 8.62 | 3.06 | 2.94 | 6.52 | 5.67 | 6.04 | 4.01 | 9.52 | 3.66 | 5.83 | 4.78 | 9.64 | 6.97 | 6.79 | 2.08 | 54.63 | 8.00 | 13.12 | 17.51 | 8.43 | 18.42 |
| LTX-Video [442] | 3.59 | 6.78 | 4.54 | 8.53 | 3.67 | 3.20 | 6.83 | 6.17 | 6.49 | 4.76 | 7.34 | 4.95 | 5.48 | 5.13 | 8.77 | 6.73 | 8.67 | 1.24 | 57.15 | 6.91 | 12.90 | 17.02 | 3.66 | 17.37 |
| CogVideoX [179] | 3.75 | 5.90 | 5.82 | 8.29 | 4.13 | 3.72 | 6.61 | 5.55 | 5.70 | 5.15 | 8.66 | 5.29 | 6.44 | 5.01 | 8.93 | 6.37 | 8.23 | 2.04 | 58.66 | 7.66 | 11.30 | 17.53 | 7.11 | 21.45 |
| Easy Animate [507] | 3.78 | 7.11 | 5.10 | 8.70 | 4.33 | 3.59 | 7.35 | 6.36 | 7.81 | 4.94 | 7.85 | 5.29 | 6.01 | 5.58 | 7.95 | 6.78 | 8.71 | 2.98 | 61.23 | 6.55 | 9.72 | 15.17 | 5.98 | 23.03 |
| Wan2.2-I2V-14B [273] | 3.32 | 7.61 | 6.57 | 8.54 | 4.00 | 3.80 | 7.37 | 6.10 | 6.27 | 5.33 | 8.37 | 5.24 | 6.69 | 6.17 | 9.51 | 7.11 | 6.84 | 2.46 | 61.83 | 5.82 | 6.42 | 12.67 | 5.98 | 23.83 |
| SkyReels-V2 [508] | 3.16 | 7.45 | 5.29 | 8.89 | 4.03 | 3.74 | 7.92 | 5.80 | 7.93 | 5.39 | 8.87 | 5.66 | 7.70 | 6.75 | 9.07 | 8.18 | 8.18 | 3.66 | 65.37 | 6.60 | 6.28 | 11.89 | 2.47 | 5.95 |
| **Open-Source Image-Generation Models** |
| Qwen-Image [509] | 0.72 | 2.21 | 7.73 | 2.10 | 0.41 | 1.64 | 1.89 | 1.70 | 1.35 | 1.72 | 0.84 | 1.61 | 1.69 | 0.61 | 1.71 | 2.96 | 0.77 | 0.32 | 17.77 | | | | | |
| BAGEL [8] | 5.01 | 5.86 | 5.53 | 6.27 | 5.01 | 3.96 | 5.33 | 6.43 | 7.68 | 4.45 | 8.60 | 4.91 | 7.08 | 5.22 | 8.89 | 5.51 | 6.00 | 5.08 | 59.34 | 5.30 | 5.36 | 11.86 | 7.24 | 58.43 |
| UniVideo [510] | 4.07 | 7.27 | 4.14 | 8.58 | 3.87 | 3.08 | 6.84 | 6.15 | 6.81 | 4.29 | 8.72 | 5.69 | 6.26 | 5.09 | 9.08 | 7.34 | 7.49 | 3.16 | 59.96 | 6.06 | 7.49 | 14.10 | 4.60 | 58.59 |
| Emu3.5 [511] | 6.15 | 8.76 | 8.61 | 8.77 | 5.31 | 4.72 | 8.81 | 8.62 | 8.77 | 5.70 | 9.42 | 6.10 | 8.47 | 8.61 | 9.76 | 8.58 | 9.22 | 5.58 | 77.76 | 5.58 | 6.31 | 12.70 | 5.56 | 77.76 |

遮擋。為了解決此一限制，我們採用細粒度的原子檢驗，明確量化模型在長範圍生成中對於模態、空間和時間約束的穩健性，而不是僅依賴語義對齊。

**複雜性來源：認知深度對比時空糾纏。** MANBench 主要評估模型的高階認知能力，其中任務難度在很大程度上歸因於超越人類水準性能所需的邏輯推理深度和知識調用的廣度 [513]。相比之下，CoW-Bench 的難度源於時空動力學的內在糾纏。實驗結果表明，即使是具有強大認知推理能力的模型（例如 GPT-4V）在跨一致性任務上也表現出明顯的失敗，特別是涉及模態-時間耦合的任務（例如 $M \times T$）。這些發現表明，世界模型的核心挑戰並不在於抽象的問題解決能力，而是在於在多個相互作用的物理約束下保持連貫的動態推理。

## **5.4 主要結果**

表 9 報告了 CoW-Bench 的任務級別分數，涵蓋 18 個子任務，跨越模態、時間、空間和跨一致性政策。總體排名突出了一個清晰的趨勢：閉源圖像生成模型在平均分數上佔據主導地位，而開源視頻生成器在大多數一致性敏感任務上仍然明顯落後。特別是，GPT-image-1.5 達到了最佳的整體性能，其次是 Nano Banana Pro 和 GPT-image-1。這個差距表明，當今最強的統一多模態先驗已經編碼了豐富的靜態世界規律性，但當一致性約束要求長期視角、多因素執行時，仍然面臨系統性故障模式。

---

(1) **時間控制是瓶頸，而非一致性。** 在多個模型系列中，T-WL（世界線持續性）即使對於多個視頻模型也保持一致地高（例如，Sora 達到 9.32），這表明生成視覺連續的視頻不再是最難的部分。然而，要求基於規則的演化或結構化狀態進展的時間任務顯示了更加不均勻的景況（例如，T-Rule 和 T-Stage-Order 在模型間差異很大）。這種分離支持了 CoW-Bench 的一個關鍵論點：世界模型需要隨時間的約束滿足，而不僅僅是平滑性。一個模型可以看起來在時間上合理，同時仍然違反因果約束。

(2) **空間一致性在單視圖 3D 中表現強勁，但跨視圖錨定仍然存在問題。** 大多數頂級模型在 S-3D（單場景 3D 合理性）上得分很高，幾個超過 9.0（例如，Nano Banana Pro 達到 9.61）。然而，跨一致性任務揭示了更緊密的瓶頸：儘管 MS-3D（文本到 3D 視點控制）對於領先模型仍然保持高分（通常 ≥ 9），TS-Maze-2D 和一些時空設置仍然低得多。這種模式表明，局部幾何合理性比在運動和決策類軌跡下維持全局錨定的空間結構更容易。

(3) **融合任務揭示了真正的世界模型差距：動態下的持久語義。** 最先進的模型與其他模型之間的最強分離發生在跨一致性族群（MT、MS、TS）中。例如，領先模型在 MT-PropKeep（時間演化下的屬性持續性）上獲得接近天花板的性能，但在 MT-PropChange（屬性變化對齊）上性能下降，特別是在 TS-Maze-2D（導航風格結構保留）上。值得注意的是，一些高平均分模型在 TS-Maze-2D 上仍然表現出明顯的弱點（例如，Nano Banana Pro 報告 4.46），表明全局世界狀態維護和軌跡級別約束執行即使在單幀保真度優秀時仍然未解決。這正是 UMM 必須從感知生成器演變為真正內部模擬器的領域。

(4) **開源模型暴露了與 CoW-Bench 動機相符的故障模式。** 開源視頻生成器通常在模態基礎（M-Subj-Attr）和跨一致性任務上表現不佳，與定性觀察一致，即它們要麼 (i) 將罕見約束放寬為常見預設，要麼 (ii) 在保留運動的同時漂移身份/屬性。同時，開源圖像模型表現出大方差：Emu3.5 在許多列中具有競爭力，但仍在以時間為中心和時空設置上下降，強化了 CoW-Bench 針對單次合理性和多步一致性之間差距的定位。

**要點。** UMM 在靜態或單視圖設置中表現良好，其中局部合理性就足夠了。然而，當任務要求在變化隨時間和空間展開時維持穩定的世界，性能會急劇下降。這些跨一致性場景是是否真正表現為世界模型而非幀生成器的最清晰指標。

## 5.5 單軸一致性

### 5.5.1 模態一致性結果

表 10 報告了跨三個指標族的模態一致性效能：主體屬性保真度（M1）、局部編輯精度（M2）以及多約束滿足（M3）。總體而言，該表強化了 CoW-Bench 的核心動機：即使生成結果看起來合理，模型也經常會削弱、誤繫結或默默重新詮釋指定的條件，這正是世界模型介面無法承受的失敗模式。

（1）身分和屬性繫結是最難的模態介面基元。在幾乎所有模型組別中，Id+Attr 明顯低於其他 M1 維度。即使是頂級的閉源圖像

# 一致性三位一體作為通用世界模型的定義原則

表 10：CoW-Bench 上的模態一致性結果（0–2 刻度；數值越高越好），分為三個指標族：**M1** 主體–屬性保真度（IDAT、BKOF、DOMN、CLAR、EXCL），**M2** 局部編輯精度（TARG、MNCH、LEAK、CLAR、NEXA），以及 **M3** 多約束滿足（CMPL、ATCO、RLCO、OMIS、NEXA）。縮寫說明詳見表 9。BKOF 衡量罕見約束是否被常見預設值替換，而 NEXA 對超出指令的虛假添加進行懲罰。

| 模型 | 主體–屬性（SUAT） ||||||| 局部編輯（LCED） ||| 多約束（MCON） ||||
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| | IDAT | BKOF | DOMN | CLAR | EXCL | TARG | MNCH | LEAK | CLAR | NEXA | CMPL | ATCO | RLCO | OMIS | NEXA |
| **閉源視頻生成模型** |||||||||||||||||
| Sora [2] | 0.40 | 1.48 | 0.91 | 1.15 | 1.22 | 1.12 | 1.96 | 1.82 | 1.45 | 2.00 | 1.96 | 1.67 | 1.32 | 1.59 | 1.84 |
| Kling [279] | 0.33 | 1.32 | 0.58 | 0.95 | 0.93 | 1.23 | 1.82 | 1.74 | 1.49 | 1.91 | 1.36 | 1.14 | 0.74 | 0.86 | 1.53 |
| **閉源圖像生成模型** |||||||||||||||||
| GPT-image-1 [501] | 0.96 | 1.71 | 1.69 | 1.26 | 1.75 | 1.40 | 1.98 | 1.91 | 1.73 | 1.95 | 1.89 | 1.45 | 1.34 | 1.47 | 1.80 |
| GPT-image-1.5 [504] | 1.19 | 1.76 | 1.77 | 1.35 | 1.68 | 1.50 | 1.94 | 1.92 | 1.73 | 1.90 | 1.89 | 1.51 | 1.49 | 1.66 | 1.79 |
| Seedream-4-0 [502] | 0.94 | 1.44 | 1.35 | 0.89 | 1.10 | 1.25 | 1.77 | 1.70 | 1.54 | 1.31 | 1.78 | 1.44 | 1.28 | 1.44 | 0.78 |
| Seedream-4-5 [502] | 1.10 | 1.53 | 1.52 | 1.16 | 1.38 | 1.43 | 1.84 | 1.78 | 1.54 | 1.20 | 1.82 | 1.41 | 1.24 | 1.45 | 0.99 |
| Nano Banana [503] | 1.17 | 1.59 | 1.62 | 1.22 | 1.59 | 1.36 | 1.79 | 1.77 | 1.67 | 1.89 | 1.38 | 1.04 | 0.89 | 1.09 | 1.23 |
| Nano Banana Pro [503] | 1.25 | 1.68 | 1.72 | 1.22 | 1.52 | 1.55 | 1.80 | 1.81 | 1.71 | 1.95 | 1.63 | 1.42 | 1.21 | 1.32 | 1.40 |
| **開源視頻生成模型** |||||||||||||||||
| Allegro [505] | 0.10 | 0.60 | 0.24 | 0.47 | 0.56 | 0.70 | 1.31 | 1.28 | 0.83 | 1.67 | 1.19 | 0.82 | 0.59 | 0.70 | 1.11 |
| Easy Animate [507] | 0.13 | 1.23 | 0.55 | 0.90 | 0.97 | 0.71 | 1.76 | 1.67 | 1.11 | 1.86 | 1.51 | 0.95 | 0.68 | 0.87 | 1.09 |
| CogVideoX [179] | 0.09 | 1.29 | 0.53 | 0.98 | 0.86 | 0.70 | 1.53 | 1.26 | 0.77 | 1.64 | 1.69 | 1.22 | 0.65 | 0.95 | 1.31 |
| Wan2.2-I2V-14B [273] | 0.11 | 1.10 | 0.40 | 0.81 | 0.90 | 0.83 | 1.87 | 1.77 | 1.32 | 1.82 | 1.68 | 1.38 | 1.11 | 1.32 | 1.08 |
| SkyReels-V2 [508] | 0.14 | 1.00 | 0.38 | 0.76 | 0.88 | 0.77 | 1.80 | 1.63 | 1.34 | 1.91 | 1.20 | 1.05 | 0.67 | 0.93 | 1.44 |
| HunyuanVideo [506] | 0.01 | 1.23 | 0.31 | 0.76 | 0.60 | 0.30 | 1.97 | 1.64 | 0.98 | 2.00 | 0.28 | 0.23 | 0.09 | 0.12 | 1.69 |
| LTX-Video [442] | 0.15 | 1.20 | 0.49 | 0.93 | 0.82 | 0.62 | 1.80 | 1.48 | 0.97 | 1.91 | 1.38 | 0.96 | 0.52 | 0.69 | 0.99 |
| **開源圖像生成模型** |||||||||||||||||
| BAGEL [8] | 0.73 | 1.13 | 1.16 | 0.63 | 1.36 | 1.01 | 1.30 | 1.14 | 0.79 | 1.62 | 1.53 | 1.01 | 0.75 | 0.85 | 1.39 |
| UniVideo [510] | 0.27 | 1.09 | 0.77 | 0.74 | 1.20 | 0.79 | 1.90 | 1.57 | 1.07 | 1.94 | 0.79 | 0.65 | 0.35 | 0.89 | 1.46 |
| Emu3.5 [511] | 0.81 | 1.37 | 1.25 | 1.03 | 1.69 | 1.41 | 1.90 | 1.86 | 1.70 | 1.89 | 1.87 | 1.79 | 1.68 | 1.74 | 1.53 |
| Qwen-Image [509] | 0.00 | 0.18 | 0.04 | 0.02 | 0.48 | 0.10 | 0.40 | 0.29 | 0.09 | 1.33 | 1.84 | 1.66 | 1.49 | 1.60 | 1.14 |

---

**** (4) 多約束滿足強調完整性和角色綁定，而非僅增加更多文字。**M3** 揭示了不同的瓶頸。像 Emu3.5 這樣的強大模型在完整性/屬性相關性/關係相關性（1.87/1.79/1.68）上始終保持較高分數，而某些系統則呈現不均衡的配置：例如，Qwen-Image 在完整性和屬性/關係分數上相對較高，但在 M1 身份綁定上表現極其糟糕。這種不匹配表明，如果模型無法維持這些約束的穩定指涉對象，僅滿足多個列表約束是不夠的。同時，幾個模型在 M3 下的無額外添加上表現下降（例如 Seedream 變體），表明在合成壓力下它們可能會引入虛假實體——這種錯誤對下游規劃和驗證特別有害。 ****

**要點。** 在 SUAT 任務中，模型在統一對齊不同模態間的約束時經常出現歧義。它無法準確從文本和圖像中提取對應的約束以指導按要求生成。相反，它將來自兩種模態的提取約束混合成單一的、混亂的約束來指導生成，導致結果混亂。

## 5.5.2 時間一致性結果

表 **11** 報告了三個指標族上的時間一致性性能：**T1** 世界線持久性，**T2** 規則引導的緩慢演變，以及 **T3** 有序的階段轉移。出現了兩個與 CoW-Bench 中心論題相一致的模式：時間合理性不等同於時間約束滿足，最難的失敗出現在模型必須強制執行結構化動態而非僅維持視覺連續性時。

世界線持久性相對較強，即使對許多視頻生成器也是如此。大多數閉源視頻模型在 T1 上的分數接近上限（例如，Sora：高環境穩定性和視覺性），幾個開源視頻模型也達到了紮實的 T1 配置（例如，SkyReels-V2 和 Wan2.2-I2V）。這表明維持穩定的場景佈局和避免逐幀重新初始化正在成為高容量生成器的大體上已解決的能力。

**主要瓶頸是規則遵循演變，而非連續性。** 相比之下，T2 對許多視頻模型暴露了在趨勢、時間尺度和州間轉移上的急劇下降（通常低於 0.6），即使當主體鎖定高時也是如此。這個差距指示了一個常見的失敗模式：模型保持相同的主體和背景，但卻未能實現具有可識別中間狀態的單調、正確節奏的過程。值得注意的是，強大的閉源圖像模型在 T2 分數上表現明顯更高（例如，GPT-image-1.5 在趨勢/時間尺度/州間轉移上維持高值），表明更強大的指令遵循先驗在時間約束按語義表達且必須在整個序列中被遵守時提供幫助。

**要點。** 我們發現在 STOR 任務中，圖像生成模型普遍把握了時間上的整體進展。然而，不同狀態之間的轉移呈現出明顯的不連續性而非平滑演變，在狀態間出現序列反轉的情況，使得難以維持一致的過程和內容。相反，在 SLEV 任務中，圖像生成模型展示了對世界規則更高程度的理解和遵守。視頻生成模型則表現出對規則的概率遵守，對相同或相似的情景任務產生發散的結果。

舞台排序在明確的多步驟結構下保持脆弱。對於 T3，最弱的列集中在順序和識別上，尤其是對開源視頻模型（通常接近 0.3 或更低）。即使當 T3 末尾的世界線保持高位時，低順序/識別也意味著序列

---

Algorithms.png

表 11：CoW-Bench 上的時間一致性結果（0–2 刻度；數值越高越好）。我們報告三個指標族：**T1** 世界線持久性（SBJC、ATST、ENST、VISU、EVCT），**T2** 規則引導的緩慢演變（SBJL、TREN、TSCL、INTS、RULE），以及 **T3** 有序的階段轉移（ORDR、IDEN、TIME、PROC、WLIN）。縮寫說明詳見表 9。INTS 評估可信的中間狀態的可見性，而 TSCL 衡量演變速度是否符合提示指定的過程。

| 模型 | 世界線（WLIN） ||||| 緩慢演變（SLEV） ||||| 階段順序（STOR） ||||
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| | SBJC | ATST | ENST | VISU | EVCT | SBJL | TREN | TSCL | INTS | RULE | ORDR | IDEN | TIME | PROC | WLIN |
| **閉源視頻生成模型** |||||||||||||||||
| Sora [2] | 1.87 | 1.84 | 1.94 | 1.91 | 1.76 | 1.95 | 0.96 | 0.99 | 0.91 | 0.99 | 0.93 | 0.91 | 1.12 | 1.42 | 1.84 |
| Kling [279] | 1.85 | 1.82 | 1.93 | 1.86 | 1.64 | 1.98 | 0.80 | 0.73 | 0.70 | 0.96 | 0.94 | 0.81 | 0.76 | 1.13 | 1.89 |
| **閉源圖像生成模型** |||||||||||||||||
| GPT-image-1 [501] | 1.75 | 1.84 | 1.93 | 1.91 | 1.71 | 1.98 | 1.49 | 1.43 | 1.49 | 1.38 | 0.79 | 0.86 | 1.06 | 1.19 | 1.79 |
| GPT-image-1.5 [504] | 1.77 | 1.86 | 1.91 | 1.93 | 1.76 | 1.98 | 1.66 | 1.69 | 1.70 | 1.62 | 1.26 | 1.18 | 1.35 | 1.45 | 1.90 |
| Seedream-4-0 [502] | 1.70 | 1.66 | 1.85 | 1.84 | 1.59 | 1.75 | 1.14 | 1.14 | 1.05 | 1.17 | 1.14 | 1.13 | 1.05 | 1.24 | 1.71 |
| Seedream-4-5 [502] | 1.71 | 1.76 | 1.89 | 1.78 | 1.63 | 1.85 | 1.49 | 1.40 | 1.40 | 1.32 | 1.29 | 1.20 | 1.12 | 1.32 | 1.83 |
| Nano Banana [503] | 1.71 | 1.72 | 1.84 | 1.84 | 1.76 | 1.88 | 1.54 | 1.51 | 1.53 | 1.41 | 1.20 | 1.23 | 1.12 | 1.35 | 1.80 |
| Nano Banana Pro [503] | 1.74 | 1.73 | 1.83 | 1.89 | 1.69 | 1.90 | 1.65 | 1.65 | 1.64 | 1.54 | 1.40 | 1.37 | 1.41 | 1.48 | 1.83 |
| **開源視頻生成模型** |||||||||||||||||
| Allegro [505] | 1.38 | 1.37 | 1.57 | 1.45 | 1.26 | 0.90 | 0.27 | 0.23 | 0.19 | 0.32 | 0.64 | 0.29 | 0.36 | 0.74 | 1.30 |
| Easy Animate [507] | 1.69 | 1.69 | 1.90 | 1.82 | 1.60 | 1.98 | 0.60 | 0.54 | 0.49 | 0.72 | 0.50 | 0.33 | 0.26 | 0.70 | 1.80 |
| CogVideoX [179] | 1.63 | 1.56 | 1.90 | 1.67 | 1.53 | 1.85 | 0.59 | 0.49 | 0.46 | 0.74 | 0.69 | 0.30 | 0.27 | 0.63 | 1.83 |
| Wan2.2-I2V-14B [273] | 1.76 | 1.70 | 1.86 | 1.78 | 1.44 | 1.88 | 0.56 | 0.43 | 0.38 | 0.75 | 0.71 | 0.42 | 0.26 | 0.65 | 1.76 |
| SkyReels-V2 [508] | 1.79 | 1.74 | 1.84 | 1.90 | 1.62 | 1.86 | 0.57 | 0.48 | 0.48 | 0.64 | 0.61 | 0.35 | 0.26 | 0.71 | 1.81 |
| HunyuanVideo [506] | 1.87 | 1.77 | 1.86 | 1.86 | 1.26 | 2.00 | 0.22 | 0.21 | 0.11 | 0.52 | 0.50 | 0.08 | 0.06 | 0.48 | 1.82 |
| LTX-Video [442] | 1.76 | 1.67 | 1.85 | 1.76 | 1.49 | 1.65 | 0.53 | 0.49 | 0.40 | 0.60 | 0.52 | 0.18 | 0.18 | 0.55 | 1.77 |
| **開源圖像生成模型** |||||||||||||||||
| BAGEL [8] | 1.31 | 1.13 | 1.57 | 1.20 | 1.06 | 1.49 | 1.01 | 0.81 | 0.91 | 0.79 | 0.69 | 0.59 | 0.67 | 0.96 | 1.05 |
| UniVideo [510] | 1.82 | 1.74 | 1.90 | 1.79 | 1.33 | 1.99 | 0.48 | 0.41 | 0.32 | 0.67 | 0.31 | 0.19 | 0.17 | 0.60 | 1.81 |
| Emu3.5 [511] | 1.66 | 1.72 | 1.91 | 1.85 | 1.63 | 1.95 | 0.84 | 0.74 | 0.78 | 1.00 | 0.54 | 0.59 | 0.73 | 1.13 | 1.73 |
| Qwen-Image [509] | 0.22 | 0.22 | 0.58 | 0.70 | 0.38 | 0.28 | 0.07 | 0.04 | 0.00 | 0.02 | 0.26 | 0.08 | 0.02 | 0.44 | 0.84 |

可能保持在一個世界內但無法可靠地實現預期的離散階段結構。這一發現激發了 CoW-Bench 的分解：一個模型可以在時間上穩定，同時仍然違反高級時間邏輯。

### **5.5.3 空間一致性結果**

表 12 報告了 S1 語義平面、S2 遮擋/包含關係和 S3 多視圖 3D 一致性的空間一致性。這些結果呼應了 CoW-Bench 的核心觀點：空間世界建模不僅關於在單一畫面中生成合理的幾何結構，更關於維持在遮擋、包含關係和視點變化等交互作用下可驗證的結構約束。

(1) 平面佈局是入門級測試，但方向性定位仍然脆弱。大多數模型在佈局上得分相對較高（通常 ≥1.8），表明生成全局一致的 2D 構圖越來越可靠。相比之下，方向性（Dir）在模型族中始終是最低的子指標（例如 Sora：0.64；多個開源視訊模型 ≤0.5；Qwen-Image：0.02）。這個差距表明模型可以維持視覺上穩定的排列，同時仍然無法以高保真度執行明確的方向性約束（左/右/內/外）。對於世界模型來說，方向性定位

---

**表 12：** CoW-Bench 上的空間一致性結果（0-2 量表；越高越好）。我們報告三個指標族群：**S1** 語義平面（DIRC、COUNT、RULE、BNDY、LAYT），**S2** 遮擋/包含關係（OCCL、BNDY、VISB、RSTB、LAYR），和 **S3** 多視圖 3D 一致性（STRC、SURF、PSCL、OUPD、GEOS）。VISB 評估可見區域是否與隱含的遮擋關係相符，而 OUPD 測量遮擋邊界在視點變化下是否合理地更新。

<table><thead><tr><th rowspan="3">模型</th><th colspan="4">語義平面（SEPL）</th><th colspan="5">遮擋-包含（OCCO）</th><th colspan="5">多視圖-3D</th></tr><tr><th>DIRC</th><th>COUNT</th><th>RULE</th><th>BNDY</th><th>LAYT</th><th>OCCL</th><th>BNDY</th><th>VISB</th><th>RSTB</th><th>LAYR</th><th>STRC</th><th>SURF</th><th>PSCL</th><th>OUPD</th><th>GEOS</th></tr><tr><th colspan="14" style="text-align:center;">閉源視訊生成模型</th></tr></thead><tbody><tr><td>Sora [2]</td><td>0.64</td><td>1.22</td><td>1.00</td><td>1.47</td><td>1.88</td><td>1.51</td><td>1.49</td><td>1.74</td><td>1.76</td><td>1.91</td><td>1.77</td><td>1.72</td><td>1.56</td><td>1.67</td><td>1.79</td></tr><tr><td>Kling [279]</td><td>1.10</td><td>1.52</td><td>1.53</td><td>1.82</td><td>1.91</td><td>1.67</td><td>1.62</td><td>1.70</td><td>1.78</td><td>1.95</td><td>1.93</td><td>1.85</td><td>1.80</td><td>1.86</td><td>1.88</td></tr><tr><th colspan="14" style="text-align:center;">閉源圖像生成模型</th></tr><tr><td>GPT-image-1 [501]</td><td>0.81</td><td>1.58</td><td>1.45</td><td>1.66</td><td>1.92</td><td>1.54</td><td>1.37</td><td>1.55</td><td>1.75</td><td>1.89</td><td>1.91</td><td>1.85</td><td>1.77</td><td>1.86</td><td>1.88</td></tr><tr><td>GPT-image-1.5 [504]</td><td>1.36</td><td>1.70</td><td>1.62</td><td>1.87</td><td>1.98</td><td>1.59</td><td>1.46</td><td>1.57</td><td>1.83</td><td>1.87</td><td>1.92</td><td>1.89</td><td>1.81</td><td>1.85</td><td>1.85</td></tr><tr><td>Seedream-4-0 [502]</td><td>0.96</td><td>1.31</td><td>1.28</td><td>1.51</td><td>1.79</td><td>1.07</td><td>0.91</td><td>1.05</td><td>1.30</td><td>1.77</td><td>1.90</td><td>1.78</td><td>1.59</td><td>1.77</td><td>1.78</td></tr><tr><td>Seedream-4-5 [502]</td><td>1.13</td><td>1.40</td><td>1.37</td><td>1.66</td><td>1.66</td><td>1.22</td><td>1.00</td><td>1.06</td><td>1.31</td><td>1.69</td><td>1.87</td><td>1.78</td><td>1.77</td><td>1.82</td><td>1.82</td></tr><tr><td>Nano Banana [503]</td><td>1.23</td><td>1.59</td><td>1.41</td><td>1.71</td><td>1.89</td><td>1.35</td><td>1.36</td><td>1.43</td><td>1.69</td><td>1.82</td><td>1.96</td><td>1.90</td><td>1.75</td><td>1.87</td><td>1.87</td></tr><tr><td>Nano Banana Pro [503]</td><td>1.54</td><td>1.56</td><td>1.66</td><td>1.79</td><td>1.93</td><td>1.52</td><td>1.46</td><td>1.58</td><td>1.70</td><td>1.84</td><td>1.94</td><td>1.94</td><td>1.92</td><td>1.90</td><td>1.91</td></tr><tr><th colspan="14" style="text-align:center;">開源視訊生成模型</th></tr><tr><td>Allegro [505]</td><td>0.69</td><td>1.14</td><td>0.91</td><td>1.26</td><td>1.75</td><td>1.29</td><td>1.18</td><td>1.26</td><td>1.39</td><td>1.70</td><td>1.58</td><td>1.65</td><td>1.48</td><td>1.59</td><td>1.59</td></tr><tr><td>Easy Animate [507]</td><td>0.44</td><td>1.41</td><td>1.12</td><td>1.53</td><td>1.86</td><td>1.34</td><td>1.26</td><td>1.37</td><td>1.47</td><td>1.91</td><td>1.68</td><td>1.52</td><td>1.54</td><td>1.56</td><td>1.51</td></tr><tr><td>CogVideoX [179]</td><td>0.48</td><td>1.10</td><td>0.95</td><td>1.18</td><td>1.84</td><td>1.30</td><td>1.16</td><td>1.13</td><td>1.26</td><td>1.76</td><td>1.42</td><td>1.23</td><td>1.08</td><td>1.01</td><td>0.96</td></tr><tr><td>Wan2.2-I2V-14B [273]</td><td>0.29</td><td>1.41</td><td>1.03</td><td>1.48</td><td>1.89</td><td>1.44</td><td>1.16</td><td>1.34</td><td>1.54</td><td>1.89</td><td>1.58</td><td>1.25</td><td>1.09</td><td>1.20</td><td>1.15</td></tr><tr><td>SkyReels-V2 [508]</td><td>0.37</td><td>1.27</td><td>0.98</td><td>1.38</td><td>1.80</td><td>1.40</td><td>1.43</td><td>1.49</td><td>1.66</td><td>1.94</td><td>1.67</td><td>1.67</td><td>1.46</td><td>1.62</td><td>1.51</td></tr><tr><td>HunyuanVideo [506]</td><td>0.15</td><td>1.34</td><td>0.92</td><td>1.38</td><td>1.88</td><td>1.15</td><td>0.81</td><td>1.18</td><td>1.44</td><td>1.94</td><td>1.75</td><td>1.15</td><td>0.89</td><td>0.98</td><td>1.27</td></tr><tr><td>LTX-Video [442]</td><td>0.64</td><td>1.35</td><td>1.14</td><td>1.30</td><td>1.74</td><td>1.30</td><td>1.08</td><td>1.29</td><td>1.37</td><td>1.79</td><td>1.57</td><td>1.33</td><td>1.21</td><td>1.13</td><td>1.25</td></tr><tr><th colspan="14" style="text-align:center;">開源圖像生成模型</th></tr><tr><td>BAGEL [8]</td><td>0.56</td><td>1.52</td><td>1.10</td><td>1.47</td><td>1.78</td><td>1.17</td><td>0.98</td><td>0.85</td><td>0.90</td><td>1.43</td><td>1.68</td><td>1.58</td><td>1.42</td><td>1.49</td><td>1.51</td></tr><tr><td>UniVideo [510]</td><td>0.36</td><td>1.38</td><td>1.10</td><td>1.42</td><td>1.89</td><td>1.29</td><td>1.18</td><td>1.34</td><td>1.28</td><td>1.75</td><td>1.74</td><td>1.38</td><td>1.12</td><td>1.23</td><td>1.34</td></tr><tr><td>Emu3.5 [511]</td><td>1.41</td><td>1.63</td><td>1.81</td><td>1.81</td><td>1.96</td><td>1.78</td><td>1.64</td><td>1.67</td><td>1.81</td><td>1.91</td><td>1.82</td><td>1.78</td><td>1.72</td><td>1.72</td><td>1.73</td></tr><tr><td>Qwen-Image [509]</td><td>0.02</td><td>0.15</td><td>0.08</td><td>0.15</td><td>1.30</td><td>0.24</td><td>0.17</td><td>0.15</td><td>0.21</td><td>1.12</td><td>0.54</td><td>0.26</td><td>0.12</td><td>0.25</td><td>0.18</td></tr></tbody></table>

是一項核心介面需求，因為它將語言轉化為可測試的空間關係。

(2) **遮擋/包含關係在很大程度上是已學習的，但可見部分證據是薄弱環節。** 閉源模型在圖層和相對穩定性上表現強勢（通常 ~1.8-1.95），意味著它們通常在沒有明顯矛盾的情況下保持一致的深度順序。然而，可見性顯示更寬的分佈，尤其是開源圖像模型（例如 BAGEL：0.85；Qwen-Image：0.15）。這種模式表明模型可能捕捉粗略的分層意圖，同時在操作證據上失敗——實際可見的部分是否與隱含的遮擋邊界相符。這正是 CoW-Bench 設計用來揭示的那種看起來合理但違反可檢查約束的失敗。

(3) **多視圖 3D 一致性將幾何可信度與世界狀態不變性分離開來。** 頂級閉源圖像模型在結構和透視/尺度上達到接近天花板的分數（例如 Nano Banana Pro：1.94/1.92；GPT-image-1.5：1.92/1.81），表明單個物體 3D 可信度強。然而多個開源視訊模型在遮擋更新和幾何自洽性上大幅下降（例如 CogVideoX：1.01/0.96），表明它們在跨視點更新遮擋和維持自洽 3D 解釋方面困難。這支持了一項關鍵的世界模型含義：生成可信的視圖比維持一個在視點變化中存活的持續 3D 場景假說更容易。

---

**要點。** 在多視圖 3D 中，圖像生成模型往往錯誤地將鏡像對稱應用於場景以模擬不同的視點，而不是準確捕捉同一場景內的不同視角。此外，當切換視點時，同一物體在不同角度的詳細屬性通常無法保持一致。在處理遮擋-包含關係任務時，模型頻繁地生成違反常識的不現實變化，例如物體相互穿透、融合在一起，或表現出不自然的運動以維持階層關係。

# 5.6 跨軸一致性

Cross-axis consistency refers to the property that the model's predictions remain stable when the input data is transformed along different axes. This concept is crucial for ensuring robust and generalizable machine learning models.

跨軸一致性是指當輸入資料沿著不同軸進行變換時，模型的預測保持穩定的特性。這個概念對於確保機器學習模型的穩健性和泛化能力至關重要。

## 5.6.1 Definition and Motivation

## 5.6.1 定義與動機

The formal definition of cross-axis consistency can be expressed as follows: for a model $f$ and input $x$, if we apply a transformation $T_i$ along axis $i$, the prediction should satisfy:

對於模型 $f$ 和輸入 $x$ 的跨軸一致性的正式定義可以表示如下：若我們沿著軸 $i$ 應用變換 $T_i$，預測應滿足：

$$f(T_i(x)) \approx f(x)$$

This property ensures that the model's behavior is not arbitrarily dependent on the coordinate system chosen.

此特性確保模型的行為不會任意依賴於所選的坐標系。

## 5.6.1 Modal-Space 一致性結果：語義至幾何綁定

Figure 35 視覺化 Modal-Space 一致性，其中模型必須將語言約束（實體、屬性、關係）映射到可執行的空間角色，並在佈局和視角變化下保持其可驗證性。該圖支持 CoW-Bench 的核心目標：區分看起來合理的與約束忠實的語義空間接地。

Figure 35：CoW-Bench 上的 Modal-Space 一致性，以熱力圖形式呈現，行為子指標，列為模型，分數在 0-2 刻度上（越高越好）。行分組為 **Sem-Planar**（Ent-match、Act-align、NT-stab、Attr-bind、Global）、**Sem-Hier**（Pos-rel、Neg-rel、Excl.、Vis+Layer、Id-stab）和 **Sem-MV**（Anchor、View-stab、Lateral、Scene、Marker）。

**語義角色綁定是主要的瓶頸。** 在許多模型中，最顯著的效能下降出現在 Act-align 和 Pos-rel 上。這表明經常出現以下失敗：(i) 將指定的動作/關係綁定到錯誤的實體，以及 (ii) 無法精確實現建設性的正向空間關係。重要的是，這些失敗可以與幾何相關線索上的強分數共存（例如 Id-stab、Scene），產生特徵性的「看起來合理但綁定錯誤」的結果：場景是一致的，但約束附加到了錯誤的物體或只是微弱反映。

**避免違反通常比構建精確關係更容易。** 對於廣泛的中級模型，Neg-rel 和 Excl. 明顯強於 Pos-rel。這種不對稱性表明模型更可靠地避免禁止配置，而不是強制實現精確的所需放置。對於世界模型使用，這種差距很重要，因為規劃和驗證依賴於建設性滿足（將正確實體放在正確角色中），而不僅僅是沒有明顯違反。

---

**多視角語義穩定性對頂級模型來說很強，但仍然暴露了尾端風險失敗。** Sem-MV 區塊（Anchor、View-stab、Lateral、Scene、Marker）對於領先的閉源影像模型和競爭系統通常很高，表明穩定的參考框架和視角變化下的身份標記越來越容易達到。然而，熱力圖也顯示較弱的模型可能在這些錨點上災難性地失敗，這使得多視角穩定性成為模型是否維持不變場景狀態而不是每個視角都重新繪製新世界的敏感探針。

**要點。** 在處理 3D 透視問題時，模型也傾向於生成鏡像對稱結果。此外，在透視轉變過程中，非主要內容可能保留其原始視角，導致部分物體透視偏移。

**5.6.2 Modal–Time 一致性結果：執行時間程序**

Figure 36 視覺化 Modal–Time 一致性，其中模型必須 (i) 在長時間範圍內保持語言指定的錨點穩定，(ii) 執行提示指定的屬性動態，以及 (iii) 對離散觸發事件做出反應，而不破壞世界線。熱力圖支持 CoW-Bench 的核心目標：將視覺上合理的時間輸出與*約束忠實的*時間執行分離。

**Figure 36：** CoW-Bench 上的 Modal–Time 一致性，以熱力圖形式呈現，行為子指標，列為模型，分數在 0–2 刻度上（越高越好）。行涵蓋 **Long-Horizon** 錨定（Init-anchor、Long-stab、Cross-scene、Attr-bind、No-unexp）、**Attr-Dyn** 對齊（Target(E,A)、Follow、Smooth、Rate、Env-stab）和 **Trigger-Event** 合規性（Pre-hold、Trigger、Post-comp、State-stab、Env-stab）。

**錨定通常很強；主要的變異集中在最弱的系統中。** Long-Horizon 區塊對於領先的閉源影像模型一致很高，並對許多影片生成器保持競爭力，表明一旦錨點可觀察，持久的身份/屬性錨定通常是可達到的。最顯著的失敗表現為孤立的低分列（例如，最弱模型中的 Init-anchor/Long-stab 非常低），然後與下游時間控制故障相關。

**動態屬性是主要的瓶頸，以指令跟隨和速率控制為主。** 在 Attr-Dyn 中，Env-stab 對大多數模型保持在高端附近，而 Follow 和 Rate 仍然明顯較低——特別是對於影片生成器。這種模式表明一個常見的失敗模式：

---

模型保持場景穩定，但無法可靠地執行指定的演化（方向/時程/步調），產生看起來光滑但違反語義時間承諾的序列。

**觸發事件暴露時間和事件後持久性失敗。** 在 Trigger-Event 中，Pre-hold 通常相對較強，但 *Trigger* 和 *Post-comp* 對許多影片模型明顯降級，揭示兩個相關的問題：事件在正確的時間沒有變得顯著，事件後狀態沒有持久。這些錯誤對規劃風格的用途特別有害，其中離散事件用作因果檢查點。

**<u>要點。</u>** 影片生成模型更容易引入文字約束中未指定的狀態條件，導致混亂的變化。相比之下，影像生成模型展示了對文本指令約束的更高合規性。

**5.6.3 Time-Space 一致性結果：導航暴露缺失的世界狀態**

Time-Space 一致性評估模型在執行時間擴展運動時是否維持不變的空間結構。Figure 37 視覺化在 ST1–ST3 上的效能，並突出與 CoW-Bench 動機一致的中心訊息：模型可以實現強*局部*運動合理性，甚至穩定的環境，但當任務需要持久的、目標導向的世界狀態時失敗。

Figure 37：CoW-Bench 上的 Time-Space 一致性，以熱力圖形式呈現，行為子指標，列為模型，分數在 0–2 刻度上（越高越好）。行涵蓋三個指標族：ST1 Maze-2D（Start/Goal、Traj-cont、Legal、Correct、Struct-stab）、ST2 運動下的遮擋動態（Occ-move、Parallax、Rigid、Natural、Env-stab）和 ST3 3D 迴圈導航（Struct、Rel、View-smooth、Physical、Entity-stab）。

**(1) Maze-2D 仍然是最清晰的鑑別器。** 在 ST1 中，許多影片生成器在 Legal 上獲得非平凡分數，有時在 Struct-stab 上也是如此，但在 Start/Goal 和 Correct 上仍然接近零（例如 Sora 和 Kling 儘管 Legal 適度但 Correct 較低）。這種模式表明核心失敗不是產生合理的迷宮狀運動，而是維持單一的、可識別的軌跡，該軌跡從正確的錨點開始並到達正確的目標，沒有隱含的重置或捷徑。相比之下，更強的以影像為中心的模型獲得實質上更高的 ST1 正確性，這表明明確的目標條件狀態追蹤仍然是影片風格生成的限制因素。

---

**(2) 運動下的遮擋相對成熟，剩餘錯誤集中在深度層更新上。** 對於 ST2，許多模型在 *Rigid*、*Natural* 和 *Env-stab* 上實現高分，這表明分層運動和全局時間穩定性越來越得到很好的處理。剩餘的分散集中在 *Occ-move* 和 *Parallax* 上，表明最困難的情況涉及一致的深度順序和運動下的可見性更新，而不是整體平滑度。

**(3) 3D 迴圈導航強調視角連續性和關係穩定性。** 在 ST3 中，領先的模型維持強大的 *Struct* 和 *Entity-stab*，但較弱的系統在 *View-smooth* 和 *Rel* 上下降。這與「視角重置」失敗模式一致：幾何可能在逐幀基礎上看起來合理，但序列無法解釋為沿著連續相機路徑遍歷的單個 3D 場景。因此，CoW-Bench 將迴圈導航視為模型是否在轉換下保存狀態的探針，超越單視角現實性。

**要點。** 影片生成模型通常產生具有更大自由度和更明顯時間進展感的結果，而影像生成模型傾向於以更不連貫的方式描繪時間上的變化，具有較少的創意表達範圍。

# 5.7 樣本分析

為了深入分析模型在多維度約束下的推理機制，我們基於先前定義的六個核心一致性挑戰，在 CoW-Bench 中構建了一套新的評估準則。超越傳統評估僅關注視覺質量的局限，本基準測試從資料集構建和模型能力邊界的角度，利用逐幀的物理狀態真值來精確量化「生成器」和「世界模擬器」之間的根本差異。

**5.7.1 單一一致性任務**

單一一致性任務的設計理念是隔離複雜干擾，利用模擬資料的純淨性來建立模型基礎推理能力的基準線。各子任務的具體效果如圖 38–40 所示。

**模態一致性任務。** 此任務檢驗模型是否能清晰區分來自不同模態的約束，並避免資訊混融。在*主體屬性保真度*中，模型展現了強大的特徵解耦能力，成功地從參考影像中提取「蝴蝶」的紋理和材質，並將其映射到「魚」的幾何結構上。生成的生物體具有獨特的尺度和光澤特徵，而不會納入蝴蝶的翅膀形態。對於更精細粒度的*局部編輯精度*，模型在時鐘編輯任務中展現出精確的像素控制，只根據指令修改時針和分針，而背景牆壁和時鐘框架保持嚴格「鎖定」。此外，在*多約束滿足*的複雜指令下，模型精確捕捉多個角色的服裝、動作和位置屬性，不出現錯誤的角色分配或屬性洩露，證明了其在解析長文本約束中的精確性。

**空間一致性任務。** 此任務評估模型建立的場景是否在幾何上自洽，而非僅在 2D 影像中「看起來合理」。在*語義平面性*中，模型正確理解非遮擋情景中的相對位置，兩隻貓分別向左右移動而不混淆方向語義。對於更複雜的*遮擋/包含*，當抽屜緩慢關閉時，模型正確渲染了內部書籍逐漸進入黑暗的過程；書籍遵循遮擋的物理規律，而非突然消失，反映了對「容器」概念的理解。在*多視角 3D* 測試中，隨著視點緩慢變化，桌燈自然地在視野邊緣消失，而床的佈局逐漸顯露出來。在整個過程中，房間內物體的相對位置保持不變，光照環境保持穩定。

**時間一致性任務。** 我們將時間一致性從「視覺平滑性」提升到「遵循規則的演化」，檢驗模型是否遵循世界的隱含規律。在*世界線持續性*中，電風扇在長期旋轉期間保持葉片的物理完整性，沒有葉片破損或突然的材質變化。*規則引導的緩慢演化*進一步展示了模型對物理熵的理解：在模擬的一小時期間，蠟燭按照燃燒規律逐漸變短，而不是違背常識地保持不變。在*有序階段轉變*的房屋坍塌任務中，模型清晰地呈現了從結構完整到廢墟的連續狀態。坍塌序列遵循重力邏輯，廢墟在下落後保持靜止，避免了「坍塌後又恢復」這樣的非因果抖動。

## 5.7.2 複合一致性任務

複合一致性任務模擬真實世界的複雜性，審視當多維度約束相互限制（甚至衝突）時，模型的權衡和推理能力。每個子任務的具體效果如圖 41–43 所示。

**Modal-Spatial 一致性任務**評估模型將語義信息轉換為可執行空間約束的能力，實現「語義–空間耦合」。模型不僅必須理解物體是什麼，還必須精確執行關於物體位置的幾何指令，確保語義指稱對象在空間維度上的準確接地。如圖 41 所示，在 *Sem-Planar* 中，模型成功在複雜交通流中識別具有「藍色車頂」的特定車輛，並控制該車輛向右移動，實現精確的語義-空間綁定。在 *Sem-Hier* 任務中，模型準確生成「蘋果在碗裡」且「梨在碗外」的場景，嚴格遵守包含和排斥的空間語義。然而，*Sem-MV* 暴露了一個當前的弱點：在視角轉換過程中，雖然路標的視角變化保持合理，但書籍和筆相對於路標的遮擋關係發生了錯誤漂移（從後方移至側方），表明模型在動態視角下維持微觀空間語義的能力仍需改進。

**Modal-Temporal 一致性任務**評估模型在長序列生成過程中對指令的保真度。核心是審視模型是否將提示詞視為高優先級的「時間憲法」，在整個視頻中實施語義元素和邏輯約束，以抵抗語義漂移和時間遺忘。如圖 42 所示，在 *Long Horizon* 任務中，車輛的車身顏色、圖案和相對位置在長距離移動過程中保持高度穩定，無模糊或紋理改變。*Attribute Dynamic* 進一步測試時間編程能力，其中模型成功控制球體顏色按「紅色 → 橙色 → 綠色 → 藍色 → 紅色」的複雜序列變化，具有清晰的步驟且無顏色溢出。在 *Trigger Event* 中，模型展現了對因果邏輯的敏銳捕捉：按鈕按下前手機螢幕保持黑色，按下後立即點亮，完全符合事件的觸發點。

**Spatial-Temporal 一致性任務**評估模型在弱模態約束下是否具有「內置物理引擎」的原型。其重點是模型是否能在動態演化過程中維持空間拓撲和運動視差的自洽性，而非僅依賴像素級平滑插值。如圖 43 所示，在 *Maze-2D* 中，雖然模型保持了迷宮牆壁的靜態結構，但主體最終未能正確規劃到目標的路徑，暗示空間推理能力的局限。相比之下，*Occlusion Dynamics under Motion* 完美再現了運動視差：近處樹木以高速移動產生運動模糊，而背景緩慢移動，車輛保持相對靜止，實現了雙重時空自洽性。最後，*3D Loop Navigation* 實現了從卧室到城市再返回的閉環漫遊，具有平滑的結構連續性且無空間坍塌，展示了長期漫遊中潛在的參考框架穩定性。

為全面驗證這些機制，我們使用 CoW-Bench 測試了包括 Sora [2]、Kling [279]、GPT-Image-1.5 [504]、Seedream-4-5 [502]、Nano Banana Pro [503]、Wan2.2-I2V-14B [273]、SkyReels-V2 [508]、HunyuanVideo [506]、BAGEL [8] 和 Emu3.5 [511] 在內的主流世界模型。我們展示了部分結果：單一一致性比較如圖 44（Modal）、圖 45（Spatial）和圖 46（Temporal）所示；複合一致性結果如圖 47（Modal-Spatial）、圖 48（Modal-Temporal）和圖 49（Spatial-Temporal）所示。

**要點。** CoW-Bench 為評估世界模型建立了新標準。儘管視頻模型在指令遵循方面超越了圖像模型，但它們根本上缺乏物理演化邏輯。這些模型過度依賴基於像素的插值而非真正的推理，表現出非物理失真和拓撲坍塌。

# 6 結論

本調查透過*一致性三位一體*的視角重新檢視生成式人工智慧的軌跡，建立了構成世界模型的通用框架。透過將能力空間解構為模態、空間和時間維度，我們論證真正的物理理解不是源於單軸效能，而是源於跨維度互動的穩健性。我們的分析強調當前系統中最關鍵的失敗不是視覺偽影，而是一致性的破裂：無法將語義指令綁定到幾何角色的失敗（模態-空間）、無法在長序列演化中維持身份認同的失敗（模態-時間），以及在導航期間環境永久性喪失（時間-空間）。

圖 50：基於互動動作空間的世界模型範式演變譜系。此圖說明該領域從早期「向量即動作」範式（左側：例如 JEPA，依賴於不可解釋的潛在空間預測）、經過中間「鍵即動作」範式（中間：例如 Genie 系列，受限於預定義的離散控制空間），最終發展到「提示即動作」範式的軌跡。在後者中，語義編譯器將自然語言意圖轉譯成通用時空動態模擬。

為了嚴格診斷這些跨維度破裂，我們引入了 CoW-Bench，這是一個全面的基準，在共享協議下統一了主流視頻生成模型和 UMM 的評估。CoW-Bench 採用源自人類專家推理的精心設計的多幀評估協議。透過針對細粒度原子檢查列表分析時間採樣網格，我們將一致性操作化為嚴格的約束滿足問題。這種嚴格的方法暴露了普遍存在的*約束退避*現象，其中模型生成看起來合理的紋理，同時無聲地違反邏輯承諾，從而提供必要的診斷解析度以區分視覺模仿和真正的物理模擬。

至關重要的是，我們的發現表明約束退避不僅是訓練數據不足或規模不足的結果，而是當前模型表示互動方式的結構性偽影。當動作空間不可解釋或僵硬預定義時，模型缺乏表達能力來將語義承諾固定在物理動態中。在這些約束下，一致性違反不是偶然錯誤，而是幾乎不可避免的結果。因此，解決這一限制需要在世界模型中如何形式化互動本身的範式轉變。

為了系統地表徵這一轉變，我們根據互動動作空間的表達性組織了世界模型範式的演變（圖 50）。如圖左側所示，早期探索如 JEPA [1] 在*向量即動作*層級運作。雖然實現了潛在空間預測，其互動機制仍然不透明且缺乏語義可解釋性。中間部分展示了*鍵即動作*範式，以 Genie 系列 [438, 514, 515] 為例。雖然引入了有限的互動性，這些模型仍然侷限於狹窄、離散且預定義的動作空間。

---

圖的右側說明了一個前瞻性的範式：提示即動作範式，其中具有模態一致性的 UMM 和具有空間時間一致性的視頻生成模型被統一。這類模型配備了內部語義編譯器，可以解釋高維自然語言提示，並將其轉譯為符合*一致性三位一體*的通用時空模擬。最近的系統如 PixVerse-R1 [516] 提供了這一方向的初步預視，展示了能夠即時回應使用者輸入並在自迴歸架構中統一多種模態的實時世界建模。透過超越預定義的動作抽象，這個範式開始縮小人類語義意圖與物理世界基本動態之間的差距。

因此，本調查的核心信念簡單而堅定：一致性不是世界模型的可選屬性—它是其存在的標準。無論規模如何，產生視覺上引人注目的像素但未能維持跨維度一致性的系統，根本上仍然是紋理合成器而非世界模擬器。*一致性三位一體*因此不僅劃定了一個分析框架；它標記了一個邊界—一個範式分界，區分生成類似於世界的影像與構建理解世界的模型之間的區別。

---

圖 39：空間子任務的示例圖表。

---

作為通用世界模型定義原則的一致性三位一體

**時間**

世界線

天花板風扇連續旋轉五分鐘，葉片平穩旋轉，機身保持靜止。

檢查列表：

- 相同風扇身份；無單位交換或形狀變化。

- 穩定的材質、顏色和紋理；無邊緣重繪。

- 天花板背景穩定；無照明或紋理跳躍。

- 無閃爍、抖動、扭曲或呼吸偽影。

- 以一致速度平穩旋轉；無停止、反向或不規則加速。

緩慢演變

縮時：一小時過去。

檢查列表：

- 背景穩定；無場景重繪或彈入/彈出。

- 火焰閃爍並穩定減弱；無重新點燃或回退。

- 蠟燭高度穩定下降；滴蠟累積並凝固；無瞬間融化/消失。

- 至少三個關鍵階段（完整 → 融化/滴蠟 → 縮短並凝固滴蠟）；

- 平穩轉變。

- 燭光隨著燃燒同步變暗；無突然照明跳躍。

階段順序

拆除用積木製成的房屋。

檢查列表：

- 初始階段：移除門/窗/屋頂；無過早坍塌。

- 中間階段：依序拆除全部四面牆；無無序坍塌。

- 後續階段：僅移除基礎積木；無其他變化。

- 最終狀態：所有積木平穩地沉降到地面；無額外動作。

- 場景和積木外觀保持一致；無新增物體。

**圖 40：時間子任務的示例圖表。**

---

圖 41：模態和空間子任務的示例圖表。

---

圖 42：模態和時間子任務的示例圖表。

---

圖 43：空間和時間子任務的示例圖表。

---

## 一致性三位一體作為通用世界模型的核心原則

圖 44：不同模型在模態一致性任務中的比較。

---

## 模型比較

**模型 Sota**แข Admiral oga

**比較資產** Alogisticbinary

** Cohen's_d**

---

## 圖 45：不同模型在空間一致性任務中的比較。

---

圖 46：不同模型在時間一致性任務中的比較。

---

圖 47：不同模型在模態和空間一致性任務中的比較。

---

- 圖 48：不同模型在模態和時間一致性任務中的比較。

---

圖 49：不同模型在空間和時間一致性任務中的比較。

---

# 7 貢獻

**主要作者**

Jingxuan Wei²、Siyuan Li³、Cheng Tan¹

**核心貢獻者**

Yuhang Xu²、Zheng Sun²、Junjie Jiang²、Hexuan Jin²、Caijun Jia²、Honghao He²、Xinglong Xu²、Xi Bai²

**其他貢獻者**

Chang Yu³、Yumou Liu⁵、Junnan Zhu²、Xuanhe Zhou⁵、Jintao Chen⁶、Xiaobin Hu⁴、Shancheng Pang⁷、Bihui Yu²、Ran He²、Zhen Lei²、Stan Z. Li³

**通訊作者**

Conghui He¹、Shuicheng Yan⁴、Cheng Tan¹

**所屬機構**

¹上海人工智慧實驗室

²中國科學院大學

³西湖大學

⁴新加坡國立大學

⁵上海交通大學

⁶浙江大學

⁷中國石油大學（華東）

參考文獻
[1] Y. LeCun, "A path towards autonomous machine intelligence," OpenReview, 2022.
[2] T. Brooks, B. Peebles, C. Holmes et al., "Video generation models as world simulators," OpenAI Blog, vol. 1, no. 8, p. 1, 2024.
[3] A. Bardes, Q. Garrido, J. Ponce et al., "Revisiting feature prediction for learning visual representations from video," TMLR, 2025.
[4] H. Lu, W. Liu, B. Zhang et al., "Deepseek-vl: towards real-world vision-language understanding," arxiv, 2024.
[5] R. Team, Z. Gao, Q. Wang et al., "Advancing open-source world models," arxiv, 2026.
[6] Runway Research, "Gen-3 alpha: A new frontier for video generation," https://runwayml.com/research/introducing-gen-3-alpha, 2024, runway Technical Report.
[7] G. Team, R. Anil, S. Borgeaud et al., "Gemini: A family of highly capable multimodal models," arxiv, 2023.
[8] C. Deng, D. Zhu, K. Li et al., "Emerging properties in unified multimodal pretraining," arxiv, 2025.
[9] M. Huh, B. Cheung, T. Wang et al., "The platonic representation hypothesis," arxiv, 2024.
[10] A. Radford, J. W. Kim, C. Hallacy et al., "Learning transferable visual models from natural language supervision," in ICML. PMLR, 2021, pp. 8748-8763.
[11] C. Jia, Y. Yang, Y. Xia et al., "Scaling up visual and vision-language representation learning with noisy text supervision," in ICML, 2021.
[12] X. Zhai, B. Mustafa, A. Kolesnikov et al., "Sigmoid loss for language image pre-training," in ICCV, 2023.
[13] H. Xu, S. Xie, X. E. Tan et al., "Demystifying clip data," arxiv, 2023.
[14] J.-B. Alayrac, J. Donahue, P. Luc et al., "Flamingo: a visual language model for few-shot learning," NeurIPS, 2022.
[15] J. Li, D. Li, C. Xiong et al., "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation," in ICML. PMLR, 2022, pp. 12 888-12 900.
[16] J. Li, D. Li, S. Savarese et al., "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," in ICML, 2023, pp. 19 730-19 742.
[17] J. Bai, S. Bai, S. Yang et al., "Qwen-vl: A frontier large vision-language model with versatile abilities," arxiv, 2023.
[18] F. Li, R. Zhang, H. Zhang et al., "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models," 2024.
[19] C. Team, "Chameleon: Mixed-modal early-fusion foundation models," arxiv, 2024.
[20] J. Lu, C. Clark, S. Lee et al., "Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action," in CVPR, 2024, pp. 26 439-26 455.
[21] J. Xie, W. Mao, Z. Bai et al., "Show-o: One single transformer to unify multimodal understanding and generation," arxiv, 2024.
[22] L. Yu, B. Shi, R. Pasunuru et al., "Scaling autoregressive multi-modal models: Pretraining and instruction tuning (2023)," arxiv, 2023.
[23] H. Liu, C. Li, Q. Wu et al., "Visual instruction tuning," in NeurIPS, 2023.
[24] W. Wang, Q. Lv, W. Yu et al., "Cogvlm: Visual expert for pretrained language models," NeurIPS, 2024.
[25] Z. Chen, J. Wu, W. Wang et al., "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks," in CVPR, 2024.

我已準備好翻譯論文段落。請提供需要翻譯的內容，我會按照您的規則進行翻譯：

1. ✓ 繁體中文（台灣用語）
2. ✓ 專有名詞、模型名稱、縮寫保留英文
3. ✓ 保持學術文章的嚴謹語氣
4. ✓ 保留所有 Markdown 格式符號
5. ✓ 表格 Markdown 格式完整保留
6. ✓ 保留 [FIGURE:xxx]、[FIGURE_CAPTION] 等標記
7. ✓ 保留 LaTeX 數學式
8. ✓ 只翻譯文字內容，不新增方括號標記

請貼上需要翻譯的論文段落。

# 一致性三位一體作為通用世界模型的定義原則

[26] D. Zhu, J. Chen, X. Shen et al., "Minigpt-4: Enhancing vision-language understanding with advanced large language models," arxiv, 2023.

[27] P. Esser, S. Kulal, A. Blattmann et al., "Scaling rectified flow transformers for high-resolution image synthesis," in ICML, 2024.

[28] X. Wang, X. Zhang, Z. Luo et al., "Emu3: Next-token prediction is all you need," arxiv, 2024.

[29] J. Chen, J. Yu, C. Ge et al., "Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis," ICLR, 2024.

[30] P. Gao, L. Zhuo, D. Liu et al., "Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers," arxiv, 2024.

[31] X. Liu, C. Gong et al., "Flow straight and fast: Learning to generate and transfer data with rectified flow," in ICLR, 2023.

[32] X. Liu, X. Zhang, J. Ma et al., "Instaflow: One step is enough for high-quality diffusion-based text-to-image generation," in ICLR, 2023.

[33] BlackForest Labs, "Flux-1: Unleashing the power of flow matching," https://blackforestlabs.ai, 2024.

[34] N. Ma, M. Goldstein, M. S. Albergo et al., "Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers," in ECCV, 2024.

[35] S. Tong, D. Fan, J. Li et al., "Metamorph: Multimodal understanding and generation via instruction tuning," in ICCV, 2025, pp. 17001-17012.

[36] W. Jin, Y. Niu, J. Liao et al., "Srum: Fine-grained self-rewarding for unified multimodal models," arxiv, 2025.

[37] J. Xu, X. Liu, Y. Wu et al., "Imagereward: Learning and evaluating human preferences for text-to-image generation," in NeurIPS, 2023.

[38] Z. Lin, D. Pathak, B. Li et al., "Evaluating text-to-visual generation with image-to-text generation," in ECCV. Springer, 2024, pp. 366-384.

[39] Z. Liang, Y. Yuan, S. Gu et al., "Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization," in CVPR, 2025.

[40] W. Wang, Z. Gao, L. Chen et al., "Visualprm: An effective process reward model for multimodal reasoning," arxiv, 2025.

[41] Y. Cai, K. Li, M. Jia et al., "Phygdpo: Physics-aware groupwise direct preference optimization for physically consistent text-to-video generation," arxiv, 2026.

[42] S. Yuan, Y. Liu, Y. Yue et al., "Ar-grpo: Training autoregressive image generation models via reinforcement learning," arxiv, 2025.

[43] T. Wang and P. Isola, "Understanding contrastive representation learning through alignment and uniformity on the hypersphere," in ICML, 2020, pp. 9929-9939.

[44] V. W. Liang, Y. Zhang, Y. Kwon et al., "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning," NeurIPS, vol. 35, pp. 17612-17625, 2022.

[45] C. Snell, J. Lee, K. Xu et al., "Scaling llm test-time compute optimally can be more effective than scaling model parameters," arxiv, 2024.

[46] S. J. Gershman and N. D. Goodman, "Amortized inference in probabilistic reasoning," in ACCSS, 2014.

[47] S. C. Lowe, "System 2 reasoning capabilities are nigh," arxiv, 2024.

[48] S. Yao, D. Yu, J. Zhao et al., "Tree of thoughts: Deliberate problem solving with large language models," in NeurIPS, vol. 36, 2023, pp. 11809-11822.

[49] A. Van Den Oord, O. Vinyals et al., "Neural discrete representation learning," NeurIPS, 2017.

我已準備就緒，可以翻譯您的論文段落。請提供需要翻譯的內容，我會按照您指定的規則進行翻譯：

1. ✓ 翻譯成繁體中文（台灣用語）
2. ✓ 保留專有名詞、模型名稱、縮寫的英文
3. ✓ 保持學術嚴謹語氣
4. ✓ 保留所有 Markdown 格式符號
5. ✓ 完整保留表格的 Markdown 格式
6. ✓ 保留所有標記如 [FIGURE:xxx]
7. ✓ 保留 LaTeX 數學式
8. ✓ 只翻譯文字內容，不自行新增方括號

請貼上您要翻譯的論文段落。

## 一致性三位一體作為通用世界模型的定義性原則

[50] A. Ramesh, M. Pavlov, G. Goh et al., "Zero-shot text-to-image generation," in ICML. PMLR, 2021, pp. 8821–8831.
[51] M. Huh, B. Cheung, P. Agrawal et al., "Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks," in ICML, 2023.
[52] T. Xiong, J. H. Liew, Z. Huang et al., "Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation," in ICCV, 2025.
[53] S. Li, L. Zhang, Z. Wang et al., "Mergevq: A unified framework for visual generation and representation with disentangled token merging and quantization," in CVPR, 2025.
[54] S. Bengio, O. Vinyals, N. Jaitly et al., "Scheduled sampling for sequence prediction with recurrent neural networks," in NeurIPS, 2015.
[55] Y. Lipman, R. T. Chen, H. Ben-Hamu et al., "Flow matching for generative modeling," arxiv, 2022.
[56] X. Liu, C. Gong, and Q. Liu, "Flow straight and fast: Learning to generate and transfer data with rectified flow," arxiv, 2022.
[57] C. Jia, Y. Yang, Y. Xia et al., "Scaling up visual and vision-language representation learning with noisy text supervision," in ICML. PMLR, 2021, pp. 4904–4916.
[58] L. Jiasen, C. Christopher, Z. Rowan et al., "Unified-io: A unified model for vision, language, and multi-modal tasks," arxiv, 2022.
[59] C. Team, "Chameleon: Mixed-modal early-fusion foundation models," arxiv, 2024.
[60] C. Ma and Y. Zhang, "Theoretical bounds of modality alignment in world models," in ICML, 2024.
[61] K. Lee, H. Liu, M. Ryu et al., "Aligning text-to-image models using human feedback," NeurIPS, 2023.
[62] Y. Zhang, Y. Li, Y. Yang et al., "Reasongen-r1: Cot for autoregressive image generation models through sft and r1," arxiv, 2025.
[63] Y. Ji, J. Li, Y. Xiang et al., "A survey of test-time compute: From intuitive inference to deliberate reasoning," arxiv, 2025.
[64] R. Tian, M. Gao, M. Xu et al., "Unigen: Enhanced training & test-time strategies for unified multimodal understanding and generation," in arxiv, 2025.
[65] H. He, J. Liang, X. Wang et al., "Scaling image and video generation via test-time evolutionary search," arxiv, 2025.
[66] D. Silver, J. Schrittwieser, K. Simonyan et al., "Mastering the game of go without human knowledge," Nature, 2017.
[67] K. Cobbe, V. Kosaraju, M. Bavarian et al., "Training verifiers to solve math word problems," arxiv, 2021.
[68] Z. Huang, N. Yu, G. Chen et al., "Vchain: Chain-of-visual-thought for reasoning in video generation," arxiv, 2025.
[69] R. Baillargeon, "Object permanence in 3½- and 4½-month-old infants," Dev. Psychol., vol. 23, no. 5, pp. 655–664, 1987.
[70] E. S. Spelke, "Principles of object perception," Cog. Sci., vol. 14, no. 1, pp. 29–56, 1990.
[71] P. Anderson, Q. Wu, D. Teney et al., "On evaluation of embodied navigation agents," arxiv, 2018.
[72] R. Hartley and A. Zisserman, *Multiple View Geometry in Computer Vision*, 2nd ed. Cambridge University Press, 2003.
[73] X. Shi, Z. Chen, H. Wang et al., "Convolutional lstm network: A machine learning approach for precipitation nowcasting," NeurIPS, 2015.
[74] Y. Wang, M. Long, J. Wang et al., "Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms," NeurIPS, 2017.

我準備好翻譯論文段落了。請提供要翻譯的具體文本內容，我將遵循您列出的所有規則：

1. ✓ 翻譯成繁體中文（台灣用語）
2. ✓ 專有名詞、模型名稱、縮寫保留英文
3. ✓ 保持學術嚴謹語氣
4. ✓ 保留所有 Markdown 格式符號
5. ✓ 表格 Markdown 格式完整保留
6. ✓ 方括號標記如 [FIGURE:xxx] 保留原樣
7. ✓ LaTeX 數學式保留原樣
8. ✓ 不自行新增方括號標記

請貼上需要翻譯的論文段落。

# 一致性三位一體作為通用世界模型的定義原則

[75] V. L. Guen and N. Thome, "Disentangling physical dynamics from unknown factors for unsupervised video prediction," in CVPR, 2020.

[76] E. Denton and R. Fergus, "Stochastic video generation with a learned prior," in ICML, 2018.

[77] M. Raissi, P. Perdikaris, and G. Karniadakis, "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations," J. Comput. Phys., vol. 378, pp. 686–707, 2019.

[78] M. Lutter, C. Ritter, and J. Peters, "Deep lagrangian networks: Using physics as model prior for deep learning," in ICLR, 2019.

[79] S. Greydanus, M. Dzamba, and J. Yosinski, "Hamiltonian neural networks," NeurIPS, vol. 32, 2019.

[80] Y. Rubanova, R. T. Chen, and D. K. Duvenaud, "Latent ordinary differential equations for irregularly-sampled time series," NeurIPS, vol. 32, 2019.

[81] B. Mildenhall, P. P. Srinivasan, M. Tancik et al., "Nerf: Representing scenes as neural radiance fields for view synthesis," in ECCV, 2020.

[82] J. T. Barron, B. Mildenhall, M. Tancik et al., "Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields," in ICCV, 2021.

[83] J. T. Barron, B. Mildenhall, D. Verbin et al., "Zip-nerf: Anti-aliased grid-based neural radiance fields," in ICCV, 2023, pp. 19797–19805.

[84] T. Muller, A. Evans, C. Schied et al., "Instant neural graphics primitives with a multiresolution hash encoding," ACM TOG, 2022.

[85] P. Wang, L. Liu, Y. Liu et al., "Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction," arxiv, 2021.

[86] L. Yariv, J. Gu, Y. Kasten et al., "Volume rendering of neural implicit surfaces," in NeurIPS, 2021.

[87] A. Gropp, L. Yariv, N. Haim et al., "Implicit geometric regularization for learning shapes," in ICML, 2020, pp. 3789–3799.

[88] Z. Yu, S. Peng, M. Niemeyer et al., "Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction," NeurIPS, vol. 35, pp. 25018–25032, 2022.

[89] B. Kerbl, G. Kopanas, T. Leimkuhler et al., "3d gaussian splatting for real-time radiance field rendering." TOG, 2023.

[90] T. Lu, M. Yu, L. Xu et al., "Scaffold-gs: Structured 3d gaussian splatting for view-adaptive rendering," in CVPR, 2024.

[91] B. Huang, Z. Yu, A. Chen et al., "2d gaussian splatting for geometrically accurate radiance fields," in SIGGRAPH, 2024.

[92] Z. Yu, A. Chen, B. Huang et al., "Mip-splatting: Alias-free 3d gaussian splatting," in CVPR, 2024.

[93] T. Xie, Z. Zong, Y. Qiu et al., "Physgaussian: Physics-integrated 3d gaussians for generative dynamics," in CVPR, 2024, pp. 4389–4398.

[94] G. Wu, T. Yi, J. Fang et al., "4d gaussian splatting for real-time dynamic scene rendering," in CVPR, 2024.

[95] Z. Yang, X. Gao, W. Zhou et al., "Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction," in CVPR, 2024.

[96] Z. Li, Z. Chen, Z. Li et al., "Spacetime gaussian feature splatting for real-time dynamic view synthesis," in CVPR, 2024.

[97] B. Poole, A. Jain, J. T. Barron et al., "Dreamfusion: Text-to-3d using 2d diffusion," arxiv, 2022.

[98] Z. Wang, C. Lu, Y. Wang et al., "Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation," in NeurIPS, 2023.

我已準備好翻譯論文段落。請提供您要翻譯的論文內容，我將：

1. 翻譯成繁體中文（台灣用語）
2. 保留所有專有名詞、模型名稱、縮寫的英文
3. 保持學術嚴謹語氣
4. 完整保留 Markdown 格式符號（#、**、*、|、-）
5. 保留表格的 Markdown 格式，只翻譯文字
6. 保留 [FIGURE:xxx]、[FIGURE_CAPTION] 等標記
7. 保留所有 LaTeX 數學式（$...$ 或 $$...$$）
8. 不新增任何方括號標記

請貼上您要翻譯的論文段落。

# The Trinity of Consistency as a Defining Principle for General World Models

[99] Y. Shi, P. Wang, J. Ye et al., "Mvdream: Multi-view diffusion for 3d generation," arxiv, 2023.
[100] J. Tang, Z. Chen, X. Chen et al., "Lgm: Large multi-view gaussian model for high-resolution 3d content creation," ECCV, 2024.
[101] M. Deitke, D. Schwenk, J. Salvador et al., "Objaverse: A universe of annotated 3d objects," in CVPR, 2023, pp. 13142-13153.
[102] V. Voleti, C.-H. Yao, M. Boss et al., "Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion," in ECCV, 2024.
[103] S. Wang, V. Leroy, Y. Cabon et al., "Dust3r: Geometric 3d vision made easy," in CVPR, 2024, pp. 20697-20709.
[104] B. Ma, H. Gao, H. Deng et al., "You see it, you got it: Learning 3d creation on pose-free videos at scale," in CVPR, 2025.
[105] N. Michael, T. B. Jonathan, M. Ben et al., "Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs," in CVPR, 2022.
[106] Y. Liu, C. Lin, Z. Zeng et al., "SyncSL☑mer: Generating multiview-consistent images from a single-view image," arxiv, 2023.
[107] J. T. Kajiya, "The rendering equation," ACM SIGGRAPH, 1986.
[108] S. Yang, S.-D. Jascha, P. K. Diederik et al., "Score-based generative modeling through stochastic differential equations," in ICLR, 2021.
[109] Y. Wang, Z. Gao, M. Long et al., "Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning," in ICML, 2018.
[110] Z. Gao, C. Tan, L. Wu et al., "Simvp: Simpler yet better video prediction," in CVPR, 2022.
[111] C. Tan, J. Wang, Z. Gao et al., "Ustep: Spatio-temporal predictive learning under a unified view," IEEE T-PAMI, 2025.
[112] C. Tan, Z. Gao, L. Wu et al., "Temporal attention unit: Towards efficient spatiotemporal predictive learning," in CVPR, 2023, pp. 18770-18782.
[113] J. Wei, C. Tan, Z. Gao et al., "Interpretable and generalizable spatiotemporal predictive learning with disentangled consistency," in ECML/PKDD. Springer, 2024, pp. 3-20.
[114] S. Liu, T. Li, W. Chen et al., "Soft rasterizer: A differentiable Worthl Oswčer for image-based 3d reasoning," in ICCV, 2019.
[115] W. Chen, H. Ling, J. Gao et al., "Learning to predict 3d objects with an interpolation-based differentiable W XVI ,\"\" NeurIPS, 3019.
[116] E. R. Chan, C. Z. Lin, M. A. Chan et al., "Efficient geometry-aware 3d generative adversarial networks," in CVPR, 2022.
[117] A. Chen, Z. Xu, A. Geiger et al., "Tensorf: Tensorial radiance fields," in ECCV, 2022.
[118] J. T. Barron, B. Mildenhall, D. Verbin et al., "Zip-nerf: Anti-aliased grid-based neural radiance fields," in ICCV, 2023.
[119] L. Yariv, Y. Kasten, D. Moran et al., "Multiview neural surface reconstruction by disentangling geometry and appearance," in NeurIPS, vol. 33, 2020, pp. 2492-2502.
[120] K. Park, U. Sinha, P. Hedman et al., "Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields," ACM TOG, 2021.
[121] G. Wu, T. Yi, J. Fang et al., "A 4d gaussian splatting for real-time dynamic scene rendering," in CVPR, 2024, pp. 20310-20320.
[122] J. Luiten, G. Kopanas, B. Leibe et al., "Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis," in 3DV, 2024.

我已準備好翻譯論文段落。請提供您要翻譯的具體論文內容，我將按照以下規則進行翻譯：

1. ✓ 翻譯成繁體中文（台灣用語）
2. ✓ 專有名詞、模型名稱、縮寫保留英文
3. ✓ 保持學術嚴謹語氣
4. ✓ 保留所有 Markdown 格式符號
5. ✓ 表格 Markdown 格式完整保留，只翻譯文字
6. ✓ [FIGURE:xxx]、[FIGURE_CAPTION] 等標記保留原樣
7. ✓ LaTeX 數學式保留原樣
8. ✓ 不自行新增方括號標記

請貼上您要翻譯的論文段落。

# 一致性三位一體作為通用世界模型的定義原則

[123] Z. Wang, C. Lu, Y. Wang et al., "Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation," PR, vol. 36, pp. 8406-8441, 2023.
[124] J. Tang, Z. Chen, X. Chen et al., "Lgm: Large multi-view gaussian model for high-resolution 3d content creation," in ECCV. Springer, 2024, pp. 1-18.
[125] X. Yu, M. Xu, Y. Zhang et al., "Mvimgnet: A large-scale dataset of multi-view images," in CVPR, 2023, pp. 9150-9161.
[126] J. Reizenstein, R. Shapovalov, P. Henzler et al., "Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction," in ICCV, 2021, pp. 10901-10911.
[127] V. Voleti, C.-H. Yao, M. Boss et al., "Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion," in ECCV. Springer, 2024, pp. 439-457.
[128] Y. J. Ma, W. Liang, G. Wang et al., "Eureka: Human-level reward design via coding large language models," in ICLR, 2024.
[129] W. Menapace, S. Lathuilière, S. Tulyakov et al., "Playable environments: Video generation in space and time," in CVPR. IEEE, 2022, pp. 3584-3594.
[130] K. Dalal, D. Koceja, G. Hussein et al., "One-minute video generation with test-time training," CVPR, 2025.
[131] L. Khachatryan, A. Movsisyan, V. Tadevosyan et al., "Text2video-zero: Text-to-image diffusion models are zero-shot video generators," in ICCV. IEEE, 2023, pp. 15954-15964.
[132] C. Qi, X. Cun, Y. Zhang et al., "Fatezero: Fusing attentions for zero-shot text-based video editing," in ICCV, 2023, pp. 15932-15942.
[133] L. Zhang, A. Rao, and M. Agrawala, "Adding conditional control to text-to-image diffusion models," in ICCV, 2023.
[134] J. Z. Wu, Y. Ge, X. Wang et al., "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation," in ICCV, 2023, pp. 7623-7633.
[135] Y. Guo, C. Yang, A. Rao et al., "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning," arxiv, 2023.
[136] H. Chen, M. Xia, Y. He et al., "Videocrafter1: Open diffusion models for high-quality video generation," arxiv, 2023.
[137] J. Wang, H. Yuan, D. Chen et al., "Modelscope text-to-video technical report," arxiv, 2023.
[138] P. Esser, J. Chiu, P. Atighehchian et al., "Structure and content-guided video synthesis with diffusion models," in ICCV, 2023, pp. 7346-7356.
[139] Y. Rao, W. Zhao, Z. Zhu et al., "Global filter networks for image classification," NeurIPS, vol. 34, pp. 980-993, 2021.
[140] J. Guibas, M. Mardani, Z. Li et al., "Adaptive fourier neural operators: Efficient token mixers for transformers," arxiv, 2021.
[141] C. Bai, Y. Li, Z. Zhao et al., "Fastinit: Fast noise initialization for temporally consistent video generation," arxiv, 2025.
[142] H. Qiu, M. Xia, Y. Zhang et al., "Freenoise: Tuning-free longer video diffusion via noise rescheduling," in ICLR, 2024.
[143] D. Kondratyuk, L. Yu, X. Gu et al., "Videopoet: A large language model for zero-shot video generation," arxiv, 2023.
[144] A. Gupta, L. Yu, K. Sohn et al., "Photorealistic video generation with diffusion models," in ECCV. Springer, 2024, pp. 393-411.
[145] L. Yu, J. Lezama, N. B. Gundavarapu et al., "Language model beats diffusion-tokenizer is key to visual generation," arxiv, 2023.

我已準備就緒，可以翻譯您提供的論文段落。請提供需要翻譯的內容，我將按照您指定的規則進行翻譯：

1. ✅ 翻譯成繁體中文（台灣用語）
2. ✅ 專有名詞、模型名稱、縮寫保留英文
3. ✅ 保持學術文章的嚴謹語氣
4. ✅ 保留所有 Markdown 格式符號
5. ✅ 完整保留表格的 Markdown 格式，只翻譯文字
6. ✅ 保留 [FIGURE:xxx]、[FIGURE_CAPTION] 等標記
7. ✅ 保留 LaTeX 數學式原樣
8. ✅ 不新增方括號標記，只翻譯文字

請貼上您要翻譯的論文段落。

# The Trinity of Consistency as a Defining Principle for General World Models

[146] NVIDIA, N. Agarwal, A. Ali et al., "Cosmos world foundation model platform for physical ai," arxiv, 2025.

[147] K. Tian, Y. Jiang, Z. Yuan et al., "Visual autoregressive modeling: Scalable image generation via next-scale prediction," in NeurIPS, vol. 37, 2024, pp. 84-85.

[148] L. Zhang, S. Cai, M. Li et al., "Frame context packing and drift prevention in next-frame-prediction video diffusion models," in NeurIPS, 2025.

[149] B. Chen, D. M. Monsó, Y. Du et al., "Diffusion forcing: Next-token prediction meets full-sequence diffusion," in NeurIPS, 2024.

[150] W. Kong, Q. Tian, Z. Zhang et al., "Hunyuanvideo: A systematic framework for large video generative models," arxiv, 2024.

[151] O. Bar-Tal, H. Chefer, O. Tov et al., "Lumiere: A space-time diffusion model for video generation," arxiv, 2024.

[152] Google DeepMind, "Veocap, "High-fidelity video generation with compressed latent representation," https://deepmind.google/technologies/veocap, 2025, technical Report.

[153] Y. Jin, Z. Sun, N. Li et al., "Pyramidal flow matching for efficient video generative modeling," arxiv, 2024.

[154] F. Liu, S. Zhang, X. Wang et al., "Timestep embedding tells: Its time to cache for video diffusion model," CVPR, pp. 7353-7363, 2025.

[155] A. Polyak, A. Zohar, A. Brown et al., "Movie gen: A cast of media foundation models," arxiv, 2024.

[156] H. Shao, S. Qian, H. Xiao et al., "Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning," NeurIPS, vol. 37, pp. 8612-8642, 2024.

[157] X. Wang and D. Zhou, "Chain-of-thought reasoning without prompting," NeurIPS, vol. 37, pp. 6678-6679, 2024.

[158] X. Lai, J. Li, W. Li et al., "Mini-im: Scaling up reasoning patterns and interaction turns for visual search," arxiv, 2025.

[159] S. Wang, J. Jin, X. Wang et al., "Video-thinker: Sparking thinking with videos via reinforcement learning," arxiv, 2025.

[160] H. Liu, K. Luo, J. Wang et al., "Thinksound: Chain-of-thought reasoning in multimodal large language models for audio generation and editing," arxiv, 2025.

[161] S. Motamed, L. Culp, K. Swersky et al., "Do generative video models understand physical principles?" arxiv, 2025.

[162] T. Aoshima, Y. Shinohara, and B. Park, "Video consistency distance: Enhancing temporal consistency for image-to-video generation via reward-based fine-tuning," arxiv, 2025.

[163] D. Ha and J. Schmidhuber, "World models," in NeurIPS, 2018.

[164] T. Zhang, H.-X. Yu, R. Wu et al., "Physdreamer: Physics-based interaction with 3d objects via video generation," in ECCV. Springer, 2024, pp. 388-406.

[165] T. Unterthiner, S. van Steenkiste, K. Kurach et al., "Towards accurate generative models of video: A new metric & challenges," arxiv, 2018.

[166] T. Aoshima, Y. Shinohara, and B. Park, "Video consistency distance: Enhancing temporal consistency for image-to-video generation via reward-based fine-tuning," arxiv, 2025.

[167] T. Wiedemer, Y. Li, P. Vicol et al., "Video models are zero-shot learners and reasoners," arxiv, 2025.

[168] F. Sun, J. Liu, J. Wu et al., "Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer," in ACM CIKM, 2019, pp. 1441-1450.

[169] H. Chen, Y. Lin, M. Pan et al., "Denoising self-attentive sequential recommendation," in ACM RecSys, 2022, pp. 92-101.

我已準備好翻譯學術論文段落。請提供需要翻譯的論文內容，我將按照以下規則進行翻譯：

✓ 翻譯成繁體中文（台灣用語）
✓ 專有名詞與模型名稱保留英文
✓ 保持學術語氣
✓ 完整保留所有 Markdown 格式符號
✓ 表格格式保持原樣，只翻譯文字
✓ 方括號標記（如 [FIGURE:xxx]）保留不翻譯
✓ LaTeX 數學式保留原樣
✓ 不新增任何額外方括號

請貼上您要翻譯的論文段落。

# The Trinity of Consistency as a Defining Principle for General World Models

[170] J. Xing, M. Xia, Y. Zhang et al., "Dynamicrafter: Animating open-domain images with video diffusion priors," in ECCV, 2024.
[171] W. Hong, M. Ding, W. Zheng et al., "Cogvideo: Large-scale pretraining for text-to-video generation via transformers," arxiv, 2022.
[172] A. K. Akan and Y. Yemez, "Compositional video synthesis by temporal object-centric learning," arxiv, 2025.
[173] K. Tian, Y. Jiang, Z. Yuan et al., "Visual autoregressive modeling: Scalable image generation via next-scale prediction," NeurIPS, vol. 37, pp. 84839-84865, 2024.
[174] L. Zhang, S. Cai, M. Li et al., "Pretraining frame preservation in autoregressive video memory compression," arxiv, 2025.
[175] Y. Bengio, N. Léonard, and A. Courville, "Estimating or propagating gradients through stochastic neurons for conditional computation," arxiv, 2013.
[176] J. Xie, W. Mao, Z. Bai et al., "Show-o: One single transformer to unify multimodal understanding and generation," arxiv, 2024.
[177] A. Polyak, A. Zohar, A. Brown et al., "Movie gen: A cast of media foundation models," arxiv, 2024.
[178] Y. Zeng, G. Wei, J. Zheng et al., "Make pixels dance: High-dynamic video generation," arxiv, 2023.
[179] Z. Yang, J. Teng, W. Zheng et al., "Cogvideox: Text-to-video diffusion models with an expert transformer," arxiv, 2024.
[180] Z. Zhang, A. Zhang, M. Li et al., "Multimodal chain-of-thought reasoning in language models," arxiv, 2023.
[181] OpenAI, "Gpt-4(vision) system card," https://cdn.openai.com/papers/GPTV_System_Card.pdf, sep 2023, technical report.
[182] "Gpt-4 technical report," arxiv, 2023.
[183] S. Yin, C. Fu, S. Zhao et al., "A survey on multimodal large language models," NSR, vol. 11, no. 12, p. nwaec403, 2024.
[184] T. Brown, B. Mann, N. Ryder et al., "Language models are few-shot learners," NeurIPS, vol. 33, pp. 1877-1901, 2020.
[185] H. Touvron, T. Lavril, G. Izacard et al., "Llama: Open and efficient foundation language models," arxiv, 2023.
[186] J. Wei, X. Wang, D. Schuurmans et al., "Chain-of-thought prompting elicits reasoning in large language models," NeurIPS, vol. 35, pp. 24824-24837, 2022.
[187] T. Kojima, S. S. Gu, M. Reidsma et al., "Large language models are zero-shot reasoners," in NeurIPS, vol. 35, 2022, pp. 22199-22213.
[188] D. Zhu, J. Chen, X. Shen et al., "MiniGpt-4: Enhancing vision-language understanding with advanced large language models," arxiv, 2023.
[189] P. Gao, J. Zhang, R. Liu et al., "Llama-adapter: Efficient fine-tuning of language models with zero-init attention," arxiv, 2023.
[190] X. Zhai, B. Mustafa, A. Kolesnikov et al., "Sigmoid loss for language image pre-training," in ICCV, 2023, pp. 11975-11986.
[191] N. Tishby and N. Zaslavsky, "Deep learning and the information bottleneck principle," ITW, 2015.
[192] W. Kim, B. Son, and I. Kim, "Vilt: Vision-and-language transformer without convolution or region supervision," in ICML, 2021, pp. 5583-5594.
[193] J. Wang, Z. Yang, X. Hu et al., "Git: A generative image-to-text transformer for vision and language," TMLR, 2022.

我準備好了，但我注意到你的請求中還沒有提供要翻譯的論文段落。

請提供要翻譯的論文內容，我會按照以下規則進行翻譯：

1. ✓ 翻譯成繁體中文（台灣用語）
2. ✓ 保留專有名詞、模型名稱、縮寫的英文
3. ✓ 保持學術嚴謹語氣
4. ✓ 保留所有 Markdown 格式符號（#、**、*、|、-）
5. ✓ 保留表格的 Markdown 格式，只翻譯文字
6. ✓ 保留方括號標記（[FIGURE:xxx] 等）
7. ✓ 保留 LaTeX 數學式
8. ✓ 不自行新增任何方括號標記

請將論文段落貼上，我會立即進行翻譯。

# The Trinity of Consistency as a Defining Principle for General World Models

[194] Z. Yang, Z. Gan, J. Wang et al., "Mm-react: Prompting chatgpt for multimodal reasoning and action," arxiv, 2023.
[195] C. Wu, S. Yin, W. Qi et al., "Visual chatgpt: Talking, drawing and editing with visual foundation models," arxiv, 2023.
[196] T. Schick, J. Dwivedi-Yu, R. Dessi et al., "Toolformer: Language models can teach themselves to use tools," in NeurIPS, 2023.
[197] D. Surís, S. Menon, and C. Vondrick, "Vipergpt: Visual inference via python execution for reasoning," in ICCV, 2023.
[198] G. Tanmay and K. Aniruddha, "Visual programming: Compositional visual reasoning without training," in CVPR, 2023.
[199] D. Driess, F. Xia, M. S. Sajjadi et al., "Palm-e: An embodied multimodal language model," arxiv, 2023.
[200] G. Wang, Y. Xie, Y. Jiang et al., "Voyager: An open-ended embodied agent with large language models," in arxiv, 2024.
[201] Y. Shen, K. Song, X. Tan et al., "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face," in NeurIPS, 2023.
[202] S. Yao, J. Zhao, D. Yu et al., "React: Synergizing reasoning and acting in language models," in ICLR, 2023.
[203] T. B. Richards, "Auto-gpt: An autonomous gpt-4 experiment," https://github.com/Significant-Gravitas/Auto-GPT, 2023.
[204] Z. Yang, S. Wang, M. Ma et al., "Unisim: A neural closed-loop sensor simulator," in CVPR, 2023, pp. 1389-1399.
[205] A. Hu, L. Russell, H. Yeo et al., "Gaia-1: A generative world model for autonomous driving," arxiv, 2023.
[206] Y. Li, H. Liu, Q. Wu et al., "Gligen: Open-set grounded text-to-image generation," in CVPR, 2023, pp. 22511-22521.
[207] L. Lian, B. Shi, A. Yala et al., "Llm-grounded video diffusion models," in ICLR, 2024.
[208] R. Liu, R. Wu, B. Van Hoorick et al., "Zero-1-to-3: Zero-shot one image to 3d object," in ICCV, 2023.
[209] J. Ho, T. Salimans, A. Gritsenko et al., "Video diffusion models," in NeurIPS, 2022.
[210] B. Zeng, L. Yang, J. Liu et al., "Editworld: Simulating world dynamics for instruction-following image editing," in ACM MM, 2025, pp. 12674-12681.
[211] Z. Zhang, D. Chen, and J. Liao, "Sgedit: Bridging llm with text2image generative model for scene graph-based image editing," arxiv, 2024.
[212] Z. M. Wang, K. Zhu, C. Xu et al., "Mio: A foundation model on multimodal tokens," in EMNLP, 2025, pp. 5077-5099.
[213] M.-S. Kwak, J. Kim, S. Yun et al., "Aligned novel view image and geometry synthesis via cross-modal attention instillation," arxiv, 2025.
[214] X. Long, Y.-C. Guo, C. Lin et al., "Wonder3d: Single image to 3d using cross-domain diffusion," in CVPR, 2024, pp. 9970-9980.
[215] M. Liu, R. Shi, K. Kuang et al., "Openshape: Scaling up 3d shape representation towards open-world understanding," PR, vol. 36, pp. 44860-44879, 2023.
[216] S. Krakovsky, G. Fiebelman, S. Benaim et al., "Lang3d-xl: Language embedded 3d gaussians for large-scale scenes," in ACM SIGGRAPH, 2025, pp. 1-11.
[217] K. Black, M. Janner, Y. Du et al., "Training diffusion models with reinforcement learning," in arxiv, 2023.
[218] B. Li, X. Li, J. Xu et al., "Test-time preference optimization for image restoration," arxiv, 2025.

我已準備好翻譯論文段落。請提供您要翻譯的論文內容，我會按照您指定的規則進行翻譯：

1. ✅ 翻譯成繁體中文（台灣用語）
2. ✅ 專有名詞、模型名稱、縮寫保留英文
3. ✅ 保持學術文章的嚴謹語氣
4. ✅ 保留所有 Markdown 格式符號
5. ✅ 表格的 Markdown 格式完整保留
6. ✅ 保留 [FIGURE:xxx] 等標記
7. ✅ 保留 LaTeX 數學式
8. ✅ 不新增方括號標記，只翻譯文字

請將論文段落貼上，我會立即進行翻譯。

# 一致性三位一體作為通用世界模型的定義原則

[219] H. Shi, J. Su, H. Ning et al., "Layoutcot: Unleashing the deep reasoning potential of large language models for layout generation," arxiv, 2025.

[220] C.-H. Lin, J. Gao, L. Tang et al., "Magic3d: High-resolution text-to-3d content creation," in CVPR, 2023, pp. 300–309.

[221] J. Ho, W. Chan, C. Saharia et al., "Adam video: High definition video generation with diffusion models," arxiv, 2022.

[222] P. Esser, R. Rombach, and B. Ommer, "Taming transformers for high-resolution image synthesis," in CVPR, 2021.

[223] R. Rombach, A. Blattmann, D. Lorenz et al., "High-resolution image synthesis with latent diffusion models," in CVPR, 2022, pp. 10684–10695.

[224] A. Blattmann, R. Rombach, H. Ling et al., "Align your latents: High-resolution video synthesis with latent diffusion models," in CVPR, 2023, pp. 22563–22575.

[225] S. Liu, Y. Han, P. Xing et al., "Step1x-edit: A practical framework for general image editing," arxiv, 2025.

[226] R. Dong, C. Han, Y. Peng et al., "Dreamllm: Synergistic multimodal comprehension and creation," arxiv, 2023.

[227] X. Chen, Z. Zhang, H. Zhang et al., "Unireal: Universal image generation and editing via learning real-world dynamics," in PR, 2025, pp. 12501–12511.

[228] H. Zhao, Z. Cai, S. Si et al., "Mentor: Efficient multimodal-conditioned tuning for autoregressive vision generation models," arxiv, 2025.

[229] N. G. Nair, S. Kaza, X. Luo et al., "Scaling transformer-based novel view synthesis with models token disentanglement and synthetic data," in ICCV, 2025, pp. 28567–28576.

[230] L. Melas-Kyriazi, I. Laina, C. Rupprecht et al., "Realfusion: 360deg reconstruction of any object from a single image," in CVPR, 2023, pp. 8446–8455.

[231] T. Hunyuan3D, B. Zhang, C. Guo et al., "Hunyuan3d-omni: A unified framework for controllable generation of 3d assets," arxiv, 2025.

[232] F. Liu, H. Li, J. Chi et al., "Langscene-x: Reconstruct generalizable 3d language-embedded scenes with trimap video diffusion," arxiv, 2025.

[233] X. Yin, Q. Zhang, J. Chang et al., "Gsfixer: Improving 3d gaussian splatting with reference-guided video diffusion priors," arxiv, 2025.

[234] J. Chen, Y. Qin, L. Liu et al., "Nerf-hugs: Improved neural radiance fields in non-static scenes using heuristics-guided segmentation," in CVPR, 2024, pp. 19436–19446.

[235] L. Xue, N. Yu, S. Zhang et al., "Ulip-2: Towards scalable multimodal pre-training for 3d understanding," in CVPR, 2024, pp. 27091–27101.

[236] J. Ye, Z. Wang, R. Zhao et al., "Shapellm-omni: A native multimodal llm for 3d generation and understanding," arxiv, 2025.

[237] X. Guo, Z. Wu, K. Xiong et al., "Genesis: Multimodal driving scene generation with spatio-temporal and cross-modal consistency," arxiv, 2025.

[238] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, "Viewset diffusion: (0-) image-conditioned 3d generative models from 2d data," in ICCV, 2023, pp. 8863–8873.

[239] J. Kerr, C. M. Kim, K. Goldberg et al., "Lerf: Language embedded radiance fields," in ICCV, 2023, pp. 19729–19739.

[240] A. Haque, M. Tancik, A. A. Efros et al., "Instruct-nerf2nerf: Editing 3d scenes with instructions," in ICCV, 2023, pp. 19740–19750.

我已準備好翻譯您提供的論文段落。不過，我注意到您提供的內容只包含了 JSON 格式的邊界框和類別標記，但沒有實際的文本內容。

請提供以下任一項：

1. **完整的文本內容** - 直接貼上需要翻譯的論文段落文字
2. **結構化內容** - 包含實際文本的 JSON 或 Markdown 格式

例如：
```
# 標題
正文內容...
| 表格 | 列 |
```

或

```json
{
  "title": "論文標題",
  "content": "需要翻譯的文本..."
}
```

請重新提交包含實際文本內容的材料，我將按照您的翻譯規則進行專業翻譯。

# The Trinity of Consistency as a Defining Principle for General World Models

[264] C. Low, W. Wang, and C. Katyal, "Ovi: Twin backbone cross-modal fusion for audio-video generation," *arxiv*, 2025.

[265] J. Liu, H. Chen, P. An et al., "Hybridvdla: Collaborative diffusion and autoregression in a unified vision-language-action model," *arxiv*, 2025.

[266] C. Zhang, Y. Liang, X. Qiu et al., "Vast 1.0: A unified framework for controllable and consistent video generation," *arxiv*, 2024.

[267] J. Feng, A. Ma, J. Wang et al., "Fancyvideo: Towards dynamic and consistent video generation via cross-frame textual guidance," *arxiv*, 2024.

[268] R. Villegas, M. Babaeizadeh, P.-J. Kindermans et al., "Phenaki: Variable length video generation from open domain textual description," in *ICLR*, 2023.

[269] Z. Tan, H. Yang, L. Qin et al., "Omni-video: Democratizing unified video understanding and generation," *arxiv*, 2025.

[270] R. Liu, H. Wu, Z. Zheng et al., "Videodpo: Omni-preference alignment for video diffusion generation," in *CVPR*, 2025, pp. 8009-8019.

[271] J. Cheng, R. Lyu, X. Gu et al., "Vpo: Aligning text-to-video generation models with prompt optimization," *arxiv*, 2025.

[272] Y. Liu, K. Zhang, Y. Li et al., "Sora: A review on background, technology, limitations, and opportunities of large vision models," *arxiv*, 2024.

[273] T. Wan, A. Wang, B. Ai et al., "Wan: Open and advanced large-scale video generative models," *arxiv*, 2025.

[274] F. Bao, C. Xiang, G. Yue et al., "Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models," *arxiv*, 2024.

[275] U. Singer, A. Polyak, T. Hayes et al., "Make-a-video: Text-to-video generation without text-video data," *arxiv*, 2022.

[276] L. Ruan, L. Tian, C. Huang et al., "Univg: Towards unified-modal video generation," in *IEEE ICME*, 2025, pp. 1-6.

[277] Pika Labs, "Pika: An idea-to-video platform," https://pika.art, 2023, accessed: 2026-02-09.

[278] W. Kong, Q. Tian, Z. Zhang et al., "Hunyuanvideo: A systematic framework for large video generative models," *arxiv*, 2024.

[279] K. Team, J. Chen, Y. Ci et al., "Kling-omni technical report," *arxiv*, 2025.

[280] J. Han, H. Chen, Y. Zhao et al., "Vision as a dialect: Unifying visual understanding and generation via text-aligned representations," *arxiv*, 2025.

[281] A. Ergasti, G. G. Tarollo, F. Botti et al., "R-flav: Rolling flow matching for infinite audio video generation," *arxiv*, 2025.

[282] L. Zhao, L. Feng, D. Ge et al., "Uniform: A unified multi-task diffusion transformer for audio-video generation," *arxiv*, 2025.

[283] Y. Wu, Z. Zhang, J. Chen et al., "Vila-u: a unified foundation model integrating visual understanding and generation," *arxiv*, 2024.

[284] H. Deng, T. Pan, H. Diao et al., "Autoregressive video generation without vector quantization," *arxiv*, 2024.

[285] X. Cheng, T. He, J. Xu et al., "Playing with transformer at 30+ fps via next-frame diffusion," *arxiv*, 2025.

[286] H. Chung, D. Lee, and J. C. Ye, "Acdc: Autoregressive coherent multimodal generation using diffusion correction," *arxiv*, 2024.

[287] Y. Li, Y. Ge, Y. Ge et al., "Difen: Diffusion-compressed deep tokens for autoregressive video generation with language models," *arxiv*, 2024.

我已準備好翻譯論文段落。請提供您想翻譯的論文文本，我會按照以下規則進行：

✓ 翻譯成繁體中文（台灣用語）
✓ 專有名詞、模型名稱、縮寫保留英文
✓ 保持學術嚴謹語氣
✓ 保留所有 Markdown 格式符號
✓ 表格 Markdown 格式完整保留
✓ 保留特殊標記 [FIGURE:xxx] 等
✓ 保留 LaTeX 數學式
✓ 不自行新增方括號標記

請粘貼論文段落，我會為您進行翻譯。

# The Trinity of Consistency as a Defining Principle for General World Models

[288] Z. Li, H. Shujie, L. Shujie et al., "Arlon: Boosting diffusion transformers with autoregressive models for long video generation," in ICLR, 2025.

[289] M. Sun, W. Wang, G. Li et al., "Ar-diffusion: Asynchronous video generation with auto-regressive diffusion," in CVPR, 2025, pp. 7364-7373.

[290] T. Yin, Q. Zhang, R. Zhang et al., "From slow bidirectional to fast autoregressive video diffusion models," in CVPR, 2025, pp. 22963-22974.

[291] E. Corona, A. Zanfir, E. G. Bazavan et al., "Vlogger: Multimodal diffusion for embodied avatar synthesis," in CVPR, 2025.

[292] P. Zhang, J. Li, M. Wang et al., "When video coding meets multimodal large language models: A unified paradigm for video coding," arxiv, 2024.

[293] W. Chen, Y. Ji, J. Wu et al., "Control-a-video: Controllable text-to-video diffusion models with motion prior and reward feedback learning," arxiv, 2023.

[294] G. Yariv, I. Gat, S. Benaim et al., "Diverse and aligned audio-to-video generation via text-to-video model adaptation," in AAAI, vol. 38, no. 7, 2024, pp. 6639-6647.

[295] K. Gong, D. Lian, H. Chang et al., "Tm2d: Bimodality driven 3d dance generation via music-text integration," in ICCV, 2023, pp. 9942-9952.

[296] X. Li, W. Chu, Y. Wu et al., "Videogen: A reference-guided latent diffusion approach for high definition text-to-video generation," arxiv, 2023.

[297] D. J. Zhang, J. Z. Wu, J.-W. Liu et al., "Show-1: Marrying pixel and latent diffusion models for text-to-video generation," IJCV, vol. 133, no. 4, pp. 1879-1893, 2025.

[298] Y. Zhang, Y. Kang, Z. Zhang et al., "Interactive video: User-centric controllable video generation with synergistic multimodal instructions," arxiv, 2024.

[299] X. Wang, J. Liu, Z. Wang et al., "Keyvid: Keyframe-aware video diffusion for audio-synchronized visual animation," arxiv, 2025.

[300] J. H. Liew, H. Yan, J. Zhang et al., "Magicedit: High-fidelity and temporally coherent video editing," arxiv, 2023.

[301] W. Wang, H. Yang, Z. Tuo et al., "Swap attention in spatiotemporal diffrusions for text-to-video generation," IJCV, pp. 1-19, 2025.

[302] D. J. Zhang, D. Li, H. Le et al., "Moonshot: Towards controllable video generation and editing with multimodal conditions," arxiv, 2024.

[303] R. Feng, W. Weng, Y. Wang et al., "Ccedit: Creative and controllable video editing via diffusion models," in CVPR, 2024, pp. 6712-6722.

[304] S. Ge, T. Hayes, H. Yang et al., "Long video generation with time-agnostic vqgan and time-sensitive transformer," in ECCV, 2022, pp. 102-118.

[305] X. Zhou, D. Liang, S. Tu et al., "Hermes: A unified self-driving world model for simultaneous 3d scene understanding and generation," arxiv, 2025.

[306] L. Chen, Y. Gu, and Q. Mao, "Univid: Unifying vision tasks with pre-trained video generation models," arxiv, 2025.

[307] M. Hu, C. Zheng, H. Zheng et al., "Unified discrete diffusion for simultaneous vision-language generation," arxiv, 2022.

[308] T. Hu, Z. Yu, Z. Zhou et al., "Hunyuancustom: A multimodal-driven architecture for customized video generation," arxiv, 2025.

[309] J. Chung, T. Zhu, M. G. Saez-Diez et al., "Unifying specialized visual encoders for video language models," arxiv, 2025.

我已準備好翻譯論文段落，但我注意到您沒有在訊息中提供要翻譯的內容。

請提供論文段落，我將按照以下規則進行翻譯：

✓ 繁體中文（台灣用語）
✓ 專有名詞、模型名稱、縮寫保留英文
✓ 保持學術嚴謹語氣
✓ 保留所有 Markdown 格式（#、**、*、|、-）
✓ 完整保留表格 Markdown 格式
✓ 保留 [FIGURE:xxx]、LaTeX 式子等
✓ 不自行新增方括號標記

請貼上您要翻譯的論文段落，我會立即為您翻譯。

# 一致性的三位一體作為通用世界模型的定義原則

[310] L. Yang, X. Zhang, Y. Tian et al., "Hermesflow: Seamlessly closing the gap in multimodal understanding and generation," arxiv, 2025.

[311] Y. Fang, W. Menapace, A. Siarohin et al., "Vimi: Grounding video generation through multi-modal instruction," in EMNLP, 2024, pp. 4444–4456.

[312] J. Li, W. Feng, T.-J. Fu et al., "T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback," in NeurIPS, vol. 37, 2024, pp. 75 692–75 726.

[313] O. Zohar, X. Wang, Y. Bitton et al., "Video-star: Self-training enables video instruction tuning with any supervision," in arxiv, 2024.

[314] X. He, D. Jiang, G. Zhang et al., "Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation," in EMNLP, 2024, pp. 2105–2123.

[315] L. Yu, J. Lezama, N. B. Gundavarapu et al., "Language model beats diffusion–(da Compiled and codes from Refrence) is key to visual generation," arxiv, 2023.

[316] W. Peebles and S. Xie, "Scalable diffusion models with transformers," in ICCV, 2023, pp. 4195–4205.

[317] Z. Zheng, X. Peng, T. Yang et al., "Open-sora: Democratizing efficient video production for all," arxiv, 2024.

[318] H. Liu, M. Zaharia, and P. Abbeel, "Ring attention with blockwise transformers for near-infinite context," arxiv, 2023.

[319] X. Ma, Y. Wang, X. Chen et al., "Latte: Latent diffusion transformer for video generation," arxiv, 2024.

[320] J. Z. Wu, Y. Ge, X. Wang et al., "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation," in ICCV, 2023, pp. 7623–7633.

[321] J. Kaplan, S. McCandlish, T. Henighan et al., "Scaling laws for neural language models," arxiv, 2020.

[322] J. Hoffmann, S. Borgeaud, A. Mensch et al., "Training compute-optimal large language models," in NeurIPS, vol. 35, 2022, pp. 30 016–30 030.

[323] A. Van Den Oord, O. Vinyals et al., "Neural discrete representation learning," in NeurIPS, 2017.

[324] S. Yin, C. Wu, J. Liang et al., "Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory," in arxiv, 2024.

[325] Z. Wang, Z. Yuan, X. Wang et al., "Motionctrl: A unified and flexible motion controller for video generation," in ACM SIGGRAPH, 2024, pp. 1–11.

[326] X. Chen, Y. Wang, L. Zhang et al., "Seine: Short-to-long video diffusion model for generative transition and prediction," in ICLR, 2024.

[327] Morph Studio Team, "Morph studio: Ai video generation platform," https://www.morphstudio.com/, 2024.

[328] A. Siarohin, S. Lathuilière, S. Tulyakov et al., "First order motion model for image animation," in NeurIPS, vol. 32, 2019.

[329] J. Zhao and H. Zhang, "Thin-plate spline motion model for image animation," in CVPR, 2022, pp. 3657–3666.

[330] L. Hu, X. Gao, P. Zhang et al., "Vectorizer: A consistent and controllable image-to-video synthesis for character animation," in CVPR, 2024.

[331] Z. Xu, J. Zhang, J. H. Liew et al., "Magicanimate: Temporally consistent human image animation using diffusion model," in CVPR, 2024.

[332] S. Zhu, J. L. Chen, Z. Dai et al., "Champ: Controllable and consistent human image animation with 3d parametric guidance," arxiv, 2024.

[333] M. Zhang, Z. Cai, L. Pan et al., "Motiondiffuse: Text-driven human motion generation with diffusion model," IEEE T-PAMI, 2022.

我已準備好進行翻譯。請提供您要翻譯的論文段落，我會按照您指定的規則進行翻譯，確保：

1. ✅ 翻譯成繁體中文（台灣用語）
2. ✅ 保留英文專有名詞、模型名稱、縮寫
3. ✅ 保持學術嚴謹語氣
4. ✅ 保留所有 Markdown 格式符號
5. ✅ 完整保留表格的 Markdown 格式
6. ✅ 保留方括號標記如 [FIGURE:xxx]、[FIGURE_CAPTION]
7. ✅ 保留 LaTeX 數學式
8. ✅ 不新增任何方括號標記

**請貼上您要翻譯的論文段落。**

# The Trinity of Consistency as a Defining Principle for General World Models

[334] D. Epstein, A. Jabri, B. Poole et al., "Diffusion self-guidance for controllable image generation," in NeurIPS, 2023.
[335] J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," in NeurIPS, vol. 33, 2020, pp. 6840-6851.
[336] C. Wu, Y. Xia, S. Gao et al., "Janus: Decoupling visual encoding for unified multimodal understanding and generation," CVPR, 2025.
[337] X. Dai, J. Hou, C.-Y. Ma et al., "Emu: Enhancing image generation models using photogenic needles in a haystack," arxiv, 2023.
[338] H. Zhang, X. Li, and L. Bing, "Video-llama: An instruction-tuned audio-visual language model for video understanding," in EMNLP, 2023, pp. 543-553.
[339] K. Li, Y. He, Y. Wang et al., "Videochat: Chat-centric video understanding," arxiv, 2023.
[340] S. Wu, H. Fei, L. Qu et al., "Next-gpt: Any-to-any multimodal llm," in ICML, 2024.
[341] P. F. Christiano, J. Leike, T. Brown et al., "Deep reinforcement learning from human preferences," in NeurIPS, 2017.
[342] L. Ouyang, J. Wu, X. Jiang et al., "Training language models to follow instructions with human feedback," in NeurIPS, 2022.
[343] X. Wu, K. Sun, F. Zhu et al., "Human preference score: Better aligning text-to-image models with human preference," in ICCV, 2023, pp. 2096-2105.
[344] R. Rafailov, A. Sharma, E. Mitchell et al., "Direct preference optimization: Your language model is secretly a reward model," in NeurIPS, vol. 36, 2023, pp. 53702-53741.
[345] J. Schulman, F. Wolski, P. Dhariwal et al., "Proximal policy optimization algorithms," arxiv, 2017.
[346] A. Radford, J. W. Kim, C. Hallacy et al., "Learning transferable visual models from natural language supervision," in ICML, 2021.
[347] M. J. Kim, K. Pertsch, S. Karamcheti et al., "Openvla: An open-source vision-language-action model," arxiv, 2024.
[348] Z. Guan, H. Sun, Y. Guo et al., "RL-VLA 3: Reinforcement learning VLA accelerating via full asynchronism," arxiv, 2026.
[349] B. Zitkovich, T. Yu, S. Xu et al., "RT-2: Vision-language-action models transfer web knowledge to robotic control," in CoRL, 2023.
[350] Q. Xu, J. Liu, R. Zhou et al., "TwinRL-VLA: Digital twin-driven reinforcement learning for real-world robotic manipulation," arxiv, 2026.
[351] G. Lu, S. Zhang, Z. Wang et al., "ManiGaussian: Dynamic gaussian splatting for multi-task robotic manipulation," in ECCV, 2024.
[352] Y. J. Ma, W. Liang, H.-J. Wang et al., "DReureka: Language model guided sim-to-real transfer, 2024," arxiv, 2024.
[353] B. Zhang, Y. Zhang, J. Ji et al., "SafeVLA: Towards safety alignment of vision-language-action model via constrained learning," arxiv, 2025.
[354] Y. Wang, Z. Xian, F. Chen et al., "RoboGen: Towards unleashing infinite data for automated robot learning via generative simulation," arxiv, 2023.
[355] S. Liu, Y. Zhang, W. Li et al., "Video-P2P: Video editing with cross-attention control," in CVPR, 2024, pp. 8599-8608.
[356] J.-g. Kwak, E. Dong, Y. Jin et al., "Vivid-1-to-3: Novel view synthesis with video diffusion models," in CVPR, 2024, pp. 6775-6785.

我已準備好翻譯論文段落。請提供您要翻譯的論文內容，我會遵循所有規則：

1. ✓ 翻譯成繁體中文（台灣用語）
2. ✓ 保留專有名詞、模型名稱、縮寫的英文原文
3. ✓ 維持學術文章的嚴謹語氣
4. ✓ 保留所有 Markdown 格式符號
5. ✓ 完整保留表格的 Markdown 格式
6. ✓ 保留 [FIGURE:xxx] 等標記
7. ✓ 保留 LaTeX 數學式
8. ✓ 不新增任何標記，只翻譯文字

請貼上您的論文段落內容。

## 一致性的三位一體作為通用世界模型的定義原則

[357] M. You, Z. Zhu, H. Liu et al., "Nvs-solver: Video diffusion model as zero-shot novel view synthesizer," arxiv, 2024.
[358] X. Ren, T. Shen, J. Huang et al., "Gen3c: 3d-informed world-consistent video generation with precise camera control," in PR, 2025, pp. 6121-6132.
[359] Q. Zhang, S. Zhai, M. A. B. Martin et al., "World-consistent video diffusion with explicit 3d modeling," in PR, 2025, pp. 21685-21695.
[360] Z. Gu, R. Yan, J. Lu et al., "Diffusion as compiler: 3d-aware video diffusion for versatile video generation control," in PR, 2025, pp. 1-12.
[361] Y.-J. Yuan, L. Kobbelt, J. Liu et al., "4-dynamic: Text-to-4d generation with hybrid priors," arxiv, 2024.
[362] R. Wu, R. Gao, B. Poole et al., "Cat4d: Create anything in 4d with multi-view video diffusion models," in CVPR, 2025, pp. 26057-26068.
[363] J. Wang, N. Karaev, C. Rupprecht et al., "Vggsfm: Visual geometry grounded deep structure from motion," in CVPR, 2024.
[364] J. Li, Q. Long, J. Zheng et al., "T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design," arxiv, 2024.
[365] H. Yuan, S. Zhang, X. Wang et al., "Instructvideo: Instructing video diffusion models with human feedback," in CVPR, 2024, pp. 6463-6474.
[366] B. Li, C. Zheng, W. Zhu et al., "Vivid-zoo: Multi-view video generation with diffusion model," PR, vol. 37, pp. 62189-6222, 2024.
[367] Z. Yang, Z. Pan, C. Gu et al., "Diffusion 2: Dynamic 3d content generation via score composition of video and multi-view diffusion models," arxiv, 2024.
[368] X. Ren, T. Shen, J. Huang et al., "Gen3c: 3d-informed world-consistent video generation with precise camera control," in CVPR, 2025.
[369] T. Li, G. Zheng, R. Jiang et al., "Realcam-i2v: Real-world image-to-video generation with interactive complex camera control," in ICCV, 2025, pp. 28785-28796.
[370] W. Yu, J. Xing, L. Yuan et al., "Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis," arxiv, 2024.
[371] Z. Wang, J. Cho, J. Li et al., "Epic: Efficient video camera control learning with precise anchor-video guidance," arxiv, 2025.
[372] H. He, Y. Xu, Y. Guo et al., "Cameractrl: Enabling camera control for text-to-video generation," arxiv, 2024.
[373] X. Fan, S. Girish, V. Ramanujan et al., "Omniview: An all-seeing diffusion model for 3d and 4d view synthesis," arxiv, 2025.
[374] Y. Chen, Z. Ye, Z. Fang et al., "Postcam: Camera-controllable novel-view video generation with query-shared cross-attention," arxiv, 2025.
[375] L. Hollein, A. Bozic, N. Muller et al., "Viewdiff: 3d-consistent image generation with text-to-image models," in CVPR, 2024, pp. 5043-5052.
[376] S. Bahmani, I. Skorokhodov, A. SIarohin et al., "Vd3d: Taming large video diffusion transformers for 3d camera control," arxiv, 2024.
[377] S. Bahmani, X. Liu, W. Yifan et al., "Tc4d: Trajectory-conditioned text-to-4d generation," in ECCV. Springer, 2024, pp. 53-72.
[378] X. F ste, X. Liu, X. Wang et al., "3drajmaster: Mastering 3d trajectory for multi-entity motion in video generation," arxiv, 2024.
[379] V. Voleti, C.-H. Yao, M. Boss et al., "Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion," in ECCV, 2024.

我已準備好翻譯論文段落。請提供您要翻譯的內容，我會按照您指定的規則進行翻譯：

1. ✅ 繁體中文（台灣用語）
2. ✅ 專有名詞、模型名稱、縮寫保留英文
3. ✅ 保持學術嚴謹語氣
4. ✅ 保留所有 Markdown 格式符號
5. ✅ 表格 Markdown 格式完整保留
6. ✅ 保留 [FIGURE:xxx]、[FIGURE_CAPTION] 標記
7. ✅ 保留 LaTeX 數學式
8. ✅ 不自行新增方括號標記

請貼上您要翻譯的論文段落。

# The Trinity of Consistency as a Defining Principle for General World Models

[380] H. Zhang, X. Chen, Y. Wang et al., "4diffusion: Multi-view video diffusion model for 4d generation," PR, vol. 37, pp. 15 272-15 295, 2024.

[381] G. Wu, T. Yi, J. Fang et al., "4d gaussian splatting for real-time dynamic scene rendering," in CVPR, 2024, pp. 20 310-20 320.

[382] Y. Jiang, L. Zhang, J. Gao et al., "Consistent4d: Consistent 360 dynamic object generation from monocular video," arxiv, 2023.

[383] H. Sun, X. Li, L. Shen et al., "Dyblurf: Dynamic neural radiance fields from blurry monocular video," in CVPR, 2024, pp. 7517-7527.

[384] Z. Li, Q. Wang, F. Cole et al., "Dynibar: Neural dynamic image-based rendering," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 4273-4284.

[385] S. Bahmani, I. Skorokhodov, V. Rong et al., "4d-fy: Text-to-4d generation using hybrid score distillation sampling," in CVPR, 2024, pp. 7996-8006.

[386] J. Fang, T. Yi, X. Wang et al., "Fast dynamic radiance fields with time-aware neural voxels," in ACM SIGGRAPH, 2022, pp. 1-9.

[387] M.-Q. V. Bui, J. Park, J. Oh et al., "Moblurf: Motion deblurring neural radiance fields for blurry monocular video," IEEE T-PAMI, 2025.

[388] Y. Xie, C.-H. Yao, V. Voleti et al., "Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency," arxiv, 2024.

[389] S. Fridovich-Keil, G. Meanti, F. R. Warburg et al., "K-planes: Explicit radiance fields in space, time, and appearance," in CVPR, 2023.

[390] Y. Zeng, Y. Jiang, S. Zhu et al., "Stag4d: Spatial-temporal anchored generative 4d gaussians," in ECCV. Springer, 2024, pp. 163-179.

[391] J. Ren, L. Pan, J. Tang et al., "Dreamgaussian4d: Generative 4d gaussian splatting," arxiv, 2023.

[392] H. Ling, S. W. Kim, A. Torralba et al., "Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models," in CVPR, 2024, pp. 8576-8588.

[393] B. He, Y. Chen, G. Lu et al., "H3d-dgs: Exploring heterogeneous 3d motion representation for deformable 3d gaussian splatting," in NeurIPS, 2025.

[394] H. Liang, Y. Yin, D. Xu et al., "Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models," arxiv, 2024.

[395] X. Liu, Y. Xiao, D. Y. Chen et al., "Trace anything: Representing any video in 4d via trajectory fields," arxiv, 2025.

[396] Q. Wang, Y.-Y. Chang, R. Cai et al., "Tracking everything everywhere all at once," in ICCV, 2023.

[397] Y. Xiao, Q. Wang, S. Zhang et al., "Spatialtracker: Tracking any 2d pixels in 3d space," in CVPR, 2024.

[398] V. Leroy, Y. Cabon, and J. Revaud, "Grounding image matching in 3d with mast3r," in ECCV, 2024.

[399] H. Huang, H. Chen, S. Wu et al., "Vistadpo: Video hierarchical spatial-temporal direct preference optimization for large video models," arxiv, 2025.

[400] N. Zhou, J. Chen, and D. Huang, "Dr-tune: Improving fine-tuning of pretrained visual models by distribution regularization with semantic calibration," in ICCV, 2023, pp. 1547-1556.

[401] H. Wang, X. Du, J. Li et al., "Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation," in CVPR, 2023.

[402] B. Efron, "Tweedie's formula and selection bias," JASA, vol. 106, no. 496, pp. 1602-1614, 2011.

[403] D. Kim, Y. Kim, S. J. Kwon et al., "Refining generative process with discriminator guidance in score-based diffusion models," in ICML. ML Research Press, 2023, pp. 16 567-16 598.

我已準備好翻譯論文段落。請提供您要翻譯的具體論文段落內容，我將按照您指定的翻譯規則進行翻譯：

1. ✅ 翻譯成繁體中文（台灣用語）
2. ✅ 專有名詞、模型名稱、縮寫保留英文
3. ✅ 保持學術文章的嚴謹語氣
4. ✅ 保留所有 Markdown 格式符號
5. ✅ 表格的 Markdown 格式完整保留
6. ✅ 保留方括號標記如 [FIGURE:xxx]、[FIGURE_CAPTION]
7. ✅ 保留 LaTeX 數學式
8. ✅ 只翻譯文字內容，不新增方括號標記

請貼上需要翻譯的段落內容。

# 一致性三位一體作為通用世界模型的定義原則

[404] Y. Du, C. Durkan, R. Strudel et al., "Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc," in ICML. PMLR, 2023, pp. 8489–8510.

[405] N. Liu, S. Li, Y. Du et al., "Compositional visual generation with composable diffusion models," in ECCV, 2022.

[406] Y. Wei, S. Zhang, Z. Qing et al., "Dreamvideo: Composing your dream videos with customized subject and motion," in CVPR, 2024.

[407] C. Lu, Y. Zhou, F. Bao et al., "Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps," in NeurIPS, 2022.

[408] W. Zhao, L. Bai, Y. Rao et al., "Unipc: A unified predictor-corrector framework for fast sampling of diffusion models," in NeurIPS, 2023.

[409] J. Zhu and P. Zhuang, "Hifa: High-fidelity text-to-3d generation with advanced diffusion guidance," in arxiv, 2024.

[410] S. Yang, Y. Zhou, Z. Liu et al., "Freeu: Free-lunch for diffusion models," in CVPR, 2024.

[411] W. Li, R. Chen, X. Chen et al., "Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d," in arxiv, 2024.

[412] Z. Yang, Z. Pan, C. Gu et al., "Diffusion 2: Dynamic 3d content generation via score composition of video and multi-view diffusion models," arxiv, 2024.

[413] Y. Zhang, Y. Wei, D. Jiang et al., "Controlvideo: Training-free controllable text-to-video generation," in arxiv, 2023.

[414] C. Mou, X. Wang, L. Xie et al., "T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models," in AAAI, 2024.

[415] J. L. Schonberger and J.-M. Frahm, "Structure-from-motion revisited," in CVPR, 2016.

[416] Z. Teed and J. Deng, "Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras," in NeurIPS, 2021.

[417] A. Yu, V. Ye, M. Tancik et al., "pixelnerf: Neural radiance fields from one or few images," in CVPR, 2021, pp. 4578–4587.

[418] M. Suhail, C. E. a. L. S. Perez, and A. Makadia, "Generalizable patch-based neural rendering," in ECCV, 2022.

[419] H. Ni, C. Shi, K. Li et al., "Conditional image-to-video generation with latent flow diffusion models," in CVPR, 2023.

[420] J. Zeng, L. Qiu, F. Li et al., "Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior," in ICCV, 2023.

[421] A. Cao and J. Johnson, "Hexplane: A fast representation for dynamic scenes," in CVPR, 2023.

[422] X. Liu, Y. Xiao, D. Y. Chen et al., "Trace anything: Representing any video in 4d via trajectory fields," arxiv, 2025.

[423] A. Yu, S. Fridovich-Keil, M. Tancik et al., "Plenoxels: Radiance fields without neural networks," in CVPR, 2022, pp. 5501–5510.

[424] S. Singh, S. Abu-El-Haija, N. Johnston et al., "End-to-end learning of compressible features," in NeurIPS, vol. 35, 2022, pp. 11750–11762.

[425] S. Fridovich-Keil, G. Meanti, F. R. Warburg et al., "K-planes: Explicit radiance fields in space, time, and appearance," in CVPR, 2023.

[426] R. Shao, Z. Zheng, H. Tu et al., "Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering," in CVPR, 2023.

我已準備好翻譯論文段落。

請提供您要翻譯的論文段落，我會：

1. ✓ 翻譯成繁體中文（台灣用語）
2. ✓ 保留專有名詞、模型名稱、縮寫的英文（如 Transformer、RLHF、LoRA 等）
3. ✓ 維持學術文章的嚴謹語氣，完整保留所有內容
4. ✓ 保留所有 Markdown 格式符號（#、**、*、|、-）
5. ✓ 表格的 Markdown 格式完整保留，只翻譯內容
6. ✓ 保留 [FIGURE:xxx]、[FIGURE_CAPTION] 等方括號標記
7. ✓ 保留 LaTeX 數學式（$...$ 或 $$...$$）
8. ✓ 不自行新增方括號標記

請貼上您要翻譯的論文段落。

# The Trinity of Consistency as a Defining Principle for General World Models

[427] Z. Wu, C. Yu, Y. Jiang et al., "Sc4d: Sparse-controlled video-to-4d generation and motion transfer," in ECCV, 2024.
[428] M. Zwicker, H. Pfister, J. Van Baar et al., "Surface splatting," in ACM SIGGRAPH, 2001, pp. 371-378.
[429] C. Lassner and M. Zollhofer, "Pulsar: Efficient sphere-based neural rendering," in CVPR, 2021, pp. 1440-1449.
[430] Y.-H. Huang, Y.-T. Sun, Z. Yang et al., "Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes," in CVPR, 2024.
[431] C. Chen, S. Huang, X. Chen et al., "Ct4d: Consistent text-to-4d generation with animatable meshes," arxiv, 2024.
[432] N. Karaev, Y. Makarov, J. Wang et al., "Cotracker3: Simpler and better point tracking by pseudo-labelling real videos," in ICCV, 2025.
[433] R. Villegas, J. Yang, S. Hong et al., "Decomposing motion and content for natural video sequence prediction," in ICLR, 2017.
[434] R. Villegas, J. Yang, Y. Zou et al., "Learning to generate long-term future via hierarchical prediction," in ICML, 2017.
[435] J. Wei, Y. Tay, R. Bommasani et al., "Emergent abilities of large language models," TMLR, 2022.
[436] A. Ramesh, P. Dhariwal, A. Nichol et al., "Hierarchical text-conditional image generation with clip latents," arxiv, vol. 1, no. 2, p. 3, 2022.
[437] J. Gu, S. Wang, H. Zhao et al., "Reuse and diffuse: Iterative denoising for text-to-video generation," arxiv, 2023.
[438] J. Bruce, M. D. Dennis, A. Edwards et al., "Genie: Generative interactive environments," in ICML. PMLR, 2024, pp. 4603-4623.
[439] D. Valevski, Y. Leviathan, M. Arar et al., "Diffusion models are real-time game engines," in ICLR, 2024.
[440] Runway, "Introducing gen-3 alpha: A new frontier for video generation," https://runwayml.com/research/introducing-gen-3-alpha, 2024, accessed: 2025-02-24.
[441] HailuoAI, "Hailuo," https://tailuoai.video/, 2024, accessed: 2025-02-24.
[442] Y. HaCohen, N. Chiprut, B. Brazowski et al., "Ltx-video: Realtime video latent diffusion," arxiv, 2024.
[443] X. He, C. Peng, Z. Liu et al., "Matrix-game 2.0: An open-source real-time and streaming interactive world model," arxiv, 2025.
[444] H. Team, Z. Wang, Y. Liu et al., "Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels," arxiv, 2025.
[445] M. Assran, Q. Duval, I. Misra et al., "Self-supervised learning from images with a joint-embedding predictive architecture," in CVPR, 2023, pp. 15619-15629.
[446] D. Hafner, J. Pasukonis, J. Ba et al., "Mastering diverse domains through world models," in arxiv, 2024.
[447] J. Cen, C. Yu, H. Yuan et al., "Worldvla: Towards autoregressive action world model," arxiv, 2025.
[448] H. Zhen, X. Qiu, P. Chen et al., "3d-vla: A 3d vision-language-action generative world model," in arxiv, 2024.
[449] Y. Hu, J. Yang, L. Chen et al., "Planning-oriented autonomous driving," in CVPR, 2023.
[450] X. Tian, J. Gu, B. Li et al., "Drivevlm: The convergence of autonomous driving and large vision-language models," in ECCV, 2024.
[451] X. Li, K. Hsu, J. Gu et al., "Evaluating real-world robot manipulation policies in simulation," arxiv, 2024.
[452] J. Gu, F. Xiang, X. Li et al., "Maniskill2: A unified benchmark for generalizable manipulation skills," arxiv, 2023.

我已準備好翻譯學術論文段落。請提供您要翻譯的論文段落文本，我會根據以下規則進行翻譯：

✓ 翻譯成繁體中文（台灣用語）
✓ 保留專有名詞、模型名稱、縮寫的英文
✓ 保持學術嚴謹語氣
✓ 完整保留所有 Markdown 格式符號（#、**、*、|、-）
✓ 保留表格的 Markdown 格式，只翻譯文字
✓ 保留 [FIGURE:xxx]、[FIGURE_CAPTION] 等標記
✓ 保留 LaTeX 數學式（$...$ 或 $$...$$）
✓ 不新增任何方括號標記

請將論文段落貼上，我會立即進行翻譯。

# The Trinity of Consistency as a Defining Principle for General World Models

[453] L. Wang, Y. Ling, Z. Yuan et al., "Gensim: Generating robotic simulation tasks via large language models," arxiv, 2023.
[454] J. Pearl, *Causality*. Cambridge university press, 2009.
[455] Y. Bengio, "From system 1 deep learning to system 2 deep learning," in NeurIPS, 2019.
[456] R. Firoozi, J. Tucker, S. Tian et al., "Foundation models in robotics: Applications, challenges, and the future," arxiv, 2023.
[457] Z. Ji, N. Lee, R. Frieske et al., "Survey of hallucination in natural language generation," ACM Comput. Surv., vol. 55, pp. 1–38, 2023.
[458] B. Goertzel, "Artificial general intelligence: Concept, state of the art, and future prospects," J. Artif. Gen. Intell., vol. 5, no. 1, pp. 1–48, 2014.
[459] G. Marcus, "The next decade in ai: Four steps towards robust artificial intelligence," arxiv, 2020.
[460] D. Bear, E. Wang, D. Mrowca et al., "Physion: Evaluating physical prediction from vision in humans and machines," in NeurIPS, 2021.
[461] J. Mao, C. Gan, P. Kohli et al., "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision," in ICLR, 2019.
[462] V. Saxena, J. Ba, and D. Hafner, "Clockwork variational autoencoders," NeurIPS, vol. 34, pp. 29 246–29 257, 2021.
[463] X. Pan, A. Tewari, T. Leimkuhler et al., "Drag your gan: Interactive point-based manipulation on the generative image manifold," in ACM SIGGRAPH. ACM, 2023, pp. 1–11.
[464] T. Brooks, A. Holynski, and A. A. Efros, "Instructpix2pix: Learning to follow image editing instructions," in CVPR, 2023, pp. 18 392–18 402.
[465] Y. Hu, L. Anderson, T.-M. Li et al., "Difftaichi: Differentiable programming for physical simulation," in ICLR, 2020.
[466] J. S. Park, J. O'Brien, C. J. Cai et al., "Generative agents: Interactive simulacra of human behavior," in ACM UIST, 2023, pp. 1–22.
[467] J. Leibo, V. Zambaldi, M. Lanctot et al., "Multi-agent reinforcement learning in sequential social dilemmas," in AAMAS, vol. 16. ACM, 2017, pp. 464–473.
[468] Y. Shoham and K. Leyton-Brown, "Multiagent systems," Cambridge Books, 2009.
[469] D. Silver, T. Hubert, J. Schrittwieser et al., "A general reinforcement learning algorithm that masters chess, shogi, and go through self-play," Science, vol. 362, no. 6419, pp. 1140–1144, 2018.
[470] W. Hong, W. Wang, Q. Lv et al., "Cogagent: A visual language model for gui agents," in CVPR, 2024, pp. 14 281–14 290.
[471] Z. Xi, W. Chen, X. Guo et al., "The rise and potential of large language model based agents: A survey," Sci. China Inf. Sci., vol. 68, no. 2, p. 121101, 2025.
[472] L. Wang, C. Ma, X. Feng et al., "A survey on large language model based autonomous agents," Front. Comput. Sci., vol. 18, no. 6, p. 186345, 2024.
[473] Y. Liang, W. Chow, F. Li et al., "Rover: Benchmarking reciprocal cross-modal reasoning for omnimodal generation," arxiv, 2025.
[474] Y. Niu, W. Jin, J. Liao et al., "Does understanding inform generation in unified multimodal models? from analysis to path forward," arxiv, 2025.
[475] Y. Niu, M. Ning, M. Zheng et al., "Wise: A world knowledge-informed semantic evaluation for text-to-image generation," arxiv, 2025.
[476] T. Zhang, H.-X. Yu, R. Wu et al., "Physdreamer: Physics-based interaction with 3d objects via video generation," in ECCV, 2024.

我已準備好進行翻譯。請提供您想要翻譯的論文段落，我會按照以下規則進行翻譯：

✓ 翻譯成繁體中文（台灣用語）
✓ 保留專有名詞、模型名稱、縮寫的英文
✓ 維持學術文章的嚴謹語氣
✓ 保留所有 Markdown 格式符號
✓ 完整保留表格的 Markdown 格式
✓ 保留方括號標記和 LaTeX 數學式
✓ 不自行新增任何標記

請貼上論文段落，我會立即翻譯。

# The Trinity of Consistency as a Defining Principle for General World Models

[477] C. Yang, H. Wan, Y. Peng et al., "Reasoning via video: The first evaluation of video models' reasoning abilities through maze-solving tasks," arxiv, 2025.

[478] Z. Huang, Y. He, J. Yu et al., "Vbench: Comprehensive benchmark suite for video generative models," in CVPR, 2024.

[479] F. Meng, J. Liao, X. Tan et al., "Towards world simulator: Crafting physical commonsense-based benchmark for video generation," arxiv, 2024.

[480] H. H. Chen, D. Lan, W.-J. Shu et al., "Tivibench: Benchmarking think-in-video reasoning for video generative models," arxiv, 2025.

[481] Z. Guo, X. Chen, R. Zhang et al., "Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark," arxiv, 2025.

[482] W. Chow, J. Pan, Y. Liang et al., "Weave: Unleashing and benchmarking the in-context interleaved comprehension and generation," arxiv, 2025.

[483] J. Tong, Y. Mou, H. Li et al., "Thinking with video: Video generation as a promising multimodal reasoning paradigm," arxiv, 2025.

[484] Y. Luo, X. Zhao, B. Lin et al., "V-reasonbench: Toward unified reasoning benchmark suite for video generation models," arxiv, 2025.

[485] J. Wei, C. Jia, X. Bai et al., "Ggbench: A geometric generative reasoning benchmark for unified multimodal models," arxiv, 2025.

[486] C. Holtermann, N. Krebs, and A. Lauscher, "Tempviz: On the evaluation of temporal knowledge in text-to-image models," arxiv, 2026.

[487] M. Heusel, H. Ramsauer, T. Unterthiner et al., "Gans trained by a two time-scale update rule converge to a local nash equilibrium," NeurIPS, vol. 30, 2017.

[488] A. Borji, "Pros and cons of gan evaluation measures," CVIU, vol. 179, pp. 41-65, 2019.

[489] S. Tong, Z. Liu, Y. Zhai et al., "Eyes wide shut? exploring the visual shortcomings of multimodal llms," in CVPR, 2024.

[490] Z. Shao, P. Wang, Q. Zhu et al., "Deepseekmath-v2: Towards self-verifiable mathematical reasoning," arxiv, 2025.

[491] N. Carlini, J. Hayes, M. Nasr et al., "Extracting training data from diffusion models," in USENIX SEC, 2023, pp. 5253-5270.

[492] L. S. Piloto, A. Weinstein, P. Battaglia et al., "Intuitive physics learning in a deep-learning model inspired by developmental psychology," Nat. Hum. Behav, vol. 6, no. 9, pp. 1257-1267, 2022.

[493] K. Yi, C. Gan, Y. Li et al., "Clevrer: Collision events for video representation and reasoning," in ICLR, 2021.

[494] V. Voleti, A. Jolicoeur-Martineau, and C. Pal, "Mcvd-masked conditional video diffusion for prediction, generation, and interpolation," NeurIPS, vol. 35, pp. 23-371-23 385, 2022.

[495] H. Lightman, V. Kosaraju, Y. Burda et al., "Let's verify step by step," in ICLR, 2023.

[496] G. E. Karniadakis, I. G. Kevrekidis, L. Lu et al., "Physics-informed machine learning," Nat. Rev. Phys., vol. 3, no. 6, pp. 422-440, 2021.

[497] O. Ahmed, F. Trauble, A. Goyal et al., "Causalworld: A robotic manipulation benchmark for causal structure and transfer learning," in ICLR, 2021.

[498] J. Hessel, A. Holtzman, M. Forbes et al., "Clipscore: A reference-free evaluation metric for image captioning," in EMNLP, 2021, pp. 7514-7528.

[499] J. Tobin, R. Fong, A. Ray et al., "Domain randomization for transferring deep neural networks from simulation to the real world," in IEEE/RSJ IROS. IEEE, 2017, pp. 23-30.

[500] D. Silver, S. Singh, D. Precup et al., "Reward is enough," AI, vol. 299, p. 103535, 2021.

# The Trinity of Consistency as a Defining Principle for General World Models

[501] OpenAI, "GPT-Image-1: Image Generation API," https://openai.com/index/image-generation-api/, 2025, accessed: 2025-01.
[502] T. Seedream, Y. Chen, Y. Gao et al., "Seedream 4.0: Toward next-generation multimodal image generation," arxiv, 2025.
[503] G. Comanici, E. Bieber, M. Schaekermann et al., "Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities," arxiv, 2025.
[504] OpenAI, "GPT-Image-1.5: New ChatGPT Image Generation," https://openai.com/zh-Hans-CN/index/new-chatgpt-images-is-here, 2025, accessed: 2025-01.
[505] R. Mroczkowski, P. Rybak, A. Króblewska et al., "Herbert: Efficiently pretrained transformer-based language model for polish," in BSNLP, 2021, pp. 1-10.
[506] B. Wu, C. Zou, C. Li et al., "Hunyuanvideo 1.5 technical report," arxiv, 2025.
[507] J. Xu, X. Zou, K. Huang et al., "Easyanimate: A high-performance long video generation method based on transformer architecture," arxiv, 2024.
[508] D. Li, Z. Fei, T. Li et al., "Skyreels-v3 technique report," arxiv, 2026.
[509] C. Wu, J. Li, J. Zhou et al., "Qwen-image technical report," arxiv, 2025.
[510] C. Wei, Q. Liu, Z. Ye et al., "Univideo: Unified understanding, generation, and editing for videos," arxiv, 2025.
[511] Y. Cui, H. Chen, H. Deng et al., "Emu3.5: Native multimodal models are world learners," arxiv, 2025.
[512] H. Al-Tahan, Q. Garrido, R. Balestrieri et al., "Unibench: Visual reasoning requires rethinking vision-language beyond scaling," Advances in Neural Information Processing Systems, vol. 37, pp. 82411-82437, 2024.
[513] H. Zhou, Q. Xu, Y. Dong et al., "Manbench: Is your multimodal model smarter than human?" arxiv, 2025.
[514] J. Parker-Holder, P. Ball, J. Bruce et al., "Genie 2: A large-scale foundation world model," https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model, 2024, accessed: 2026-02.
[515] Google DeepMind, "Genie 3: A real-time interactive world model," 2025, technical report.
[516] PixVerse Research, "Pixverse-r1: Next-generation real-time world model," https://pixverse.ai/en/blog/pixverse-r1-next-generation-real-time-world-model, 2026, technical report on real-time multimodal world model for interactive video generation.