# 論文摘要翻譯

視覺-語言-動作（VLA）模型統一了具身智能體的感知、語言和控制能力，但在實際部署中面臨重大挑戰，特別是隨著模型擴展到更長的時間範圍和更大的主幹網路時，計算和記憶體需求迅速增加。為了解決這些瓶頸，我們引入了 QuantVLA，一個無訓練的後訓練量化（PTQ）框架。據我們所知，這是首個針對 VLA 系統的 PTQ 方法，也是首個成功量化擴散 Transformer（DiT）動作頭的方法。QuantVLA 包含三個經過尺度校準的組件：（1）選擇性量化佈局，將語言主幹和 DiT 中的所有線性層整數化，同時保持注意力投影為浮點格式以保留原始運算子排程；（2）注意力溫度匹配，一種輕量級的逐頭縮放機制，用於穩定注意力 logits，並在推論時折疊到反量化尺度中；（3）輸出頭平衡，一種逐層殘差介面校準，用於緩解投影後的能量漂移。該框架無需額外訓練，僅使用一個小型無標籤校準緩衝區，並支援低位寬權重和激活值的整數核心，同時保持架構不變。在 LIBERO 上的代表性 VLA 模型中，QuantVLA 超越了全精度基線的任務成功率，在量化組件上達到約 70% 的相對記憶體節省，並在端到端推論延遲上實現 1.22 倍的加速，為在嚴格的計算、記憶體和功耗限制下實現可擴展的低位寬具身智能提供了實用途徑。