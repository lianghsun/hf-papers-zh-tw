# 論文摘要翻譯

近期 AI 輔助程式設計的進展使得代理能夠透過命令列介面執行複雜的工作流程，然而現有的基準測試受限於短期任務範圍、GitHub 爬蟲的資料污染，以及缺乏細粒度的評估指標，無法嚴格評估現實軟體工程中必需的長期規劃和執行能力。為了解決這些問題，我們推出 LongCLI-Bench，一個全面的基準測試，旨在評估代理在長期、現實任務中的能力。我們從超過 1,000 個電腦科學課程作業和現實工作流程中精心挑選了 20 個高品質、長期任務，涵蓋四個工程類別：從零開始、功能添加、臭蟲修復和重構。我們為 LongCLI-Bench 提出了雙集測試協議，該協議測量需求滿足度（失敗轉通過）和迴歸避免（通過轉通過），並納入步驟級評分以精確定位執行失敗。廣泛的實驗顯示，即使是最先進的代理在 LongCLI-Bench 上的通過率也低於 20%。步驟級分析進一步表明，大多數任務在完成度不足 30% 時停滯，突顯出關鍵失敗往往發生在早期階段。儘管自我修正能夠帶來邊際收益，但透過計畫注入和互動式指導的人-代理協作能產生顯著更高的改進。這些結果強調，未來的研究必須強調開發協同人-代理工作流程，同時推進代理的規劃和執行能力，以克服長期任務性能中的關鍵挑戰。