# 論文摘要翻譯

使用 Transformer 模型高效處理長序列通常需要通過上下文並行化在加速器之間分割計算。這類方法中的主流方法，例如 Ring Attention 或 DeepSpeed Ulysses，能夠在上下文維度上進行擴展，但不著重於記憶體效率，這限制了它們能支援的序列長度。更先進的技術，例如 Fully Pipelined Distributed Transformer 或啟動值卸載，可以進一步延伸可能的上下文長度，但代價是訓練吞吐量的降低。在本論文中，我們提出 UPipe，一種簡單但有效的上下文並行化技術，在注意力頭級別進行細粒度分塊。該技術顯著減少了自注意力的啟動值記憶體使用，突破了啟動值記憶體的瓶頸，釋放了更長的上下文長度。我們的方法將 32B Transformer 中注意力層的中間張量記憶體使用量減少了多達 87.5%，同時在訓練速度方面與先前的上下文並行化技術相當。UPipe 在單個 8×H100 節點上訓練 Llama3-8B 時可支援 5M tokens 的上下文長度，相比之前的方法改進超過 25%。