# LLM 智能體通用基準測試

LLM 智能體越來越需要作為通用系統來運作，以解決開放性的用戶請求。雖然現有基準測試側重於領域感知的環境來開發專門化智能體，但評估通用智能體需要更加真實的設置，使其在統一環境中跨越多項技能和工具進行運作。我們引入 General AgentBench，一個提供統一框架的基準測試，用於在搜尋、編碼、推理和工具使用等領域評估通用 LLM 智能體。利用 General AgentBench，我們系統地研究了在順序擴展（迭代交互）和平行擴展（採樣多個軌跡）下的測試時間擴展行為。對十個領先 LLM 智能體的評估揭示了從領域特定評估轉移到此通用智能體設置時的顯著性能衰退。此外，我們發現由於兩個根本限制——順序擴展中的上下文天花板和平行擴展中的驗證差距——兩種擴展方法在實踐中都無法產生有效的性能改進。代碼已在 https://github.com/cxcscmu/General-AgentBench 公開發佈。