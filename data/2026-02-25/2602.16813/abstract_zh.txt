# 論文摘要翻譯

基於離散擴散的語言模型因其相比自迴歸模型提供更快生成速度的潛力而受到廣泛關注。然而在實踐中，這些模型在少步驟（few-step）情景下表現出樣本品質的急劇下降，未能實現這一承諾。本文顯示利用基於流的連續去噪的語言模型可以在品質和速度上都優於離散擴散。通過重新審視離散模態上的流的基礎理論，我們構建了一個流基語言模型（FLM），它在獨熱編碼令牌上執行歐幾里得去噪。我們證明該模型可以通過交叉熵目標預測乾淨數據進行訓練，其中我們引入了簡單的時間重參數化，大幅改進了訓練穩定性和生成品質。通過將 FLM 蒸餾到其相關的流映射中，我們獲得了一個蒸餾流映射語言模型（FMLM），能夠執行少步驟生成。在 LM1B 和 OWT 語言數據集上，FLM 實現了與最先進離散擴散模型相匹配的生成品質。使用 FMLM，我們的方法在各方面都優於最近的少步驟語言模型，其單步生成超過了它們的八步品質。我們的工作質疑了廣泛持有的假設——即離散擴散過程對於離散模態上的生成建模是必要的，並為大規模加速流基語言模型開闢了道路。代碼可在 https://github.com/david3684/flm 獲取。