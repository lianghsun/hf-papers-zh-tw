# 論文摘要翻譯

大型視覺語言模型（VLMs）已透過反覆優化方法在複雜視覺理解任務上展示出顯著的潛力。然而，這些模型普遍缺乏有效的自我修正機制，使得它們難以獨立地糾正認知偏差。因此，在多輪修訂過程中，它們往往陷入重複且無效的嘗試，無法實現答案品質的穩定改善。為了解決這個問題，我們提出了一個新穎的反覆自我修正框架，賦予模型兩項關鍵能力：能力反思（Capability Reflection）和記憶反思（Memory Reflection）。該框架引導模型首先透過能力反思診斷錯誤並生成修正計畫，隨後利用記憶反思來回顧過去的嘗試以避免重複並探索新的解決方案，最後透過嚴謹的重新推理來優化答案。在具有挑戰性的 OCRBench v2 基準測試上的實驗表明，OCR-Agent 在英文子集上超越現有開源最先進模型 InternVL3-8B 達 +2.0，在中文子集上達 +1.2，同時在視覺理解（79.9）和推理（66.5）方面取得最先進的結果，甚至超越了更大的微調模型。我們的方法表明，結構化的自我意識反思可以顯著增強 VLMs 的推理穩健性，而無需額外訓練。程式碼：https://github.com/AIGeeksGroup/OCR-Agent。