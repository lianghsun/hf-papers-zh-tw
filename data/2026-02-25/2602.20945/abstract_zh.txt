# 論文摘要翻譯

大型語言模型（LLMs）持續受益於大規模思維鏈（Chain-of-Thought, CoT）推理，但同時也面臨沉重的計算開銷。為解決此問題，高效推理旨在透過強化學習（Reinforcement Learning, RL）進行獎勵塑形，激勵簡短而準確的思考軌跡。在本論文中，我們系統性地調查 LLMs 高效推理的機制。為進行全面評估，我們倡導採用更精細的指標，包括基於正確性的長度分佈以及涵蓋 2k 到 32k tokens 廣泛範圍內的性能表現。首先，我們揭示訓練過程遵循兩階段範式：長度自適應與推理精化。隨後，我們進行廣泛實驗（約 0.2 百萬 GPU 小時），在統一協議下解構訓練提示詞與推出、獎勵塑形以及最佳化策略。特別是，一個關鍵發現是在相對較簡單的提示詞上進行訓練，確保正獎勵信號的密度，從而避免長度崩潰。同時，所學習的長度偏差可以跨領域推廣。我們將所有發現凝煉為寶貴的洞察與實踐指南，並進一步在 Qwen3 系列中驗證，涵蓋 0.6B 到 30B，展示其穩健性與泛化能力。